{"db":[{"meta":{"exported_on":1625088520974,"version":"4.8.4"},"data":{"posts":[{"id":"5adb8922351ffe0018a5770f","uuid":"1d2725d6-9032-4b43-b81b-be7cb9852989","title":"About Me","slug":"about","mobiledoc":"{\"version\":\"0.3.1\",\"atoms\":[],\"cards\":[[\"markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\"###TL;DR\\n\\n==Kevin Hoyt== is a ==Developer Advocate== with ==IBM==, focused on driving customer success with machine learning (Watson) and IoT in conjunction with IBM's Bluemix PaaS (Platform as a Service).\\n\\n###Backstory\\n\\nCan I let you in on a little secret?  I am an Army Veteran.  I served primarily with the 110 Military Intelligence Battalion out of Fort Drum, New York (==10th Mountain Division==).  Oh, great, now I have to kill you!  Or is that an oxymoron?  I served from 1991 to 1995.\\n\\nAt one point in my service I became sidelined with an injury.  Youthful energy abounding, and bored out of my mind, I picked up ... ==crochet==.  You thought I was going to say \\\"a computer\\\".\\n\\nBeneath the craft store where I purchase my skeins (yarn), was a computer store.  I had dabbled with computers in high school, but never really gave them much attention.  At the urging of one of my fellow soldiers (an officer actually), I decided to piece together a ==486== system.\\n\\nAt the same time as I was exploring my new crafty side (er, recovering from my injury), back on base, I was assigned to a logistics roll.  Every morning the senior staff would look to us to report on the status of the troops, equipment, etc. in what was called a \\\"readiness report\\\".\\n\\nIt was tedious, and filled with pushing paper.  There was a computer in the office that got some use, but was not the mainstay of the reporting functionality.  Back on my 486 I taught myself to program a computer (==C/C++==) to avoid pushing paper.\\n\\nEventually I recovered, and was back out in the field.\\n\\nAs the term of my service drew to a close, a college recruiter from DeVry came by and talked about careers in Information Technology.  The was a DeVry campus in Kansas City, Missouri.  Close enough to my home in Denver, Colorado, but not too close.  The tuition matched up nicely with my G.I. Bill benefits.  Sure, why not - I had nothing else planned.\\n\\nAnd so began my career in technology.\\n\\n###The Early Years\\n\\nI started off as a computer technician at a ==Best Buy== near the campus (pre-Geek Squad).  We would install printer drivers for people, memory and hard drive upgrades, etc.\\n\\nI do not remember the exact chain of events that followed, but I eventually landed myself the role of ==Webmaster== at a Internet Service Provider (ISP) called Grapevine.  I would build web sites for companies, field support calls, and occasionally setup new servers.\\n\\nGrapevine eventually went under, but not before I landed a ==Web Developer== role at Jack Henry &amp; Associates ([JKHY](https://www.etrade.wallst.com/v1/stocks/snapshot/snapshot.asp?AuthnContext=prospect&prospectnavyear=2011&reinitiate-handshake=0&ChallengeUrl=https://idp.etrade.com/idp/SSO.saml2&env=PRD&symbol=jkhy&rsO=new&country=US&User_EncryptionID=220)).  Jack Henry provided banking software for small banks that did not have their own IT staff.  \\n\\nThe majority of my work at Jack Henry consisted of building dynamic web sites for banks using ==Allaire ColdFusion==.  I would also dabble in ==Java applets== for interactivity (pre-Flash).  I even got to do some work on their Internet banking software, that connected to ==AS400== systems.\\n\\nNow back in Denver, Colorado, this baseline experience landed me a job at a consulting firm by the name of Whittman-Hart.  There I did more ColdFusion and Java work.  Much of that Java was on the server, in the form of ==servlets==, which preceded JSP, and eventually became Java EE.\\n\\n###Off to the Races\\n\\nThen one day I got a call from ==Allaire==.  Yes, the same Allaire that made ColdFusion.  They wanted to hire a ==Sales Engineer== for a product they recently acquired called, ==JRun==.  JRun eventually became a full ==Java EE== server, and competed with the likes of BEA WebLogic, and IBM WebSphere.\\n\\nIn 2001, Allaire was acquired by ==Macromedia==.  Macromedia had seen a demand for dynamic ==Flash-based== content, and even rolled out a product called Generator around this need.  \\n\\nEventually saner minds prevailed, and ==Flash Remoting== was introduced.  Flash Remoting allowed Flash content to directly exchange data with application server technologies.  Still a bit before the AJAX technique was popularized, this feature really gained traction with enterprise customers.\\n\\nSomewhere in there Macromedia had a ==learning management system== (LMS) called, Breeze, and even ==content management system== (CMS).  My server skills allowed me to be very successful in closing sales of these products.\\n\\nFlash Remoting was such a hit with the enterprise that work on a new product called, ==Flex==, was started.  Then in 2005, Macromedia was acquired by ==Adobe== ([ADBE](https://www.etrade.wallst.com/v1/stocks/snapshot/snapshot.asp?YYY220_3OkaVgYPqBclXSsAU+ST3xwmir1KBnTmnoMq0an6l/2FMOyuaM/i6Rgj0rMMnuHztZG6e9OMv2QBd+AUEAtXtcKIGoaV3vnwgLCJ90tuIlvYKhPJIyI7ub0UzPjbFLJj8NdFQiL7Ckh8ve+fLzAApjcUyqOZ664byjE3A/JHwGAiqOKe6gJUIk5JvgkqP0D6MKI3hpKLC5I2lXlUK1GuYg)).  Flex was eventually placed into open source.  Around that time (2007) I made the leap from sales to marketing as an ==Evangelist== for the ==JavaScript== side of ==Adobe AIR==.  I even co-authored a [book](http://shop.oreilly.com/product/9780596515195.do) about it.\\n\\nThe number of technologies I got to learn through my years at Adobe was vast.  At one point I was developing ==mobile applications== for feature phones (pre-iPhone).  At another point, I was integrating with ==business process management== (BPM) systems.  Then ==Android== development.  Then ==iOS== development.  Then ... well, you get the picture.\\n\\nMy employment at Adobe ended in 2013, when the company decided that developers were no longer a focus, and related staff were laid off.\\n\\n###Startup-land\\n\\nAfter years at a big company like Adobe, I decided to have my hand in the startup world, and joined ==[Kaazing](http://kaazing.com)== in 2014 as a ==Principal Evangelist==.  The founders of Kaazing were pioneers on a web standard called ==WebSocket==.  Kaazing Gateway allows WebSocket clients (across both Web and native) to integrate with ==message brokers== such as ==ActiveMQ==.\\n\\nWhile I learned a lot about implementing publish/subscribe for decoupled systems, in the end, the startup life was just not quite the right fit for me.  I moved on from Kaazing in June of 2015.\\n\\n###Today\\n\\nIn July of 2015, I started at ==IBM== as a ==Developer Advocate==, joining Adobe colleagues [Ray Camden](http://www.raymondcamden.com), and [Andy Trice](http://www.tricedesigns.com) on the MobileFirst team.  Did you know that IBM invented the ATM?  Maybe I will get to be a part of something as great down the road. When I first joined IBM, I spent a lot of time focusing on ==Android== application development, then I ==led a team== focused on emerging technology. Currently, I oversee ==data analysis== for our developer relations activities.\\n\\nI started picking up electronics hardware as a gimmick in my demonstrations around 2007.  Electronics connected to the Internet has since become called, the ==Internet of Things== (IoT), and is something I am deeply passionate about.  I have traveled the world talking about IoT, including teaching hands-on workshops for audiences ranging from grade school to professionals.\\n\\nAn emerging market closely related to IoT is what I refer to as \\\"desktop fabrication\\\".  This includes ==3D printers, laser cutting, and CNC==, and is one of the many ways I enjoy spending my leisure time.  I also have a ==private pilots license==, see loads of movies (even the bad ones), and spend as much time as possible with my ==wife== of 21 years, and our fifteen (15) year-old ==daughter==. \"}]],\"markups\":[],\"sections\":[[10,0]],\"ghostVersion\":\"3.0\"}","html":"<!--kg-card-begin: markdown--><h3 id=\"tldr\">TL;DR</h3>\n<p><mark>Kevin Hoyt</mark> is a <mark>Developer Advocate</mark> with <mark>IBM</mark>, focused on driving customer success with machine learning (Watson) and IoT in conjunction with IBM's Bluemix PaaS (Platform as a Service).</p>\n<h3 id=\"backstory\">Backstory</h3>\n<p>Can I let you in on a little secret?  I am an Army Veteran.  I served primarily with the 110 Military Intelligence Battalion out of Fort Drum, New York (<mark>10th Mountain Division</mark>).  Oh, great, now I have to kill you!  Or is that an oxymoron?  I served from 1991 to 1995.</p>\n<p>At one point in my service I became sidelined with an injury.  Youthful energy abounding, and bored out of my mind, I picked up ... <mark>crochet</mark>.  You thought I was going to say &quot;a computer&quot;.</p>\n<p>Beneath the craft store where I purchase my skeins (yarn), was a computer store.  I had dabbled with computers in high school, but never really gave them much attention.  At the urging of one of my fellow soldiers (an officer actually), I decided to piece together a <mark>486</mark> system.</p>\n<p>At the same time as I was exploring my new crafty side (er, recovering from my injury), back on base, I was assigned to a logistics roll.  Every morning the senior staff would look to us to report on the status of the troops, equipment, etc. in what was called a &quot;readiness report&quot;.</p>\n<p>It was tedious, and filled with pushing paper.  There was a computer in the office that got some use, but was not the mainstay of the reporting functionality.  Back on my 486 I taught myself to program a computer (<mark>C/C++</mark>) to avoid pushing paper.</p>\n<p>Eventually I recovered, and was back out in the field.</p>\n<p>As the term of my service drew to a close, a college recruiter from DeVry came by and talked about careers in Information Technology.  The was a DeVry campus in Kansas City, Missouri.  Close enough to my home in Denver, Colorado, but not too close.  The tuition matched up nicely with my G.I. Bill benefits.  Sure, why not - I had nothing else planned.</p>\n<p>And so began my career in technology.</p>\n<h3 id=\"theearlyyears\">The Early Years</h3>\n<p>I started off as a computer technician at a <mark>Best Buy</mark> near the campus (pre-Geek Squad).  We would install printer drivers for people, memory and hard drive upgrades, etc.</p>\n<p>I do not remember the exact chain of events that followed, but I eventually landed myself the role of <mark>Webmaster</mark> at a Internet Service Provider (ISP) called Grapevine.  I would build web sites for companies, field support calls, and occasionally setup new servers.</p>\n<p>Grapevine eventually went under, but not before I landed a <mark>Web Developer</mark> role at Jack Henry &amp; Associates (<a href=\"https://www.etrade.wallst.com/v1/stocks/snapshot/snapshot.asp?AuthnContext=prospect&amp;prospectnavyear=2011&amp;reinitiate-handshake=0&amp;ChallengeUrl=https://idp.etrade.com/idp/SSO.saml2&amp;env=PRD&amp;symbol=jkhy&amp;rsO=new&amp;country=US&amp;User_EncryptionID=220\">JKHY</a>).  Jack Henry provided banking software for small banks that did not have their own IT staff.</p>\n<p>The majority of my work at Jack Henry consisted of building dynamic web sites for banks using <mark>Allaire ColdFusion</mark>.  I would also dabble in <mark>Java applets</mark> for interactivity (pre-Flash).  I even got to do some work on their Internet banking software, that connected to <mark>AS400</mark> systems.</p>\n<p>Now back in Denver, Colorado, this baseline experience landed me a job at a consulting firm by the name of Whittman-Hart.  There I did more ColdFusion and Java work.  Much of that Java was on the server, in the form of <mark>servlets</mark>, which preceded JSP, and eventually became Java EE.</p>\n<h3 id=\"offtotheraces\">Off to the Races</h3>\n<p>Then one day I got a call from <mark>Allaire</mark>.  Yes, the same Allaire that made ColdFusion.  They wanted to hire a <mark>Sales Engineer</mark> for a product they recently acquired called, <mark>JRun</mark>.  JRun eventually became a full <mark>Java EE</mark> server, and competed with the likes of BEA WebLogic, and IBM WebSphere.</p>\n<p>In 2001, Allaire was acquired by <mark>Macromedia</mark>.  Macromedia had seen a demand for dynamic <mark>Flash-based</mark> content, and even rolled out a product called Generator around this need.</p>\n<p>Eventually saner minds prevailed, and <mark>Flash Remoting</mark> was introduced.  Flash Remoting allowed Flash content to directly exchange data with application server technologies.  Still a bit before the AJAX technique was popularized, this feature really gained traction with enterprise customers.</p>\n<p>Somewhere in there Macromedia had a <mark>learning management system</mark> (LMS) called, Breeze, and even <mark>content management system</mark> (CMS).  My server skills allowed me to be very successful in closing sales of these products.</p>\n<p>Flash Remoting was such a hit with the enterprise that work on a new product called, <mark>Flex</mark>, was started.  Then in 2005, Macromedia was acquired by <mark>Adobe</mark> (<a href=\"https://www.etrade.wallst.com/v1/stocks/snapshot/snapshot.asp?YYY220_3OkaVgYPqBclXSsAU+ST3xwmir1KBnTmnoMq0an6l/2FMOyuaM/i6Rgj0rMMnuHztZG6e9OMv2QBd+AUEAtXtcKIGoaV3vnwgLCJ90tuIlvYKhPJIyI7ub0UzPjbFLJj8NdFQiL7Ckh8ve+fLzAApjcUyqOZ664byjE3A/JHwGAiqOKe6gJUIk5JvgkqP0D6MKI3hpKLC5I2lXlUK1GuYg\">ADBE</a>).  Flex was eventually placed into open source.  Around that time (2007) I made the leap from sales to marketing as an <mark>Evangelist</mark> for the <mark>JavaScript</mark> side of <mark>Adobe AIR</mark>.  I even co-authored a <a href=\"http://shop.oreilly.com/product/9780596515195.do\">book</a> about it.</p>\n<p>The number of technologies I got to learn through my years at Adobe was vast.  At one point I was developing <mark>mobile applications</mark> for feature phones (pre-iPhone).  At another point, I was integrating with <mark>business process management</mark> (BPM) systems.  Then <mark>Android</mark> development.  Then <mark>iOS</mark> development.  Then ... well, you get the picture.</p>\n<p>My employment at Adobe ended in 2013, when the company decided that developers were no longer a focus, and related staff were laid off.</p>\n<h3 id=\"startupland\">Startup-land</h3>\n<p>After years at a big company like Adobe, I decided to have my hand in the startup world, and joined <mark><a href=\"http://kaazing.com\">Kaazing</a></mark> in 2014 as a <mark>Principal Evangelist</mark>.  The founders of Kaazing were pioneers on a web standard called <mark>WebSocket</mark>.  Kaazing Gateway allows WebSocket clients (across both Web and native) to integrate with <mark>message brokers</mark> such as <mark>ActiveMQ</mark>.</p>\n<p>While I learned a lot about implementing publish/subscribe for decoupled systems, in the end, the startup life was just not quite the right fit for me.  I moved on from Kaazing in June of 2015.</p>\n<h3 id=\"today\">Today</h3>\n<p>In July of 2015, I started at <mark>IBM</mark> as a <mark>Developer Advocate</mark>, joining Adobe colleagues <a href=\"http://www.raymondcamden.com\">Ray Camden</a>, and <a href=\"http://www.tricedesigns.com\">Andy Trice</a> on the MobileFirst team.  Did you know that IBM invented the ATM?  Maybe I will get to be a part of something as great down the road. When I first joined IBM, I spent a lot of time focusing on <mark>Android</mark> application development, then I <mark>led a team</mark> focused on emerging technology. Currently, I oversee <mark>data analysis</mark> for our developer relations activities.</p>\n<p>I started picking up electronics hardware as a gimmick in my demonstrations around 2007.  Electronics connected to the Internet has since become called, the <mark>Internet of Things</mark> (IoT), and is something I am deeply passionate about.  I have traveled the world talking about IoT, including teaching hands-on workshops for audiences ranging from grade school to professionals.</p>\n<p>An emerging market closely related to IoT is what I refer to as &quot;desktop fabrication&quot;.  This includes <mark>3D printers, laser cutting, and CNC</mark>, and is one of the many ways I enjoy spending my leisure time.  I also have a <mark>private pilots license</mark>, see loads of movies (even the bad ones), and spend as much time as possible with my <mark>wife</mark> of 21 years, and our fifteen (15) year-old <mark>daughter</mark>.</p>\n<!--kg-card-end: markdown-->","comment_id":"2","plaintext":"TL;DR\nKevin Hoyt is a Developer Advocate with IBM, focused on driving customer success\nwith machine learning (Watson) and IoT in conjunction with IBM's Bluemix PaaS\n(Platform as a Service).\n\nBackstory\nCan I let you in on a little secret? I am an Army Veteran. I served primarily\nwith the 110 Military Intelligence Battalion out of Fort Drum, New York (10th\nMountain Division). Oh, great, now I have to kill you! Or is that an oxymoron? I\nserved from 1991 to 1995.\n\nAt one point in my service I became sidelined with an injury. Youthful energy\nabounding, and bored out of my mind, I picked up ... crochet. You thought I was\ngoing to say \"a computer\".\n\nBeneath the craft store where I purchase my skeins (yarn), was a computer store.\nI had dabbled with computers in high school, but never really gave them much\nattention. At the urging of one of my fellow soldiers (an officer actually), I\ndecided to piece together a 486 system.\n\nAt the same time as I was exploring my new crafty side (er, recovering from my\ninjury), back on base, I was assigned to a logistics roll. Every morning the\nsenior staff would look to us to report on the status of the troops, equipment,\netc. in what was called a \"readiness report\".\n\nIt was tedious, and filled with pushing paper. There was a computer in the\noffice that got some use, but was not the mainstay of the reporting\nfunctionality. Back on my 486 I taught myself to program a computer (C/C++) to\navoid pushing paper.\n\nEventually I recovered, and was back out in the field.\n\nAs the term of my service drew to a close, a college recruiter from DeVry came\nby and talked about careers in Information Technology. The was a DeVry campus in\nKansas City, Missouri. Close enough to my home in Denver, Colorado, but not too\nclose. The tuition matched up nicely with my G.I. Bill benefits. Sure, why not -\nI had nothing else planned.\n\nAnd so began my career in technology.\n\nThe Early Years\nI started off as a computer technician at a Best Buy near the campus (pre-Geek\nSquad). We would install printer drivers for people, memory and hard drive\nupgrades, etc.\n\nI do not remember the exact chain of events that followed, but I eventually\nlanded myself the role of Webmaster at a Internet Service Provider (ISP) called\nGrapevine. I would build web sites for companies, field support calls, and\noccasionally setup new servers.\n\nGrapevine eventually went under, but not before I landed a Web Developer role at\nJack Henry & Associates (JKHY\n[https://www.etrade.wallst.com/v1/stocks/snapshot/snapshot.asp?AuthnContext=prospect&prospectnavyear=2011&reinitiate-handshake=0&ChallengeUrl=https://idp.etrade.com/idp/SSO.saml2&env=PRD&symbol=jkhy&rsO=new&country=US&User_EncryptionID=220]\n). Jack Henry provided banking software for small banks that did not have their\nown IT staff.\n\nThe majority of my work at Jack Henry consisted of building dynamic web sites\nfor banks using Allaire ColdFusion. I would also dabble in Java applets for\ninteractivity (pre-Flash). I even got to do some work on their Internet banking\nsoftware, that connected to AS400 systems.\n\nNow back in Denver, Colorado, this baseline experience landed me a job at a\nconsulting firm by the name of Whittman-Hart. There I did more ColdFusion and\nJava work. Much of that Java was on the server, in the form of servlets, which\npreceded JSP, and eventually became Java EE.\n\nOff to the Races\nThen one day I got a call from Allaire. Yes, the same Allaire that made\nColdFusion. They wanted to hire a Sales Engineer for a product they recently\nacquired called, JRun. JRun eventually became a full Java EE server, and\ncompeted with the likes of BEA WebLogic, and IBM WebSphere.\n\nIn 2001, Allaire was acquired by Macromedia. Macromedia had seen a demand for\ndynamic Flash-based content, and even rolled out a product called Generator\naround this need.\n\nEventually saner minds prevailed, and Flash Remoting was introduced. Flash\nRemoting allowed Flash content to directly exchange data with application server\ntechnologies. Still a bit before the AJAX technique was popularized, this\nfeature really gained traction with enterprise customers.\n\nSomewhere in there Macromedia had a learning management system (LMS) called,\nBreeze, and even content management system (CMS). My server skills allowed me to\nbe very successful in closing sales of these products.\n\nFlash Remoting was such a hit with the enterprise that work on a new product\ncalled, Flex, was started. Then in 2005, Macromedia was acquired by Adobe (ADBE\n[https://www.etrade.wallst.com/v1/stocks/snapshot/snapshot.asp?YYY220_3OkaVgYPqBclXSsAU+ST3xwmir1KBnTmnoMq0an6l/2FMOyuaM/i6Rgj0rMMnuHztZG6e9OMv2QBd+AUEAtXtcKIGoaV3vnwgLCJ90tuIlvYKhPJIyI7ub0UzPjbFLJj8NdFQiL7Ckh8ve+fLzAApjcUyqOZ664byjE3A/JHwGAiqOKe6gJUIk5JvgkqP0D6MKI3hpKLC5I2lXlUK1GuYg]\n). Flex was eventually placed into open source. Around that time (2007) I made\nthe leap from sales to marketing as an Evangelist for the JavaScript side of \nAdobe AIR. I even co-authored a book\n[http://shop.oreilly.com/product/9780596515195.do] about it.\n\nThe number of technologies I got to learn through my years at Adobe was vast. At\none point I was developing mobile applications for feature phones (pre-iPhone).\nAt another point, I was integrating with business process management (BPM)\nsystems. Then Android development. Then iOS development. Then ... well, you get\nthe picture.\n\nMy employment at Adobe ended in 2013, when the company decided that developers\nwere no longer a focus, and related staff were laid off.\n\nStartup-land\nAfter years at a big company like Adobe, I decided to have my hand in the\nstartup world, and joined Kaazing [http://kaazing.com] in 2014 as a Principal\nEvangelist. The founders of Kaazing were pioneers on a web standard called \nWebSocket. Kaazing Gateway allows WebSocket clients (across both Web and native)\nto integrate with message brokers such as ActiveMQ.\n\nWhile I learned a lot about implementing publish/subscribe for decoupled\nsystems, in the end, the startup life was just not quite the right fit for me. I\nmoved on from Kaazing in June of 2015.\n\nToday\nIn July of 2015, I started at IBM as a Developer Advocate, joining Adobe\ncolleagues Ray Camden [http://www.raymondcamden.com], and Andy Trice\n[http://www.tricedesigns.com] on the MobileFirst team. Did you know that IBM\ninvented the ATM? Maybe I will get to be a part of something as great down the\nroad. When I first joined IBM, I spent a lot of time focusing on Android \napplication development, then I led a team focused on emerging technology.\nCurrently, I oversee data analysis for our developer relations activities.\n\nI started picking up electronics hardware as a gimmick in my demonstrations\naround 2007. Electronics connected to the Internet has since become called, the \nInternet of Things (IoT), and is something I am deeply passionate about. I have\ntraveled the world talking about IoT, including teaching hands-on workshops for\naudiences ranging from grade school to professionals.\n\nAn emerging market closely related to IoT is what I refer to as \"desktop\nfabrication\". This includes 3D printers, laser cutting, and CNC, and is one of\nthe many ways I enjoy spending my leisure time. I also have a private pilots\nlicense, see loads of movies (even the bad ones), and spend as much time as\npossible with my wife of 21 years, and our fifteen (15) year-old daughter.","feature_image":"__GHOST_URL__/content/images/2019/02/skyhawk.n5327j.jpg","featured":0,"status":"published","locale":null,"visibility":"public","author_id":"1","created_at":"2014-10-28T19:49:08.000Z","updated_at":"2019-07-24T23:17:29.000Z","published_at":"2014-10-28T19:49:34.000Z","custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"type":"page","email_recipient_filter":"none"},{"id":"5adb8922351ffe0018a57710","uuid":"bcccc061-9f20-4376-a618-3b7e3cb45a3c","title":"IoT and the New Web","slug":"iot-and-the-new-web","mobiledoc":"{\"version\":\"0.3.1\",\"markups\":[],\"atoms\":[],\"cards\":[[\"markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\"*All too often, we as developers, focus on technology for what it can do for us - and usually in a particularly one-sided conversation. Every now and again however, there comes a time when we need not to look at technology for what it can do for us, but rather what we can do for it. Such is the case with the Internet of Things (IoT), which was the topic of my recent presentation (October 2014) at [IoTa Conference](http://www.iotaconf.com/kevin_hoyt.html) in San Francisco, California.*\\n\\n---\\n\\nBefore we get too far in the technology weeds, let us take a moment to step back and talk about today's consumer.\\n\\n### Consumer Expectations\\n\\nI am an Army veteran (1991 - 1995). As a veteran, the United States government provides me with various benefits that I can opt to use. I have, from time to time, leveraged these benefits, as was the case recently. In order to secure said benefits, I was instructed to print a form from the Veterans Affairs (VA) website, fill it out, and then fax it to the St. Louis, Missouri office. At that point, my request would be put in a queue for completion, and it would be an estimated four weeks before I heard back.\\n\\nIf I filled out the form incorrectly, or was missing pertinent information, I would be required to go through the process again.\\n\\nThis is not a consumer experience we have come to expect from the rest of our daily interactions in a modern society. Sure, there was a day when the fax machine ruled supreme; when mobile phones were far and few between, and the Internet was still just a dream. In fact, the first photo facsimile was sent in 1924 (of then President Calvin Coolidge, from New York, New York to London, England). Nearly 100 years later, and I'm not sure most teenagers would even know what a fax machine does.\\n\\n![The first photo facsimile was of Calvin Coolidge in 1924.](http://images.kevinhoyt.com/calvin.coolidge.jpg)\\n\\nConversely, business today thrives (some might say \\\"chokes\\\") on email. Near immediate responses are expected. And if email is not fast enough, you can lean on some fashion of instant messaging. You can even track your FedEx or UPS packages in near real-time. Amazon.com currently provides two-day delivery, and is working on delivery by drone in under thirty minutes.\\n\\nBusiness today is driven on instant gratification - it is the very fabric of the current generation. While the long-term impact of such a mindset is being researched (building a personal savings is a long-term financial goal), the reality for business is that consumers expect real-time interactions.\\n\\n### Enter the Web\\n\\nIt surprises me then that in a world driven by real-time expectations, so much of our technology is driven by the request/response technology of HTTP. A client makes a request, the server sends a response, and then the connection between the two is terminated.\\n\\nFrom a physical computing perspective, this is effectively my fax experience with the VA.\\n\\nI make a request for benefits by faxing a document. At some point in the future, an actual person picks that document up, and does something with it. Eventually, I get an email that the fax was received. And finally, the connection is severed. I have no idea what else is going on at the VA, nor does the VA understand the urgency of that benefit in my world. I can only hope that at some point in the future, my benefits come through.\\n\\nOn the Web, we have managed to gloss over this inconvenience by using approaches such as Ajax. Using Ajax we can effectively impact the user experience - specifically the perceived performance of the application. The time it takes for the user to move their mouse to a point on the screen and click, is valuable background processing time for our application.\\n\\n> I suppose it is tempting, if the only tool you have is a hammer, to treat everything as if it were a nail. - Abraham Maslow\\n\\nOf course we know that there are many dominoes stacked along the way, and that the connection may be slow, the server may be slow, or the client may be slow in processing the data. So long as we play our cards right though, some degree of instant gratification is achieved.\\n\\nAnd so goes the technology trap. When we come to the world of IoT, we bring with us this set of tools. And as the old adage goes, if you have a hammer, everything looks like a nail (Law of Instrument, Maslow, 1966).\\n\\n### The Arrival of IoT\\n\\nWhen it comes to IoT, the first thing that most developers will do, is to take something like an Arduino, and put a web server on it. The reasoning behind this is that we can request a resource from the Arduino, it will gather some information about the hardware, and then send a response with the details. While this is certainly possible, the side effects are not without consequence.\\n\\n> The scalability of a micro-controller-based web server, is certainly not equipped to handle anything more than one-off requests.\\n\\nRunning a web server on a microprocessor as limited as the Arduino, means that you are likely going to need to plug into a wall outlet for power before too long. The scalability of a micro-controller-based web server, is certainly not equipped to handle anything more than one-off requests (many are written this way intentionally). Good luck getting security features into that limited profile. And then there is the user experience, which is slow and intentional - far from the business demands of immediate gratification.\\n\\nThrough my next series of posts, I will introduce you to my brand new (fictional) startup called \\\"The Widget Company\\\". Each post will walk through introducing a brand new, life-changing (not really) product. We will take a look at how these devices are connected the Internet, and the resulting user experience. Along the way, we will fight to leave our technology toolset baggage behind, and broaden our horizons as to the possibilities that may have come before HTTP and request/response, that may be better suited to our IoT needs.\\n\"}]],\"sections\":[[10,0]],\"ghostVersion\":\"3.0\"}","html":"<!--kg-card-begin: markdown--><p><em>All too often, we as developers, focus on technology for what it can do for us - and usually in a particularly one-sided conversation. Every now and again however, there comes a time when we need not to look at technology for what it can do for us, but rather what we can do for it. Such is the case with the Internet of Things (IoT), which was the topic of my recent presentation (October 2014) at <a href=\"http://www.iotaconf.com/kevin_hoyt.html\">IoTa Conference</a> in San Francisco, California.</em></p>\n<hr>\n<p>Before we get too far in the technology weeds, let us take a moment to step back and talk about today's consumer.</p>\n<h3 id=\"consumerexpectations\">Consumer Expectations</h3>\n<p>I am an Army veteran (1991 - 1995). As a veteran, the United States government provides me with various benefits that I can opt to use. I have, from time to time, leveraged these benefits, as was the case recently. In order to secure said benefits, I was instructed to print a form from the Veterans Affairs (VA) website, fill it out, and then fax it to the St. Louis, Missouri office. At that point, my request would be put in a queue for completion, and it would be an estimated four weeks before I heard back.</p>\n<p>If I filled out the form incorrectly, or was missing pertinent information, I would be required to go through the process again.</p>\n<p>This is not a consumer experience we have come to expect from the rest of our daily interactions in a modern society. Sure, there was a day when the fax machine ruled supreme; when mobile phones were far and few between, and the Internet was still just a dream. In fact, the first photo facsimile was sent in 1924 (of then President Calvin Coolidge, from New York, New York to London, England). Nearly 100 years later, and I'm not sure most teenagers would even know what a fax machine does.</p>\n<p><img src=\"http://images.kevinhoyt.com/calvin.coolidge.jpg\" alt=\"The first photo facsimile was of Calvin Coolidge in 1924.\" loading=\"lazy\"></p>\n<p>Conversely, business today thrives (some might say &quot;chokes&quot;) on email. Near immediate responses are expected. And if email is not fast enough, you can lean on some fashion of instant messaging. You can even track your FedEx or UPS packages in near real-time. Amazon.com currently provides two-day delivery, and is working on delivery by drone in under thirty minutes.</p>\n<p>Business today is driven on instant gratification - it is the very fabric of the current generation. While the long-term impact of such a mindset is being researched (building a personal savings is a long-term financial goal), the reality for business is that consumers expect real-time interactions.</p>\n<h3 id=\"entertheweb\">Enter the Web</h3>\n<p>It surprises me then that in a world driven by real-time expectations, so much of our technology is driven by the request/response technology of HTTP. A client makes a request, the server sends a response, and then the connection between the two is terminated.</p>\n<p>From a physical computing perspective, this is effectively my fax experience with the VA.</p>\n<p>I make a request for benefits by faxing a document. At some point in the future, an actual person picks that document up, and does something with it. Eventually, I get an email that the fax was received. And finally, the connection is severed. I have no idea what else is going on at the VA, nor does the VA understand the urgency of that benefit in my world. I can only hope that at some point in the future, my benefits come through.</p>\n<p>On the Web, we have managed to gloss over this inconvenience by using approaches such as Ajax. Using Ajax we can effectively impact the user experience - specifically the perceived performance of the application. The time it takes for the user to move their mouse to a point on the screen and click, is valuable background processing time for our application.</p>\n<blockquote>\n<p>I suppose it is tempting, if the only tool you have is a hammer, to treat everything as if it were a nail. - Abraham Maslow</p>\n</blockquote>\n<p>Of course we know that there are many dominoes stacked along the way, and that the connection may be slow, the server may be slow, or the client may be slow in processing the data. So long as we play our cards right though, some degree of instant gratification is achieved.</p>\n<p>And so goes the technology trap. When we come to the world of IoT, we bring with us this set of tools. And as the old adage goes, if you have a hammer, everything looks like a nail (Law of Instrument, Maslow, 1966).</p>\n<h3 id=\"thearrivalofiot\">The Arrival of IoT</h3>\n<p>When it comes to IoT, the first thing that most developers will do, is to take something like an Arduino, and put a web server on it. The reasoning behind this is that we can request a resource from the Arduino, it will gather some information about the hardware, and then send a response with the details. While this is certainly possible, the side effects are not without consequence.</p>\n<blockquote>\n<p>The scalability of a micro-controller-based web server, is certainly not equipped to handle anything more than one-off requests.</p>\n</blockquote>\n<p>Running a web server on a microprocessor as limited as the Arduino, means that you are likely going to need to plug into a wall outlet for power before too long. The scalability of a micro-controller-based web server, is certainly not equipped to handle anything more than one-off requests (many are written this way intentionally). Good luck getting security features into that limited profile. And then there is the user experience, which is slow and intentional - far from the business demands of immediate gratification.</p>\n<p>Through my next series of posts, I will introduce you to my brand new (fictional) startup called &quot;The Widget Company&quot;. Each post will walk through introducing a brand new, life-changing (not really) product. We will take a look at how these devices are connected the Internet, and the resulting user experience. Along the way, we will fight to leave our technology toolset baggage behind, and broaden our horizons as to the possibilities that may have come before HTTP and request/response, that may be better suited to our IoT needs.</p>\n<!--kg-card-end: markdown-->","comment_id":"4","plaintext":"All too often, we as developers, focus on technology for what it can do for us -\nand usually in a particularly one-sided conversation. Every now and again\nhowever, there comes a time when we need not to look at technology for what it\ncan do for us, but rather what we can do for it. Such is the case with the\nInternet of Things (IoT), which was the topic of my recent presentation (October\n2014) at IoTa Conference [http://www.iotaconf.com/kevin_hoyt.html] in San\nFrancisco, California.\n\n\n--------------------------------------------------------------------------------\n\nBefore we get too far in the technology weeds, let us take a moment to step back\nand talk about today's consumer.\n\nConsumer Expectations\nI am an Army veteran (1991 - 1995). As a veteran, the United States government\nprovides me with various benefits that I can opt to use. I have, from time to\ntime, leveraged these benefits, as was the case recently. In order to secure\nsaid benefits, I was instructed to print a form from the Veterans Affairs (VA)\nwebsite, fill it out, and then fax it to the St. Louis, Missouri office. At that\npoint, my request would be put in a queue for completion, and it would be an\nestimated four weeks before I heard back.\n\nIf I filled out the form incorrectly, or was missing pertinent information, I\nwould be required to go through the process again.\n\nThis is not a consumer experience we have come to expect from the rest of our\ndaily interactions in a modern society. Sure, there was a day when the fax\nmachine ruled supreme; when mobile phones were far and few between, and the\nInternet was still just a dream. In fact, the first photo facsimile was sent in\n1924 (of then President Calvin Coolidge, from New York, New York to London,\nEngland). Nearly 100 years later, and I'm not sure most teenagers would even\nknow what a fax machine does.\n\n\n\nConversely, business today thrives (some might say \"chokes\") on email. Near\nimmediate responses are expected. And if email is not fast enough, you can lean\non some fashion of instant messaging. You can even track your FedEx or UPS\npackages in near real-time. Amazon.com currently provides two-day delivery, and\nis working on delivery by drone in under thirty minutes.\n\nBusiness today is driven on instant gratification - it is the very fabric of the\ncurrent generation. While the long-term impact of such a mindset is being\nresearched (building a personal savings is a long-term financial goal), the\nreality for business is that consumers expect real-time interactions.\n\nEnter the Web\nIt surprises me then that in a world driven by real-time expectations, so much\nof our technology is driven by the request/response technology of HTTP. A client\nmakes a request, the server sends a response, and then the connection between\nthe two is terminated.\n\nFrom a physical computing perspective, this is effectively my fax experience\nwith the VA.\n\nI make a request for benefits by faxing a document. At some point in the future,\nan actual person picks that document up, and does something with it. Eventually,\nI get an email that the fax was received. And finally, the connection is\nsevered. I have no idea what else is going on at the VA, nor does the VA\nunderstand the urgency of that benefit in my world. I can only hope that at some\npoint in the future, my benefits come through.\n\nOn the Web, we have managed to gloss over this inconvenience by using approaches\nsuch as Ajax. Using Ajax we can effectively impact the user experience -\nspecifically the perceived performance of the application. The time it takes for\nthe user to move their mouse to a point on the screen and click, is valuable\nbackground processing time for our application.\n\n> I suppose it is tempting, if the only tool you have is a hammer, to treat\neverything as if it were a nail. - Abraham Maslow\n\n\nOf course we know that there are many dominoes stacked along the way, and that\nthe connection may be slow, the server may be slow, or the client may be slow in\nprocessing the data. So long as we play our cards right though, some degree of\ninstant gratification is achieved.\n\nAnd so goes the technology trap. When we come to the world of IoT, we bring with\nus this set of tools. And as the old adage goes, if you have a hammer,\neverything looks like a nail (Law of Instrument, Maslow, 1966).\n\nThe Arrival of IoT\nWhen it comes to IoT, the first thing that most developers will do, is to take\nsomething like an Arduino, and put a web server on it. The reasoning behind this\nis that we can request a resource from the Arduino, it will gather some\ninformation about the hardware, and then send a response with the details. While\nthis is certainly possible, the side effects are not without consequence.\n\n> The scalability of a micro-controller-based web server, is certainly not\nequipped to handle anything more than one-off requests.\n\n\nRunning a web server on a microprocessor as limited as the Arduino, means that\nyou are likely going to need to plug into a wall outlet for power before too\nlong. The scalability of a micro-controller-based web server, is certainly not\nequipped to handle anything more than one-off requests (many are written this\nway intentionally). Good luck getting security features into that limited\nprofile. And then there is the user experience, which is slow and intentional -\nfar from the business demands of immediate gratification.\n\nThrough my next series of posts, I will introduce you to my brand new\n(fictional) startup called \"The Widget Company\". Each post will walk through\nintroducing a brand new, life-changing (not really) product. We will take a look\nat how these devices are connected the Internet, and the resulting user\nexperience. Along the way, we will fight to leave our technology toolset baggage\nbehind, and broaden our horizons as to the possibilities that may have come\nbefore HTTP and request/response, that may be better suited to our IoT needs.","feature_image":"http://images.kevinhoyt.com/john.f.kennedy.jpg","featured":0,"status":"published","locale":null,"visibility":"public","author_id":"1","created_at":"2015-04-20T19:39:59.000Z","updated_at":"2015-04-21T14:25:31.000Z","published_at":"2014-11-04T21:00:00.000Z","custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"type":"post","email_recipient_filter":"none"},{"id":"5adb8922351ffe0018a57711","uuid":"69b579bd-ee26-48c4-bfb5-bc734c516566","title":"WidgetX and the Old Web","slug":"widgetx-and-the-old-web","mobiledoc":"{\"version\":\"0.3.1\",\"markups\":[],\"atoms\":[],\"cards\":[[\"markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\"*In my last article on \\\"[IoT and the New Web](http://kevinhoyt.com/blog/2014/11/04/iot-and-the-new-web.html)\\\" I talked about how consumer expectations are trending towards instant gratification. I went on to talk about how that experience impacts the Web and the Internet of Things (IoT). I closed by introducing \\\"The Widget Company\\\" (fictional) which will be launching brand new, life-changing (not really) products in the IoT space.*\\n\\n*Well, the day has finally arrived. The press has gathered. We have dawned our black turtleneck sweaters. Live-blogging is keeping the Interwebs informed. Suddenly, in a surprise \\\"one more thing\\\" moment, The Widget Company CEO introduces \\\"WidgetX!\\\" Now we can all gather around a glass enclosure to finally get our first glimpse of the future.*\\n\\n---\\n\\nOkay, so WidgetX is really just a Light Emitting Diode (LED). Not really a life-changing product, but not too dissimilar from other commercial products on the market, such as the [Philips Hue](http://www2.meethue.com/en-us/). Let us sit in on one of the launch event sessions that talks about how this device is connected to the Internet.\\n\\n### Circuit Diagram\\n\\nMaking an LED blink is the \\\"Hello World\\\" of the physical computing space. It effectively demonstrates basic programming, electronics, and digital input/output (IO). Wiring up an LED requires that you know a little about the LED itself, and a little about Ohm's Law.\\n\\n> Resistance (Ohms) = Voltage (Volts) / Current (Amps)\\n\\nEffectively, I will be using an LED with a 2.0V forward voltage and a forward current of 20mA. We also know that voltage provided by the Arduino digital pins is 5.0V. If all of that sounds Greek to you, that is totally fine at this point. It did to me too at one point.\\n\\nAfter the voltage passes through the LED, it should go from 5.0V to 3.0V (5.0V - 2.0V). The current needed is 20mA, which gives us a resistance value of 150 Ohms (3.0V / 20mA). So then electricity will come out of a digital pin on the Arduino, through a 150 Ohm resistor, across the LED, and then to ground to make a complete circuit. This looks something like the following diagram.\\n\\n![Digital pin 12 to 150 Ohm resistor. Resistor to LED anode. LED cathode to ground.](http://images.kevinhoyt.com/fritzing.led.png)\\n\\n### Arduino Yun\\n\\nThe [Arduino Yun](http://arduino.cc/en/Main/ArduinoBoardYun) is one of my favorite hobbyist platforms for the Internet of Things. Besides being an Arduino Leonardo, it has a Linux System on a Chip (SoC), micro SD slot, ethernet jack, USB Type-A, and even 802.11 b/g/n wireless. The wireless even supports ad hoc mode, which will let us configure SSID and password details from a browser on a different machine.\\n\\n*The details on configuring an Arduino Yun are beyond the scope of this post.*\\n\\nThe Arduino Yun SoC gives us handy server and client libraries to use in our Arduino code. You can use these libraries to create servers and clients not only for the web, but for any other TCP/IP protocols.\\n\\nRather than write our own HTTP handling however, the SoC will serve content uploaded with our sketch. To get this going, create an \\\"/arduino/www\\\" directory structure on the Yun SD, and then create a \\\"www\\\" folder inside your sketch folder. That content will be uploaded for you automatically.\\n\\nAnother reason that the Yun is so great for prototyping is that it can automatically serve up a REST interface for your sketch. Anything after \\\"http://arduino.local/arduino/\\\" can be easily read through the aforementioned client library as a stream. You can then parse the incoming string, and even write content back to the requesting client.\\n\\n> Woah! That is a lot of functionality provided out of the box that we would otherwise have to figure out all ourselves.\\n\\nWoah! That is a lot of functionality provided out of the box that we would otherwise have to figure out ourselves. To recap, the Arduino Yun gives us all the following for the purposes of building the IoT device, WidgetX:\\n\\n1. Arduino Leonardo (16 MHz MCU)\\n2. Wireless (802.11 b/g/n)\\n3. Hostname (no IP hunting)\\n4. Configuration Panel (SSID)\\n5. Micro SD (disk space)\\n6. HTTP Server (for static assets)\\n7. REST API (passthrough to our sketch)\\n8. TCP Server\\n9. TCP Client\\n\\nThis is almost more a really lightweight computer like the Raspberry Pi, than it is an Arduino. And that is an important observation. Using our request/response hammer, this project really looks like a nail, and we load up accordingly. That seems like a lot of pieces to put into every little IoT device we want to bring to market. While you consider the long-term implications of that hammer, here is the Arduino code WidgetX uses.\\n\\n```\\n// Libraries\\n#include <Bridge.h>\\n#include <YunClient.h> \\n#include <YunServer.h>\\n\\n// Literals\\n#define LED_OFF          \\\"OFF\\\"\\n#define LED_ON           \\\"ON\\\"\\n#define MESSAGE_LED      \\\"LED\\\"\\n#define TERMINATOR_SLASH '/'\\n\\n// Constants\\nconst int LED = 13;\\n\\n// Server\\nYunServer server;\\n \\n// Setup\\nvoid setup() \\n{\\n  // Digital output\\n  // Set low (off) initially\\n  pinMode( LED, OUTPUT ); \\n  digitalWrite( LED, LOW );\\n  \\n  // Leverage the SoC libraries\\n  // Start listening for clients\\n  Bridge.begin();\\n  server.begin();\\n}\\n \\nvoid loop() \\n{\\n  String    command;\\n  String    message;\\n  YunClient client;\\n  \\n  // Accept incoming client request\\n  client = server.accept();\\n \\n  // If there is a client\\n  if( client ) \\n  {\\n     // Parse the command from the URL\\n    command = client.readStringUntil( \\n      TERMINATOR_SLASH \\n    );\\n    command.toUpperCase();\\n\\n    // Message for the LED\\n    // Could have multiple controls\\n    if( command == MESSAGE_LED ) \\n    {\\n      // Parse the message from the URL\\n      // Trim - invisible - newline from end\\n      message = client.readString();\\n      message.toUpperCase();\\n      message.trim();\\n  \\n      if( message == LED_ON )\\n      {\\n        // Turn the LED on\\n        // Tell the client something happened\\n        digitalWrite( LED, HIGH );\\n        server.println( message );                      \\n      } else if( message == LED_OFF ) {\\n        // Turn the LED on\\n        // Tell the client something happened        \\n        digitalWrite( LED, LOW );        \\n        server.println( message );                      \\n      } else {\\n        // What did you want me to do?\\n        server.print( \\\"Unknonwn: \\\" );\\n        server.println( message );        \\n      }\\n    }\\n  \\n    // Close connection with client\\n    client.stop();\\n  }\\n}\\n```\\n\\n### Web Client\\n\\n![A little CSS magic helps us toggle the appearance of the button.](http://images.kevinhoyt.com/iota.web.led.png)\\n\\nFor the Web stack, we will have a single HTML page with a button in the center. When you press the button, an XMLHttpRequest (XHR) instance will be used to make the GET request with the appropriate command to turn on the LED. When you release the button, the XHR instance will send the command to turn off the LED.\\n\\nHere is what our JavaScript will look like:\\n\\n```\\nvar xhr = null;\\n\\nfunction doButtonDown()\\n{\\n  xhr = new XMLHttpRequest();\\n  xhr.addEventListener( \\\"load\\\", doButtonLoad );\\n  xhr.open( \\n    \\\"GET\\\", \\n    \\\"http://arduino.local/arduino/led/on\\\" \\n  );\\n  xhr.send( null );\\n}\\n```\\n\\nBecause our Arduino Yun is serving all the static content, including the JavaScript, we do not have to worry about cross-domain security. This seems fine for a single IoT device, but will really introduce problems as we add more IoT to our product line-up. How will these devices communicate across one another? How about devices that we did not make? How will they even be aware of one another? When we start swinging hammers around, we can quickly arrive at an \\\"intranet\\\" of things, as opposed to the far more powerful \\\"internet\\\" of things.\\n\\nFor the button, I am using an old CSS trick. Both the up and down states of the button are loaded as one image. The element that shows the button does not show overflow content which hides the state we are not showing at the moment. To change states, we can adjust the style of the \\\"background-position\\\" property.\\n\\n```\\n.button {\\n  background-image: url( 'button.png' );\\n  background-position: 0;\\n  background-repeat: no-repeat;\\n  height: 283px;\\n  outline: none;\\n  overflow: hidden;\\n  width: 298px;\\n}\\n\\n.button:active {\\n  background-position: -298px;\\n}\\n```\\n\\n*Note that I am using a DIV element for my button to make styling a little easier across screens - I am looking at you iOS!*\\n\\nIf you are a web developer, all of these web stack pieces probably feel pretty comfortable. That is a good thing because the hardest part of IoT is actually the Internet. As a Web Developer you have a much shorter way to go to master IoT than an Electrical Engineer. On the hardware side, patterns start to emerge pretty quickly. On the Web side, there is a never ending permutation of screens and technologies to support.\\n\\n### Next Steps\\n\\nLike any other technology, there are a myriad of ways to architect the Yun into our web stack. We could for example, run PHP on the Yun itself (on the Linux SoC side), and even Apache or Nginx. We could run PHP on a central server (EC2) and then proxy communication to and from our devices. The list goes on and on.\\n\\n> What happens when the device needs to notify us of state changes?\\n\\nIf you have put this particular code in place, one thing you will notice is that the LED is particularly slow to respond. Clicking the button in the browser takes a second or two to actually light the LED. While this seems like an acceptable margin, we are only issuing commands right now. What happens when the device needs to notify us of state changes?\\n\\nThis is exactly the challenge that The Widget Company is going to have to face as it introduces yet another game-changing IoT device. We will take a look at how they solved that problem in the next post.\"}]],\"sections\":[[10,0]],\"ghostVersion\":\"3.0\"}","html":"<!--kg-card-begin: markdown--><p><em>In my last article on &quot;<a href=\"http://kevinhoyt.com/blog/2014/11/04/iot-and-the-new-web.html\">IoT and the New Web</a>&quot; I talked about how consumer expectations are trending towards instant gratification. I went on to talk about how that experience impacts the Web and the Internet of Things (IoT). I closed by introducing &quot;The Widget Company&quot; (fictional) which will be launching brand new, life-changing (not really) products in the IoT space.</em></p>\n<p><em>Well, the day has finally arrived. The press has gathered. We have dawned our black turtleneck sweaters. Live-blogging is keeping the Interwebs informed. Suddenly, in a surprise &quot;one more thing&quot; moment, The Widget Company CEO introduces &quot;WidgetX!&quot; Now we can all gather around a glass enclosure to finally get our first glimpse of the future.</em></p>\n<hr>\n<p>Okay, so WidgetX is really just a Light Emitting Diode (LED). Not really a life-changing product, but not too dissimilar from other commercial products on the market, such as the <a href=\"http://www2.meethue.com/en-us/\">Philips Hue</a>. Let us sit in on one of the launch event sessions that talks about how this device is connected to the Internet.</p>\n<h3 id=\"circuitdiagram\">Circuit Diagram</h3>\n<p>Making an LED blink is the &quot;Hello World&quot; of the physical computing space. It effectively demonstrates basic programming, electronics, and digital input/output (IO). Wiring up an LED requires that you know a little about the LED itself, and a little about Ohm's Law.</p>\n<blockquote>\n<p>Resistance (Ohms) = Voltage (Volts) / Current (Amps)</p>\n</blockquote>\n<p>Effectively, I will be using an LED with a 2.0V forward voltage and a forward current of 20mA. We also know that voltage provided by the Arduino digital pins is 5.0V. If all of that sounds Greek to you, that is totally fine at this point. It did to me too at one point.</p>\n<p>After the voltage passes through the LED, it should go from 5.0V to 3.0V (5.0V - 2.0V). The current needed is 20mA, which gives us a resistance value of 150 Ohms (3.0V / 20mA). So then electricity will come out of a digital pin on the Arduino, through a 150 Ohm resistor, across the LED, and then to ground to make a complete circuit. This looks something like the following diagram.</p>\n<p><img src=\"http://images.kevinhoyt.com/fritzing.led.png\" alt=\"Digital pin 12 to 150 Ohm resistor. Resistor to LED anode. LED cathode to ground.\" loading=\"lazy\"></p>\n<h3 id=\"arduinoyun\">Arduino Yun</h3>\n<p>The <a href=\"http://arduino.cc/en/Main/ArduinoBoardYun\">Arduino Yun</a> is one of my favorite hobbyist platforms for the Internet of Things. Besides being an Arduino Leonardo, it has a Linux System on a Chip (SoC), micro SD slot, ethernet jack, USB Type-A, and even 802.11 b/g/n wireless. The wireless even supports ad hoc mode, which will let us configure SSID and password details from a browser on a different machine.</p>\n<p><em>The details on configuring an Arduino Yun are beyond the scope of this post.</em></p>\n<p>The Arduino Yun SoC gives us handy server and client libraries to use in our Arduino code. You can use these libraries to create servers and clients not only for the web, but for any other TCP/IP protocols.</p>\n<p>Rather than write our own HTTP handling however, the SoC will serve content uploaded with our sketch. To get this going, create an &quot;/arduino/www&quot; directory structure on the Yun SD, and then create a &quot;www&quot; folder inside your sketch folder. That content will be uploaded for you automatically.</p>\n<p>Another reason that the Yun is so great for prototyping is that it can automatically serve up a REST interface for your sketch. Anything after &quot;<a href=\"http://arduino.local/arduino/\">http://arduino.local/arduino/</a>&quot; can be easily read through the aforementioned client library as a stream. You can then parse the incoming string, and even write content back to the requesting client.</p>\n<blockquote>\n<p>Woah! That is a lot of functionality provided out of the box that we would otherwise have to figure out all ourselves.</p>\n</blockquote>\n<p>Woah! That is a lot of functionality provided out of the box that we would otherwise have to figure out ourselves. To recap, the Arduino Yun gives us all the following for the purposes of building the IoT device, WidgetX:</p>\n<ol>\n<li>Arduino Leonardo (16 MHz MCU)</li>\n<li>Wireless (802.11 b/g/n)</li>\n<li>Hostname (no IP hunting)</li>\n<li>Configuration Panel (SSID)</li>\n<li>Micro SD (disk space)</li>\n<li>HTTP Server (for static assets)</li>\n<li>REST API (passthrough to our sketch)</li>\n<li>TCP Server</li>\n<li>TCP Client</li>\n</ol>\n<p>This is almost more a really lightweight computer like the Raspberry Pi, than it is an Arduino. And that is an important observation. Using our request/response hammer, this project really looks like a nail, and we load up accordingly. That seems like a lot of pieces to put into every little IoT device we want to bring to market. While you consider the long-term implications of that hammer, here is the Arduino code WidgetX uses.</p>\n<pre><code>// Libraries\n#include &lt;Bridge.h&gt;\n#include &lt;YunClient.h&gt; \n#include &lt;YunServer.h&gt;\n\n// Literals\n#define LED_OFF          &quot;OFF&quot;\n#define LED_ON           &quot;ON&quot;\n#define MESSAGE_LED      &quot;LED&quot;\n#define TERMINATOR_SLASH '/'\n\n// Constants\nconst int LED = 13;\n\n// Server\nYunServer server;\n \n// Setup\nvoid setup() \n{\n  // Digital output\n  // Set low (off) initially\n  pinMode( LED, OUTPUT ); \n  digitalWrite( LED, LOW );\n  \n  // Leverage the SoC libraries\n  // Start listening for clients\n  Bridge.begin();\n  server.begin();\n}\n \nvoid loop() \n{\n  String    command;\n  String    message;\n  YunClient client;\n  \n  // Accept incoming client request\n  client = server.accept();\n \n  // If there is a client\n  if( client ) \n  {\n     // Parse the command from the URL\n    command = client.readStringUntil( \n      TERMINATOR_SLASH \n    );\n    command.toUpperCase();\n\n    // Message for the LED\n    // Could have multiple controls\n    if( command == MESSAGE_LED ) \n    {\n      // Parse the message from the URL\n      // Trim - invisible - newline from end\n      message = client.readString();\n      message.toUpperCase();\n      message.trim();\n  \n      if( message == LED_ON )\n      {\n        // Turn the LED on\n        // Tell the client something happened\n        digitalWrite( LED, HIGH );\n        server.println( message );                      \n      } else if( message == LED_OFF ) {\n        // Turn the LED on\n        // Tell the client something happened        \n        digitalWrite( LED, LOW );        \n        server.println( message );                      \n      } else {\n        // What did you want me to do?\n        server.print( &quot;Unknonwn: &quot; );\n        server.println( message );        \n      }\n    }\n  \n    // Close connection with client\n    client.stop();\n  }\n}\n</code></pre>\n<h3 id=\"webclient\">Web Client</h3>\n<p><img src=\"http://images.kevinhoyt.com/iota.web.led.png\" alt=\"A little CSS magic helps us toggle the appearance of the button.\" loading=\"lazy\"></p>\n<p>For the Web stack, we will have a single HTML page with a button in the center. When you press the button, an XMLHttpRequest (XHR) instance will be used to make the GET request with the appropriate command to turn on the LED. When you release the button, the XHR instance will send the command to turn off the LED.</p>\n<p>Here is what our JavaScript will look like:</p>\n<pre><code>var xhr = null;\n\nfunction doButtonDown()\n{\n  xhr = new XMLHttpRequest();\n  xhr.addEventListener( &quot;load&quot;, doButtonLoad );\n  xhr.open( \n    &quot;GET&quot;, \n    &quot;http://arduino.local/arduino/led/on&quot; \n  );\n  xhr.send( null );\n}\n</code></pre>\n<p>Because our Arduino Yun is serving all the static content, including the JavaScript, we do not have to worry about cross-domain security. This seems fine for a single IoT device, but will really introduce problems as we add more IoT to our product line-up. How will these devices communicate across one another? How about devices that we did not make? How will they even be aware of one another? When we start swinging hammers around, we can quickly arrive at an &quot;intranet&quot; of things, as opposed to the far more powerful &quot;internet&quot; of things.</p>\n<p>For the button, I am using an old CSS trick. Both the up and down states of the button are loaded as one image. The element that shows the button does not show overflow content which hides the state we are not showing at the moment. To change states, we can adjust the style of the &quot;background-position&quot; property.</p>\n<pre><code>.button {\n  background-image: url( 'button.png' );\n  background-position: 0;\n  background-repeat: no-repeat;\n  height: 283px;\n  outline: none;\n  overflow: hidden;\n  width: 298px;\n}\n\n.button:active {\n  background-position: -298px;\n}\n</code></pre>\n<p><em>Note that I am using a DIV element for my button to make styling a little easier across screens - I am looking at you iOS!</em></p>\n<p>If you are a web developer, all of these web stack pieces probably feel pretty comfortable. That is a good thing because the hardest part of IoT is actually the Internet. As a Web Developer you have a much shorter way to go to master IoT than an Electrical Engineer. On the hardware side, patterns start to emerge pretty quickly. On the Web side, there is a never ending permutation of screens and technologies to support.</p>\n<h3 id=\"nextsteps\">Next Steps</h3>\n<p>Like any other technology, there are a myriad of ways to architect the Yun into our web stack. We could for example, run PHP on the Yun itself (on the Linux SoC side), and even Apache or Nginx. We could run PHP on a central server (EC2) and then proxy communication to and from our devices. The list goes on and on.</p>\n<blockquote>\n<p>What happens when the device needs to notify us of state changes?</p>\n</blockquote>\n<p>If you have put this particular code in place, one thing you will notice is that the LED is particularly slow to respond. Clicking the button in the browser takes a second or two to actually light the LED. While this seems like an acceptable margin, we are only issuing commands right now. What happens when the device needs to notify us of state changes?</p>\n<p>This is exactly the challenge that The Widget Company is going to have to face as it introduces yet another game-changing IoT device. We will take a look at how they solved that problem in the next post.</p>\n<!--kg-card-end: markdown-->","comment_id":"5","plaintext":"In my last article on \"IoT and the New Web\n[http://kevinhoyt.com/blog/2014/11/04/iot-and-the-new-web.html]\" I talked about\nhow consumer expectations are trending towards instant gratification. I went on\nto talk about how that experience impacts the Web and the Internet of Things\n(IoT). I closed by introducing \"The Widget Company\" (fictional) which will be\nlaunching brand new, life-changing (not really) products in the IoT space.\n\nWell, the day has finally arrived. The press has gathered. We have dawned our\nblack turtleneck sweaters. Live-blogging is keeping the Interwebs informed.\nSuddenly, in a surprise \"one more thing\" moment, The Widget Company CEO\nintroduces \"WidgetX!\" Now we can all gather around a glass enclosure to finally\nget our first glimpse of the future.\n\n\n--------------------------------------------------------------------------------\n\nOkay, so WidgetX is really just a Light Emitting Diode (LED). Not really a\nlife-changing product, but not too dissimilar from other commercial products on\nthe market, such as the Philips Hue [http://www2.meethue.com/en-us/]. Let us sit\nin on one of the launch event sessions that talks about how this device is\nconnected to the Internet.\n\nCircuit Diagram\nMaking an LED blink is the \"Hello World\" of the physical computing space. It\neffectively demonstrates basic programming, electronics, and digital\ninput/output (IO). Wiring up an LED requires that you know a little about the\nLED itself, and a little about Ohm's Law.\n\n> Resistance (Ohms) = Voltage (Volts) / Current (Amps)\n\n\nEffectively, I will be using an LED with a 2.0V forward voltage and a forward\ncurrent of 20mA. We also know that voltage provided by the Arduino digital pins\nis 5.0V. If all of that sounds Greek to you, that is totally fine at this point.\nIt did to me too at one point.\n\nAfter the voltage passes through the LED, it should go from 5.0V to 3.0V (5.0V -\n2.0V). The current needed is 20mA, which gives us a resistance value of 150 Ohms\n(3.0V / 20mA). So then electricity will come out of a digital pin on the\nArduino, through a 150 Ohm resistor, across the LED, and then to ground to make\na complete circuit. This looks something like the following diagram.\n\n\n\nArduino Yun\nThe Arduino Yun [http://arduino.cc/en/Main/ArduinoBoardYun] is one of my\nfavorite hobbyist platforms for the Internet of Things. Besides being an Arduino\nLeonardo, it has a Linux System on a Chip (SoC), micro SD slot, ethernet jack,\nUSB Type-A, and even 802.11 b/g/n wireless. The wireless even supports ad hoc\nmode, which will let us configure SSID and password details from a browser on a\ndifferent machine.\n\nThe details on configuring an Arduino Yun are beyond the scope of this post.\n\nThe Arduino Yun SoC gives us handy server and client libraries to use in our\nArduino code. You can use these libraries to create servers and clients not only\nfor the web, but for any other TCP/IP protocols.\n\nRather than write our own HTTP handling however, the SoC will serve content\nuploaded with our sketch. To get this going, create an \"/arduino/www\" directory\nstructure on the Yun SD, and then create a \"www\" folder inside your sketch\nfolder. That content will be uploaded for you automatically.\n\nAnother reason that the Yun is so great for prototyping is that it can\nautomatically serve up a REST interface for your sketch. Anything after \"\nhttp://arduino.local/arduino/\" can be easily read through the aforementioned\nclient library as a stream. You can then parse the incoming string, and even\nwrite content back to the requesting client.\n\n> Woah! That is a lot of functionality provided out of the box that we would\notherwise have to figure out all ourselves.\n\n\nWoah! That is a lot of functionality provided out of the box that we would\notherwise have to figure out ourselves. To recap, the Arduino Yun gives us all\nthe following for the purposes of building the IoT device, WidgetX:\n\n 1. Arduino Leonardo (16 MHz MCU)\n 2. Wireless (802.11 b/g/n)\n 3. Hostname (no IP hunting)\n 4. Configuration Panel (SSID)\n 5. Micro SD (disk space)\n 6. HTTP Server (for static assets)\n 7. REST API (passthrough to our sketch)\n 8. TCP Server\n 9. TCP Client\n\nThis is almost more a really lightweight computer like the Raspberry Pi, than it\nis an Arduino. And that is an important observation. Using our request/response\nhammer, this project really looks like a nail, and we load up accordingly. That\nseems like a lot of pieces to put into every little IoT device we want to bring\nto market. While you consider the long-term implications of that hammer, here is\nthe Arduino code WidgetX uses.\n\n// Libraries\n#include <Bridge.h>\n#include <YunClient.h> \n#include <YunServer.h>\n\n// Literals\n#define LED_OFF          \"OFF\"\n#define LED_ON           \"ON\"\n#define MESSAGE_LED      \"LED\"\n#define TERMINATOR_SLASH '/'\n\n// Constants\nconst int LED = 13;\n\n// Server\nYunServer server;\n \n// Setup\nvoid setup() \n{\n  // Digital output\n  // Set low (off) initially\n  pinMode( LED, OUTPUT ); \n  digitalWrite( LED, LOW );\n  \n  // Leverage the SoC libraries\n  // Start listening for clients\n  Bridge.begin();\n  server.begin();\n}\n \nvoid loop() \n{\n  String    command;\n  String    message;\n  YunClient client;\n  \n  // Accept incoming client request\n  client = server.accept();\n \n  // If there is a client\n  if( client ) \n  {\n     // Parse the command from the URL\n    command = client.readStringUntil( \n      TERMINATOR_SLASH \n    );\n    command.toUpperCase();\n\n    // Message for the LED\n    // Could have multiple controls\n    if( command == MESSAGE_LED ) \n    {\n      // Parse the message from the URL\n      // Trim - invisible - newline from end\n      message = client.readString();\n      message.toUpperCase();\n      message.trim();\n  \n      if( message == LED_ON )\n      {\n        // Turn the LED on\n        // Tell the client something happened\n        digitalWrite( LED, HIGH );\n        server.println( message );                      \n      } else if( message == LED_OFF ) {\n        // Turn the LED on\n        // Tell the client something happened        \n        digitalWrite( LED, LOW );        \n        server.println( message );                      \n      } else {\n        // What did you want me to do?\n        server.print( \"Unknonwn: \" );\n        server.println( message );        \n      }\n    }\n  \n    // Close connection with client\n    client.stop();\n  }\n}\n\n\nWeb Client\n\n\nFor the Web stack, we will have a single HTML page with a button in the center.\nWhen you press the button, an XMLHttpRequest (XHR) instance will be used to make\nthe GET request with the appropriate command to turn on the LED. When you\nrelease the button, the XHR instance will send the command to turn off the LED.\n\nHere is what our JavaScript will look like:\n\nvar xhr = null;\n\nfunction doButtonDown()\n{\n  xhr = new XMLHttpRequest();\n  xhr.addEventListener( \"load\", doButtonLoad );\n  xhr.open( \n    \"GET\", \n    \"http://arduino.local/arduino/led/on\" \n  );\n  xhr.send( null );\n}\n\n\nBecause our Arduino Yun is serving all the static content, including the\nJavaScript, we do not have to worry about cross-domain security. This seems fine\nfor a single IoT device, but will really introduce problems as we add more IoT\nto our product line-up. How will these devices communicate across one another?\nHow about devices that we did not make? How will they even be aware of one\nanother? When we start swinging hammers around, we can quickly arrive at an\n\"intranet\" of things, as opposed to the far more powerful \"internet\" of things.\n\nFor the button, I am using an old CSS trick. Both the up and down states of the\nbutton are loaded as one image. The element that shows the button does not show\noverflow content which hides the state we are not showing at the moment. To\nchange states, we can adjust the style of the \"background-position\" property.\n\n.button {\n  background-image: url( 'button.png' );\n  background-position: 0;\n  background-repeat: no-repeat;\n  height: 283px;\n  outline: none;\n  overflow: hidden;\n  width: 298px;\n}\n\n.button:active {\n  background-position: -298px;\n}\n\n\nNote that I am using a DIV element for my button to make styling a little easier\nacross screens - I am looking at you iOS!\n\nIf you are a web developer, all of these web stack pieces probably feel pretty\ncomfortable. That is a good thing because the hardest part of IoT is actually\nthe Internet. As a Web Developer you have a much shorter way to go to master IoT\nthan an Electrical Engineer. On the hardware side, patterns start to emerge\npretty quickly. On the Web side, there is a never ending permutation of screens\nand technologies to support.\n\nNext Steps\nLike any other technology, there are a myriad of ways to architect the Yun into\nour web stack. We could for example, run PHP on the Yun itself (on the Linux SoC\nside), and even Apache or Nginx. We could run PHP on a central server (EC2) and\nthen proxy communication to and from our devices. The list goes on and on.\n\n> What happens when the device needs to notify us of state changes?\n\n\nIf you have put this particular code in place, one thing you will notice is that\nthe LED is particularly slow to respond. Clicking the button in the browser\ntakes a second or two to actually light the LED. While this seems like an\nacceptable margin, we are only issuing commands right now. What happens when the\ndevice needs to notify us of state changes?\n\nThis is exactly the challenge that The Widget Company is going to have to face\nas it introduces yet another game-changing IoT device. We will take a look at\nhow they solved that problem in the next post.","feature_image":"http://images.kevinhoyt.com/led.throwies.chaos.jpg","featured":0,"status":"published","locale":null,"visibility":"public","author_id":"1","created_at":"2015-04-20T19:54:38.000Z","updated_at":"2015-04-21T14:23:39.000Z","published_at":"2014-11-05T16:00:00.000Z","custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"type":"post","email_recipient_filter":"none"},{"id":"5adb8922351ffe0018a57712","uuid":"9ee2bbc6-a8bd-4ae2-bd89-23679c9d81e0","title":"WidgetY and the New Web","slug":"widgety-and-the-new-web","mobiledoc":"{\"version\":\"0.3.1\",\"markups\":[],\"atoms\":[],\"cards\":[[\"markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\"*My how time flies! The Widget Company has gone on to make many new friends with the revolutionary WidgetX device. Connected to the Internet, you have the power to control an LED through a simple web page. Magnificent! Sales are at an all-time high, and customers love it.*\\n\\n*With that kind of success comes new demands in the form of feature requests. The Widget Company has listened, and today is proud to introduce the world to WidgetY. WidgetY is a marvel of modern technology, allowing you to remote monitor the amount of light in a room.*\\n\\n---\\n\\nWhile the press is amazed with how The Widget Company has managed to pull off yet another disruptive product, the developer audience is wondering how it is done. Luckily, there is a session at the launch event, and a seat with our name on it (after waiting in line for two days). How does one remotely monitor light levels?\\n\\n### Circuit Diagram\\n\\nOkay, so the workhorse under the hood in WidgetY is a light-dependent resistor (LDR), also commonly known as a photocell. The leads (wires) on the photocell are connected to a thin piece of silicon in the center. Silicon reacts to light. The more light, the more electricity it will let pass through. The less light, the more resistance it gets, slowing down the flow of electricity.\\n\\n![5V to photocell. Out through 10K resistor to ground. Analog pin 0 to same row as ground.](http://images.kevinhoyt.com/fritzing.photocell.png)\\n\\nProbably the trickiest part of this circuit is the pull-down resistor. If the pull-down resistor is too small, even the slightest light will cause the photocell readings to spike. We want a nice range of readings for our given light source (a room in a house). To achieve this range of readings, we will use a 10K resistor.\\n\\n> A pull-down resistor holds the logic signal near zero volts when no other active device is connected.\\n\\n### Arduino Yun\\n\\nCertainly we could have treated this device the same as the LED from the previous post on WidgetX. We could poll from the web page using XMLHttpRequest (XHR) in the background, updating the web page over time, but at updating every few seconds it would have been horrendously slow.\\n\\nWhy every few seconds? The 16 MHz processor on the Arduino is simply not going to handle much more. Even for our LED, there was a second or two lag while the HTTP request was parsed.\\n\\nWhy does that matter? Recall from the first post in this series that consumers have come to expect immediate gratification. While a few seconds delay between updates may have worked, our consumers would have seen this as half-baked. Our competitors would have seen it as opportunity. In this end, this will very much be the way vertical IoT battles will be won - not on hardware per se, as much as the experience provided to the consumer.\\n\\n> If you are not going to wait but seconds for your HD streaming video to load, you are sure as heck not going to wait five seconds for a dedicated IoT device to respond.\\n\\nEffectively what WidgetY needs to deliver that experience is the ability to push data from the device to a web page as fast as it possibly can. In a perfect world, it would be faster than the human eye could read it, but then we would throttle (analytics) it as necessary at the client. It turns out that in the New Web, there is a standard that can do exactly this - WebSocket.\\n\\n### Enter WebSocket\\n\\nWebSocket was introduced to the web stack in February 2010, and has been stabilized in the form of [RFC 6455](http://tools.ietf.org/html/rfc6455) as of December 2011. WebSocket has been in all major browsers, both desktop and mobile, for some time now. WebSocket allows us to open a socket connection to a remote server, and keep it open. Data can then travel in either direction across the wire at any time.\\n\\nNot only is [WebSocket](http://caniuse.com/#feat=websockets) in your favorite browser, on your favorite device, but it is also available, in the form of various libraries, for Arduino (and many other boards/chipsets). As WebSocket emerged in 2010, I partnered with Branden Hall to build an Arduino implementation. He did some amazing work on supporting the [Roving Network's WiFly](https://www.sparkfun.com/products/9954) chipset as well. You can grab this library from [Branden's GitHub repository](https://github.com/brandenhall/Arduino-Websocket).\\n\\nHow the WebSocket works on the Arduino is generic to the means of communication. This means that while it can work on the WiFly, it can also work using ethernet, or in our case, the Arduino Yun.\\n\\n```\\n// Libraries\\n#include <Bridge.h>\\n#include <WebSocketServer.h>\\n#include <YunClient.h>\\n#include <YunServer.h>\\n\\n// Literals\\n#define CALLBACK_FUNCTIONS 0\\n#define DEBUGGING\\n#define MAX_FRAME_LENGTH   64\\n#define SOCKET_PORT        8383\\n\\n// Constants\\nconst int PHOTOCELL = 0;\\n\\n// Server\\nYunServer server( SOCKET_PORT );\\n\\n// WebSocket protocol\\nWebSocketServer socket;\\n\\n// Setup\\nvoid setup() \\n{  \\n  // Analog input\\n  pinMode( PHOTOCELL, INPUT );\\n  \\n  // Leveraging the SoC libraries\\n  // Not using port 80\\n  Bridge.begin();\\n  server.noListenOnLocalhost();\\n  server.begin();\\n}\\n\\n// Loop\\nvoid loop() \\n{\\n  int       light;\\n  String    data;\\n  YunClient client;\\n \\n  // Accept incoming client request\\n  client = server.accept();\\n  \\n  // If tehre is a client\\n  if( client ) \\n  {\\n    // If connected as WebSocket\\n    if( client.connected() && \\n        socket.handshake( client ) ) \\n    {  \\n      // As long as the client is connected\\n      while( client.connected() ) \\n      {\\n        // Read the analog light value\\n        // Send as string to client\\n        light = analogRead( PHOTOCELL );\\n        socket.sendData( String( light ) );      \\n      }\\n    }\\n  }\\n  \\n  // Fully disconnect before loop\\n  delay( 100 );\\n}\\n```\\n\\n### Web Client\\n\\n![Charting brought to you by the letters S, V, and G. Woot!](http://images.kevinhoyt.com/iota.web.photocell.jpg)\\n\\nLeveraging WebSockets in JavaScript is pretty straightforward work. First, you create an instance of WebSocket. As an argument, WebSocket expects a string pointing it to the server. This could be a URL, an IP address, or other routing, but will be prefixed with \\\"ws\\\" where you might generally use \\\"http\\\". Attach some event listeners, are you are on your way.\\n\\n```\\n// Global\\nvar socket = null;\\n\\n// Instantiate the WebSocket\\n// Attach event handlers\\nsocket = new WebSocket( SOCKET_URL );\\nsocket.addEventListener( \\\"open\\\", doSocketOpen );\\nsocket.addEventListener( \\\"message\\\", doSocketMessage );\\t\\nsocket.addEventListener( \\\"error\\\", doSocketError );\\n```\\n\\nWhen the data arrives, as I mentioned, it can pretty much come in faster than your user can perceive it. The best way to handle this is to decouple the data collection from WebSocket from the actual rendering. This means we will only collect the data as it arrives in our event handler.\\n\\n```\\n// Called when a message has arrived on the socket\\n// Pushes latest value into array for display\\nfunction doSocketMessage( mess )\\n{\\n  light.push( parseInt( mess.data ) );\\n  light.splice( 0, 1 );\\n}\\n```\\n\\nWith our incoming data safely tucked away in an array in memory, we are free to render the data however, and whenever we want. What is important is that user experience. To make sure rendering is buttery smooth, we will use requestAnimationFrame. To make sure it can render cleanly across devices, we will map the light data to an SVG path.\\n\\nThe result is an impressively fast display of WidgetY data. It catches you off guard at first because you do not expect the web to move that fast. And yet there it is! That same shock and awe will drive your customers wild on social media. Perfect!\\n\\n### Next Steps\\n\\nYou may have already figured out that we could also use WebSocket to control the LED, and likely with a much faster response time. WebSocket does, after all, offer bi-directional communication. Perhaps more interesting however is having the data read from the photocell impact the brightness of the LED.\\n\\nOrchestrating two IoT devices, without either of them knowing about each other, is where The Widget Company is headed next. This is also where the real power of IoT lies - having things communicate to one another without any interaction from humans. In the next post we go from the \\\"intranet\\\" of things to the Internet of Things.\\n\"}]],\"sections\":[[10,0]],\"ghostVersion\":\"3.0\"}","html":"<!--kg-card-begin: markdown--><p><em>My how time flies! The Widget Company has gone on to make many new friends with the revolutionary WidgetX device. Connected to the Internet, you have the power to control an LED through a simple web page. Magnificent! Sales are at an all-time high, and customers love it.</em></p>\n<p><em>With that kind of success comes new demands in the form of feature requests. The Widget Company has listened, and today is proud to introduce the world to WidgetY. WidgetY is a marvel of modern technology, allowing you to remote monitor the amount of light in a room.</em></p>\n<hr>\n<p>While the press is amazed with how The Widget Company has managed to pull off yet another disruptive product, the developer audience is wondering how it is done. Luckily, there is a session at the launch event, and a seat with our name on it (after waiting in line for two days). How does one remotely monitor light levels?</p>\n<h3 id=\"circuitdiagram\">Circuit Diagram</h3>\n<p>Okay, so the workhorse under the hood in WidgetY is a light-dependent resistor (LDR), also commonly known as a photocell. The leads (wires) on the photocell are connected to a thin piece of silicon in the center. Silicon reacts to light. The more light, the more electricity it will let pass through. The less light, the more resistance it gets, slowing down the flow of electricity.</p>\n<p><img src=\"http://images.kevinhoyt.com/fritzing.photocell.png\" alt=\"5V to photocell. Out through 10K resistor to ground. Analog pin 0 to same row as ground.\" loading=\"lazy\"></p>\n<p>Probably the trickiest part of this circuit is the pull-down resistor. If the pull-down resistor is too small, even the slightest light will cause the photocell readings to spike. We want a nice range of readings for our given light source (a room in a house). To achieve this range of readings, we will use a 10K resistor.</p>\n<blockquote>\n<p>A pull-down resistor holds the logic signal near zero volts when no other active device is connected.</p>\n</blockquote>\n<h3 id=\"arduinoyun\">Arduino Yun</h3>\n<p>Certainly we could have treated this device the same as the LED from the previous post on WidgetX. We could poll from the web page using XMLHttpRequest (XHR) in the background, updating the web page over time, but at updating every few seconds it would have been horrendously slow.</p>\n<p>Why every few seconds? The 16 MHz processor on the Arduino is simply not going to handle much more. Even for our LED, there was a second or two lag while the HTTP request was parsed.</p>\n<p>Why does that matter? Recall from the first post in this series that consumers have come to expect immediate gratification. While a few seconds delay between updates may have worked, our consumers would have seen this as half-baked. Our competitors would have seen it as opportunity. In this end, this will very much be the way vertical IoT battles will be won - not on hardware per se, as much as the experience provided to the consumer.</p>\n<blockquote>\n<p>If you are not going to wait but seconds for your HD streaming video to load, you are sure as heck not going to wait five seconds for a dedicated IoT device to respond.</p>\n</blockquote>\n<p>Effectively what WidgetY needs to deliver that experience is the ability to push data from the device to a web page as fast as it possibly can. In a perfect world, it would be faster than the human eye could read it, but then we would throttle (analytics) it as necessary at the client. It turns out that in the New Web, there is a standard that can do exactly this - WebSocket.</p>\n<h3 id=\"enterwebsocket\">Enter WebSocket</h3>\n<p>WebSocket was introduced to the web stack in February 2010, and has been stabilized in the form of <a href=\"http://tools.ietf.org/html/rfc6455\">RFC 6455</a> as of December 2011. WebSocket has been in all major browsers, both desktop and mobile, for some time now. WebSocket allows us to open a socket connection to a remote server, and keep it open. Data can then travel in either direction across the wire at any time.</p>\n<p>Not only is <a href=\"http://caniuse.com/#feat=websockets\">WebSocket</a> in your favorite browser, on your favorite device, but it is also available, in the form of various libraries, for Arduino (and many other boards/chipsets). As WebSocket emerged in 2010, I partnered with Branden Hall to build an Arduino implementation. He did some amazing work on supporting the <a href=\"https://www.sparkfun.com/products/9954\">Roving Network's WiFly</a> chipset as well. You can grab this library from <a href=\"https://github.com/brandenhall/Arduino-Websocket\">Branden's GitHub repository</a>.</p>\n<p>How the WebSocket works on the Arduino is generic to the means of communication. This means that while it can work on the WiFly, it can also work using ethernet, or in our case, the Arduino Yun.</p>\n<pre><code>// Libraries\n#include &lt;Bridge.h&gt;\n#include &lt;WebSocketServer.h&gt;\n#include &lt;YunClient.h&gt;\n#include &lt;YunServer.h&gt;\n\n// Literals\n#define CALLBACK_FUNCTIONS 0\n#define DEBUGGING\n#define MAX_FRAME_LENGTH   64\n#define SOCKET_PORT        8383\n\n// Constants\nconst int PHOTOCELL = 0;\n\n// Server\nYunServer server( SOCKET_PORT );\n\n// WebSocket protocol\nWebSocketServer socket;\n\n// Setup\nvoid setup() \n{  \n  // Analog input\n  pinMode( PHOTOCELL, INPUT );\n  \n  // Leveraging the SoC libraries\n  // Not using port 80\n  Bridge.begin();\n  server.noListenOnLocalhost();\n  server.begin();\n}\n\n// Loop\nvoid loop() \n{\n  int       light;\n  String    data;\n  YunClient client;\n \n  // Accept incoming client request\n  client = server.accept();\n  \n  // If tehre is a client\n  if( client ) \n  {\n    // If connected as WebSocket\n    if( client.connected() &amp;&amp; \n        socket.handshake( client ) ) \n    {  \n      // As long as the client is connected\n      while( client.connected() ) \n      {\n        // Read the analog light value\n        // Send as string to client\n        light = analogRead( PHOTOCELL );\n        socket.sendData( String( light ) );      \n      }\n    }\n  }\n  \n  // Fully disconnect before loop\n  delay( 100 );\n}\n</code></pre>\n<h3 id=\"webclient\">Web Client</h3>\n<p><img src=\"http://images.kevinhoyt.com/iota.web.photocell.jpg\" alt=\"Charting brought to you by the letters S, V, and G. Woot!\" loading=\"lazy\"></p>\n<p>Leveraging WebSockets in JavaScript is pretty straightforward work. First, you create an instance of WebSocket. As an argument, WebSocket expects a string pointing it to the server. This could be a URL, an IP address, or other routing, but will be prefixed with &quot;ws&quot; where you might generally use &quot;http&quot;. Attach some event listeners, are you are on your way.</p>\n<pre><code>// Global\nvar socket = null;\n\n// Instantiate the WebSocket\n// Attach event handlers\nsocket = new WebSocket( SOCKET_URL );\nsocket.addEventListener( &quot;open&quot;, doSocketOpen );\nsocket.addEventListener( &quot;message&quot;, doSocketMessage );\t\nsocket.addEventListener( &quot;error&quot;, doSocketError );\n</code></pre>\n<p>When the data arrives, as I mentioned, it can pretty much come in faster than your user can perceive it. The best way to handle this is to decouple the data collection from WebSocket from the actual rendering. This means we will only collect the data as it arrives in our event handler.</p>\n<pre><code>// Called when a message has arrived on the socket\n// Pushes latest value into array for display\nfunction doSocketMessage( mess )\n{\n  light.push( parseInt( mess.data ) );\n  light.splice( 0, 1 );\n}\n</code></pre>\n<p>With our incoming data safely tucked away in an array in memory, we are free to render the data however, and whenever we want. What is important is that user experience. To make sure rendering is buttery smooth, we will use requestAnimationFrame. To make sure it can render cleanly across devices, we will map the light data to an SVG path.</p>\n<p>The result is an impressively fast display of WidgetY data. It catches you off guard at first because you do not expect the web to move that fast. And yet there it is! That same shock and awe will drive your customers wild on social media. Perfect!</p>\n<h3 id=\"nextsteps\">Next Steps</h3>\n<p>You may have already figured out that we could also use WebSocket to control the LED, and likely with a much faster response time. WebSocket does, after all, offer bi-directional communication. Perhaps more interesting however is having the data read from the photocell impact the brightness of the LED.</p>\n<p>Orchestrating two IoT devices, without either of them knowing about each other, is where The Widget Company is headed next. This is also where the real power of IoT lies - having things communicate to one another without any interaction from humans. In the next post we go from the &quot;intranet&quot; of things to the Internet of Things.</p>\n<!--kg-card-end: markdown-->","comment_id":"6","plaintext":"My how time flies! The Widget Company has gone on to make many new friends with\nthe revolutionary WidgetX device. Connected to the Internet, you have the power\nto control an LED through a simple web page. Magnificent! Sales are at an\nall-time high, and customers love it.\n\nWith that kind of success comes new demands in the form of feature requests. The\nWidget Company has listened, and today is proud to introduce the world to\nWidgetY. WidgetY is a marvel of modern technology, allowing you to remote\nmonitor the amount of light in a room.\n\n\n--------------------------------------------------------------------------------\n\nWhile the press is amazed with how The Widget Company has managed to pull off\nyet another disruptive product, the developer audience is wondering how it is\ndone. Luckily, there is a session at the launch event, and a seat with our name\non it (after waiting in line for two days). How does one remotely monitor light\nlevels?\n\nCircuit Diagram\nOkay, so the workhorse under the hood in WidgetY is a light-dependent resistor\n(LDR), also commonly known as a photocell. The leads (wires) on the photocell\nare connected to a thin piece of silicon in the center. Silicon reacts to light.\nThe more light, the more electricity it will let pass through. The less light,\nthe more resistance it gets, slowing down the flow of electricity.\n\n\n\nProbably the trickiest part of this circuit is the pull-down resistor. If the\npull-down resistor is too small, even the slightest light will cause the\nphotocell readings to spike. We want a nice range of readings for our given\nlight source (a room in a house). To achieve this range of readings, we will use\na 10K resistor.\n\n> A pull-down resistor holds the logic signal near zero volts when no other active\ndevice is connected.\n\n\nArduino Yun\nCertainly we could have treated this device the same as the LED from the\nprevious post on WidgetX. We could poll from the web page using XMLHttpRequest\n(XHR) in the background, updating the web page over time, but at updating every\nfew seconds it would have been horrendously slow.\n\nWhy every few seconds? The 16 MHz processor on the Arduino is simply not going\nto handle much more. Even for our LED, there was a second or two lag while the\nHTTP request was parsed.\n\nWhy does that matter? Recall from the first post in this series that consumers\nhave come to expect immediate gratification. While a few seconds delay between\nupdates may have worked, our consumers would have seen this as half-baked. Our\ncompetitors would have seen it as opportunity. In this end, this will very much\nbe the way vertical IoT battles will be won - not on hardware per se, as much as\nthe experience provided to the consumer.\n\n> If you are not going to wait but seconds for your HD streaming video to load,\nyou are sure as heck not going to wait five seconds for a dedicated IoT device\nto respond.\n\n\nEffectively what WidgetY needs to deliver that experience is the ability to push\ndata from the device to a web page as fast as it possibly can. In a perfect\nworld, it would be faster than the human eye could read it, but then we would\nthrottle (analytics) it as necessary at the client. It turns out that in the New\nWeb, there is a standard that can do exactly this - WebSocket.\n\nEnter WebSocket\nWebSocket was introduced to the web stack in February 2010, and has been\nstabilized in the form of RFC 6455 [http://tools.ietf.org/html/rfc6455] as of\nDecember 2011. WebSocket has been in all major browsers, both desktop and\nmobile, for some time now. WebSocket allows us to open a socket connection to a\nremote server, and keep it open. Data can then travel in either direction across\nthe wire at any time.\n\nNot only is WebSocket [http://caniuse.com/#feat=websockets] in your favorite\nbrowser, on your favorite device, but it is also available, in the form of\nvarious libraries, for Arduino (and many other boards/chipsets). As WebSocket\nemerged in 2010, I partnered with Branden Hall to build an Arduino\nimplementation. He did some amazing work on supporting the Roving Network's\nWiFly [https://www.sparkfun.com/products/9954] chipset as well. You can grab\nthis library from Branden's GitHub repository\n[https://github.com/brandenhall/Arduino-Websocket].\n\nHow the WebSocket works on the Arduino is generic to the means of communication.\nThis means that while it can work on the WiFly, it can also work using ethernet,\nor in our case, the Arduino Yun.\n\n// Libraries\n#include <Bridge.h>\n#include <WebSocketServer.h>\n#include <YunClient.h>\n#include <YunServer.h>\n\n// Literals\n#define CALLBACK_FUNCTIONS 0\n#define DEBUGGING\n#define MAX_FRAME_LENGTH   64\n#define SOCKET_PORT        8383\n\n// Constants\nconst int PHOTOCELL = 0;\n\n// Server\nYunServer server( SOCKET_PORT );\n\n// WebSocket protocol\nWebSocketServer socket;\n\n// Setup\nvoid setup() \n{  \n  // Analog input\n  pinMode( PHOTOCELL, INPUT );\n  \n  // Leveraging the SoC libraries\n  // Not using port 80\n  Bridge.begin();\n  server.noListenOnLocalhost();\n  server.begin();\n}\n\n// Loop\nvoid loop() \n{\n  int       light;\n  String    data;\n  YunClient client;\n \n  // Accept incoming client request\n  client = server.accept();\n  \n  // If tehre is a client\n  if( client ) \n  {\n    // If connected as WebSocket\n    if( client.connected() && \n        socket.handshake( client ) ) \n    {  \n      // As long as the client is connected\n      while( client.connected() ) \n      {\n        // Read the analog light value\n        // Send as string to client\n        light = analogRead( PHOTOCELL );\n        socket.sendData( String( light ) );      \n      }\n    }\n  }\n  \n  // Fully disconnect before loop\n  delay( 100 );\n}\n\n\nWeb Client\n\n\nLeveraging WebSockets in JavaScript is pretty straightforward work. First, you\ncreate an instance of WebSocket. As an argument, WebSocket expects a string\npointing it to the server. This could be a URL, an IP address, or other routing,\nbut will be prefixed with \"ws\" where you might generally use \"http\". Attach some\nevent listeners, are you are on your way.\n\n// Global\nvar socket = null;\n\n// Instantiate the WebSocket\n// Attach event handlers\nsocket = new WebSocket( SOCKET_URL );\nsocket.addEventListener( \"open\", doSocketOpen );\nsocket.addEventListener( \"message\", doSocketMessage );\t\nsocket.addEventListener( \"error\", doSocketError );\n\n\nWhen the data arrives, as I mentioned, it can pretty much come in faster than\nyour user can perceive it. The best way to handle this is to decouple the data\ncollection from WebSocket from the actual rendering. This means we will only\ncollect the data as it arrives in our event handler.\n\n// Called when a message has arrived on the socket\n// Pushes latest value into array for display\nfunction doSocketMessage( mess )\n{\n  light.push( parseInt( mess.data ) );\n  light.splice( 0, 1 );\n}\n\n\nWith our incoming data safely tucked away in an array in memory, we are free to\nrender the data however, and whenever we want. What is important is that user\nexperience. To make sure rendering is buttery smooth, we will use\nrequestAnimationFrame. To make sure it can render cleanly across devices, we\nwill map the light data to an SVG path.\n\nThe result is an impressively fast display of WidgetY data. It catches you off\nguard at first because you do not expect the web to move that fast. And yet\nthere it is! That same shock and awe will drive your customers wild on social\nmedia. Perfect!\n\nNext Steps\nYou may have already figured out that we could also use WebSocket to control the\nLED, and likely with a much faster response time. WebSocket does, after all,\noffer bi-directional communication. Perhaps more interesting however is having\nthe data read from the photocell impact the brightness of the LED.\n\nOrchestrating two IoT devices, without either of them knowing about each other,\nis where The Widget Company is headed next. This is also where the real power of\nIoT lies - having things communicate to one another without any interaction from\nhumans. In the next post we go from the \"intranet\" of things to the Internet of\nThings.","feature_image":"http://images.kevinhoyt.com/modern.toolbox.jpg","featured":0,"status":"published","locale":null,"visibility":"public","author_id":"1","created_at":"2015-04-20T20:30:33.000Z","updated_at":"2015-04-21T14:20:34.000Z","published_at":"2014-11-06T16:00:00.000Z","custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"type":"post","email_recipient_filter":"none"},{"id":"5adb8922351ffe0018a57713","uuid":"cff45d99-9be6-4e97-b787-e255e5412494","title":"Circuit Friday: Thermistor","slug":"circuit-friday-thermistor","mobiledoc":"{\"version\":\"0.3.1\",\"markups\":[],\"atoms\":[],\"cards\":[[\"markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\"*My family has recently moved into a new home, and we are struggling to fine tune the thermostat in this Colorado Fall, where daytime temperatures hit 70&deg;F and drop to 30&deg;F at night. This means temperature sensing applications are fresh in my mind. For today's Circuit Friday then, I present for your reading pleasure, the humble thermistor.*\\n\\n---\\n\\nBlinking an LED is the \\\"Hello World\\\" of physical computing. It shows you how to put all the pieces in place, make sure they are working, and not blow anything up (except maybe an inexpensive LED). You are also working with low enough voltage that there is no concern of physically harming yourself. Woot!\\n\\nFunctionally, blinking an LED is an example of digital input/output (IO). Digital IO means exactly what you might expect - there is either a signal on the line, or there is not. The opposite of a digital signal is an analog signal, in which the amount of signal on the wire can increase and decrease in any variation of steps.\\n\\n> Think of digital IO as a square wave form, with analog IO being more of a sine wave.\\n\\nAnalog signals are very useful in sensing the physical world. You can run a current through some material, and get a varying degree of response based on environmental characteristics. Many packages will in turn wrap this in a digital signal for communication standards such as Inter-Integrated Circuit (I2C).\\n\\nOne such environmental characteristic that we think about almost daily is temperature. Certain materials will change their resistance based on the ambient temperature. While the study of sensing temperature can be a whole discipline unto itself, we can get started at normal room temperatures using a simple resistor called a thermistor.\\n\\n![Inexpensive, durable, temperature sensing for $0.75 at SparkFun (photo source).](http://images.kevinhoyt.com/thermistor.jpg)\\n\\n### Circuit Diagram\\n\\nAs this is an analog circuit, we will be connecting to the analog side of an Arduino Uno. We will run 5V through the thermistor, then a 10K pull-down resistor, and into ground. That really completes the circuit, but to measure the temperature we will pull a line off the junction of the thermistor and the resistor, and into an analog pin (A0 here).\\n\\n![5V into the thermistor, through a pull-down resistor, and back to ground.](http://images.kevinhoyt.com/fritzing.thermistor.png)\\n\\n### Arduino Uno\\n\\nThe readings we are going to get from the thermistor are going to be voltage levels. The Arduino Uno will give us an analog reading range of 0 to 1023, which equates to 0.0049V to 5V respectively. How do we make a voltage into a temperature? The answer is a resounding RTFM! The [data sheet](http://dlnmh9ip6v2uc.cloudfront.net/datasheets/Sensors/Temp/ntcle100.pdf) actually gives us copious numbers of formulas to use based on various situations.\\n\\nOr, you know, you can copy and paste the formula like I did! But seriously, before asking questions about your hardware in an online web forum, please make an effort to read the manual. When it comes to electronics, you can learn a ton by just asserting yourself and tackling the data sheet head on.\\n\\n```\\n// http://arduino.cc/playground/ComponentLib/Thermistor2\\n// Math used for formula temperature calculation\\n#include <math.h>\\n\\n// Thermistor pin\\nconst int THERMISTOR = 0;\\n\\n// Setup serial communication\\nvoid setup() \\n{\\n  Serial.begin( 9600 );\\n}\\n\\n// Infinite loop\\nvoid loop() \\n{\\n  double temperature = 0;\\n  \\n  // Get temperature using custom function\\n  // Send value to the serial port\\n  temperature = thermister( \\n    analogRead( THERMISTOR ) \\n  );\\n  Serial.println( temperature );\\n\\n  // Wait for next sample\\n  delay( 500 );\\n}\\n\\n// Called to calculate temperature\\ndouble thermister( int analog ) \\n{\\n  double temp = 0;\\n \\n  temp = log( ( ( 10240000 / analog ) - 10000 ) );\\n  temp = 1 / ( 0.001129148 + ( 0.000234125 + \\n    ( 0.0000000876741 * temp * temp ) ) * temp );\\n  temp = temp - 273.15;\\n  temp = ( temp * 9.0 ) / 5 + 32.0;\\n \\n  return temp;\\n}\\n```\\n\\nLoad this bad boy into your Arduino, open up the serial monitor, and you will be off to the races with temperature sensing.\\n\\n### Next Steps\\n\\nDuring this post I have tried to delve a little deeper into some of the mechanics behind electronics that we can often take for granted. Analog versus digital. Using a pull-down resistor. Knowing how these things work at a lower level, as with most technology, will help you truly become proficient in electronics.\\n\\nFrom there you can start to explore bigger questions. For example, does the thermistor heat up because of the current running through it, and does that impact the readings? How about the ambient heat from the Arduino processor? This thermistor will give you an accuracy of about +/- 3&deg;F but what happens if you need to measure within a degree? Sub-degree? What about handing extreme temperatures?\\n\\nThe world of electronics is something you never truly master. There are countless questions to explore … and countless data sheets with answers.\\n\"}]],\"sections\":[[10,0]],\"ghostVersion\":\"3.0\"}","html":"<!--kg-card-begin: markdown--><p><em>My family has recently moved into a new home, and we are struggling to fine tune the thermostat in this Colorado Fall, where daytime temperatures hit 70°F and drop to 30°F at night. This means temperature sensing applications are fresh in my mind. For today's Circuit Friday then, I present for your reading pleasure, the humble thermistor.</em></p>\n<hr>\n<p>Blinking an LED is the &quot;Hello World&quot; of physical computing. It shows you how to put all the pieces in place, make sure they are working, and not blow anything up (except maybe an inexpensive LED). You are also working with low enough voltage that there is no concern of physically harming yourself. Woot!</p>\n<p>Functionally, blinking an LED is an example of digital input/output (IO). Digital IO means exactly what you might expect - there is either a signal on the line, or there is not. The opposite of a digital signal is an analog signal, in which the amount of signal on the wire can increase and decrease in any variation of steps.</p>\n<blockquote>\n<p>Think of digital IO as a square wave form, with analog IO being more of a sine wave.</p>\n</blockquote>\n<p>Analog signals are very useful in sensing the physical world. You can run a current through some material, and get a varying degree of response based on environmental characteristics. Many packages will in turn wrap this in a digital signal for communication standards such as Inter-Integrated Circuit (I2C).</p>\n<p>One such environmental characteristic that we think about almost daily is temperature. Certain materials will change their resistance based on the ambient temperature. While the study of sensing temperature can be a whole discipline unto itself, we can get started at normal room temperatures using a simple resistor called a thermistor.</p>\n<p><img src=\"http://images.kevinhoyt.com/thermistor.jpg\" alt=\"Inexpensive, durable, temperature sensing for $0.75 at SparkFun (photo source).\" loading=\"lazy\"></p>\n<h3 id=\"circuitdiagram\">Circuit Diagram</h3>\n<p>As this is an analog circuit, we will be connecting to the analog side of an Arduino Uno. We will run 5V through the thermistor, then a 10K pull-down resistor, and into ground. That really completes the circuit, but to measure the temperature we will pull a line off the junction of the thermistor and the resistor, and into an analog pin (A0 here).</p>\n<p><img src=\"http://images.kevinhoyt.com/fritzing.thermistor.png\" alt=\"5V into the thermistor, through a pull-down resistor, and back to ground.\" loading=\"lazy\"></p>\n<h3 id=\"arduinouno\">Arduino Uno</h3>\n<p>The readings we are going to get from the thermistor are going to be voltage levels. The Arduino Uno will give us an analog reading range of 0 to 1023, which equates to 0.0049V to 5V respectively. How do we make a voltage into a temperature? The answer is a resounding RTFM! The <a href=\"http://dlnmh9ip6v2uc.cloudfront.net/datasheets/Sensors/Temp/ntcle100.pdf\">data sheet</a> actually gives us copious numbers of formulas to use based on various situations.</p>\n<p>Or, you know, you can copy and paste the formula like I did! But seriously, before asking questions about your hardware in an online web forum, please make an effort to read the manual. When it comes to electronics, you can learn a ton by just asserting yourself and tackling the data sheet head on.</p>\n<pre><code>// http://arduino.cc/playground/ComponentLib/Thermistor2\n// Math used for formula temperature calculation\n#include &lt;math.h&gt;\n\n// Thermistor pin\nconst int THERMISTOR = 0;\n\n// Setup serial communication\nvoid setup() \n{\n  Serial.begin( 9600 );\n}\n\n// Infinite loop\nvoid loop() \n{\n  double temperature = 0;\n  \n  // Get temperature using custom function\n  // Send value to the serial port\n  temperature = thermister( \n    analogRead( THERMISTOR ) \n  );\n  Serial.println( temperature );\n\n  // Wait for next sample\n  delay( 500 );\n}\n\n// Called to calculate temperature\ndouble thermister( int analog ) \n{\n  double temp = 0;\n \n  temp = log( ( ( 10240000 / analog ) - 10000 ) );\n  temp = 1 / ( 0.001129148 + ( 0.000234125 + \n    ( 0.0000000876741 * temp * temp ) ) * temp );\n  temp = temp - 273.15;\n  temp = ( temp * 9.0 ) / 5 + 32.0;\n \n  return temp;\n}\n</code></pre>\n<p>Load this bad boy into your Arduino, open up the serial monitor, and you will be off to the races with temperature sensing.</p>\n<h3 id=\"nextsteps\">Next Steps</h3>\n<p>During this post I have tried to delve a little deeper into some of the mechanics behind electronics that we can often take for granted. Analog versus digital. Using a pull-down resistor. Knowing how these things work at a lower level, as with most technology, will help you truly become proficient in electronics.</p>\n<p>From there you can start to explore bigger questions. For example, does the thermistor heat up because of the current running through it, and does that impact the readings? How about the ambient heat from the Arduino processor? This thermistor will give you an accuracy of about +/- 3°F but what happens if you need to measure within a degree? Sub-degree? What about handing extreme temperatures?</p>\n<p>The world of electronics is something you never truly master. There are countless questions to explore … and countless data sheets with answers.</p>\n<!--kg-card-end: markdown-->","comment_id":"7","plaintext":"My family has recently moved into a new home, and we are struggling to fine tune\nthe thermostat in this Colorado Fall, where daytime temperatures hit 70°F and\ndrop to 30°F at night. This means temperature sensing applications are fresh in\nmy mind. For today's Circuit Friday then, I present for your reading pleasure,\nthe humble thermistor.\n\n\n--------------------------------------------------------------------------------\n\nBlinking an LED is the \"Hello World\" of physical computing. It shows you how to\nput all the pieces in place, make sure they are working, and not blow anything\nup (except maybe an inexpensive LED). You are also working with low enough\nvoltage that there is no concern of physically harming yourself. Woot!\n\nFunctionally, blinking an LED is an example of digital input/output (IO).\nDigital IO means exactly what you might expect - there is either a signal on the\nline, or there is not. The opposite of a digital signal is an analog signal, in\nwhich the amount of signal on the wire can increase and decrease in any\nvariation of steps.\n\n> Think of digital IO as a square wave form, with analog IO being more of a sine\nwave.\n\n\nAnalog signals are very useful in sensing the physical world. You can run a\ncurrent through some material, and get a varying degree of response based on\nenvironmental characteristics. Many packages will in turn wrap this in a digital\nsignal for communication standards such as Inter-Integrated Circuit (I2C).\n\nOne such environmental characteristic that we think about almost daily is\ntemperature. Certain materials will change their resistance based on the ambient\ntemperature. While the study of sensing temperature can be a whole discipline\nunto itself, we can get started at normal room temperatures using a simple\nresistor called a thermistor.\n\n\n\nCircuit Diagram\nAs this is an analog circuit, we will be connecting to the analog side of an\nArduino Uno. We will run 5V through the thermistor, then a 10K pull-down\nresistor, and into ground. That really completes the circuit, but to measure the\ntemperature we will pull a line off the junction of the thermistor and the\nresistor, and into an analog pin (A0 here).\n\n\n\nArduino Uno\nThe readings we are going to get from the thermistor are going to be voltage\nlevels. The Arduino Uno will give us an analog reading range of 0 to 1023, which\nequates to 0.0049V to 5V respectively. How do we make a voltage into a\ntemperature? The answer is a resounding RTFM! The data sheet\n[http://dlnmh9ip6v2uc.cloudfront.net/datasheets/Sensors/Temp/ntcle100.pdf] \nactually gives us copious numbers of formulas to use based on various\nsituations.\n\nOr, you know, you can copy and paste the formula like I did! But seriously,\nbefore asking questions about your hardware in an online web forum, please make\nan effort to read the manual. When it comes to electronics, you can learn a ton\nby just asserting yourself and tackling the data sheet head on.\n\n// http://arduino.cc/playground/ComponentLib/Thermistor2\n// Math used for formula temperature calculation\n#include <math.h>\n\n// Thermistor pin\nconst int THERMISTOR = 0;\n\n// Setup serial communication\nvoid setup() \n{\n  Serial.begin( 9600 );\n}\n\n// Infinite loop\nvoid loop() \n{\n  double temperature = 0;\n  \n  // Get temperature using custom function\n  // Send value to the serial port\n  temperature = thermister( \n    analogRead( THERMISTOR ) \n  );\n  Serial.println( temperature );\n\n  // Wait for next sample\n  delay( 500 );\n}\n\n// Called to calculate temperature\ndouble thermister( int analog ) \n{\n  double temp = 0;\n \n  temp = log( ( ( 10240000 / analog ) - 10000 ) );\n  temp = 1 / ( 0.001129148 + ( 0.000234125 + \n    ( 0.0000000876741 * temp * temp ) ) * temp );\n  temp = temp - 273.15;\n  temp = ( temp * 9.0 ) / 5 + 32.0;\n \n  return temp;\n}\n\n\nLoad this bad boy into your Arduino, open up the serial monitor, and you will be\noff to the races with temperature sensing.\n\nNext Steps\nDuring this post I have tried to delve a little deeper into some of the\nmechanics behind electronics that we can often take for granted. Analog versus\ndigital. Using a pull-down resistor. Knowing how these things work at a lower\nlevel, as with most technology, will help you truly become proficient in\nelectronics.\n\nFrom there you can start to explore bigger questions. For example, does the\nthermistor heat up because of the current running through it, and does that\nimpact the readings? How about the ambient heat from the Arduino processor? This\nthermistor will give you an accuracy of about +/- 3°F but what happens if you\nneed to measure within a degree? Sub-degree? What about handing extreme\ntemperatures?\n\nThe world of electronics is something you never truly master. There are\ncountless questions to explore … and countless data sheets with answers.","feature_image":"http://images.kevinhoyt.com/fall.colors.jpg","featured":0,"status":"published","locale":null,"visibility":"public","author_id":"1","created_at":"2015-04-20T20:41:28.000Z","updated_at":"2015-04-21T14:16:35.000Z","published_at":"2014-11-07T22:00:00.000Z","custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"type":"post","email_recipient_filter":"none"},{"id":"5adb8922351ffe0018a57714","uuid":"4140f8d4-0bcb-4a34-829b-ea57cc48d363","title":"Messaging and the New Web","slug":"messaging-and-the-new-web","mobiledoc":"{\"version\":\"0.3.1\",\"markups\":[],\"atoms\":[],\"cards\":[[\"markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\"*The Widget Company now has two successful devices on the market. WidgetX (LED) has a web server built into it, and is expecting REST operations for control. WidgetY (photocell) has a WebSocket server built into it, and can push sensor data to the web in real-time. But does this represent the Intranet of Things, or the Internet of Things?*\\n\\n*Neither device knows about the other. Both are based on architectures that would require them being specifically told about the other in order to communicate. Any third-party devices, or even future devices from the same vendor, are oblivious to the features offered by existing devices.*\\n\\n---\\n\\nWe came to these projects with our request/response hammer, and everything looked like a nail. What we need is an architecture that allows for event-driven communication (turn on a light), but that is decentralized. We need an architecture scalable enough to handle high volumes of device chatter in real-time. We need an architecture that allows those devices to be decoupled. What we need is an architecture called \\\"publish and subscribe.\\\"\\n\\n### Messaging\\n\\nThe \\\"web server\\\" is actually two pieces of functionality. The first, and primary objective is to route HTTP traffic. The second, and actually quite separate objective, is to process the data on that HTTP traffic. Now take these two pieces and decouple them. Replace the HTTP server with middleware capable of handling various number of protocols (not just HTTP), and you are starting to get to the idea of messaging.\\n\\nWhat about handling the data? Where does your application server go? In a message-oriented middleware architecture, your application server is just another loosely decoupled part of a larger system. In fact, you could have several application server technologies (C#, PHP, Java, etc.) in the system - all completely unaware of the internals of the others.\\n\\n![A basic publish-subscribe architecture in action.](http://images.kevinhoyt.com/iot.www.messaging.png)\\n\\nAn event occurs on some subsystem, is routed to a message broker (the HTTP server with more protocols), which in turns broadcasts (publishes) that event to all the subsystems that care about it. Those systems that care about the event (subscribe) take the data and perform some processing on it. Subsystems may or may not in turn generate events of their own.\\n\\n> In a publish/subscribe architecture, It does not matter if the subsystem handing the data is an IoT device or a quantum computer.\\n\\nAlong the way, our HTTP server, now called a message broker, is going to get a lot more functionality. You can, for example, keep connections open, and still grow to web scale. That means you can push data to clients in real-time. The broker can handle queuing data (messages) for systems that are not online at the moment. It can handle disaster recovery for that data should the server crash. And of course, it can handle a lot more than just HTTP.\\n\\n### Publish\\n\\nMany message brokers can speak several different protocols. Protocols you may have heard of in the past include XMPP and AMQP. If you are a Java developer, you might look at JMS as a protocol (though it is not technically). For IoT, where we may not have the memory or CPU to handle complex protocols, there are other options. One such option is the [Simple Text Oriented Messaging Protocol](http://stomp.github.io/) (STOMP).\\n\\n> If you take a look at the STOMP web site, chances are you will find an implementation for your favorite technology. If not, there is a good chance you can write your own implementation in an afternoon.\\n\\nI have written a basic STOMP implementation for the Arduino Yun, which we are going to apply to WidgetY (photocell) to allow it to generate events (publish) around the light levels is senses. Who handles this event, where they are on the system, what technology they are using, and what they intend to do with it are completely irrelevant to WidgetY.\\n\\n```\\n// Libraries\\n#include <Bridge.h>\\n#include <Console.h>\\n#include <YunClient.h>\\n\\n// Literals\\n#define ENDPOINT  \\\"kaazing.kevinhoyt.com\\\"\\n#define LOGIN     \\\" \\\"\\n#define PASSCODE  \\\" \\\"\\n#define PORT      61613\\n#define TOPIC     \\\"/topic/iota.photocell\\\"\\n\\n// Constants\\nconst int PHOTOCELL = 0;\\n\\n// Connectivity\\nString    response;\\nString    session;\\nYunClient client;\\n\\n// Setup\\nvoid setup() \\n{\\n  // Yun network connectivity\\n  Bridge.begin();\\n\\n  // Start client\\n  if( connect() ) {;}\\n}\\n\\n// Loop\\nvoid loop() \\n{\\n  // Process STOMP data\\n  stomp();\\n  \\n  // Get analog pin reading\\n  light = analogRead( PHOTOCELL );\\n\\n  // Publish to clients\\n  publish( TOPIC, String( light ) );  \\n\\n  // Wait to send next\\n  delay( 100 );\\n}\\n```\\n\\nThere are a few things going on here. In the setup routine, you will notice that we connect from the Arduino to the message broker. Then in the loop, we check for any incoming STOMP data (generally, events). We read the light value from the photocell, and the publish the data to the message broker. You can think of a \\\"topic\\\" as the name of this event.\\n\\n### Subscribe\\n\\nWhat happens next? As far as WidgetY is concerned, it does not care. Notice that the message broker is on the public Internet. This means that any system can participate in the conversation - listen for events.\\n\\nIn this case, there are two systems listening for events. The first system is WidgetX (LED). We previously looked at WidgetX as an on or off scenario, but we could alternatively control the brightness of the LED as well. The less light in the room (an event now published by WidgetY), the brighter our LED should be.\\n\\n```\\n// Libraries\\n#include <Bridge.h>\\n#include <Console.h>\\n#include <YunClient.h>\\n\\n// Literals\\n#define ENDPOINT  \\\"kaazing.kevinhoyt.com\\\"\\n#define LOGIN     \\\" \\\"\\n#define PASSCODE  \\\" \\\"\\n#define PORT      61613\\n#define TOPIC     \\\"/topic/iota.photocell\\\"\\n\\n// Constants\\nconst int LED = 9;\\n\\n// Connectivity\\nString    response;\\nString    session;\\nYunClient client;\\n\\n// Setup\\nvoid setup() \\n{\\n  // Use designated pin as an output\\n  // Set to off initially\\n  pinMode( LED, OUTPUT );\\n  analogWrite( LED, 0 );    \\n  \\n  // Yun network connectivity\\n  Bridge.begin();\\n  \\n  // Start client\\n  if( connect() ) \\n  {\\n    subscribe( TOPIC );\\n  }\\n}\\n\\n// Loop\\nvoid loop() \\n{\\n  // Process STOMP data\\n  stomp();\\n  \\n  // Wait for next bits\\n  delay( 100 );\\n}\\n\\n// Callback when messages arrive\\nvoid callback() \\n{\\n  int value;\\n  \\n  // Frame data  \\n  String frame;\\n  String message;\\n  \\n  // Get header\\n  frame = getValue( response, 0, \\\"\\\\n\\\" );\\n  \\n  // Receipt messages\\n  if( frame == \\\"MESSAGE\\\" ) \\n  {\\n    // Message arrived\\n    message = getValue( response, 1, \\\"\\\\n\\\\n\\\" );\\n\\n    // Map photocell to PWM\\n    value = message.toInt();\\n    value = map( value, 0, 1100, 0, 255 );\\n\\n    // Control LED\\n    analogWrite( LED, value );\\n  }\\n}\\n```\\n\\nBecause the LED is interested in knowing about external light events, it subscribes to those events. When messages for that event arrive, the data is parsed, mapped to a Pulse-Width Modulation (PWM) range, and then applied to the LED.\\n\\nA few changes could be made to further decouple the LED from the photocell. For example, rather than listening for a \\\"iota.photocell\\\" event, the LED might just listen for a \\\"light\\\" event. Or it might just listen for an \\\"iota\\\" event, where the event data contains the type of operation to be performed.\\n\\nLikewise, we could publish percentage values from the photocell in place of a concrete range. This would allow for a broader range of applications, without having to additionally send range minimum and maximum values. In this example, the range is hard-coded.\\n\\n### Web Client\\n\\nThe second system listening for WidgetY (photocell) events is the browser, which would like to display and chart the light values coming from the photocell. Real-time display of data in the browser is going to take place using WebSockets. Once we have a WebSocket connection to the message broker, is it up to use to manage the data (protocol) coming across the wire.\\n\\nThere are a growing number of message brokers that support WebSockets. For this example, I am going to use Kaazing Web Gateway to efficiently handle Web communications.\\n\\n```\\n<script src=\\\"http://kaazing.kevinhoyt.com/demo/jms/javascript/WebSocket.js\\\" type=\\\"text/javascript\\\"></script>\\n<script src=\\\"http://kaazing.kevinhoyt.com/demo/jms/javascript/JmsClient.js\\\" type=\\\"text/javascript\\\"></script>\\n<script type=\\\"text/javascript\\\" src=\\\"kaazing.js\\\"></script>\\n\\n...\\n\\nvar kzng = new Kaazing();\\n\\n// New Kaazing connection\\nkzng = new Kaazing();\\n\\n// Connect to Gateway\\nkzng.connect( KAAZING_ID, null, {\\n    success: doConnectSuccess,\\n    error: doConnectError\\n} );\\n\\n// Listen for messages\\nkzng.on( \\\"message\\\", doMessage );\\n\\n...\\n\\n// Problem connecting to broker\\nfunction doConnectError()\\n{\\n    console.log( \\\"Error connecting.\\\" );\\n}\\n\\n// Connected to message broker\\nfunction doConnectSuccess()\\n{\\n    // Listen for photocell values\\n    kzng.subscribe( TOPIC );\\n}\\n\\n// Called when a message has arrived on the socket\\n// Pushes latest value into array for display\\nfunction doMessage( topic, message )\\n{\\n    light.push( parseInt( message ) );\\n\\tlight.splice( 0, 1 );\\n}\\n```\\n\\nThe overall approach here is not considerably different from straight WebSocket. The underlying differences however, are considerable. You think of WebSocket as a means to connect, but messaging is more like adding an event listener to DOM. It just happens that in this case, your events are occurring somewhere else, on a device you know nothing about.\\n\\n### The New Web Unlocked\\n\\nThis decoupling allows WidgetX and WidgetY to communicate with one another, without knowing anything about one another. The light sensor could be from a completely different vendor altogether. The hardware requirements on the devices is much smaller, and the scalability and features for the Web is much greater.\\n\\n> As disparate devices communicate freely with one another, we start to unlock the true potential of the Internet of Things.\\n\\nAt this point we start getting into the semantics that devices should use to communicate - and there is progress on this front ([AllJoyn](https://www.alljoyn.org/) as an example).\\n\\nA little closer home, you might consider if you need to be using HTTP for your data exchange (outside of assets) at all. Rather than reach for XMLHttpRequest (Ajax) next time, consider reaching for the publish-subscribe architecture using WebSocket. Doing so will move the Web forward, and at the same time encourage IoT device vendors to move forward as well.\\n\\nIf you are a device vendor, consider defaulting to publish-subscribe for your API. A real-time configuration makes for a compelling differentiator. Consumers used to instant gratification will gravitate towards your product, hackers will be choose your product over less robust solutions, and the long-term effect will be to drive the Web forward.\\n\\nAsk not what the Internet can do for your Thing, but what your Thing can do for the Internet.\\n\"}]],\"sections\":[[10,0]],\"ghostVersion\":\"3.0\"}","html":"<!--kg-card-begin: markdown--><p><em>The Widget Company now has two successful devices on the market. WidgetX (LED) has a web server built into it, and is expecting REST operations for control. WidgetY (photocell) has a WebSocket server built into it, and can push sensor data to the web in real-time. But does this represent the Intranet of Things, or the Internet of Things?</em></p>\n<p><em>Neither device knows about the other. Both are based on architectures that would require them being specifically told about the other in order to communicate. Any third-party devices, or even future devices from the same vendor, are oblivious to the features offered by existing devices.</em></p>\n<hr>\n<p>We came to these projects with our request/response hammer, and everything looked like a nail. What we need is an architecture that allows for event-driven communication (turn on a light), but that is decentralized. We need an architecture scalable enough to handle high volumes of device chatter in real-time. We need an architecture that allows those devices to be decoupled. What we need is an architecture called &quot;publish and subscribe.&quot;</p>\n<h3 id=\"messaging\">Messaging</h3>\n<p>The &quot;web server&quot; is actually two pieces of functionality. The first, and primary objective is to route HTTP traffic. The second, and actually quite separate objective, is to process the data on that HTTP traffic. Now take these two pieces and decouple them. Replace the HTTP server with middleware capable of handling various number of protocols (not just HTTP), and you are starting to get to the idea of messaging.</p>\n<p>What about handling the data? Where does your application server go? In a message-oriented middleware architecture, your application server is just another loosely decoupled part of a larger system. In fact, you could have several application server technologies (C#, PHP, Java, etc.) in the system - all completely unaware of the internals of the others.</p>\n<p><img src=\"http://images.kevinhoyt.com/iot.www.messaging.png\" alt=\"A basic publish-subscribe architecture in action.\" loading=\"lazy\"></p>\n<p>An event occurs on some subsystem, is routed to a message broker (the HTTP server with more protocols), which in turns broadcasts (publishes) that event to all the subsystems that care about it. Those systems that care about the event (subscribe) take the data and perform some processing on it. Subsystems may or may not in turn generate events of their own.</p>\n<blockquote>\n<p>In a publish/subscribe architecture, It does not matter if the subsystem handing the data is an IoT device or a quantum computer.</p>\n</blockquote>\n<p>Along the way, our HTTP server, now called a message broker, is going to get a lot more functionality. You can, for example, keep connections open, and still grow to web scale. That means you can push data to clients in real-time. The broker can handle queuing data (messages) for systems that are not online at the moment. It can handle disaster recovery for that data should the server crash. And of course, it can handle a lot more than just HTTP.</p>\n<h3 id=\"publish\">Publish</h3>\n<p>Many message brokers can speak several different protocols. Protocols you may have heard of in the past include XMPP and AMQP. If you are a Java developer, you might look at JMS as a protocol (though it is not technically). For IoT, where we may not have the memory or CPU to handle complex protocols, there are other options. One such option is the <a href=\"http://stomp.github.io/\">Simple Text Oriented Messaging Protocol</a> (STOMP).</p>\n<blockquote>\n<p>If you take a look at the STOMP web site, chances are you will find an implementation for your favorite technology. If not, there is a good chance you can write your own implementation in an afternoon.</p>\n</blockquote>\n<p>I have written a basic STOMP implementation for the Arduino Yun, which we are going to apply to WidgetY (photocell) to allow it to generate events (publish) around the light levels is senses. Who handles this event, where they are on the system, what technology they are using, and what they intend to do with it are completely irrelevant to WidgetY.</p>\n<pre><code>// Libraries\n#include &lt;Bridge.h&gt;\n#include &lt;Console.h&gt;\n#include &lt;YunClient.h&gt;\n\n// Literals\n#define ENDPOINT  &quot;kaazing.kevinhoyt.com&quot;\n#define LOGIN     &quot; &quot;\n#define PASSCODE  &quot; &quot;\n#define PORT      61613\n#define TOPIC     &quot;/topic/iota.photocell&quot;\n\n// Constants\nconst int PHOTOCELL = 0;\n\n// Connectivity\nString    response;\nString    session;\nYunClient client;\n\n// Setup\nvoid setup() \n{\n  // Yun network connectivity\n  Bridge.begin();\n\n  // Start client\n  if( connect() ) {;}\n}\n\n// Loop\nvoid loop() \n{\n  // Process STOMP data\n  stomp();\n  \n  // Get analog pin reading\n  light = analogRead( PHOTOCELL );\n\n  // Publish to clients\n  publish( TOPIC, String( light ) );  \n\n  // Wait to send next\n  delay( 100 );\n}\n</code></pre>\n<p>There are a few things going on here. In the setup routine, you will notice that we connect from the Arduino to the message broker. Then in the loop, we check for any incoming STOMP data (generally, events). We read the light value from the photocell, and the publish the data to the message broker. You can think of a &quot;topic&quot; as the name of this event.</p>\n<h3 id=\"subscribe\">Subscribe</h3>\n<p>What happens next? As far as WidgetY is concerned, it does not care. Notice that the message broker is on the public Internet. This means that any system can participate in the conversation - listen for events.</p>\n<p>In this case, there are two systems listening for events. The first system is WidgetX (LED). We previously looked at WidgetX as an on or off scenario, but we could alternatively control the brightness of the LED as well. The less light in the room (an event now published by WidgetY), the brighter our LED should be.</p>\n<pre><code>// Libraries\n#include &lt;Bridge.h&gt;\n#include &lt;Console.h&gt;\n#include &lt;YunClient.h&gt;\n\n// Literals\n#define ENDPOINT  &quot;kaazing.kevinhoyt.com&quot;\n#define LOGIN     &quot; &quot;\n#define PASSCODE  &quot; &quot;\n#define PORT      61613\n#define TOPIC     &quot;/topic/iota.photocell&quot;\n\n// Constants\nconst int LED = 9;\n\n// Connectivity\nString    response;\nString    session;\nYunClient client;\n\n// Setup\nvoid setup() \n{\n  // Use designated pin as an output\n  // Set to off initially\n  pinMode( LED, OUTPUT );\n  analogWrite( LED, 0 );    \n  \n  // Yun network connectivity\n  Bridge.begin();\n  \n  // Start client\n  if( connect() ) \n  {\n    subscribe( TOPIC );\n  }\n}\n\n// Loop\nvoid loop() \n{\n  // Process STOMP data\n  stomp();\n  \n  // Wait for next bits\n  delay( 100 );\n}\n\n// Callback when messages arrive\nvoid callback() \n{\n  int value;\n  \n  // Frame data  \n  String frame;\n  String message;\n  \n  // Get header\n  frame = getValue( response, 0, &quot;\\n&quot; );\n  \n  // Receipt messages\n  if( frame == &quot;MESSAGE&quot; ) \n  {\n    // Message arrived\n    message = getValue( response, 1, &quot;\\n\\n&quot; );\n\n    // Map photocell to PWM\n    value = message.toInt();\n    value = map( value, 0, 1100, 0, 255 );\n\n    // Control LED\n    analogWrite( LED, value );\n  }\n}\n</code></pre>\n<p>Because the LED is interested in knowing about external light events, it subscribes to those events. When messages for that event arrive, the data is parsed, mapped to a Pulse-Width Modulation (PWM) range, and then applied to the LED.</p>\n<p>A few changes could be made to further decouple the LED from the photocell. For example, rather than listening for a &quot;iota.photocell&quot; event, the LED might just listen for a &quot;light&quot; event. Or it might just listen for an &quot;iota&quot; event, where the event data contains the type of operation to be performed.</p>\n<p>Likewise, we could publish percentage values from the photocell in place of a concrete range. This would allow for a broader range of applications, without having to additionally send range minimum and maximum values. In this example, the range is hard-coded.</p>\n<h3 id=\"webclient\">Web Client</h3>\n<p>The second system listening for WidgetY (photocell) events is the browser, which would like to display and chart the light values coming from the photocell. Real-time display of data in the browser is going to take place using WebSockets. Once we have a WebSocket connection to the message broker, is it up to use to manage the data (protocol) coming across the wire.</p>\n<p>There are a growing number of message brokers that support WebSockets. For this example, I am going to use Kaazing Web Gateway to efficiently handle Web communications.</p>\n<pre><code>&lt;script src=&quot;http://kaazing.kevinhoyt.com/demo/jms/javascript/WebSocket.js&quot; type=&quot;text/javascript&quot;&gt;&lt;/script&gt;\n&lt;script src=&quot;http://kaazing.kevinhoyt.com/demo/jms/javascript/JmsClient.js&quot; type=&quot;text/javascript&quot;&gt;&lt;/script&gt;\n&lt;script type=&quot;text/javascript&quot; src=&quot;kaazing.js&quot;&gt;&lt;/script&gt;\n\n...\n\nvar kzng = new Kaazing();\n\n// New Kaazing connection\nkzng = new Kaazing();\n\n// Connect to Gateway\nkzng.connect( KAAZING_ID, null, {\n    success: doConnectSuccess,\n    error: doConnectError\n} );\n\n// Listen for messages\nkzng.on( &quot;message&quot;, doMessage );\n\n...\n\n// Problem connecting to broker\nfunction doConnectError()\n{\n    console.log( &quot;Error connecting.&quot; );\n}\n\n// Connected to message broker\nfunction doConnectSuccess()\n{\n    // Listen for photocell values\n    kzng.subscribe( TOPIC );\n}\n\n// Called when a message has arrived on the socket\n// Pushes latest value into array for display\nfunction doMessage( topic, message )\n{\n    light.push( parseInt( message ) );\n\tlight.splice( 0, 1 );\n}\n</code></pre>\n<p>The overall approach here is not considerably different from straight WebSocket. The underlying differences however, are considerable. You think of WebSocket as a means to connect, but messaging is more like adding an event listener to DOM. It just happens that in this case, your events are occurring somewhere else, on a device you know nothing about.</p>\n<h3 id=\"thenewwebunlocked\">The New Web Unlocked</h3>\n<p>This decoupling allows WidgetX and WidgetY to communicate with one another, without knowing anything about one another. The light sensor could be from a completely different vendor altogether. The hardware requirements on the devices is much smaller, and the scalability and features for the Web is much greater.</p>\n<blockquote>\n<p>As disparate devices communicate freely with one another, we start to unlock the true potential of the Internet of Things.</p>\n</blockquote>\n<p>At this point we start getting into the semantics that devices should use to communicate - and there is progress on this front (<a href=\"https://www.alljoyn.org/\">AllJoyn</a> as an example).</p>\n<p>A little closer home, you might consider if you need to be using HTTP for your data exchange (outside of assets) at all. Rather than reach for XMLHttpRequest (Ajax) next time, consider reaching for the publish-subscribe architecture using WebSocket. Doing so will move the Web forward, and at the same time encourage IoT device vendors to move forward as well.</p>\n<p>If you are a device vendor, consider defaulting to publish-subscribe for your API. A real-time configuration makes for a compelling differentiator. Consumers used to instant gratification will gravitate towards your product, hackers will be choose your product over less robust solutions, and the long-term effect will be to drive the Web forward.</p>\n<p>Ask not what the Internet can do for your Thing, but what your Thing can do for the Internet.</p>\n<!--kg-card-end: markdown-->","comment_id":"8","plaintext":"The Widget Company now has two successful devices on the market. WidgetX (LED)\nhas a web server built into it, and is expecting REST operations for control.\nWidgetY (photocell) has a WebSocket server built into it, and can push sensor\ndata to the web in real-time. But does this represent the Intranet of Things, or\nthe Internet of Things?\n\nNeither device knows about the other. Both are based on architectures that would\nrequire them being specifically told about the other in order to communicate.\nAny third-party devices, or even future devices from the same vendor, are\noblivious to the features offered by existing devices.\n\n\n--------------------------------------------------------------------------------\n\nWe came to these projects with our request/response hammer, and everything\nlooked like a nail. What we need is an architecture that allows for event-driven\ncommunication (turn on a light), but that is decentralized. We need an\narchitecture scalable enough to handle high volumes of device chatter in\nreal-time. We need an architecture that allows those devices to be decoupled.\nWhat we need is an architecture called \"publish and subscribe.\"\n\nMessaging\nThe \"web server\" is actually two pieces of functionality. The first, and primary\nobjective is to route HTTP traffic. The second, and actually quite separate\nobjective, is to process the data on that HTTP traffic. Now take these two\npieces and decouple them. Replace the HTTP server with middleware capable of\nhandling various number of protocols (not just HTTP), and you are starting to\nget to the idea of messaging.\n\nWhat about handling the data? Where does your application server go? In a\nmessage-oriented middleware architecture, your application server is just\nanother loosely decoupled part of a larger system. In fact, you could have\nseveral application server technologies (C#, PHP, Java, etc.) in the system -\nall completely unaware of the internals of the others.\n\n\n\nAn event occurs on some subsystem, is routed to a message broker (the HTTP\nserver with more protocols), which in turns broadcasts (publishes) that event to\nall the subsystems that care about it. Those systems that care about the event\n(subscribe) take the data and perform some processing on it. Subsystems may or\nmay not in turn generate events of their own.\n\n> In a publish/subscribe architecture, It does not matter if the subsystem handing\nthe data is an IoT device or a quantum computer.\n\n\nAlong the way, our HTTP server, now called a message broker, is going to get a\nlot more functionality. You can, for example, keep connections open, and still\ngrow to web scale. That means you can push data to clients in real-time. The\nbroker can handle queuing data (messages) for systems that are not online at the\nmoment. It can handle disaster recovery for that data should the server crash.\nAnd of course, it can handle a lot more than just HTTP.\n\nPublish\nMany message brokers can speak several different protocols. Protocols you may\nhave heard of in the past include XMPP and AMQP. If you are a Java developer,\nyou might look at JMS as a protocol (though it is not technically). For IoT,\nwhere we may not have the memory or CPU to handle complex protocols, there are\nother options. One such option is the Simple Text Oriented Messaging Protocol\n[http://stomp.github.io/] (STOMP).\n\n> If you take a look at the STOMP web site, chances are you will find an\nimplementation for your favorite technology. If not, there is a good chance you\ncan write your own implementation in an afternoon.\n\n\nI have written a basic STOMP implementation for the Arduino Yun, which we are\ngoing to apply to WidgetY (photocell) to allow it to generate events (publish)\naround the light levels is senses. Who handles this event, where they are on the\nsystem, what technology they are using, and what they intend to do with it are\ncompletely irrelevant to WidgetY.\n\n// Libraries\n#include <Bridge.h>\n#include <Console.h>\n#include <YunClient.h>\n\n// Literals\n#define ENDPOINT  \"kaazing.kevinhoyt.com\"\n#define LOGIN     \" \"\n#define PASSCODE  \" \"\n#define PORT      61613\n#define TOPIC     \"/topic/iota.photocell\"\n\n// Constants\nconst int PHOTOCELL = 0;\n\n// Connectivity\nString    response;\nString    session;\nYunClient client;\n\n// Setup\nvoid setup() \n{\n  // Yun network connectivity\n  Bridge.begin();\n\n  // Start client\n  if( connect() ) {;}\n}\n\n// Loop\nvoid loop() \n{\n  // Process STOMP data\n  stomp();\n  \n  // Get analog pin reading\n  light = analogRead( PHOTOCELL );\n\n  // Publish to clients\n  publish( TOPIC, String( light ) );  \n\n  // Wait to send next\n  delay( 100 );\n}\n\n\nThere are a few things going on here. In the setup routine, you will notice that\nwe connect from the Arduino to the message broker. Then in the loop, we check\nfor any incoming STOMP data (generally, events). We read the light value from\nthe photocell, and the publish the data to the message broker. You can think of\na \"topic\" as the name of this event.\n\nSubscribe\nWhat happens next? As far as WidgetY is concerned, it does not care. Notice that\nthe message broker is on the public Internet. This means that any system can\nparticipate in the conversation - listen for events.\n\nIn this case, there are two systems listening for events. The first system is\nWidgetX (LED). We previously looked at WidgetX as an on or off scenario, but we\ncould alternatively control the brightness of the LED as well. The less light in\nthe room (an event now published by WidgetY), the brighter our LED should be.\n\n// Libraries\n#include <Bridge.h>\n#include <Console.h>\n#include <YunClient.h>\n\n// Literals\n#define ENDPOINT  \"kaazing.kevinhoyt.com\"\n#define LOGIN     \" \"\n#define PASSCODE  \" \"\n#define PORT      61613\n#define TOPIC     \"/topic/iota.photocell\"\n\n// Constants\nconst int LED = 9;\n\n// Connectivity\nString    response;\nString    session;\nYunClient client;\n\n// Setup\nvoid setup() \n{\n  // Use designated pin as an output\n  // Set to off initially\n  pinMode( LED, OUTPUT );\n  analogWrite( LED, 0 );    \n  \n  // Yun network connectivity\n  Bridge.begin();\n  \n  // Start client\n  if( connect() ) \n  {\n    subscribe( TOPIC );\n  }\n}\n\n// Loop\nvoid loop() \n{\n  // Process STOMP data\n  stomp();\n  \n  // Wait for next bits\n  delay( 100 );\n}\n\n// Callback when messages arrive\nvoid callback() \n{\n  int value;\n  \n  // Frame data  \n  String frame;\n  String message;\n  \n  // Get header\n  frame = getValue( response, 0, \"\\n\" );\n  \n  // Receipt messages\n  if( frame == \"MESSAGE\" ) \n  {\n    // Message arrived\n    message = getValue( response, 1, \"\\n\\n\" );\n\n    // Map photocell to PWM\n    value = message.toInt();\n    value = map( value, 0, 1100, 0, 255 );\n\n    // Control LED\n    analogWrite( LED, value );\n  }\n}\n\n\nBecause the LED is interested in knowing about external light events, it\nsubscribes to those events. When messages for that event arrive, the data is\nparsed, mapped to a Pulse-Width Modulation (PWM) range, and then applied to the\nLED.\n\nA few changes could be made to further decouple the LED from the photocell. For\nexample, rather than listening for a \"iota.photocell\" event, the LED might just\nlisten for a \"light\" event. Or it might just listen for an \"iota\" event, where\nthe event data contains the type of operation to be performed.\n\nLikewise, we could publish percentage values from the photocell in place of a\nconcrete range. This would allow for a broader range of applications, without\nhaving to additionally send range minimum and maximum values. In this example,\nthe range is hard-coded.\n\nWeb Client\nThe second system listening for WidgetY (photocell) events is the browser, which\nwould like to display and chart the light values coming from the photocell.\nReal-time display of data in the browser is going to take place using\nWebSockets. Once we have a WebSocket connection to the message broker, is it up\nto use to manage the data (protocol) coming across the wire.\n\nThere are a growing number of message brokers that support WebSockets. For this\nexample, I am going to use Kaazing Web Gateway to efficiently handle Web\ncommunications.\n\n<script src=\"http://kaazing.kevinhoyt.com/demo/jms/javascript/WebSocket.js\" type=\"text/javascript\"></script>\n<script src=\"http://kaazing.kevinhoyt.com/demo/jms/javascript/JmsClient.js\" type=\"text/javascript\"></script>\n<script type=\"text/javascript\" src=\"kaazing.js\"></script>\n\n...\n\nvar kzng = new Kaazing();\n\n// New Kaazing connection\nkzng = new Kaazing();\n\n// Connect to Gateway\nkzng.connect( KAAZING_ID, null, {\n    success: doConnectSuccess,\n    error: doConnectError\n} );\n\n// Listen for messages\nkzng.on( \"message\", doMessage );\n\n...\n\n// Problem connecting to broker\nfunction doConnectError()\n{\n    console.log( \"Error connecting.\" );\n}\n\n// Connected to message broker\nfunction doConnectSuccess()\n{\n    // Listen for photocell values\n    kzng.subscribe( TOPIC );\n}\n\n// Called when a message has arrived on the socket\n// Pushes latest value into array for display\nfunction doMessage( topic, message )\n{\n    light.push( parseInt( message ) );\n\tlight.splice( 0, 1 );\n}\n\n\nThe overall approach here is not considerably different from straight WebSocket.\nThe underlying differences however, are considerable. You think of WebSocket as\na means to connect, but messaging is more like adding an event listener to DOM.\nIt just happens that in this case, your events are occurring somewhere else, on\na device you know nothing about.\n\nThe New Web Unlocked\nThis decoupling allows WidgetX and WidgetY to communicate with one another,\nwithout knowing anything about one another. The light sensor could be from a\ncompletely different vendor altogether. The hardware requirements on the devices\nis much smaller, and the scalability and features for the Web is much greater.\n\n> As disparate devices communicate freely with one another, we start to unlock the\ntrue potential of the Internet of Things.\n\n\nAt this point we start getting into the semantics that devices should use to\ncommunicate - and there is progress on this front (AllJoyn\n[https://www.alljoyn.org/] as an example).\n\nA little closer home, you might consider if you need to be using HTTP for your\ndata exchange (outside of assets) at all. Rather than reach for XMLHttpRequest\n(Ajax) next time, consider reaching for the publish-subscribe architecture using\nWebSocket. Doing so will move the Web forward, and at the same time encourage\nIoT device vendors to move forward as well.\n\nIf you are a device vendor, consider defaulting to publish-subscribe for your\nAPI. A real-time configuration makes for a compelling differentiator. Consumers\nused to instant gratification will gravitate towards your product, hackers will\nbe choose your product over less robust solutions, and the long-term effect will\nbe to drive the Web forward.\n\nAsk not what the Internet can do for your Thing, but what your Thing can do for\nthe Internet.","feature_image":"http://images.kevinhoyt.com/pony.express.jpg","featured":0,"status":"published","locale":null,"visibility":"public","author_id":"1","created_at":"2015-04-20T20:48:00.000Z","updated_at":"2015-04-21T14:14:31.000Z","published_at":"2014-11-11T22:00:00.000Z","custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"type":"post","email_recipient_filter":"none"},{"id":"5adb8922351ffe0018a57715","uuid":"97f4923c-659c-471d-9713-93fdc200ff39","title":"Arduino Yun to Parse.com","slug":"arduino-yun-to-parse-com","mobiledoc":"{\"version\":\"0.3.1\",\"markups\":[],\"atoms\":[],\"cards\":[[\"markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\"*There are no shortage of cloud-based solutions, and this is a very good thing. What used to require substantial investment of skill, time and money, can effectively be outsourced for pennies on the gigabyte. As an application developer, this represents the opening of a door to a vast world of opportunity. Opportunity that is yours for the taking.*\\n\\n*For some years now, I have been using [Parse.com](http://parse.com/) for application data storage. The good folks at Parse.com provide many other feature, but it is the beautiful console and thorough documentation of the data storage that got me hooked. Among many API options is a REST offering, which one can use from an Arduino Yun.*\\n\\n---\\n\\nI want to be clear here that when I say \\\"application data storage\\\" I am talking about the types of data we developers usually put into a relational database. In the case of Parse.com, this is more of a NoSQL approach. Without a clear label however, I will just call it data storage for the purposes of this post.\\n\\n### A Word on Security\\n\\nThere is a specific reason this post uses an [Arduino Yun](http://arduino.cc/en/Main/ArduinoBoardYun), and that is security. Not security of the Yun, but the security requirements of using the Parse.com APIs - all traffic uses HTTPS. The cryptography requirements of HTTPS make it difficult to handle for most embedded systems. This would be the case even for any other Arduino.\\n\\nThe Yun however has a Linux System On a Chip (SoC) that has cURL available to it, including support for SSL. From our Arduino code, using the Yun's innovative bridge, we can invoke that cURL process to send data to the Parse.com REST API.\\n\\n> The approach I am about to show you originally comes from an [article](http://hypernephelist.com/2014/08/19/https_on_arduino_yun.html) about posting data from Arduino Yun to Microsoft Azure. The blog has no social media or other contact information, so while I would love to give full attribution, I can only point you to that [article](http://hypernephelist.com/2014/08/19/https_on_arduino_yun.html) for more information.\\n\\n### Parse.com\\n\\nIf you do not already have a Parse.com account, you should create one. Not just to use this tutorial, but to take in all the Parse.com awesomeness. Once you have an account created, you will be asked to create an application. Once you have an application, you can create a data table to hold the data coming from our Yun.\\n\\n![The Core section for one of my applications, showing the Temperature table.](http://images.kevinhoyt.com/parse.temperature.png)\\n\\nAs you can see, I use Parse.com to power several facets of this site - namely the scrolling header on the landing page. That is another story, but for now, I have created a \\\"table\\\" called \\\"Temperature\\\" with two columns - fahrenheit and celcius. Each of those columns expect numerical data. Note that Parse.com tables have some default columns, but we do not need to worry ourselves with them at this stage in the game. Just leave them alone.\\n\\nYour Parse.com application will have several security keys associated with it for you to use from the Parse.com APIs. You can find these by putting your mouse over your avatar in the upper right hand corner of the screen, selecting your application, clicking on the \\\"Settings\\\" tab at the top, and then the \\\"Keys\\\" option along the left side of the screen.\\n\\n### Arduino Yun\\n\\nIn the setup function of our Arduino code, we will setup communication with the Linux side of the Yun by called \\\"Bridge.begin()\\\". In the loop function, we will get the temperature reading, send the value to Parse.com, and then wait before doing it again. Parse.com charges based on frequency of usage, and I have had the Arduino post fast enough to eat up my (reasonably high) free-tier allotment in just minutes. Do not forget the delay.\\n\\n*Note that I am using a thermistor for sensor data in this example, since I covered it in last week's [Circuit Friday](http://kevinhoyt.com/blog/2014/11/07/circuit-friday-thermistor.html). You can read all about the thermistor, how to wire it, and more in that [post](http://kevinhoyt.com/blog/2014/11/07/circuit-friday-thermistor.html).*\\n\\n```\\n// Libraries\\n#include <math.h>\\n#include <Process.h>\\n\\n// Constants\\nconst char *PARSE_API = \\\"api.parse.com\\\";\\nconst char *PARSE_APP = \\\"_YOUR_APP_KEY_\\\";\\nconst char *PARSE_CLASS = \\\"Temperature\\\";\\nconst char *PARSE_KEY = \\\"_YOUR_REST_KEY_\\\";\\nconst char *PARSE_VERSION = \\\"1\\\";\\nconst int  THERMISTOR = 0;\\nconst int  UPDATE_RATE = 5000;\\n\\n// Leverage Yun Linux (curl)\\nProcess process;\\n\\n// Buffer for parameters\\nchar buffer[80];\\n\\n// Setup\\nvoid setup() \\n{\\n  // Bridge communication\\n  Bridge.begin();\\n}\\n\\n// Loop\\nvoid loop() \\n{\\n  double temperature;\\n  int    analog;    \\n\\n  // Get temperature\\n  analog = analogRead( THERMISTOR );\\n  temperature = thermister( analog );  \\n  \\n  // Put value in data store\\n  request( temperature );\\n  wait();\\n  response();    \\n\\n  delay( UPDATE_RATE );\\n}\\n\\n// Send the data to Parse.com\\nvoid request( double value )\\n{\\n  // Buffers for string conversion\\n  // The sprintf function does not like doubles\\n  char celcius[10];\\n  char farenheit[10];\\n  \\n  // Farenheit as character string\\n  dtostrf( value, 3, 2, farenheit );\\n  \\n  // Convert to celcius as character string\\n  dtostrf( ( value - 32 ) / 1.80, 3, 2, celcius );\\n  \\n  // Build curl command line\\n  // Includes HTTPS support\\n  // POST\\n  // JSON\\n  process.begin( \\\"curl\\\" );\\n  process.addParameter( \\\"-k\\\" );\\n  process.addParameter( \\\"-X\\\" );\\n  process.addParameter( \\\"POST\\\" );\\n  process.addParameter( \\\"-H\\\" );\\n  process.addParameter( \\n    \\\"Content-Type:application/json\\\" \\n  );\\n \\n  // Parse.com application\\n  process.addParameter( \\\"-H\\\" );\\n  sprintf( \\n    buffer, \\n    \\\"X-Parse-Application-Id: %s\\\", \\n    PARSE_APP \\n  );\\n  process.addParameter( buffer );\\n  \\n  // Parse.com key\\n  process.addParameter( \\\"-H\\\" );\\n  sprintf( \\n    buffer, \\n    \\\"X-Parse-REST-API-Key: %s\\\", \\n    PARSE_KEY \\n  );\\n  process.addParameter( buffer );  \\n \\n  // JSON body\\n  process.addParameter( \\\"-d\\\" );\\n  sprintf( \\n    buffer, \\n    \\\"{\\\\\\\"farenheit\\\\\\\": %s, \\\\\\\"celcius\\\\\\\": %s}\\\", \\n    farenheit, \\n    celcius \\n  );\\n  process.addParameter( buffer );\\n \\n  // URI\\n  sprintf( \\n    buffer, \\n    \\\"https://%s/%s/classes/%s\\\", \\n    PARSE_API, \\n    PARSE_VERSION, \\n    PARSE_CLASS \\n  );\\n  process.addParameter( buffer );  \\n\\n  // Run the command \\n  // Synchronous\\n  process.run();\\n}\\n\\n// Response from Parse.com\\nvoid response()\\n{\\n  bool print = true;\\n  char c;\\n  \\n  // While there is data to read\\n  while( process.available() ) \\n  {\\n    // Get character\\n    c = process.read();\\n  }\\n}\\n\\n// Calculate temperature\\ndouble thermister( int analog ) \\n{\\n  double temp = 0;\\n \\n  temp = log( ( ( 10240000 / analog ) - 10000 ) );\\n  temp = 1 / ( 0.001129148 + ( 0.000234125 + ( 0.0000000876741 * temp * temp ) ) * temp );\\n  temp = temp - 273.15;\\n  temp = ( temp * 9.0 ) / 5 + 32.0;\\n \\n  return temp;\\n}\\n\\n// Wait for a response from Parse.com\\nvoid wait()\\n{\\n  // Periodically check curl process\\n  while( !process.available() ) \\n  {\\n    delay( 100 );\\n  }\\n}\\n```\\n\\nFrom the loop, there are three functions called to send data to Parse.com. The first is request, which formulates the cURL request and sends it off. The second is to wait for the cURL response to return before moving on to other processing. And finally, the third function handles the response data.\\n\\nThe most interesting of these three is the request function. The Parse.com documentation will give you the code necessary to use their APIs. In this case the cURL code to store a value in the Parse.com system look like the following command.\\n\\n```\\ncurl -X POST \\\\\\n  -H \\\"X-Parse-Application-Id: APPLICATION_ID\\\" \\\\\\n  -H \\\"X-Parse-REST-API-Key: REST_API_KEY\\\" \\\\\\n  -H \\\"Content-Type: application/json\\\" \\\\\\n  -d '{\\\"reading\\\":123}' \\\\\\n  https://api.parse.com/1/classes/Temperature \\n```\\n\\nUsing the Arduino Yun \\\"Process\\\" library, we effectively build out this exact cURL command on the Linux SoC. We use \\\"dtostrf\\\" to convert float values to their character representation. We do this because we leverage \\\"sprintf\\\" to format the cURL parameters, and \\\"sprintf\\\" does not work with float values. Once we feel we have the cURL command built correctly, we call \\\"process.run()\\\" to kick off cURL.\\n\\nIn this case, I do not want to do anything else until I know that the data has been stored in the Parse.com system - or at the very least that the request has completed. That is the nature of the \\\"wait\\\" function, which periodically checks the process for return data.\\n\\nOnce return data has arrived, it is our job to parse it (no pun intended) into something useful. The Parse.com response will be a JSON string. No doubt that this presents its own challenges for an Arduino. The response is short however, and you can parse the JSON should you need some of the response data for a subsequent call (foreign key). In this example, we just accept that data and keep moving.\\n\\n### Next Steps\\n\\nThat is it! The Arduino Yun will now merrily POST sensor data over the Parse.com REST API until it is told otherwise.\\n\\nDespite the ease of issuing a single rest like this, there are some technical challenges to consider for more robust application. One consideration might be for what happens if the network is down. Do you cache failed requests on the Arduino? How long can you do that before running out of space? What if the a client wants the data for the same time the network is down? Perhaps you interpolate (fill in the blanks) the data at some later point (on the client)?\\n\\nIt is these use-cases beyond the basics that should tap you on the shoulder like a child begging for your attention. These are all solvable problems. Do not let them discourage you. And remember that rapid iteration is an acceptable solution. You do not have to solve all these problems in your first project.\\n\"}]],\"sections\":[[10,0]],\"ghostVersion\":\"3.0\"}","html":"<!--kg-card-begin: markdown--><p><em>There are no shortage of cloud-based solutions, and this is a very good thing. What used to require substantial investment of skill, time and money, can effectively be outsourced for pennies on the gigabyte. As an application developer, this represents the opening of a door to a vast world of opportunity. Opportunity that is yours for the taking.</em></p>\n<p><em>For some years now, I have been using <a href=\"http://parse.com/\">Parse.com</a> for application data storage. The good folks at Parse.com provide many other feature, but it is the beautiful console and thorough documentation of the data storage that got me hooked. Among many API options is a REST offering, which one can use from an Arduino Yun.</em></p>\n<hr>\n<p>I want to be clear here that when I say &quot;application data storage&quot; I am talking about the types of data we developers usually put into a relational database. In the case of Parse.com, this is more of a NoSQL approach. Without a clear label however, I will just call it data storage for the purposes of this post.</p>\n<h3 id=\"awordonsecurity\">A Word on Security</h3>\n<p>There is a specific reason this post uses an <a href=\"http://arduino.cc/en/Main/ArduinoBoardYun\">Arduino Yun</a>, and that is security. Not security of the Yun, but the security requirements of using the Parse.com APIs - all traffic uses HTTPS. The cryptography requirements of HTTPS make it difficult to handle for most embedded systems. This would be the case even for any other Arduino.</p>\n<p>The Yun however has a Linux System On a Chip (SoC) that has cURL available to it, including support for SSL. From our Arduino code, using the Yun's innovative bridge, we can invoke that cURL process to send data to the Parse.com REST API.</p>\n<blockquote>\n<p>The approach I am about to show you originally comes from an <a href=\"http://hypernephelist.com/2014/08/19/https_on_arduino_yun.html\">article</a> about posting data from Arduino Yun to Microsoft Azure. The blog has no social media or other contact information, so while I would love to give full attribution, I can only point you to that <a href=\"http://hypernephelist.com/2014/08/19/https_on_arduino_yun.html\">article</a> for more information.</p>\n</blockquote>\n<h3 id=\"parsecom\">Parse.com</h3>\n<p>If you do not already have a Parse.com account, you should create one. Not just to use this tutorial, but to take in all the Parse.com awesomeness. Once you have an account created, you will be asked to create an application. Once you have an application, you can create a data table to hold the data coming from our Yun.</p>\n<p><img src=\"http://images.kevinhoyt.com/parse.temperature.png\" alt=\"The Core section for one of my applications, showing the Temperature table.\" loading=\"lazy\"></p>\n<p>As you can see, I use Parse.com to power several facets of this site - namely the scrolling header on the landing page. That is another story, but for now, I have created a &quot;table&quot; called &quot;Temperature&quot; with two columns - fahrenheit and celcius. Each of those columns expect numerical data. Note that Parse.com tables have some default columns, but we do not need to worry ourselves with them at this stage in the game. Just leave them alone.</p>\n<p>Your Parse.com application will have several security keys associated with it for you to use from the Parse.com APIs. You can find these by putting your mouse over your avatar in the upper right hand corner of the screen, selecting your application, clicking on the &quot;Settings&quot; tab at the top, and then the &quot;Keys&quot; option along the left side of the screen.</p>\n<h3 id=\"arduinoyun\">Arduino Yun</h3>\n<p>In the setup function of our Arduino code, we will setup communication with the Linux side of the Yun by called &quot;Bridge.begin()&quot;. In the loop function, we will get the temperature reading, send the value to Parse.com, and then wait before doing it again. Parse.com charges based on frequency of usage, and I have had the Arduino post fast enough to eat up my (reasonably high) free-tier allotment in just minutes. Do not forget the delay.</p>\n<p><em>Note that I am using a thermistor for sensor data in this example, since I covered it in last week's <a href=\"http://kevinhoyt.com/blog/2014/11/07/circuit-friday-thermistor.html\">Circuit Friday</a>. You can read all about the thermistor, how to wire it, and more in that <a href=\"http://kevinhoyt.com/blog/2014/11/07/circuit-friday-thermistor.html\">post</a>.</em></p>\n<pre><code>// Libraries\n#include &lt;math.h&gt;\n#include &lt;Process.h&gt;\n\n// Constants\nconst char *PARSE_API = &quot;api.parse.com&quot;;\nconst char *PARSE_APP = &quot;_YOUR_APP_KEY_&quot;;\nconst char *PARSE_CLASS = &quot;Temperature&quot;;\nconst char *PARSE_KEY = &quot;_YOUR_REST_KEY_&quot;;\nconst char *PARSE_VERSION = &quot;1&quot;;\nconst int  THERMISTOR = 0;\nconst int  UPDATE_RATE = 5000;\n\n// Leverage Yun Linux (curl)\nProcess process;\n\n// Buffer for parameters\nchar buffer[80];\n\n// Setup\nvoid setup() \n{\n  // Bridge communication\n  Bridge.begin();\n}\n\n// Loop\nvoid loop() \n{\n  double temperature;\n  int    analog;    \n\n  // Get temperature\n  analog = analogRead( THERMISTOR );\n  temperature = thermister( analog );  \n  \n  // Put value in data store\n  request( temperature );\n  wait();\n  response();    \n\n  delay( UPDATE_RATE );\n}\n\n// Send the data to Parse.com\nvoid request( double value )\n{\n  // Buffers for string conversion\n  // The sprintf function does not like doubles\n  char celcius[10];\n  char farenheit[10];\n  \n  // Farenheit as character string\n  dtostrf( value, 3, 2, farenheit );\n  \n  // Convert to celcius as character string\n  dtostrf( ( value - 32 ) / 1.80, 3, 2, celcius );\n  \n  // Build curl command line\n  // Includes HTTPS support\n  // POST\n  // JSON\n  process.begin( &quot;curl&quot; );\n  process.addParameter( &quot;-k&quot; );\n  process.addParameter( &quot;-X&quot; );\n  process.addParameter( &quot;POST&quot; );\n  process.addParameter( &quot;-H&quot; );\n  process.addParameter( \n    &quot;Content-Type:application/json&quot; \n  );\n \n  // Parse.com application\n  process.addParameter( &quot;-H&quot; );\n  sprintf( \n    buffer, \n    &quot;X-Parse-Application-Id: %s&quot;, \n    PARSE_APP \n  );\n  process.addParameter( buffer );\n  \n  // Parse.com key\n  process.addParameter( &quot;-H&quot; );\n  sprintf( \n    buffer, \n    &quot;X-Parse-REST-API-Key: %s&quot;, \n    PARSE_KEY \n  );\n  process.addParameter( buffer );  \n \n  // JSON body\n  process.addParameter( &quot;-d&quot; );\n  sprintf( \n    buffer, \n    &quot;{\\&quot;farenheit\\&quot;: %s, \\&quot;celcius\\&quot;: %s}&quot;, \n    farenheit, \n    celcius \n  );\n  process.addParameter( buffer );\n \n  // URI\n  sprintf( \n    buffer, \n    &quot;https://%s/%s/classes/%s&quot;, \n    PARSE_API, \n    PARSE_VERSION, \n    PARSE_CLASS \n  );\n  process.addParameter( buffer );  \n\n  // Run the command \n  // Synchronous\n  process.run();\n}\n\n// Response from Parse.com\nvoid response()\n{\n  bool print = true;\n  char c;\n  \n  // While there is data to read\n  while( process.available() ) \n  {\n    // Get character\n    c = process.read();\n  }\n}\n\n// Calculate temperature\ndouble thermister( int analog ) \n{\n  double temp = 0;\n \n  temp = log( ( ( 10240000 / analog ) - 10000 ) );\n  temp = 1 / ( 0.001129148 + ( 0.000234125 + ( 0.0000000876741 * temp * temp ) ) * temp );\n  temp = temp - 273.15;\n  temp = ( temp * 9.0 ) / 5 + 32.0;\n \n  return temp;\n}\n\n// Wait for a response from Parse.com\nvoid wait()\n{\n  // Periodically check curl process\n  while( !process.available() ) \n  {\n    delay( 100 );\n  }\n}\n</code></pre>\n<p>From the loop, there are three functions called to send data to Parse.com. The first is request, which formulates the cURL request and sends it off. The second is to wait for the cURL response to return before moving on to other processing. And finally, the third function handles the response data.</p>\n<p>The most interesting of these three is the request function. The Parse.com documentation will give you the code necessary to use their APIs. In this case the cURL code to store a value in the Parse.com system look like the following command.</p>\n<pre><code>curl -X POST \\\n  -H &quot;X-Parse-Application-Id: APPLICATION_ID&quot; \\\n  -H &quot;X-Parse-REST-API-Key: REST_API_KEY&quot; \\\n  -H &quot;Content-Type: application/json&quot; \\\n  -d '{&quot;reading&quot;:123}' \\\n  https://api.parse.com/1/classes/Temperature \n</code></pre>\n<p>Using the Arduino Yun &quot;Process&quot; library, we effectively build out this exact cURL command on the Linux SoC. We use &quot;dtostrf&quot; to convert float values to their character representation. We do this because we leverage &quot;sprintf&quot; to format the cURL parameters, and &quot;sprintf&quot; does not work with float values. Once we feel we have the cURL command built correctly, we call &quot;process.run()&quot; to kick off cURL.</p>\n<p>In this case, I do not want to do anything else until I know that the data has been stored in the Parse.com system - or at the very least that the request has completed. That is the nature of the &quot;wait&quot; function, which periodically checks the process for return data.</p>\n<p>Once return data has arrived, it is our job to parse it (no pun intended) into something useful. The Parse.com response will be a JSON string. No doubt that this presents its own challenges for an Arduino. The response is short however, and you can parse the JSON should you need some of the response data for a subsequent call (foreign key). In this example, we just accept that data and keep moving.</p>\n<h3 id=\"nextsteps\">Next Steps</h3>\n<p>That is it! The Arduino Yun will now merrily POST sensor data over the Parse.com REST API until it is told otherwise.</p>\n<p>Despite the ease of issuing a single rest like this, there are some technical challenges to consider for more robust application. One consideration might be for what happens if the network is down. Do you cache failed requests on the Arduino? How long can you do that before running out of space? What if the a client wants the data for the same time the network is down? Perhaps you interpolate (fill in the blanks) the data at some later point (on the client)?</p>\n<p>It is these use-cases beyond the basics that should tap you on the shoulder like a child begging for your attention. These are all solvable problems. Do not let them discourage you. And remember that rapid iteration is an acceptable solution. You do not have to solve all these problems in your first project.</p>\n<!--kg-card-end: markdown-->","comment_id":"9","plaintext":"There are no shortage of cloud-based solutions, and this is a very good thing.\nWhat used to require substantial investment of skill, time and money, can\neffectively be outsourced for pennies on the gigabyte. As an application\ndeveloper, this represents the opening of a door to a vast world of opportunity.\nOpportunity that is yours for the taking.\n\nFor some years now, I have been using Parse.com [http://parse.com/] for\napplication data storage. The good folks at Parse.com provide many other\nfeature, but it is the beautiful console and thorough documentation of the data\nstorage that got me hooked. Among many API options is a REST offering, which one\ncan use from an Arduino Yun.\n\n\n--------------------------------------------------------------------------------\n\nI want to be clear here that when I say \"application data storage\" I am talking\nabout the types of data we developers usually put into a relational database. In\nthe case of Parse.com, this is more of a NoSQL approach. Without a clear label\nhowever, I will just call it data storage for the purposes of this post.\n\nA Word on Security\nThere is a specific reason this post uses an Arduino Yun\n[http://arduino.cc/en/Main/ArduinoBoardYun], and that is security. Not security\nof the Yun, but the security requirements of using the Parse.com APIs - all\ntraffic uses HTTPS. The cryptography requirements of HTTPS make it difficult to\nhandle for most embedded systems. This would be the case even for any other\nArduino.\n\nThe Yun however has a Linux System On a Chip (SoC) that has cURL available to\nit, including support for SSL. From our Arduino code, using the Yun's innovative\nbridge, we can invoke that cURL process to send data to the Parse.com REST API.\n\n> The approach I am about to show you originally comes from an article\n[http://hypernephelist.com/2014/08/19/https_on_arduino_yun.html] about posting\ndata from Arduino Yun to Microsoft Azure. The blog has no social media or other\ncontact information, so while I would love to give full attribution, I can only\npoint you to that article\n[http://hypernephelist.com/2014/08/19/https_on_arduino_yun.html] for more\ninformation.\n\n\nParse.com\nIf you do not already have a Parse.com account, you should create one. Not just\nto use this tutorial, but to take in all the Parse.com awesomeness. Once you\nhave an account created, you will be asked to create an application. Once you\nhave an application, you can create a data table to hold the data coming from\nour Yun.\n\n\n\nAs you can see, I use Parse.com to power several facets of this site - namely\nthe scrolling header on the landing page. That is another story, but for now, I\nhave created a \"table\" called \"Temperature\" with two columns - fahrenheit and\ncelcius. Each of those columns expect numerical data. Note that Parse.com tables\nhave some default columns, but we do not need to worry ourselves with them at\nthis stage in the game. Just leave them alone.\n\nYour Parse.com application will have several security keys associated with it\nfor you to use from the Parse.com APIs. You can find these by putting your mouse\nover your avatar in the upper right hand corner of the screen, selecting your\napplication, clicking on the \"Settings\" tab at the top, and then the \"Keys\"\noption along the left side of the screen.\n\nArduino Yun\nIn the setup function of our Arduino code, we will setup communication with the\nLinux side of the Yun by called \"Bridge.begin()\". In the loop function, we will\nget the temperature reading, send the value to Parse.com, and then wait before\ndoing it again. Parse.com charges based on frequency of usage, and I have had\nthe Arduino post fast enough to eat up my (reasonably high) free-tier allotment\nin just minutes. Do not forget the delay.\n\nNote that I am using a thermistor for sensor data in this example, since I\ncovered it in last week's Circuit Friday\n[http://kevinhoyt.com/blog/2014/11/07/circuit-friday-thermistor.html]. You can\nread all about the thermistor, how to wire it, and more in that post\n[http://kevinhoyt.com/blog/2014/11/07/circuit-friday-thermistor.html].\n\n// Libraries\n#include <math.h>\n#include <Process.h>\n\n// Constants\nconst char *PARSE_API = \"api.parse.com\";\nconst char *PARSE_APP = \"_YOUR_APP_KEY_\";\nconst char *PARSE_CLASS = \"Temperature\";\nconst char *PARSE_KEY = \"_YOUR_REST_KEY_\";\nconst char *PARSE_VERSION = \"1\";\nconst int  THERMISTOR = 0;\nconst int  UPDATE_RATE = 5000;\n\n// Leverage Yun Linux (curl)\nProcess process;\n\n// Buffer for parameters\nchar buffer[80];\n\n// Setup\nvoid setup() \n{\n  // Bridge communication\n  Bridge.begin();\n}\n\n// Loop\nvoid loop() \n{\n  double temperature;\n  int    analog;    \n\n  // Get temperature\n  analog = analogRead( THERMISTOR );\n  temperature = thermister( analog );  \n  \n  // Put value in data store\n  request( temperature );\n  wait();\n  response();    \n\n  delay( UPDATE_RATE );\n}\n\n// Send the data to Parse.com\nvoid request( double value )\n{\n  // Buffers for string conversion\n  // The sprintf function does not like doubles\n  char celcius[10];\n  char farenheit[10];\n  \n  // Farenheit as character string\n  dtostrf( value, 3, 2, farenheit );\n  \n  // Convert to celcius as character string\n  dtostrf( ( value - 32 ) / 1.80, 3, 2, celcius );\n  \n  // Build curl command line\n  // Includes HTTPS support\n  // POST\n  // JSON\n  process.begin( \"curl\" );\n  process.addParameter( \"-k\" );\n  process.addParameter( \"-X\" );\n  process.addParameter( \"POST\" );\n  process.addParameter( \"-H\" );\n  process.addParameter( \n    \"Content-Type:application/json\" \n  );\n \n  // Parse.com application\n  process.addParameter( \"-H\" );\n  sprintf( \n    buffer, \n    \"X-Parse-Application-Id: %s\", \n    PARSE_APP \n  );\n  process.addParameter( buffer );\n  \n  // Parse.com key\n  process.addParameter( \"-H\" );\n  sprintf( \n    buffer, \n    \"X-Parse-REST-API-Key: %s\", \n    PARSE_KEY \n  );\n  process.addParameter( buffer );  \n \n  // JSON body\n  process.addParameter( \"-d\" );\n  sprintf( \n    buffer, \n    \"{\\\"farenheit\\\": %s, \\\"celcius\\\": %s}\", \n    farenheit, \n    celcius \n  );\n  process.addParameter( buffer );\n \n  // URI\n  sprintf( \n    buffer, \n    \"https://%s/%s/classes/%s\", \n    PARSE_API, \n    PARSE_VERSION, \n    PARSE_CLASS \n  );\n  process.addParameter( buffer );  \n\n  // Run the command \n  // Synchronous\n  process.run();\n}\n\n// Response from Parse.com\nvoid response()\n{\n  bool print = true;\n  char c;\n  \n  // While there is data to read\n  while( process.available() ) \n  {\n    // Get character\n    c = process.read();\n  }\n}\n\n// Calculate temperature\ndouble thermister( int analog ) \n{\n  double temp = 0;\n \n  temp = log( ( ( 10240000 / analog ) - 10000 ) );\n  temp = 1 / ( 0.001129148 + ( 0.000234125 + ( 0.0000000876741 * temp * temp ) ) * temp );\n  temp = temp - 273.15;\n  temp = ( temp * 9.0 ) / 5 + 32.0;\n \n  return temp;\n}\n\n// Wait for a response from Parse.com\nvoid wait()\n{\n  // Periodically check curl process\n  while( !process.available() ) \n  {\n    delay( 100 );\n  }\n}\n\n\nFrom the loop, there are three functions called to send data to Parse.com. The\nfirst is request, which formulates the cURL request and sends it off. The second\nis to wait for the cURL response to return before moving on to other processing.\nAnd finally, the third function handles the response data.\n\nThe most interesting of these three is the request function. The Parse.com\ndocumentation will give you the code necessary to use their APIs. In this case\nthe cURL code to store a value in the Parse.com system look like the following\ncommand.\n\ncurl -X POST \\\n  -H \"X-Parse-Application-Id: APPLICATION_ID\" \\\n  -H \"X-Parse-REST-API-Key: REST_API_KEY\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"reading\":123}' \\\n  https://api.parse.com/1/classes/Temperature \n\n\nUsing the Arduino Yun \"Process\" library, we effectively build out this exact\ncURL command on the Linux SoC. We use \"dtostrf\" to convert float values to their\ncharacter representation. We do this because we leverage \"sprintf\" to format the\ncURL parameters, and \"sprintf\" does not work with float values. Once we feel we\nhave the cURL command built correctly, we call \"process.run()\" to kick off cURL.\n\nIn this case, I do not want to do anything else until I know that the data has\nbeen stored in the Parse.com system - or at the very least that the request has\ncompleted. That is the nature of the \"wait\" function, which periodically checks\nthe process for return data.\n\nOnce return data has arrived, it is our job to parse it (no pun intended) into\nsomething useful. The Parse.com response will be a JSON string. No doubt that\nthis presents its own challenges for an Arduino. The response is short however,\nand you can parse the JSON should you need some of the response data for a\nsubsequent call (foreign key). In this example, we just accept that data and\nkeep moving.\n\nNext Steps\nThat is it! The Arduino Yun will now merrily POST sensor data over the Parse.com\nREST API until it is told otherwise.\n\nDespite the ease of issuing a single rest like this, there are some technical\nchallenges to consider for more robust application. One consideration might be\nfor what happens if the network is down. Do you cache failed requests on the\nArduino? How long can you do that before running out of space? What if the a\nclient wants the data for the same time the network is down? Perhaps you\ninterpolate (fill in the blanks) the data at some later point (on the client)?\n\nIt is these use-cases beyond the basics that should tap you on the shoulder like\na child begging for your attention. These are all solvable problems. Do not let\nthem discourage you. And remember that rapid iteration is an acceptable\nsolution. You do not have to solve all these problems in your first project.","feature_image":"http://images.kevinhoyt.com/the.cloud.jpg","featured":0,"status":"published","locale":null,"visibility":"public","author_id":"1","created_at":"2015-04-20T20:55:07.000Z","updated_at":"2015-04-21T14:11:23.000Z","published_at":"2014-11-13T16:11:00.000Z","custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"type":"post","email_recipient_filter":"none"},{"id":"5adb8922351ffe0018a57716","uuid":"9ba69a88-5e66-4140-af56-cec86001c130","title":"XBee Mesh Networking","slug":"xbee-mesh-networking","mobiledoc":"{\"version\":\"0.3.1\",\"markups\":[],\"atoms\":[],\"cards\":[[\"markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\"*As you start working with the Internet of Things, one of the patterns that will emerge pretty early is the concept of serial communication. Initially this takes the form of using something like the Serial Monitor with an Arduino. While this communication happens over a wire, a more practical for IoT would be wireless. This is exactly what [XBee](http://en.wikipedia.org/wiki/XBee) modules provide.*\\n\\n---\\n\\n### XBee Overview\\n\\nXBee is a commercial kin of the [802.15 protocol](http://en.wikipedia.org/wiki/IEEE_802.15.4) - not to be confused with 802.11, which is the ubiquitous wi-fi we know and love today. The 802.15 protocol was designed to be less power hungry than 802.11, predominantly by sacrificing speed. It does however gain an amazing potential in distance between points.\\n\\nXBee comes in a wide variety of [flavors](https://www.sparkfun.com/pages/xbee_guide?_ga=1.233870046.199580068.1415136666). My favorite is the Series 1, which gives me a simple, dumb pipe between my remote sensors, and my master device. It requires no additional configuration out of the box. Strap an XBee modules on your Arduino, and you are wireless.\\n\\nI have also worked with the Series 2 modules. These can be extensively configured - and almost seem to require it. Why would you want to configure an serial pipe? An example might be running two different meshes. Naming the modules network would keep them separate. Be warned however that the configuration does not come without a substantial degree of complexity.\\n\\nXBee modules also come in a variety of distances they can span. A 1mW module will get you 300 feet, while a 100mW module will get you 15 miles. Earlier this year, I strapped a 60mW module (1 mile range) to a [DJI Phantom](http://www.dji.com/product/phantom-2) to get [real-time data telemetry](http://vimeo.com/93023138) onto the Web. Try that with traditional wireless (802.11)!\\n\\n### XBee Advantage\\n\\nMesh networks are interesting beasts. You see, not everything IoT needs to be on the Internet. In fact, a 802.11 endpoint may be far out of range. That is perfectly acceptable in a mesh environment. Mesh networks do not need to go directly to a single point.\\n\\nData can be passed from one module to another so long as it is in range. If the destination module is out of range of the initiating module, that too is acceptable. The message can be transmitted from unit to unit until the destination is within reach.\\n\\nImagine then trying to cover the vast oil fields in Texas with IoT sensors. Clearly there is important data to be had, but the Internet is nowhere to be found. Yet with XBee, these pumps can create a mesh network hundreds of miles in size. Only one module need have access to the Internet.\\n\\nThe communication can go both ways as well. A control center somewhere on the Internet, can in turn, tell a pump in the middle of nowhere to stop for maintenance.\\n\\nXBee is not the only means of this type of communication. I used to have an [Oregon Scientific](http://www.oregonscientific.com/us/en/Silver-Advanced-Weather-Station-with-Atomic-Time-Weather-500-BAR208S-P) weather station with various sensors placed around the house. Those devices used a proprietary 433MHz frequency signal to communicate. I have since replaced the gear with my own custom weather station ... Because I can!\\n\\n### Arduino Walkthrough\\n\\nSo now you have gone out and purchased two [XBee Series 1](https://www.sparkfun.com/products/11216) modules, an [XBee Shield](https://www.sparkfun.com/products/12847) for your Arduino, and an [XBee Explorer](https://www.sparkfun.com/products/11697). How do you get all this working?\\n\\nWell, let us start with the Arduino. Let us give it a little code to just increment a counter. Then we will tell it to send that value over the serial port. If you have worked with the Arduino for a while, this should not look like anything new.\\n\\n```\\n// Counter\\nint counter = 0;\\n\\n// Setup\\nvoid setup()\\n{\\n    Serial.begin( 9600 );\\n}\\n\\n// Loop\\nvoid loop()\\n{\\n    // Increment the counter\\n    counter = counter + 1;\\n    \\n    // Display the current value\\n    Serial.println( counter );\\n    \\n    // Wait a second and repeat\\n    delay( 1000 );\\n}\\n```\\n\\nNotice that there is no XBee library to include. When the XBee is strapped onto the shield, and placed on the Arduino, it will effectively take the Serial output, and send it across to the other XBee module(s).\\n\\n> You cannot program the Arduino with the XBee attached.\\n\\nYou cannot program the Arduino with the XBee attached. The IDE will use that serial access to load your program. If the XBee is sitting there, the Arduino will never be reached. Functionally, this is not a big deal for development, because you can debug against the serial port the same without the XBee as with it.\\n\\nOkay, so now put one XBee module on the shield, and then onto your Arduino. Detach the Arduino from the computer, and give it a power source all its own. This guy is now merrily sitting out there, counting numbers, sending them across the air.\\n\\nPut the other XBee module on the Explorer, and plug it into your computer. Using the Arduino IDE, set the port to the XBee Explorer. Since we are not loading code, we do not need to worry about the board type. Open the Serial Monitor from the IDE, and you should start seeing numbers come across from the Arduino.\\n\\n### Next Steps\\n\\nNow that you can talk wirelessly between a remote Arduino and your serial port, there is no shortage of new adventures. You can sense environmental conditions throughout your entire neighborhood, and collect them on a Raspberry Pi dashboard. You can remotely control servos and motors, and more.\"}]],\"sections\":[[10,0]],\"ghostVersion\":\"3.0\"}","html":"<!--kg-card-begin: markdown--><p><em>As you start working with the Internet of Things, one of the patterns that will emerge pretty early is the concept of serial communication. Initially this takes the form of using something like the Serial Monitor with an Arduino. While this communication happens over a wire, a more practical for IoT would be wireless. This is exactly what <a href=\"http://en.wikipedia.org/wiki/XBee\">XBee</a> modules provide.</em></p>\n<hr>\n<h3 id=\"xbeeoverview\">XBee Overview</h3>\n<p>XBee is a commercial kin of the <a href=\"http://en.wikipedia.org/wiki/IEEE_802.15.4\">802.15 protocol</a> - not to be confused with 802.11, which is the ubiquitous wi-fi we know and love today. The 802.15 protocol was designed to be less power hungry than 802.11, predominantly by sacrificing speed. It does however gain an amazing potential in distance between points.</p>\n<p>XBee comes in a wide variety of <a href=\"https://www.sparkfun.com/pages/xbee_guide?_ga=1.233870046.199580068.1415136666\">flavors</a>. My favorite is the Series 1, which gives me a simple, dumb pipe between my remote sensors, and my master device. It requires no additional configuration out of the box. Strap an XBee modules on your Arduino, and you are wireless.</p>\n<p>I have also worked with the Series 2 modules. These can be extensively configured - and almost seem to require it. Why would you want to configure an serial pipe? An example might be running two different meshes. Naming the modules network would keep them separate. Be warned however that the configuration does not come without a substantial degree of complexity.</p>\n<p>XBee modules also come in a variety of distances they can span. A 1mW module will get you 300 feet, while a 100mW module will get you 15 miles. Earlier this year, I strapped a 60mW module (1 mile range) to a <a href=\"http://www.dji.com/product/phantom-2\">DJI Phantom</a> to get <a href=\"http://vimeo.com/93023138\">real-time data telemetry</a> onto the Web. Try that with traditional wireless (802.11)!</p>\n<h3 id=\"xbeeadvantage\">XBee Advantage</h3>\n<p>Mesh networks are interesting beasts. You see, not everything IoT needs to be on the Internet. In fact, a 802.11 endpoint may be far out of range. That is perfectly acceptable in a mesh environment. Mesh networks do not need to go directly to a single point.</p>\n<p>Data can be passed from one module to another so long as it is in range. If the destination module is out of range of the initiating module, that too is acceptable. The message can be transmitted from unit to unit until the destination is within reach.</p>\n<p>Imagine then trying to cover the vast oil fields in Texas with IoT sensors. Clearly there is important data to be had, but the Internet is nowhere to be found. Yet with XBee, these pumps can create a mesh network hundreds of miles in size. Only one module need have access to the Internet.</p>\n<p>The communication can go both ways as well. A control center somewhere on the Internet, can in turn, tell a pump in the middle of nowhere to stop for maintenance.</p>\n<p>XBee is not the only means of this type of communication. I used to have an <a href=\"http://www.oregonscientific.com/us/en/Silver-Advanced-Weather-Station-with-Atomic-Time-Weather-500-BAR208S-P\">Oregon Scientific</a> weather station with various sensors placed around the house. Those devices used a proprietary 433MHz frequency signal to communicate. I have since replaced the gear with my own custom weather station ... Because I can!</p>\n<h3 id=\"arduinowalkthrough\">Arduino Walkthrough</h3>\n<p>So now you have gone out and purchased two <a href=\"https://www.sparkfun.com/products/11216\">XBee Series 1</a> modules, an <a href=\"https://www.sparkfun.com/products/12847\">XBee Shield</a> for your Arduino, and an <a href=\"https://www.sparkfun.com/products/11697\">XBee Explorer</a>. How do you get all this working?</p>\n<p>Well, let us start with the Arduino. Let us give it a little code to just increment a counter. Then we will tell it to send that value over the serial port. If you have worked with the Arduino for a while, this should not look like anything new.</p>\n<pre><code>// Counter\nint counter = 0;\n\n// Setup\nvoid setup()\n{\n    Serial.begin( 9600 );\n}\n\n// Loop\nvoid loop()\n{\n    // Increment the counter\n    counter = counter + 1;\n    \n    // Display the current value\n    Serial.println( counter );\n    \n    // Wait a second and repeat\n    delay( 1000 );\n}\n</code></pre>\n<p>Notice that there is no XBee library to include. When the XBee is strapped onto the shield, and placed on the Arduino, it will effectively take the Serial output, and send it across to the other XBee module(s).</p>\n<blockquote>\n<p>You cannot program the Arduino with the XBee attached.</p>\n</blockquote>\n<p>You cannot program the Arduino with the XBee attached. The IDE will use that serial access to load your program. If the XBee is sitting there, the Arduino will never be reached. Functionally, this is not a big deal for development, because you can debug against the serial port the same without the XBee as with it.</p>\n<p>Okay, so now put one XBee module on the shield, and then onto your Arduino. Detach the Arduino from the computer, and give it a power source all its own. This guy is now merrily sitting out there, counting numbers, sending them across the air.</p>\n<p>Put the other XBee module on the Explorer, and plug it into your computer. Using the Arduino IDE, set the port to the XBee Explorer. Since we are not loading code, we do not need to worry about the board type. Open the Serial Monitor from the IDE, and you should start seeing numbers come across from the Arduino.</p>\n<h3 id=\"nextsteps\">Next Steps</h3>\n<p>Now that you can talk wirelessly between a remote Arduino and your serial port, there is no shortage of new adventures. You can sense environmental conditions throughout your entire neighborhood, and collect them on a Raspberry Pi dashboard. You can remotely control servos and motors, and more.</p>\n<!--kg-card-end: markdown-->","comment_id":"10","plaintext":"As you start working with the Internet of Things, one of the patterns that will\nemerge pretty early is the concept of serial communication. Initially this takes\nthe form of using something like the Serial Monitor with an Arduino. While this\ncommunication happens over a wire, a more practical for IoT would be wireless.\nThis is exactly what XBee [http://en.wikipedia.org/wiki/XBee] modules provide.\n\n\n--------------------------------------------------------------------------------\n\nXBee Overview\nXBee is a commercial kin of the 802.15 protocol\n[http://en.wikipedia.org/wiki/IEEE_802.15.4] - not to be confused with 802.11,\nwhich is the ubiquitous wi-fi we know and love today. The 802.15 protocol was\ndesigned to be less power hungry than 802.11, predominantly by sacrificing\nspeed. It does however gain an amazing potential in distance between points.\n\nXBee comes in a wide variety of flavors\n[https://www.sparkfun.com/pages/xbee_guide?_ga=1.233870046.199580068.1415136666]\n. My favorite is the Series 1, which gives me a simple, dumb pipe between my\nremote sensors, and my master device. It requires no additional configuration\nout of the box. Strap an XBee modules on your Arduino, and you are wireless.\n\nI have also worked with the Series 2 modules. These can be extensively\nconfigured - and almost seem to require it. Why would you want to configure an\nserial pipe? An example might be running two different meshes. Naming the\nmodules network would keep them separate. Be warned however that the\nconfiguration does not come without a substantial degree of complexity.\n\nXBee modules also come in a variety of distances they can span. A 1mW module\nwill get you 300 feet, while a 100mW module will get you 15 miles. Earlier this\nyear, I strapped a 60mW module (1 mile range) to a DJI Phantom\n[http://www.dji.com/product/phantom-2] to get real-time data telemetry\n[http://vimeo.com/93023138] onto the Web. Try that with traditional wireless\n(802.11)!\n\nXBee Advantage\nMesh networks are interesting beasts. You see, not everything IoT needs to be on\nthe Internet. In fact, a 802.11 endpoint may be far out of range. That is\nperfectly acceptable in a mesh environment. Mesh networks do not need to go\ndirectly to a single point.\n\nData can be passed from one module to another so long as it is in range. If the\ndestination module is out of range of the initiating module, that too is\nacceptable. The message can be transmitted from unit to unit until the\ndestination is within reach.\n\nImagine then trying to cover the vast oil fields in Texas with IoT sensors.\nClearly there is important data to be had, but the Internet is nowhere to be\nfound. Yet with XBee, these pumps can create a mesh network hundreds of miles in\nsize. Only one module need have access to the Internet.\n\nThe communication can go both ways as well. A control center somewhere on the\nInternet, can in turn, tell a pump in the middle of nowhere to stop for\nmaintenance.\n\nXBee is not the only means of this type of communication. I used to have an \nOregon Scientific\n[http://www.oregonscientific.com/us/en/Silver-Advanced-Weather-Station-with-Atomic-Time-Weather-500-BAR208S-P] \nweather station with various sensors placed around the house. Those devices used\na proprietary 433MHz frequency signal to communicate. I have since replaced the\ngear with my own custom weather station ... Because I can!\n\nArduino Walkthrough\nSo now you have gone out and purchased two XBee Series 1\n[https://www.sparkfun.com/products/11216] modules, an XBee Shield\n[https://www.sparkfun.com/products/12847] for your Arduino, and an XBee Explorer\n[https://www.sparkfun.com/products/11697]. How do you get all this working?\n\nWell, let us start with the Arduino. Let us give it a little code to just\nincrement a counter. Then we will tell it to send that value over the serial\nport. If you have worked with the Arduino for a while, this should not look like\nanything new.\n\n// Counter\nint counter = 0;\n\n// Setup\nvoid setup()\n{\n    Serial.begin( 9600 );\n}\n\n// Loop\nvoid loop()\n{\n    // Increment the counter\n    counter = counter + 1;\n    \n    // Display the current value\n    Serial.println( counter );\n    \n    // Wait a second and repeat\n    delay( 1000 );\n}\n\n\nNotice that there is no XBee library to include. When the XBee is strapped onto\nthe shield, and placed on the Arduino, it will effectively take the Serial\noutput, and send it across to the other XBee module(s).\n\n> You cannot program the Arduino with the XBee attached.\n\n\nYou cannot program the Arduino with the XBee attached. The IDE will use that\nserial access to load your program. If the XBee is sitting there, the Arduino\nwill never be reached. Functionally, this is not a big deal for development,\nbecause you can debug against the serial port the same without the XBee as with\nit.\n\nOkay, so now put one XBee module on the shield, and then onto your Arduino.\nDetach the Arduino from the computer, and give it a power source all its own.\nThis guy is now merrily sitting out there, counting numbers, sending them across\nthe air.\n\nPut the other XBee module on the Explorer, and plug it into your computer. Using\nthe Arduino IDE, set the port to the XBee Explorer. Since we are not loading\ncode, we do not need to worry about the board type. Open the Serial Monitor from\nthe IDE, and you should start seeing numbers come across from the Arduino.\n\nNext Steps\nNow that you can talk wirelessly between a remote Arduino and your serial port,\nthere is no shortage of new adventures. You can sense environmental conditions\nthroughout your entire neighborhood, and collect them on a Raspberry Pi\ndashboard. You can remotely control servos and motors, and more.","feature_image":"http://images.kevinhoyt.com/chicken.wire.jpg","featured":0,"status":"published","locale":null,"visibility":"public","author_id":"1","created_at":"2015-04-21T13:18:42.000Z","updated_at":"2015-04-21T13:24:52.000Z","published_at":"2014-12-02T15:00:00.000Z","custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"type":"post","email_recipient_filter":"none"},{"id":"5adb8922351ffe0018a57717","uuid":"cdfebd6b-0a42-42e6-a1bd-4b6764e0a608","title":"Tessel to Parse.com","slug":"tessel-to-parse-com","mobiledoc":"{\"version\":\"0.3.1\",\"markups\":[],\"atoms\":[],\"cards\":[[\"markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\"The [Electric Imp](http://kevinhoyt.com/blog/2014/11/17/electric-imp-to-parse-dot-com.html) has Squirrel. The [Spark Core](http://kevinhoyt.com/blog/2014/11/18/spark-core-to-parse-dot-com.html) has Wiring/C/C++. Today I give you the [Tessel](https://tessel.io/), which has JavaScript. At $75 the Tessel is not exactly your cost-effective choice for prototype to production. If however you are a Web Developer looking to get started down the road of the Internet of Things, then the Tessel could not have provided an easier on-ramp.\\n\\n---\\n\\nThe Tessel is an interesting board. For wireless, it has the same Texas Instruments chipset that the Spark Core has in it - the CC3000. This means it has the same 802.11 b/g limitations. Technical Machines, the maker of the Tessel, have not implemented an application for setting the SSID either - it has to be done via the command line.\\n\\nI could go on about how there is no cloud IDE, or remote firmware update support. I could talk about how big it is compared to a Spark Core, with headers coming off at right angles (though it is still roughly the same size as an Arduino). Of course there is the cost issue. But in the end … it is programmed in JavaScript!\\n\\nSeriously though, comparing the Tessel to the Electric Imp and Spark Core boards would not be an apples to apples comparison. These guys are for different markets. As an IoT enabler goes however, if you are a Web Developer that has ever had so much as a passing interest in IoT, then this guy will get you hooked in no time flat.\\n\\n### Tooling and Workflow\\n\\nThe first piece of kit you will need to work with Tessel is Node.js. After that, a little NPM action will get you the Tessel tooling. With that tooling you have all sorts of fun control over the hardware, for example updating the firmware. And I have put two firmware updates on it in the last week alone, so the staff over at Technical Machines is hard at work.\\n\\nLike any other hardware kit, the first project you are likely to do with the Tessel is blink and LED. Their tutorial will take you through blinking the on-board LEDs, but there is a bank of GPIO headers along one side of the board. These give you analog, digital, SPI, I2C, and UART capabilities. In short, you can control most every sensor you might otherwise control with an Arduino.\\n\\nIn addition to the GPIO pins, there are also four dedicated \\\"ports\\\" for Tessel Modules. Modules are pieces of kit designed specifically for the Tessel, and they support a wide variety of functionality including GPS, audio, camera, GPRS, relays, servos and on and on. Modules have their own Tessel libraries already wrapped up and ready to go, so you can easily and quickly add functionality.\\n\\n> Put simply, this thing is a blast to use!\\n\\nWith your JavaScript code written, another trip to the command line will upload the script and required libraries to the Tessel. If your script uses access to the Internet, you will want to tell the Tessel your network SSID and password - again through the command line. You can alternatively use a JavaScript library to control the network connection.\\n\\nWhen your program is running on the Tessel, it is just like any other running Node.js program. The various output streams will send information to the command line window. If something breaks, you will see the error logged in the window as well. While there is no need for a clunky serial monitor, normal syntax errors that Node.js would catch right off the bat, go unnoticed until trying to run.\\n\\n### Climate Module\\n\\nThroughout this tour of IoT enablers, we have been using a thermistor as our sensor data point. The Tessel however has a climate module that will report the temperature and humidity without any tricky extra math. You can place the modules into one of the dedicated ports, code to the module library, and be up and running in no time flat.\\n\\n```\\n// Libraries\\nvar climate = require( 'climate-si7020' );\\nvar tessel = require( 'tessel' );\\n\\n// Use port\\nvar port = climate.use( tessel.port['A'] );\\n\\n// Port is ready\\nport.on( 'ready', function () {\\n    console.log( 'Connected to si7020.' );\\n\\n    // Queue execution of port access\\n    setImmediate( function loop() {\\n        // Read Fahrenheit temperature\\n        port.readTemperature( 'f', function( err, temperature ) {\\n            // Read relative humidity\\n            port.readHumidity( function( err, humidity ) {\\n                console.log(\\n                    'Degrees: ', temperature.toFixed( 4 ) + 'F ',    \\n                    'Humidity: ', humidity.toFixed( 4 ) + '%RH'\\n                );                \\n            \\n                // Wait and do it again\\n                setTimeout( loop, 1000 );                \\n            } );\\n        } );\\n    } );\\n} );\\n\\n// Error accessing port\\nport.on( 'error', function( err ) {\\n    console.log( 'error connecting module', err );\\n} ); \\n```\\n\\nThis was so easy, especially with the provided documentation, that it almost felt like I was cheating. No C/C++ libraries? No character buffers to maintain? No pointers? Surely I am cheating! It dawned on me however that this really represents an opportunity. Everything about the modules is documented, so you could build your own modules as well.\\n\\nPrinted Circuit Board (PCB) design is pretty tricky, especially if you are dealing with a complex set of chips. Designing for one chip as a module however seems totally approachable. So not only has the Tessel set you up to learn IoT development, it also really sets you up to take the next step to the PCB. Once you have IoT and PCB under your belt, you are well on your way to bringing your own product to market.\\n\\n### Climate Module with Parse Library\\n\\nSince Tessel is all about JavaScript, there is a good chance that most of your favorite libraries will work right out of the box. Parse.com makes a JavaScript library for accessing its services, so another trip to NPM and I was ready to store my data in the cloud … Well, almost.\\n\\nAs it turns out, while you are writing JavaScript, it is actually Lua that is working behind the scenes. This means there is some translation that happens. It also means that not all translation will work exactly right. Such is the case for the Parse.com library. A trip to the Tessel forums revealed a change to the Parse.com JavaScript files to get everything running smoothly.\\n\\nTo their credit, the Technical Machines staff are all over making sure this translation process is perfected.\\n\\n```\\n// Libraries\\nvar climate = require( 'climate-si7020' );\\nvar parse = require( 'parse' ).Parse;\\nvar tessel = require( 'tessel' );\\nvar wifi = require( 'wifi-cc3000' );\\n\\n// Constants\\nvar PARSE_APP = '_YOUR_APP_KEY_';\\nvar PARSE_KEY = '_YOUR_JAVASCRIPT_KEY_';\\n\\n// Parse objects\\nvar Temperature = parse.Object.extend( 'Temperature' );                    \\n\\n// Use port\\nvar port = climate.use( tessel.port['A'] );\\n\\n// Initialize Parse library\\n// Note Tessel bug requiring change to Parse module\\n// https://github.com/tessel/runtime/issues/429\\nparse.initialize( PARSE_APP, PARSE_KEY );    \\n\\n// Port is ready\\nport.on( 'ready', function () {\\n    console.log( 'Connected to si7020.' );\\n\\n    // Queue execution of port access\\n    setImmediate( function loop() {\\n        // Read Fahrenheit temperature\\n        port.readTemperature( 'f', function( err, temperature ) {\\n            var storage = null;\\n\\n            // Store in cloud if wireless\\n            if( wifi.isConnected() ) \\n            {    \\n                // New Parse object\\n                // Values\\n                // Save\\n                storage = new Temperature();\\n                storage.set( 'fahrenheit', temperature );\\n                storage.set( 'celcius', ( temperature - 32 ) / 1.8 );\\n                storage.save( null, {\\n                    success: function( result ) {\\n                        console.log( 'Saved: ' + result.id );\\n                    },\\n                    error: function( result, error ) {\\n                        console.log( error.message );\\n                    }\\n                } );\\n            }\\n\\n            // Log to console\\n            console.log( 'Temperature: ' + temperature.toFixed( 4 ) + 'F' );\\n        \\n            // Wait and do it again\\n            setTimeout( loop, 1000 );\\n        } );\\n    } );\\n} );\\n\\n// Error accessing port\\nport.on( 'error', function( err ) {\\n    console.log( 'error connecting module', err );\\n} ); \\n```\\n\\nThis program starts off by turning on one of the Tessel's four ports. An event is fired when the port is ready, and then we set up a loop to same the climate data. If the Tessel is connected to wireless, then we go ahead and use the Parse.com JavaScript library to store the temperature reading in the cloud. The program waits for a second, and then does it again.\\n\\n### Climate Module with Parse REST API\\n\\nAgain, this is so easy, it almost feels like cheating. My other examples to this point have used the Parse.com REST API, and in some cases, we have had to do some proxy gymnastics to make things work. Maybe moving to the REST API would make this problem more complex.\\n\\n```\\n// Store in cloud if wireless\\nif( wifi.isConnected() ) \\n{    \\n    options = {\\n        headers: {\\n            'Content-Type': 'application/json',\\n            'X-Parse-Application-Id': PARSE_APP,\\n            'X-Parse-REST-API-Key': PARSE_KEY  \\n        },\\n        hostname: 'api.parse.com',\\n        method: 'POST',                    \\n        path: '/1/classes/Temperature',                    \\n        port: 443\\n    };\\n            \\n    // Make request\\n    request = https.request( options, function( response ) {                \\n        // Got a response\\n        response.on( 'data', function( data ) {\\n            console.log( data.toString().trim() );\\n        } );\\n    } );\\n    request.write( JSON.stringify( {\\n        fahrenheit: temperature,\\n        celcius: ( temperature - 32 ) / 1.8\\n    } ) );\\n    request.end();\\n            \\n    // Handle HTTPS error\\n    request.on( 'error', function( err ) {\\n        console.error( err );\\n    } );\\n}\\n```\\n\\nEverything around accessing the port and reading the temperature is exactly the same. What is listed above is the only part that changes, which is the script inside the wireless connectivity check.\\n\\nTo get the Tessel to communicate with the Parse.com REST API, we setup an options object, and then use the HTTPS library to make a request. The Parse.com REST API expect JSON content, so we format our request data accordingly and send it along. When the response comes back from the API, we can access the data and do with it as we please.\\n\\nThe difference here however is that Node.js, being JavaScript, provides great JSON handling. We can parse and serialize JSON data all day long on the Tessel without any crazy string manipulation. In fact, I found the REST API to be more responsive than the formal Parse.com JavaScript library. It just does not get any easier than this.\\n\\n### Thermistor with Parse Library\\n\\nMaybe I am getting away with something here because I am not using a thermistor. Grabbing ye olde thermistor and assorted components, I wired from pin 3 on the Tessel (3.3V) to the thermistor. From the thermistor I went across a 10k resistor. From the resistor I went to pin 1 on the Tessel (Ground), and also pin 6 (10-bit DAC).\\n\\nUsing the same calculations as the other examples, I was able to same temperature using the thermistor as well.\\n\\n```\\n// GPIO access\\nvar gpio = tessel.port['GPIO'];\\n\\n// Thermistor\\nvar thermistor = gpio.pin['A5'];\\n\\n...\\n\\n// Analog reading\\n// Resulting range is zero to one\\nanalog = thermistor.read();\\n\\n// Voltage to temperature values\\n// Multiply by 10-bit steps\\ncelcius = temperature( analog * ANALOG_STEPS );\\nfahrenheit = celcius * 1.80 + 32.0;\\n\\n...\\n\\n// Calculate celcius temperature\\nfunction temperature( analog ) \\n{\\n    var kelvin = 0.0;\\n    var l_therm = 0.0;\\n    var r_therm = 0.0;    \\n    var v_out = 0.0;\\n\\n    // Thermistor calculations\\n    v_out = VOLTAGE * analog / ANALOG_STEPS;\\n    r_therm = ( RESISTOR * VOLTAGE / v_out ) - RESISTOR;\\n  \\n    l_therm = Math.log( THERMISTOR_OHM / r_therm );\\n    kelvin = ( T_THERM * B_THERM ) / ( B_THERM - T_THERM * l_therm );    \\n\\n    // Celcius value\\n    return kelvin - 273.15;\\n}\\n```\\n\\nThe code here is almost identical to the Parse.com example using the climate module. The difference is that I first get a reference to the GPIO pins. From there I get a reference to the \\\"A5\\\" pin (pin 6). In my loop, I use that reference to read the analog value. The reading that comes back is between zero and one, so I multiply that value times the 10-bit resolution (1023 steps).\\n\\nEverything else is the same as using the Parse.com JavaScript library with the Climate Module.\\n\\n### Thermistor with Parse REST API\\n\\nAt this point I had no other choice but to finish this game of comparisons by using a thermistor, and communicating with the Parse.com REST API. This is about as apples to apples as a comparison as we can get to the Spark Core and Electric Imp examples. And as you might have guessed at this point, the Tessel performed brilliantly.\\n\\nIf you want to view the complete code for this example (or any of the examples from this post), you can find them in my [GitHub](https://gist.github.com/krhoyt/3ce601ecb520229d87bf) repository. So far as posting thermistor readings to Parse.com via the REST API, it is effectively a combination of two previous example. Pull the REST API access from the Climate Module example, and the thermistor code from the previous example using the Parse.com JavaScript library.\\n\\nWhen you put it all together, including my comments, GPIO access, HTTPS POST, and the temperature calculation, it comes in at just over 100 lines of JavaScript. Pretty lean.\\n\\n### Next Steps\\n\\nIf you get a sense that I was able to crank out these four examples relatively easily, you would be absolutely right. I actually broke down several other atomic examples along the way. I found that I could just keep going all day long because the JavaScript was so familiar, and the Tessel just so eager to perform. Put simply, this thing is a blast to use!\\n\\nFrom here I think I would really like to design my own module, though I am not sure around what exactly. Perhaps an RGB LED matrix using WS2812 modules. I have taken classes on PCB design using Eagle, but have never quite found the right project to commit to printing. The Tessel has been so much fun, but our march of IoT enablers continues tomorrow.\\n\"}]],\"sections\":[[10,0]],\"ghostVersion\":\"3.0\"}","html":"<!--kg-card-begin: markdown--><p>The <a href=\"http://kevinhoyt.com/blog/2014/11/17/electric-imp-to-parse-dot-com.html\">Electric Imp</a> has Squirrel. The <a href=\"http://kevinhoyt.com/blog/2014/11/18/spark-core-to-parse-dot-com.html\">Spark Core</a> has Wiring/C/C++. Today I give you the <a href=\"https://tessel.io/\">Tessel</a>, which has JavaScript. At $75 the Tessel is not exactly your cost-effective choice for prototype to production. If however you are a Web Developer looking to get started down the road of the Internet of Things, then the Tessel could not have provided an easier on-ramp.</p>\n<hr>\n<p>The Tessel is an interesting board. For wireless, it has the same Texas Instruments chipset that the Spark Core has in it - the CC3000. This means it has the same 802.11 b/g limitations. Technical Machines, the maker of the Tessel, have not implemented an application for setting the SSID either - it has to be done via the command line.</p>\n<p>I could go on about how there is no cloud IDE, or remote firmware update support. I could talk about how big it is compared to a Spark Core, with headers coming off at right angles (though it is still roughly the same size as an Arduino). Of course there is the cost issue. But in the end … it is programmed in JavaScript!</p>\n<p>Seriously though, comparing the Tessel to the Electric Imp and Spark Core boards would not be an apples to apples comparison. These guys are for different markets. As an IoT enabler goes however, if you are a Web Developer that has ever had so much as a passing interest in IoT, then this guy will get you hooked in no time flat.</p>\n<h3 id=\"toolingandworkflow\">Tooling and Workflow</h3>\n<p>The first piece of kit you will need to work with Tessel is Node.js. After that, a little NPM action will get you the Tessel tooling. With that tooling you have all sorts of fun control over the hardware, for example updating the firmware. And I have put two firmware updates on it in the last week alone, so the staff over at Technical Machines is hard at work.</p>\n<p>Like any other hardware kit, the first project you are likely to do with the Tessel is blink and LED. Their tutorial will take you through blinking the on-board LEDs, but there is a bank of GPIO headers along one side of the board. These give you analog, digital, SPI, I2C, and UART capabilities. In short, you can control most every sensor you might otherwise control with an Arduino.</p>\n<p>In addition to the GPIO pins, there are also four dedicated &quot;ports&quot; for Tessel Modules. Modules are pieces of kit designed specifically for the Tessel, and they support a wide variety of functionality including GPS, audio, camera, GPRS, relays, servos and on and on. Modules have their own Tessel libraries already wrapped up and ready to go, so you can easily and quickly add functionality.</p>\n<blockquote>\n<p>Put simply, this thing is a blast to use!</p>\n</blockquote>\n<p>With your JavaScript code written, another trip to the command line will upload the script and required libraries to the Tessel. If your script uses access to the Internet, you will want to tell the Tessel your network SSID and password - again through the command line. You can alternatively use a JavaScript library to control the network connection.</p>\n<p>When your program is running on the Tessel, it is just like any other running Node.js program. The various output streams will send information to the command line window. If something breaks, you will see the error logged in the window as well. While there is no need for a clunky serial monitor, normal syntax errors that Node.js would catch right off the bat, go unnoticed until trying to run.</p>\n<h3 id=\"climatemodule\">Climate Module</h3>\n<p>Throughout this tour of IoT enablers, we have been using a thermistor as our sensor data point. The Tessel however has a climate module that will report the temperature and humidity without any tricky extra math. You can place the modules into one of the dedicated ports, code to the module library, and be up and running in no time flat.</p>\n<pre><code>// Libraries\nvar climate = require( 'climate-si7020' );\nvar tessel = require( 'tessel' );\n\n// Use port\nvar port = climate.use( tessel.port['A'] );\n\n// Port is ready\nport.on( 'ready', function () {\n    console.log( 'Connected to si7020.' );\n\n    // Queue execution of port access\n    setImmediate( function loop() {\n        // Read Fahrenheit temperature\n        port.readTemperature( 'f', function( err, temperature ) {\n            // Read relative humidity\n            port.readHumidity( function( err, humidity ) {\n                console.log(\n                    'Degrees: ', temperature.toFixed( 4 ) + 'F ',    \n                    'Humidity: ', humidity.toFixed( 4 ) + '%RH'\n                );                \n            \n                // Wait and do it again\n                setTimeout( loop, 1000 );                \n            } );\n        } );\n    } );\n} );\n\n// Error accessing port\nport.on( 'error', function( err ) {\n    console.log( 'error connecting module', err );\n} ); \n</code></pre>\n<p>This was so easy, especially with the provided documentation, that it almost felt like I was cheating. No C/C++ libraries? No character buffers to maintain? No pointers? Surely I am cheating! It dawned on me however that this really represents an opportunity. Everything about the modules is documented, so you could build your own modules as well.</p>\n<p>Printed Circuit Board (PCB) design is pretty tricky, especially if you are dealing with a complex set of chips. Designing for one chip as a module however seems totally approachable. So not only has the Tessel set you up to learn IoT development, it also really sets you up to take the next step to the PCB. Once you have IoT and PCB under your belt, you are well on your way to bringing your own product to market.</p>\n<h3 id=\"climatemodulewithparselibrary\">Climate Module with Parse Library</h3>\n<p>Since Tessel is all about JavaScript, there is a good chance that most of your favorite libraries will work right out of the box. Parse.com makes a JavaScript library for accessing its services, so another trip to NPM and I was ready to store my data in the cloud … Well, almost.</p>\n<p>As it turns out, while you are writing JavaScript, it is actually Lua that is working behind the scenes. This means there is some translation that happens. It also means that not all translation will work exactly right. Such is the case for the Parse.com library. A trip to the Tessel forums revealed a change to the Parse.com JavaScript files to get everything running smoothly.</p>\n<p>To their credit, the Technical Machines staff are all over making sure this translation process is perfected.</p>\n<pre><code>// Libraries\nvar climate = require( 'climate-si7020' );\nvar parse = require( 'parse' ).Parse;\nvar tessel = require( 'tessel' );\nvar wifi = require( 'wifi-cc3000' );\n\n// Constants\nvar PARSE_APP = '_YOUR_APP_KEY_';\nvar PARSE_KEY = '_YOUR_JAVASCRIPT_KEY_';\n\n// Parse objects\nvar Temperature = parse.Object.extend( 'Temperature' );                    \n\n// Use port\nvar port = climate.use( tessel.port['A'] );\n\n// Initialize Parse library\n// Note Tessel bug requiring change to Parse module\n// https://github.com/tessel/runtime/issues/429\nparse.initialize( PARSE_APP, PARSE_KEY );    \n\n// Port is ready\nport.on( 'ready', function () {\n    console.log( 'Connected to si7020.' );\n\n    // Queue execution of port access\n    setImmediate( function loop() {\n        // Read Fahrenheit temperature\n        port.readTemperature( 'f', function( err, temperature ) {\n            var storage = null;\n\n            // Store in cloud if wireless\n            if( wifi.isConnected() ) \n            {    \n                // New Parse object\n                // Values\n                // Save\n                storage = new Temperature();\n                storage.set( 'fahrenheit', temperature );\n                storage.set( 'celcius', ( temperature - 32 ) / 1.8 );\n                storage.save( null, {\n                    success: function( result ) {\n                        console.log( 'Saved: ' + result.id );\n                    },\n                    error: function( result, error ) {\n                        console.log( error.message );\n                    }\n                } );\n            }\n\n            // Log to console\n            console.log( 'Temperature: ' + temperature.toFixed( 4 ) + 'F' );\n        \n            // Wait and do it again\n            setTimeout( loop, 1000 );\n        } );\n    } );\n} );\n\n// Error accessing port\nport.on( 'error', function( err ) {\n    console.log( 'error connecting module', err );\n} ); \n</code></pre>\n<p>This program starts off by turning on one of the Tessel's four ports. An event is fired when the port is ready, and then we set up a loop to same the climate data. If the Tessel is connected to wireless, then we go ahead and use the Parse.com JavaScript library to store the temperature reading in the cloud. The program waits for a second, and then does it again.</p>\n<h3 id=\"climatemodulewithparserestapi\">Climate Module with Parse REST API</h3>\n<p>Again, this is so easy, it almost feels like cheating. My other examples to this point have used the Parse.com REST API, and in some cases, we have had to do some proxy gymnastics to make things work. Maybe moving to the REST API would make this problem more complex.</p>\n<pre><code>// Store in cloud if wireless\nif( wifi.isConnected() ) \n{    \n    options = {\n        headers: {\n            'Content-Type': 'application/json',\n            'X-Parse-Application-Id': PARSE_APP,\n            'X-Parse-REST-API-Key': PARSE_KEY  \n        },\n        hostname: 'api.parse.com',\n        method: 'POST',                    \n        path: '/1/classes/Temperature',                    \n        port: 443\n    };\n            \n    // Make request\n    request = https.request( options, function( response ) {                \n        // Got a response\n        response.on( 'data', function( data ) {\n            console.log( data.toString().trim() );\n        } );\n    } );\n    request.write( JSON.stringify( {\n        fahrenheit: temperature,\n        celcius: ( temperature - 32 ) / 1.8\n    } ) );\n    request.end();\n            \n    // Handle HTTPS error\n    request.on( 'error', function( err ) {\n        console.error( err );\n    } );\n}\n</code></pre>\n<p>Everything around accessing the port and reading the temperature is exactly the same. What is listed above is the only part that changes, which is the script inside the wireless connectivity check.</p>\n<p>To get the Tessel to communicate with the Parse.com REST API, we setup an options object, and then use the HTTPS library to make a request. The Parse.com REST API expect JSON content, so we format our request data accordingly and send it along. When the response comes back from the API, we can access the data and do with it as we please.</p>\n<p>The difference here however is that Node.js, being JavaScript, provides great JSON handling. We can parse and serialize JSON data all day long on the Tessel without any crazy string manipulation. In fact, I found the REST API to be more responsive than the formal Parse.com JavaScript library. It just does not get any easier than this.</p>\n<h3 id=\"thermistorwithparselibrary\">Thermistor with Parse Library</h3>\n<p>Maybe I am getting away with something here because I am not using a thermistor. Grabbing ye olde thermistor and assorted components, I wired from pin 3 on the Tessel (3.3V) to the thermistor. From the thermistor I went across a 10k resistor. From the resistor I went to pin 1 on the Tessel (Ground), and also pin 6 (10-bit DAC).</p>\n<p>Using the same calculations as the other examples, I was able to same temperature using the thermistor as well.</p>\n<pre><code>// GPIO access\nvar gpio = tessel.port['GPIO'];\n\n// Thermistor\nvar thermistor = gpio.pin['A5'];\n\n...\n\n// Analog reading\n// Resulting range is zero to one\nanalog = thermistor.read();\n\n// Voltage to temperature values\n// Multiply by 10-bit steps\ncelcius = temperature( analog * ANALOG_STEPS );\nfahrenheit = celcius * 1.80 + 32.0;\n\n...\n\n// Calculate celcius temperature\nfunction temperature( analog ) \n{\n    var kelvin = 0.0;\n    var l_therm = 0.0;\n    var r_therm = 0.0;    \n    var v_out = 0.0;\n\n    // Thermistor calculations\n    v_out = VOLTAGE * analog / ANALOG_STEPS;\n    r_therm = ( RESISTOR * VOLTAGE / v_out ) - RESISTOR;\n  \n    l_therm = Math.log( THERMISTOR_OHM / r_therm );\n    kelvin = ( T_THERM * B_THERM ) / ( B_THERM - T_THERM * l_therm );    \n\n    // Celcius value\n    return kelvin - 273.15;\n}\n</code></pre>\n<p>The code here is almost identical to the Parse.com example using the climate module. The difference is that I first get a reference to the GPIO pins. From there I get a reference to the &quot;A5&quot; pin (pin 6). In my loop, I use that reference to read the analog value. The reading that comes back is between zero and one, so I multiply that value times the 10-bit resolution (1023 steps).</p>\n<p>Everything else is the same as using the Parse.com JavaScript library with the Climate Module.</p>\n<h3 id=\"thermistorwithparserestapi\">Thermistor with Parse REST API</h3>\n<p>At this point I had no other choice but to finish this game of comparisons by using a thermistor, and communicating with the Parse.com REST API. This is about as apples to apples as a comparison as we can get to the Spark Core and Electric Imp examples. And as you might have guessed at this point, the Tessel performed brilliantly.</p>\n<p>If you want to view the complete code for this example (or any of the examples from this post), you can find them in my <a href=\"https://gist.github.com/krhoyt/3ce601ecb520229d87bf\">GitHub</a> repository. So far as posting thermistor readings to Parse.com via the REST API, it is effectively a combination of two previous example. Pull the REST API access from the Climate Module example, and the thermistor code from the previous example using the Parse.com JavaScript library.</p>\n<p>When you put it all together, including my comments, GPIO access, HTTPS POST, and the temperature calculation, it comes in at just over 100 lines of JavaScript. Pretty lean.</p>\n<h3 id=\"nextsteps\">Next Steps</h3>\n<p>If you get a sense that I was able to crank out these four examples relatively easily, you would be absolutely right. I actually broke down several other atomic examples along the way. I found that I could just keep going all day long because the JavaScript was so familiar, and the Tessel just so eager to perform. Put simply, this thing is a blast to use!</p>\n<p>From here I think I would really like to design my own module, though I am not sure around what exactly. Perhaps an RGB LED matrix using WS2812 modules. I have taken classes on PCB design using Eagle, but have never quite found the right project to commit to printing. The Tessel has been so much fun, but our march of IoT enablers continues tomorrow.</p>\n<!--kg-card-end: markdown-->","comment_id":"11","plaintext":"The Electric Imp\n[http://kevinhoyt.com/blog/2014/11/17/electric-imp-to-parse-dot-com.html] has\nSquirrel. The Spark Core\n[http://kevinhoyt.com/blog/2014/11/18/spark-core-to-parse-dot-com.html] has\nWiring/C/C++. Today I give you the Tessel [https://tessel.io/], which has\nJavaScript. At $75 the Tessel is not exactly your cost-effective choice for\nprototype to production. If however you are a Web Developer looking to get\nstarted down the road of the Internet of Things, then the Tessel could not have\nprovided an easier on-ramp.\n\n\n--------------------------------------------------------------------------------\n\nThe Tessel is an interesting board. For wireless, it has the same Texas\nInstruments chipset that the Spark Core has in it - the CC3000. This means it\nhas the same 802.11 b/g limitations. Technical Machines, the maker of the\nTessel, have not implemented an application for setting the SSID either - it has\nto be done via the command line.\n\nI could go on about how there is no cloud IDE, or remote firmware update\nsupport. I could talk about how big it is compared to a Spark Core, with headers\ncoming off at right angles (though it is still roughly the same size as an\nArduino). Of course there is the cost issue. But in the end … it is programmed\nin JavaScript!\n\nSeriously though, comparing the Tessel to the Electric Imp and Spark Core boards\nwould not be an apples to apples comparison. These guys are for different\nmarkets. As an IoT enabler goes however, if you are a Web Developer that has\never had so much as a passing interest in IoT, then this guy will get you hooked\nin no time flat.\n\nTooling and Workflow\nThe first piece of kit you will need to work with Tessel is Node.js. After that,\na little NPM action will get you the Tessel tooling. With that tooling you have\nall sorts of fun control over the hardware, for example updating the firmware.\nAnd I have put two firmware updates on it in the last week alone, so the staff\nover at Technical Machines is hard at work.\n\nLike any other hardware kit, the first project you are likely to do with the\nTessel is blink and LED. Their tutorial will take you through blinking the\non-board LEDs, but there is a bank of GPIO headers along one side of the board.\nThese give you analog, digital, SPI, I2C, and UART capabilities. In short, you\ncan control most every sensor you might otherwise control with an Arduino.\n\nIn addition to the GPIO pins, there are also four dedicated \"ports\" for Tessel\nModules. Modules are pieces of kit designed specifically for the Tessel, and\nthey support a wide variety of functionality including GPS, audio, camera, GPRS,\nrelays, servos and on and on. Modules have their own Tessel libraries already\nwrapped up and ready to go, so you can easily and quickly add functionality.\n\n> Put simply, this thing is a blast to use!\n\n\nWith your JavaScript code written, another trip to the command line will upload\nthe script and required libraries to the Tessel. If your script uses access to\nthe Internet, you will want to tell the Tessel your network SSID and password -\nagain through the command line. You can alternatively use a JavaScript library\nto control the network connection.\n\nWhen your program is running on the Tessel, it is just like any other running\nNode.js program. The various output streams will send information to the command\nline window. If something breaks, you will see the error logged in the window as\nwell. While there is no need for a clunky serial monitor, normal syntax errors\nthat Node.js would catch right off the bat, go unnoticed until trying to run.\n\nClimate Module\nThroughout this tour of IoT enablers, we have been using a thermistor as our\nsensor data point. The Tessel however has a climate module that will report the\ntemperature and humidity without any tricky extra math. You can place the\nmodules into one of the dedicated ports, code to the module library, and be up\nand running in no time flat.\n\n// Libraries\nvar climate = require( 'climate-si7020' );\nvar tessel = require( 'tessel' );\n\n// Use port\nvar port = climate.use( tessel.port['A'] );\n\n// Port is ready\nport.on( 'ready', function () {\n    console.log( 'Connected to si7020.' );\n\n    // Queue execution of port access\n    setImmediate( function loop() {\n        // Read Fahrenheit temperature\n        port.readTemperature( 'f', function( err, temperature ) {\n            // Read relative humidity\n            port.readHumidity( function( err, humidity ) {\n                console.log(\n                    'Degrees: ', temperature.toFixed( 4 ) + 'F ',    \n                    'Humidity: ', humidity.toFixed( 4 ) + '%RH'\n                );                \n            \n                // Wait and do it again\n                setTimeout( loop, 1000 );                \n            } );\n        } );\n    } );\n} );\n\n// Error accessing port\nport.on( 'error', function( err ) {\n    console.log( 'error connecting module', err );\n} ); \n\n\nThis was so easy, especially with the provided documentation, that it almost\nfelt like I was cheating. No C/C++ libraries? No character buffers to maintain?\nNo pointers? Surely I am cheating! It dawned on me however that this really\nrepresents an opportunity. Everything about the modules is documented, so you\ncould build your own modules as well.\n\nPrinted Circuit Board (PCB) design is pretty tricky, especially if you are\ndealing with a complex set of chips. Designing for one chip as a module however\nseems totally approachable. So not only has the Tessel set you up to learn IoT\ndevelopment, it also really sets you up to take the next step to the PCB. Once\nyou have IoT and PCB under your belt, you are well on your way to bringing your\nown product to market.\n\nClimate Module with Parse Library\nSince Tessel is all about JavaScript, there is a good chance that most of your\nfavorite libraries will work right out of the box. Parse.com makes a JavaScript\nlibrary for accessing its services, so another trip to NPM and I was ready to\nstore my data in the cloud … Well, almost.\n\nAs it turns out, while you are writing JavaScript, it is actually Lua that is\nworking behind the scenes. This means there is some translation that happens. It\nalso means that not all translation will work exactly right. Such is the case\nfor the Parse.com library. A trip to the Tessel forums revealed a change to the\nParse.com JavaScript files to get everything running smoothly.\n\nTo their credit, the Technical Machines staff are all over making sure this\ntranslation process is perfected.\n\n// Libraries\nvar climate = require( 'climate-si7020' );\nvar parse = require( 'parse' ).Parse;\nvar tessel = require( 'tessel' );\nvar wifi = require( 'wifi-cc3000' );\n\n// Constants\nvar PARSE_APP = '_YOUR_APP_KEY_';\nvar PARSE_KEY = '_YOUR_JAVASCRIPT_KEY_';\n\n// Parse objects\nvar Temperature = parse.Object.extend( 'Temperature' );                    \n\n// Use port\nvar port = climate.use( tessel.port['A'] );\n\n// Initialize Parse library\n// Note Tessel bug requiring change to Parse module\n// https://github.com/tessel/runtime/issues/429\nparse.initialize( PARSE_APP, PARSE_KEY );    \n\n// Port is ready\nport.on( 'ready', function () {\n    console.log( 'Connected to si7020.' );\n\n    // Queue execution of port access\n    setImmediate( function loop() {\n        // Read Fahrenheit temperature\n        port.readTemperature( 'f', function( err, temperature ) {\n            var storage = null;\n\n            // Store in cloud if wireless\n            if( wifi.isConnected() ) \n            {    \n                // New Parse object\n                // Values\n                // Save\n                storage = new Temperature();\n                storage.set( 'fahrenheit', temperature );\n                storage.set( 'celcius', ( temperature - 32 ) / 1.8 );\n                storage.save( null, {\n                    success: function( result ) {\n                        console.log( 'Saved: ' + result.id );\n                    },\n                    error: function( result, error ) {\n                        console.log( error.message );\n                    }\n                } );\n            }\n\n            // Log to console\n            console.log( 'Temperature: ' + temperature.toFixed( 4 ) + 'F' );\n        \n            // Wait and do it again\n            setTimeout( loop, 1000 );\n        } );\n    } );\n} );\n\n// Error accessing port\nport.on( 'error', function( err ) {\n    console.log( 'error connecting module', err );\n} ); \n\n\nThis program starts off by turning on one of the Tessel's four ports. An event\nis fired when the port is ready, and then we set up a loop to same the climate\ndata. If the Tessel is connected to wireless, then we go ahead and use the\nParse.com JavaScript library to store the temperature reading in the cloud. The\nprogram waits for a second, and then does it again.\n\nClimate Module with Parse REST API\nAgain, this is so easy, it almost feels like cheating. My other examples to this\npoint have used the Parse.com REST API, and in some cases, we have had to do\nsome proxy gymnastics to make things work. Maybe moving to the REST API would\nmake this problem more complex.\n\n// Store in cloud if wireless\nif( wifi.isConnected() ) \n{    \n    options = {\n        headers: {\n            'Content-Type': 'application/json',\n            'X-Parse-Application-Id': PARSE_APP,\n            'X-Parse-REST-API-Key': PARSE_KEY  \n        },\n        hostname: 'api.parse.com',\n        method: 'POST',                    \n        path: '/1/classes/Temperature',                    \n        port: 443\n    };\n            \n    // Make request\n    request = https.request( options, function( response ) {                \n        // Got a response\n        response.on( 'data', function( data ) {\n            console.log( data.toString().trim() );\n        } );\n    } );\n    request.write( JSON.stringify( {\n        fahrenheit: temperature,\n        celcius: ( temperature - 32 ) / 1.8\n    } ) );\n    request.end();\n            \n    // Handle HTTPS error\n    request.on( 'error', function( err ) {\n        console.error( err );\n    } );\n}\n\n\nEverything around accessing the port and reading the temperature is exactly the\nsame. What is listed above is the only part that changes, which is the script\ninside the wireless connectivity check.\n\nTo get the Tessel to communicate with the Parse.com REST API, we setup an\noptions object, and then use the HTTPS library to make a request. The Parse.com\nREST API expect JSON content, so we format our request data accordingly and send\nit along. When the response comes back from the API, we can access the data and\ndo with it as we please.\n\nThe difference here however is that Node.js, being JavaScript, provides great\nJSON handling. We can parse and serialize JSON data all day long on the Tessel\nwithout any crazy string manipulation. In fact, I found the REST API to be more\nresponsive than the formal Parse.com JavaScript library. It just does not get\nany easier than this.\n\nThermistor with Parse Library\nMaybe I am getting away with something here because I am not using a thermistor.\nGrabbing ye olde thermistor and assorted components, I wired from pin 3 on the\nTessel (3.3V) to the thermistor. From the thermistor I went across a 10k\nresistor. From the resistor I went to pin 1 on the Tessel (Ground), and also pin\n6 (10-bit DAC).\n\nUsing the same calculations as the other examples, I was able to same\ntemperature using the thermistor as well.\n\n// GPIO access\nvar gpio = tessel.port['GPIO'];\n\n// Thermistor\nvar thermistor = gpio.pin['A5'];\n\n...\n\n// Analog reading\n// Resulting range is zero to one\nanalog = thermistor.read();\n\n// Voltage to temperature values\n// Multiply by 10-bit steps\ncelcius = temperature( analog * ANALOG_STEPS );\nfahrenheit = celcius * 1.80 + 32.0;\n\n...\n\n// Calculate celcius temperature\nfunction temperature( analog ) \n{\n    var kelvin = 0.0;\n    var l_therm = 0.0;\n    var r_therm = 0.0;    \n    var v_out = 0.0;\n\n    // Thermistor calculations\n    v_out = VOLTAGE * analog / ANALOG_STEPS;\n    r_therm = ( RESISTOR * VOLTAGE / v_out ) - RESISTOR;\n  \n    l_therm = Math.log( THERMISTOR_OHM / r_therm );\n    kelvin = ( T_THERM * B_THERM ) / ( B_THERM - T_THERM * l_therm );    \n\n    // Celcius value\n    return kelvin - 273.15;\n}\n\n\nThe code here is almost identical to the Parse.com example using the climate\nmodule. The difference is that I first get a reference to the GPIO pins. From\nthere I get a reference to the \"A5\" pin (pin 6). In my loop, I use that\nreference to read the analog value. The reading that comes back is between zero\nand one, so I multiply that value times the 10-bit resolution (1023 steps).\n\nEverything else is the same as using the Parse.com JavaScript library with the\nClimate Module.\n\nThermistor with Parse REST API\nAt this point I had no other choice but to finish this game of comparisons by\nusing a thermistor, and communicating with the Parse.com REST API. This is about\nas apples to apples as a comparison as we can get to the Spark Core and Electric\nImp examples. And as you might have guessed at this point, the Tessel performed\nbrilliantly.\n\nIf you want to view the complete code for this example (or any of the examples\nfrom this post), you can find them in my GitHub\n[https://gist.github.com/krhoyt/3ce601ecb520229d87bf] repository. So far as\nposting thermistor readings to Parse.com via the REST API, it is effectively a\ncombination of two previous example. Pull the REST API access from the Climate\nModule example, and the thermistor code from the previous example using the\nParse.com JavaScript library.\n\nWhen you put it all together, including my comments, GPIO access, HTTPS POST,\nand the temperature calculation, it comes in at just over 100 lines of\nJavaScript. Pretty lean.\n\nNext Steps\nIf you get a sense that I was able to crank out these four examples relatively\neasily, you would be absolutely right. I actually broke down several other\natomic examples along the way. I found that I could just keep going all day long\nbecause the JavaScript was so familiar, and the Tessel just so eager to perform.\nPut simply, this thing is a blast to use!\n\nFrom here I think I would really like to design my own module, though I am not\nsure around what exactly. Perhaps an RGB LED matrix using WS2812 modules. I have\ntaken classes on PCB design using Eagle, but have never quite found the right\nproject to commit to printing. The Tessel has been so much fun, but our march of\nIoT enablers continues tomorrow.","feature_image":"http://images.kevinhoyt.com/ceramic.tessellation.jpg","featured":0,"status":"published","locale":null,"visibility":"public","author_id":"1","created_at":"2015-04-21T13:25:41.000Z","updated_at":"2015-04-21T13:33:50.000Z","published_at":"2014-11-19T15:00:00.000Z","custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"type":"post","email_recipient_filter":"none"},{"id":"5adb8922351ffe0018a57718","uuid":"87ae4b49-8930-4bc0-bf3e-415db8877fc3","title":"Spark Core to Parse.com","slug":"spark-core-to-parse-com","mobiledoc":"{\"version\":\"0.3.1\",\"markups\":[],\"atoms\":[],\"cards\":[[\"markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\"*Compared to the [Electric Imp](http://kevinhoyt.com/blog/2014/11/18/electric-imp-to-parse-dot-com.html), the [Spark Core](https://www.spark.io/) is a relative newcomer. It launched via [Kickstarter](https://www.kickstarter.com/projects/sparkdevices/spark-core-wi-fi-for-everything-arduino-compatible), picking up almost viral momentum early, due mostly to the use of the Texas Instruments [CC3000](http://www.ti.com/product/cc3000) wireless stack. Just last week Spark announced an upgrade to the Core, called the Photon, which ditches the CC3000 and is due in March 2015. While I am really excited about the Photon, let us take a look at what the Core offers us today.*\\n\\n---\\n\\nBefore getting too far, I would like to talk a little about the brand. The name of the company is Spark, and the name of the device is the Core. Put them together and you get the Spark Core. This is one area I think the Photon will help Spark from a marketing perspective. With the arrival of its second device, Spark will really start to emerge as an IoT-enabler as opposed to a one-trick pony.\\n\\n### TI CC3000\\n\\nSecond, some details about the CC3000. This innovative chip picked up a lot of interest mostly because wirelessly linking hardware to the Web is actually a really challenging problem. With no keyboard or screen, setting SSID becomes a real feat of engineering. If you are just a hobbyist, you can hard-code these credentials, but in production, consumers are not going to update your code with their SSID, recompile, and flash new firmware.\\n\\nThe CC3000 series chipset uses a little loophole in 802.11 b/g networking, that allows you to set the SSID from another device on the target network. Notice the catch there - 802.11 b/g. If you are running an 802.11 n network, as I was when I received my Spark Core, then you will be out of luck. Not to mention the now growing list of 802.11 ac implementations.\\n\\nTo address this, the Spark Photon moves to a [Broadcom](http://www.broadcom.com/products/wiced/wifi/) chipset that provides \\\"soft access point\\\" (Soft AP). Soft AP allows the device to setup its own wireless network when it does not have an SSID specified, or cannot attach to the one that has been specified. Once the Soft AP wireless has been established, you connect your smartphone (or other device) to that network. You can then set the SSID information on the device, usually using a custom application, or web page.\\n\\nFor somebody looking to move from prototype to production, where the consumer's wireless network is unpredictable, this represents a big step forward. And the Spark Photon is not alone here, as the [Google Nest](https://nest.com/) (among others) also uses the same chipset. Oh, and the Broadcom chipset supports 802.11 n networks as well.\\n\\n### Spark Core\\n\\nOutside of the innovative CC3000, the Spark Core uses an ARM Cortex M3. Unlike the Electric Imp however, the Core is Arduino compatible. This means that if you are already familiar with the the Wiring framework used by the Arduino, then you will feel pretty comfortable with the Spark Core. And of course, you can use C/C++ to further extend your capabilities.\\n\\n![The Spark Core](http://images.kevinhoyt.com/spark.core.jpg)\\n\\nThis does not mean however that all your favorite Arduino libraries can be used. It also does not mean that the Arduino tooling workflow can be used. In fact, neither is true. To address this, the folks at Spark make many libraries available in their cloud IDE. Many, but not all. You will have to import and migrate any libraries not already supported.\\n\\nAs for the IDE, this was originally cloud-based only - just like the Electric Imp. Unlike the Imp however, there is no debugging console for the cloud IDE. With the Spark Photon announcement came the release of Spark Dev - a desktop IDE that promotes Arduino-like development workflow. There is also a command-line tool chain available for working with your Spark Core.\\n\\n**Pro Tip:** *If you struggle with setting the SSID on your Spark Core using the Spark application, you can also set it over USB. I actually used the Arduino Serial Monitor for this. You send the Core a \\\"w\\\" with no line feed, and it will prompt you for SSID and password. Enter each followed by a newline.*\\n\\nUpdating the firmware on the Spark Core happens wirelessly, which is great. You click a button in the cloud IDE, it finds your Core, and uploads new firmware. Unfortunately, this process is not always reliable, sometimes requiring multiple attempts, or resetting of the Core entirely. I do love the RGB LED included on the board however, as I could easily tell when problems arose, or when everything was going smoothly.\\n\\n### Circuit Diagram\\n\\nThere is not much difference in this circuit than wiring it up with any other controller. You send it 3.3V, of which the Core has two sources, run through a 10k resistor to ground, and take analog samples off the same thermistor/resistor junction. The Spark Core has a 12-bit DAC, which means 4,096 steps of resolution.\\n\\n![The Spark Core is so small that it fits entirely on a mini breadboard.](http://images.kevinhoyt.com/spark.core.fritzing.thermistor.png)\\n\\n### Device Code\\n\\nSince the Spark Core is Arduino compatible, the code from the perspective of interacting with the sensor is similar to the [Circuit Friday](http://kevinhoyt.com/blog/2014/11/07/circuit-friday-thermistor.html) example, with a few constant changes for working with 3.3V and 12-bit DAC. I will skip over posting that code here again, and instead move onto the meat of communicating with the Parse.com system.\\n\\nSpark provides a number of value-add classes that you can leverage from the Core. There are classes to expose variables through a REST-based API. There are classes to publish/subscribe to data from other Core devices. There are classes to learn all about the wireless environment. But for our purposes we are interested in the TCPClient class (there is also TCPServer and even UDP support).\\n\\n```\\nTCPClient client;\\n\\n...\\n\\n// Loop\\nvoid loop() \\n{\\n    double temperature = 0.0;\\n    int    analog = 0;\\n\\n    // Analog reading\\n    analog = analogRead( THERMISTOR );\\n\\n    // Thermistor reading as celcius\\n    temperature = thermistor( analog );\\n\\n    // Store data in cloud\\n    if( client.connect( KRHOYT_URL, KRHOYT_PORT ) )\\n    {\\n        request( temperature );\\n        wait();\\n        response();\\n    }\\n\\n    // Wait for next sample\\n    delay( UPDATE_RATE );\\n} \\n```\\n\\nThe TCPClient is pretty similar to any other networking library you may have used on the Arduino. In this case a call to TCPClient.connect() will start a TCP connection to a desired endpoint. The call is blocking, and returns a true/false upon completing. Once we know that the call has been completed successfully, we can go about logging the data.\\n\\nNotice that I am using my own proxy server here, as opposed to going directly to [Parse.com](https://parse.com/). The reason for this is that the Spark Core does not have the horsepower to handle HTTPS connections. By offloading networking requests to the cloud, Electric Imp overcomes this challenge, but introduces dependency on their cloud. By comparison the Spark Core makes network requests directly, but is then limited by the capabilities of the controller.\\n\\n```\\nvoid request( double value )\\n{\\n    // Buffers for string conversion\\n    char celcius[10];\\n    char farenheit[10];\\n  \\n    // Farenheit as character string\\n    sprintf( celcius, \\\"%2.2f\\\", value );\\n  \\n    // Convert to celcius as character string\\n    sprintf( farenheit, \\\"%2.2f\\\", value * 1.80 + 32 );\\n\\n    // Action - PARSE\\n    // APP_ID\\n    // Celcius\\n    // Farenheit\\n    // Data type - Temperature\\n    // Operation - create\\n    // REST key\\n    // Unit - object\\n    // Version - 1\\n\\n    sprintf( \\n        buffer, \\n        \\\"{\\\\\\\"action\\\\\\\":\\\\\\\"%s\\\\\\\",\\\\\\\"details\\\\\\\":{\\\\\\\"app_id\\\\\\\":\\\\\\\"%s\\\\\\\",\\\\\\\"data\\\\\\\":{\\\\\\\"celcius\\\\\\\":%s,\\\\\\\"farenheit\\\\\\\":%s},\\\\\\\"data_type\\\\\\\":\\\\\\\"%s\\\\\\\",\\\\\\\"operation\\\\\\\":\\\\\\\"%s\\\\\\\",\\\\\\\"rest_key\\\\\\\":\\\\\\\"%s\\\\\\\",\\\\\\\"unit\\\\\\\":\\\\\\\"%s\\\\\\\",\\\\\\\"version\\\\\\\":%s}}\\\", \\n        KRHOYT_ACTION,\\n        PARSE_APP,\\n        celcius,\\n        farenheit,\\n        PARSE_TYPE,\\n        KRHOYT_OPERATION,\\n        PARSE_KEY,\\n        KRHOYT_UNIT,\\n        PARSE_VERSION\\n    );\\n\\n    client.println( \\\"POST / HTTP/1.1\\\" );\\n    client.println( \\\"Host: proxy.kevinhoyt.com\\\" );\\n    client.print( \\\"Content-Length: \\\" );\\n    client.println( strlen( buffer ) );\\n    client.println( \\\"User-Agent: Spark Core\\\" );\\n    client.println( \\\"Content-Type: text/plain;charset=UTF-8\\\" );\\n    client.println();        \\n    client.print( buffer );\\n}\\n```\\n\\nMy custom-designed proxy has evolved over time to allow me to communicate with all my favorite cloud-based systems. To handle these variations, clients must POST a JSON structure that provides instructions as to handling the destination. My proxy system aside, you may have a non-HTTPS endpoint that needs JSON, so the approach would be similar if you were going directly from the Spark Core to any other API endpoint.\\n\\nOn the Arduino, I have become a big fan of the sprintf() function to format data. While the string here is really long (character array actually), I can pop values into the JSON structure with ease. The sprintf() function also allows me to convert the temperature readings from double to character arrays.\\n\\nAt this point, what we have is a TCP connection. We are connected to a server, and ready to send data to it. To make this into an HTTP connection, we need to send the appropriate values, in the correct form, across the wire. You can add headers here, or improvise on them, as I have with the User-Agent header.\\n\\n```\\n// Wait for a response from proxy\\nvoid wait()\\n{\\n    // Periodically check curl process\\n    while( !client.available() ) \\n    {\\n        Serial.println( \\\"Waiting ...\\\" );\\n        delay( 100 );\\n    }\\n}\\n```\\n\\nOnce the request bits have been sent, the server may take some time to respond. To handle this we will sit in a loop, checking for available data. When data arrives, execution will go back to the main loop, which will them move onto handling the response. One might also use a finite state machine to handle this in the main loop, but I feel my code reads a bit cleaner with the secondary loop approach.\\n\\n```\\n// Response from proxy\\nvoid response()\\n{\\n    char c;\\n  \\n    // While there is data to read\\n    while( client.available() ) \\n    {\\n        // Get character\\n        c = client.read();\\n        Serial.print( c );\\n    }\\n  \\n    client.stop();\\n}\\n```\\n\\nAnd finally comes the response! So long as the server, in this case my proxy server, is sending data back to the Spark Core, we will read those characters in and echo them to the serial port (USB). Once all the data has arrived, we will close off the client connection by called client.stop().\\n\\nIn the case of Parse.com, the data coming back is in JSON format. We could further parse this data, change settings on the device. We would have to do this however, using our own implementation. We would also need to be aware of the memory (or lack thereof) available to us. By comparison, the Electric Imp gives us a class to handle JSON out of the box.\\n\\n### Next Steps\\n\\nMost of the shortcomings of the Spark Core can be overcome with software updates, and it looks like the Spark team is hard at work doing exactly that. Those shortcomings that require hardware updates should be taken care of by the upcoming Spark Photon in March 2015.\\n\\nThere are many things to love about the Spark Core however including an attractive price, compact footprint, and the adorable RGB LED indicator. The developer kit even comes with a battery and charging circuit shaped as a mustache. Because, why not!\\n\\nAs for going from prototype to production, I think the Spark Core (and Photon) hold a lot of promise, but they are just not there yet. However, if you are familiar with Arduino, and looking at a comfortable way to start playing with IoT, then it will be hard to beat the Spark Core. You could get a [CC3000 shield](https://www.sparkfun.com/products/12071), but you would sacrifice size, considerable features, and cost.\"}]],\"sections\":[[10,0]],\"ghostVersion\":\"3.0\"}","html":"<!--kg-card-begin: markdown--><p><em>Compared to the <a href=\"http://kevinhoyt.com/blog/2014/11/18/electric-imp-to-parse-dot-com.html\">Electric Imp</a>, the <a href=\"https://www.spark.io/\">Spark Core</a> is a relative newcomer. It launched via <a href=\"https://www.kickstarter.com/projects/sparkdevices/spark-core-wi-fi-for-everything-arduino-compatible\">Kickstarter</a>, picking up almost viral momentum early, due mostly to the use of the Texas Instruments <a href=\"http://www.ti.com/product/cc3000\">CC3000</a> wireless stack. Just last week Spark announced an upgrade to the Core, called the Photon, which ditches the CC3000 and is due in March 2015. While I am really excited about the Photon, let us take a look at what the Core offers us today.</em></p>\n<hr>\n<p>Before getting too far, I would like to talk a little about the brand. The name of the company is Spark, and the name of the device is the Core. Put them together and you get the Spark Core. This is one area I think the Photon will help Spark from a marketing perspective. With the arrival of its second device, Spark will really start to emerge as an IoT-enabler as opposed to a one-trick pony.</p>\n<h3 id=\"ticc3000\">TI CC3000</h3>\n<p>Second, some details about the CC3000. This innovative chip picked up a lot of interest mostly because wirelessly linking hardware to the Web is actually a really challenging problem. With no keyboard or screen, setting SSID becomes a real feat of engineering. If you are just a hobbyist, you can hard-code these credentials, but in production, consumers are not going to update your code with their SSID, recompile, and flash new firmware.</p>\n<p>The CC3000 series chipset uses a little loophole in 802.11 b/g networking, that allows you to set the SSID from another device on the target network. Notice the catch there - 802.11 b/g. If you are running an 802.11 n network, as I was when I received my Spark Core, then you will be out of luck. Not to mention the now growing list of 802.11 ac implementations.</p>\n<p>To address this, the Spark Photon moves to a <a href=\"http://www.broadcom.com/products/wiced/wifi/\">Broadcom</a> chipset that provides &quot;soft access point&quot; (Soft AP). Soft AP allows the device to setup its own wireless network when it does not have an SSID specified, or cannot attach to the one that has been specified. Once the Soft AP wireless has been established, you connect your smartphone (or other device) to that network. You can then set the SSID information on the device, usually using a custom application, or web page.</p>\n<p>For somebody looking to move from prototype to production, where the consumer's wireless network is unpredictable, this represents a big step forward. And the Spark Photon is not alone here, as the <a href=\"https://nest.com/\">Google Nest</a> (among others) also uses the same chipset. Oh, and the Broadcom chipset supports 802.11 n networks as well.</p>\n<h3 id=\"sparkcore\">Spark Core</h3>\n<p>Outside of the innovative CC3000, the Spark Core uses an ARM Cortex M3. Unlike the Electric Imp however, the Core is Arduino compatible. This means that if you are already familiar with the the Wiring framework used by the Arduino, then you will feel pretty comfortable with the Spark Core. And of course, you can use C/C++ to further extend your capabilities.</p>\n<p><img src=\"http://images.kevinhoyt.com/spark.core.jpg\" alt=\"The Spark Core\" loading=\"lazy\"></p>\n<p>This does not mean however that all your favorite Arduino libraries can be used. It also does not mean that the Arduino tooling workflow can be used. In fact, neither is true. To address this, the folks at Spark make many libraries available in their cloud IDE. Many, but not all. You will have to import and migrate any libraries not already supported.</p>\n<p>As for the IDE, this was originally cloud-based only - just like the Electric Imp. Unlike the Imp however, there is no debugging console for the cloud IDE. With the Spark Photon announcement came the release of Spark Dev - a desktop IDE that promotes Arduino-like development workflow. There is also a command-line tool chain available for working with your Spark Core.</p>\n<p><strong>Pro Tip:</strong> <em>If you struggle with setting the SSID on your Spark Core using the Spark application, you can also set it over USB. I actually used the Arduino Serial Monitor for this. You send the Core a &quot;w&quot; with no line feed, and it will prompt you for SSID and password. Enter each followed by a newline.</em></p>\n<p>Updating the firmware on the Spark Core happens wirelessly, which is great. You click a button in the cloud IDE, it finds your Core, and uploads new firmware. Unfortunately, this process is not always reliable, sometimes requiring multiple attempts, or resetting of the Core entirely. I do love the RGB LED included on the board however, as I could easily tell when problems arose, or when everything was going smoothly.</p>\n<h3 id=\"circuitdiagram\">Circuit Diagram</h3>\n<p>There is not much difference in this circuit than wiring it up with any other controller. You send it 3.3V, of which the Core has two sources, run through a 10k resistor to ground, and take analog samples off the same thermistor/resistor junction. The Spark Core has a 12-bit DAC, which means 4,096 steps of resolution.</p>\n<p><img src=\"http://images.kevinhoyt.com/spark.core.fritzing.thermistor.png\" alt=\"The Spark Core is so small that it fits entirely on a mini breadboard.\" loading=\"lazy\"></p>\n<h3 id=\"devicecode\">Device Code</h3>\n<p>Since the Spark Core is Arduino compatible, the code from the perspective of interacting with the sensor is similar to the <a href=\"http://kevinhoyt.com/blog/2014/11/07/circuit-friday-thermistor.html\">Circuit Friday</a> example, with a few constant changes for working with 3.3V and 12-bit DAC. I will skip over posting that code here again, and instead move onto the meat of communicating with the Parse.com system.</p>\n<p>Spark provides a number of value-add classes that you can leverage from the Core. There are classes to expose variables through a REST-based API. There are classes to publish/subscribe to data from other Core devices. There are classes to learn all about the wireless environment. But for our purposes we are interested in the TCPClient class (there is also TCPServer and even UDP support).</p>\n<pre><code>TCPClient client;\n\n...\n\n// Loop\nvoid loop() \n{\n    double temperature = 0.0;\n    int    analog = 0;\n\n    // Analog reading\n    analog = analogRead( THERMISTOR );\n\n    // Thermistor reading as celcius\n    temperature = thermistor( analog );\n\n    // Store data in cloud\n    if( client.connect( KRHOYT_URL, KRHOYT_PORT ) )\n    {\n        request( temperature );\n        wait();\n        response();\n    }\n\n    // Wait for next sample\n    delay( UPDATE_RATE );\n} \n</code></pre>\n<p>The TCPClient is pretty similar to any other networking library you may have used on the Arduino. In this case a call to TCPClient.connect() will start a TCP connection to a desired endpoint. The call is blocking, and returns a true/false upon completing. Once we know that the call has been completed successfully, we can go about logging the data.</p>\n<p>Notice that I am using my own proxy server here, as opposed to going directly to <a href=\"https://parse.com/\">Parse.com</a>. The reason for this is that the Spark Core does not have the horsepower to handle HTTPS connections. By offloading networking requests to the cloud, Electric Imp overcomes this challenge, but introduces dependency on their cloud. By comparison the Spark Core makes network requests directly, but is then limited by the capabilities of the controller.</p>\n<pre><code>void request( double value )\n{\n    // Buffers for string conversion\n    char celcius[10];\n    char farenheit[10];\n  \n    // Farenheit as character string\n    sprintf( celcius, &quot;%2.2f&quot;, value );\n  \n    // Convert to celcius as character string\n    sprintf( farenheit, &quot;%2.2f&quot;, value * 1.80 + 32 );\n\n    // Action - PARSE\n    // APP_ID\n    // Celcius\n    // Farenheit\n    // Data type - Temperature\n    // Operation - create\n    // REST key\n    // Unit - object\n    // Version - 1\n\n    sprintf( \n        buffer, \n        &quot;{\\&quot;action\\&quot;:\\&quot;%s\\&quot;,\\&quot;details\\&quot;:{\\&quot;app_id\\&quot;:\\&quot;%s\\&quot;,\\&quot;data\\&quot;:{\\&quot;celcius\\&quot;:%s,\\&quot;farenheit\\&quot;:%s},\\&quot;data_type\\&quot;:\\&quot;%s\\&quot;,\\&quot;operation\\&quot;:\\&quot;%s\\&quot;,\\&quot;rest_key\\&quot;:\\&quot;%s\\&quot;,\\&quot;unit\\&quot;:\\&quot;%s\\&quot;,\\&quot;version\\&quot;:%s}}&quot;, \n        KRHOYT_ACTION,\n        PARSE_APP,\n        celcius,\n        farenheit,\n        PARSE_TYPE,\n        KRHOYT_OPERATION,\n        PARSE_KEY,\n        KRHOYT_UNIT,\n        PARSE_VERSION\n    );\n\n    client.println( &quot;POST / HTTP/1.1&quot; );\n    client.println( &quot;Host: proxy.kevinhoyt.com&quot; );\n    client.print( &quot;Content-Length: &quot; );\n    client.println( strlen( buffer ) );\n    client.println( &quot;User-Agent: Spark Core&quot; );\n    client.println( &quot;Content-Type: text/plain;charset=UTF-8&quot; );\n    client.println();        \n    client.print( buffer );\n}\n</code></pre>\n<p>My custom-designed proxy has evolved over time to allow me to communicate with all my favorite cloud-based systems. To handle these variations, clients must POST a JSON structure that provides instructions as to handling the destination. My proxy system aside, you may have a non-HTTPS endpoint that needs JSON, so the approach would be similar if you were going directly from the Spark Core to any other API endpoint.</p>\n<p>On the Arduino, I have become a big fan of the sprintf() function to format data. While the string here is really long (character array actually), I can pop values into the JSON structure with ease. The sprintf() function also allows me to convert the temperature readings from double to character arrays.</p>\n<p>At this point, what we have is a TCP connection. We are connected to a server, and ready to send data to it. To make this into an HTTP connection, we need to send the appropriate values, in the correct form, across the wire. You can add headers here, or improvise on them, as I have with the User-Agent header.</p>\n<pre><code>// Wait for a response from proxy\nvoid wait()\n{\n    // Periodically check curl process\n    while( !client.available() ) \n    {\n        Serial.println( &quot;Waiting ...&quot; );\n        delay( 100 );\n    }\n}\n</code></pre>\n<p>Once the request bits have been sent, the server may take some time to respond. To handle this we will sit in a loop, checking for available data. When data arrives, execution will go back to the main loop, which will them move onto handling the response. One might also use a finite state machine to handle this in the main loop, but I feel my code reads a bit cleaner with the secondary loop approach.</p>\n<pre><code>// Response from proxy\nvoid response()\n{\n    char c;\n  \n    // While there is data to read\n    while( client.available() ) \n    {\n        // Get character\n        c = client.read();\n        Serial.print( c );\n    }\n  \n    client.stop();\n}\n</code></pre>\n<p>And finally comes the response! So long as the server, in this case my proxy server, is sending data back to the Spark Core, we will read those characters in and echo them to the serial port (USB). Once all the data has arrived, we will close off the client connection by called client.stop().</p>\n<p>In the case of Parse.com, the data coming back is in JSON format. We could further parse this data, change settings on the device. We would have to do this however, using our own implementation. We would also need to be aware of the memory (or lack thereof) available to us. By comparison, the Electric Imp gives us a class to handle JSON out of the box.</p>\n<h3 id=\"nextsteps\">Next Steps</h3>\n<p>Most of the shortcomings of the Spark Core can be overcome with software updates, and it looks like the Spark team is hard at work doing exactly that. Those shortcomings that require hardware updates should be taken care of by the upcoming Spark Photon in March 2015.</p>\n<p>There are many things to love about the Spark Core however including an attractive price, compact footprint, and the adorable RGB LED indicator. The developer kit even comes with a battery and charging circuit shaped as a mustache. Because, why not!</p>\n<p>As for going from prototype to production, I think the Spark Core (and Photon) hold a lot of promise, but they are just not there yet. However, if you are familiar with Arduino, and looking at a comfortable way to start playing with IoT, then it will be hard to beat the Spark Core. You could get a <a href=\"https://www.sparkfun.com/products/12071\">CC3000 shield</a>, but you would sacrifice size, considerable features, and cost.</p>\n<!--kg-card-end: markdown-->","comment_id":"12","plaintext":"Compared to the Electric Imp\n[http://kevinhoyt.com/blog/2014/11/18/electric-imp-to-parse-dot-com.html], the \nSpark Core [https://www.spark.io/] is a relative newcomer. It launched via \nKickstarter\n[https://www.kickstarter.com/projects/sparkdevices/spark-core-wi-fi-for-everything-arduino-compatible]\n, picking up almost viral momentum early, due mostly to the use of the Texas\nInstruments CC3000 [http://www.ti.com/product/cc3000] wireless stack. Just last\nweek Spark announced an upgrade to the Core, called the Photon, which ditches\nthe CC3000 and is due in March 2015. While I am really excited about the Photon,\nlet us take a look at what the Core offers us today.\n\n\n--------------------------------------------------------------------------------\n\nBefore getting too far, I would like to talk a little about the brand. The name\nof the company is Spark, and the name of the device is the Core. Put them\ntogether and you get the Spark Core. This is one area I think the Photon will\nhelp Spark from a marketing perspective. With the arrival of its second device,\nSpark will really start to emerge as an IoT-enabler as opposed to a one-trick\npony.\n\nTI CC3000\nSecond, some details about the CC3000. This innovative chip picked up a lot of\ninterest mostly because wirelessly linking hardware to the Web is actually a\nreally challenging problem. With no keyboard or screen, setting SSID becomes a\nreal feat of engineering. If you are just a hobbyist, you can hard-code these\ncredentials, but in production, consumers are not going to update your code with\ntheir SSID, recompile, and flash new firmware.\n\nThe CC3000 series chipset uses a little loophole in 802.11 b/g networking, that\nallows you to set the SSID from another device on the target network. Notice the\ncatch there - 802.11 b/g. If you are running an 802.11 n network, as I was when\nI received my Spark Core, then you will be out of luck. Not to mention the now\ngrowing list of 802.11 ac implementations.\n\nTo address this, the Spark Photon moves to a Broadcom\n[http://www.broadcom.com/products/wiced/wifi/] chipset that provides \"soft\naccess point\" (Soft AP). Soft AP allows the device to setup its own wireless\nnetwork when it does not have an SSID specified, or cannot attach to the one\nthat has been specified. Once the Soft AP wireless has been established, you\nconnect your smartphone (or other device) to that network. You can then set the\nSSID information on the device, usually using a custom application, or web page.\n\nFor somebody looking to move from prototype to production, where the consumer's\nwireless network is unpredictable, this represents a big step forward. And the\nSpark Photon is not alone here, as the Google Nest [https://nest.com/] (among\nothers) also uses the same chipset. Oh, and the Broadcom chipset supports 802.11\nn networks as well.\n\nSpark Core\nOutside of the innovative CC3000, the Spark Core uses an ARM Cortex M3. Unlike\nthe Electric Imp however, the Core is Arduino compatible. This means that if you\nare already familiar with the the Wiring framework used by the Arduino, then you\nwill feel pretty comfortable with the Spark Core. And of course, you can use\nC/C++ to further extend your capabilities.\n\n\n\nThis does not mean however that all your favorite Arduino libraries can be used.\nIt also does not mean that the Arduino tooling workflow can be used. In fact,\nneither is true. To address this, the folks at Spark make many libraries\navailable in their cloud IDE. Many, but not all. You will have to import and\nmigrate any libraries not already supported.\n\nAs for the IDE, this was originally cloud-based only - just like the Electric\nImp. Unlike the Imp however, there is no debugging console for the cloud IDE.\nWith the Spark Photon announcement came the release of Spark Dev - a desktop IDE\nthat promotes Arduino-like development workflow. There is also a command-line\ntool chain available for working with your Spark Core.\n\nPro Tip: If you struggle with setting the SSID on your Spark Core using the\nSpark application, you can also set it over USB. I actually used the Arduino\nSerial Monitor for this. You send the Core a \"w\" with no line feed, and it will\nprompt you for SSID and password. Enter each followed by a newline.\n\nUpdating the firmware on the Spark Core happens wirelessly, which is great. You\nclick a button in the cloud IDE, it finds your Core, and uploads new firmware.\nUnfortunately, this process is not always reliable, sometimes requiring multiple\nattempts, or resetting of the Core entirely. I do love the RGB LED included on\nthe board however, as I could easily tell when problems arose, or when\neverything was going smoothly.\n\nCircuit Diagram\nThere is not much difference in this circuit than wiring it up with any other\ncontroller. You send it 3.3V, of which the Core has two sources, run through a\n10k resistor to ground, and take analog samples off the same thermistor/resistor\njunction. The Spark Core has a 12-bit DAC, which means 4,096 steps of\nresolution.\n\n\n\nDevice Code\nSince the Spark Core is Arduino compatible, the code from the perspective of\ninteracting with the sensor is similar to the Circuit Friday\n[http://kevinhoyt.com/blog/2014/11/07/circuit-friday-thermistor.html] example,\nwith a few constant changes for working with 3.3V and 12-bit DAC. I will skip\nover posting that code here again, and instead move onto the meat of\ncommunicating with the Parse.com system.\n\nSpark provides a number of value-add classes that you can leverage from the\nCore. There are classes to expose variables through a REST-based API. There are\nclasses to publish/subscribe to data from other Core devices. There are classes\nto learn all about the wireless environment. But for our purposes we are\ninterested in the TCPClient class (there is also TCPServer and even UDP\nsupport).\n\nTCPClient client;\n\n...\n\n// Loop\nvoid loop() \n{\n    double temperature = 0.0;\n    int    analog = 0;\n\n    // Analog reading\n    analog = analogRead( THERMISTOR );\n\n    // Thermistor reading as celcius\n    temperature = thermistor( analog );\n\n    // Store data in cloud\n    if( client.connect( KRHOYT_URL, KRHOYT_PORT ) )\n    {\n        request( temperature );\n        wait();\n        response();\n    }\n\n    // Wait for next sample\n    delay( UPDATE_RATE );\n} \n\n\nThe TCPClient is pretty similar to any other networking library you may have\nused on the Arduino. In this case a call to TCPClient.connect() will start a TCP\nconnection to a desired endpoint. The call is blocking, and returns a true/false\nupon completing. Once we know that the call has been completed successfully, we\ncan go about logging the data.\n\nNotice that I am using my own proxy server here, as opposed to going directly to \nParse.com [https://parse.com/]. The reason for this is that the Spark Core does\nnot have the horsepower to handle HTTPS connections. By offloading networking\nrequests to the cloud, Electric Imp overcomes this challenge, but introduces\ndependency on their cloud. By comparison the Spark Core makes network requests\ndirectly, but is then limited by the capabilities of the controller.\n\nvoid request( double value )\n{\n    // Buffers for string conversion\n    char celcius[10];\n    char farenheit[10];\n  \n    // Farenheit as character string\n    sprintf( celcius, \"%2.2f\", value );\n  \n    // Convert to celcius as character string\n    sprintf( farenheit, \"%2.2f\", value * 1.80 + 32 );\n\n    // Action - PARSE\n    // APP_ID\n    // Celcius\n    // Farenheit\n    // Data type - Temperature\n    // Operation - create\n    // REST key\n    // Unit - object\n    // Version - 1\n\n    sprintf( \n        buffer, \n        \"{\\\"action\\\":\\\"%s\\\",\\\"details\\\":{\\\"app_id\\\":\\\"%s\\\",\\\"data\\\":{\\\"celcius\\\":%s,\\\"farenheit\\\":%s},\\\"data_type\\\":\\\"%s\\\",\\\"operation\\\":\\\"%s\\\",\\\"rest_key\\\":\\\"%s\\\",\\\"unit\\\":\\\"%s\\\",\\\"version\\\":%s}}\", \n        KRHOYT_ACTION,\n        PARSE_APP,\n        celcius,\n        farenheit,\n        PARSE_TYPE,\n        KRHOYT_OPERATION,\n        PARSE_KEY,\n        KRHOYT_UNIT,\n        PARSE_VERSION\n    );\n\n    client.println( \"POST / HTTP/1.1\" );\n    client.println( \"Host: proxy.kevinhoyt.com\" );\n    client.print( \"Content-Length: \" );\n    client.println( strlen( buffer ) );\n    client.println( \"User-Agent: Spark Core\" );\n    client.println( \"Content-Type: text/plain;charset=UTF-8\" );\n    client.println();        \n    client.print( buffer );\n}\n\n\nMy custom-designed proxy has evolved over time to allow me to communicate with\nall my favorite cloud-based systems. To handle these variations, clients must\nPOST a JSON structure that provides instructions as to handling the destination.\nMy proxy system aside, you may have a non-HTTPS endpoint that needs JSON, so the\napproach would be similar if you were going directly from the Spark Core to any\nother API endpoint.\n\nOn the Arduino, I have become a big fan of the sprintf() function to format\ndata. While the string here is really long (character array actually), I can pop\nvalues into the JSON structure with ease. The sprintf() function also allows me\nto convert the temperature readings from double to character arrays.\n\nAt this point, what we have is a TCP connection. We are connected to a server,\nand ready to send data to it. To make this into an HTTP connection, we need to\nsend the appropriate values, in the correct form, across the wire. You can add\nheaders here, or improvise on them, as I have with the User-Agent header.\n\n// Wait for a response from proxy\nvoid wait()\n{\n    // Periodically check curl process\n    while( !client.available() ) \n    {\n        Serial.println( \"Waiting ...\" );\n        delay( 100 );\n    }\n}\n\n\nOnce the request bits have been sent, the server may take some time to respond.\nTo handle this we will sit in a loop, checking for available data. When data\narrives, execution will go back to the main loop, which will them move onto\nhandling the response. One might also use a finite state machine to handle this\nin the main loop, but I feel my code reads a bit cleaner with the secondary loop\napproach.\n\n// Response from proxy\nvoid response()\n{\n    char c;\n  \n    // While there is data to read\n    while( client.available() ) \n    {\n        // Get character\n        c = client.read();\n        Serial.print( c );\n    }\n  \n    client.stop();\n}\n\n\nAnd finally comes the response! So long as the server, in this case my proxy\nserver, is sending data back to the Spark Core, we will read those characters in\nand echo them to the serial port (USB). Once all the data has arrived, we will\nclose off the client connection by called client.stop().\n\nIn the case of Parse.com, the data coming back is in JSON format. We could\nfurther parse this data, change settings on the device. We would have to do this\nhowever, using our own implementation. We would also need to be aware of the\nmemory (or lack thereof) available to us. By comparison, the Electric Imp gives\nus a class to handle JSON out of the box.\n\nNext Steps\nMost of the shortcomings of the Spark Core can be overcome with software\nupdates, and it looks like the Spark team is hard at work doing exactly that.\nThose shortcomings that require hardware updates should be taken care of by the\nupcoming Spark Photon in March 2015.\n\nThere are many things to love about the Spark Core however including an\nattractive price, compact footprint, and the adorable RGB LED indicator. The\ndeveloper kit even comes with a battery and charging circuit shaped as a\nmustache. Because, why not!\n\nAs for going from prototype to production, I think the Spark Core (and Photon)\nhold a lot of promise, but they are just not there yet. However, if you are\nfamiliar with Arduino, and looking at a comfortable way to start playing with\nIoT, then it will be hard to beat the Spark Core. You could get a CC3000 shield\n[https://www.sparkfun.com/products/12071], but you would sacrifice size,\nconsiderable features, and cost.","feature_image":"http://images.kevinhoyt.com/tesla.coil.spark.jpg","featured":0,"status":"published","locale":null,"visibility":"public","author_id":"1","created_at":"2015-04-21T13:34:06.000Z","updated_at":"2015-04-21T13:47:13.000Z","published_at":"2014-11-18T15:00:00.000Z","custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"type":"post","email_recipient_filter":"none"},{"id":"5adb8922351ffe0018a57719","uuid":"e1909f31-2b79-4477-8350-1589ba3bba59","title":"Electric Imp to Parse.com","slug":"electric-imp-to-parse-com","mobiledoc":"{\"version\":\"0.3.1\",\"markups\":[],\"atoms\":[],\"cards\":[[\"markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\"*There are boards a plenty these days for connecting your custom hardware to the Internet. When it comes to development however, like many things, some are better tools for certain jobs than others. Every day this week I will post some details around boards that I have used, along with code on how to accomplish the same task for each. Today's IoT-enabler will be the [Electric Imp](http://electricimp.com/).*\\n\\n---\\n\\nBuilding off a recent [Circuit Friday](http://kevinhoyt.com/blog/2014/11/07/circuit-friday-thermistor.html) project, I will be using a thermistor as my example. I like the thermistor for testing because we get a general sense of data input, enough logic to see how the program looks, and it is a cheap and easy first project. By calculating both celcius and farenheit temperatures, we can also send more than one value to our data storage.\\n\\n### Parse.com\\n\\nAll this week, I will be using [Parse.com](https://parse.com/) as my data storage solution. Why Parse? Perhaps my favorite feature for Parse is a relatively easy to use REST API. They have wrappers for all your assorted needs too. Then, the dashboard for viewing our data is polished and capable, including filtering, access control lists, etc. And finally is the plethora of additional services that Parse offers for growing your business.\\n\\n### Electric Imp\\n\\nFor a long time, the Electric Imp was my favorite IoT-enabler. I have even taught a few workshops on how to use it. The Imp is an ARM Cortex M3 processor, running at 3.3V, and it includes wireless connectivity (802.11 b/g/n) using the same chip you will find in an iPhone. The [breakout board from SparkFun](https://www.sparkfun.com/products/12886) will give you hookups for battery and USB power, and six (6) GPIO pins.\\n\\n> The GPIO pins can be used in a variety of ways, from simple analog/digital IO to SPI and I2C.\\n\\n\\nAs if the chipset was not robust enough, it is the infrastructure that really shines. The Electric Imp cloud-based IDE lets you develop for your board, and see messages from it in real time. When it comes time to move to production, you can easily update all your deployed units. You can do it all at once, or even a phased roll out. Being able to solve these complex types of problems is one of the big hurdles in going from prototype to production, and the Imp lets you take off a rocket speed.\\n\\nAnother problem that you will have to solve when deploying a production device is how to set the wireless SSID and password for your user's network. The Imp uses an innovative \\\"BlinkUp\\\" process to accomplish this task. There are two parts to this system. First there is a phototransistor on the Imp. Second is an application for your phone that produces precisely time flashes of light. This effectively establishes a means of communication to the Imp, allowing you to set the SSID of your network.\\n\\n### A Few Caveats\\n\\nFor all the power and versatility that the Imp brings to the table, you will have make some trade-offs.\\n\\nThe first trade-off is that the programming language is [Squirrel](http://www.squirrel-lang.org/), which is not exactly a mainstream choice. This means there will be some learning curve in addition to the Imp API. Luckily the language is very similar to JavaScript, which makes it easy to pick up, and the Imp team does a great job on documentation as well.\\n\\nAnother trade-off might be in having to use the Imp cloud. All your lovely Internet traffic from your devices, will go through the Imp cloud before it ever gets to your servers. This can be a touch subject for some - and an outright show stopper for others. The flip side of the coin however is that awesome roll-out and production unit management feature.\\n\\nThen there is the BlinkUp. At the hands-on workshops where I have taught getting started with IoT using the Imp, this was easily the biggest hurdle. If you are using iOS, and can have very tightly controlled screen refreshes, then BlinkUp will be a snap. Android however, was far less likely to complete successfully. As for the Window Phone gang - they were just flat out of luck.\\n\\n> We did eventually find a skunkworks, native desktop Windows application that worked for most devices.\\n\\n### Circuit Diagram\\n\\nThere is not much difference here than from the Circuit Friday exercise using an Arduino. Since the Imp can assign different behaviors to different pins, I chose a Imp pin pretty much arbitrarily, and then set it in software. Also note that the Imp is running at 3.3V where the Arduino Uno is going to run at 3.3V or 5V depending on which pin you chose.\\n\\n![I like using the breadboard gap, but this can be far more compact.](http://images.kevinhoyt.com/imp.fritzing.thermistor.png)\\n\\n### Device Code\\n\\nWhen it comes to programming the Imp, there are two sides of the house you need to develop. The first is the code that runs on the Imp itself. I will start off by declaring constants for some of the temperature calculation math that needs to happen along the way.\\n\\n```\\n// Constants\\nconst ANALOG_STEPS = 65534.0;\\nconst B_THERM = 3977.0;\\nconst T_THERM = 298.15;\\nconst RESISTOR = 10000.0;\\nconst THERMISTOR_OHM = 10000.0;\\n```\\n\\nUp next I define a constant for what I call events. Events are how the Imp device code communicates with the Imp server code (called agents). You do not need to make constants, but I am picky like that.\\n\\n```\\n// Events\\nconst EVENT_TEMPERATURE = \\\"temperature\\\";\\n```\\n\\nUp next comes the pin assignment - in this case, pin 8 on the breakout will be used for analog input. While I could refer to \\\"hardware.pin8\\\" every time I wanted to access the pin, I have a created a little shortcut and assigned that value to another value called \\\"thermistor\\\". This lets me change pins without having to crawl my code. I also think it makes things a little easier to read.\\n\\n```\\n// Configure hardware pin\\nthermistor <- hardware.pin8;\\nthermistor.configure( ANALOG_IN );\\n```\\n\\nTemperature calculation is up next, and this is pretty much the exact same calculation as used on the Arduino with a couple significant tweaks. First is that since we are using 3.3V, some of the math will change. Also, the Imp gives us 16-bit readings on the analog input, so we have far more steps. And finally, the Imp will actually tell us the real voltage being used, which will make the math more accurate.\\n\\n```\\n// Calculate celcius temperature\\nfunction temperature( analog ) \\n{\\n    local kelvin = 0.0;\\n    local l_therm = 0.0;\\n    local r_therm = 0.0;    \\n    local v_in = 0.0;\\n    local v_out = 0.0;\\n\\n    // Imp voltage\\n    v_in = hardware.voltage();\\n\\n    // Thermistor calculations\\n    v_out = v_in * analog / ANALOG_STEPS;\\n    r_therm = ( RESISTOR * v_in / v_out ) - RESISTOR;\\n  \\n    l_therm = math.log( THERMISTOR_OHM / r_therm );\\n    kelvin = ( T_THERM * B_THERM ) / ( B_THERM - T_THERM * l_therm );    \\n\\n    // Celcius value\\n    return kelvin - 273.15;\\n}\\n```\\n\\nThe Imp does not have an infinite loop like the Arduino - and this is a good thing. In the long run, Imp gives you very fine-grained control over power consumption. I have had an Imp wake up every five minutes, same temperature, log it on the Internet, and then go back to sleep, and last for more than six months on a single 2,500 mAh battery.\\n\\nIf you do not tell the Imp to loop, it will not - it will execute the code from top to bottom, and then terminate. Since I want to report the temperature every few seconds in this example, the \\\"poll\\\" function will act as the entry point. When put together with the imp.wakeup() function, a loop gets created, but one that only really executes at the pace we want. No faster, no slower, saving us power.\\n\\n```\\n// Called to poll temperature\\n// Sends results to agent for storage\\nfunction poll() \\n{\\n    local analog = 0;\\n    local celcius = 0.0;\\n    local farenheit = 0.0;\\n\\n    // Analog reading\\n    analog = thermistor.read();\\n\\n    // Voltage to temperature values\\n    celcius = temperature( analog );\\n    farenheit = celcius * 9.0 / 5.0 + 32.0;\\n\\n\\t// Log to server\\n\\t// Send to agent for data storage\\n\\tserver.log( farenheit );\\n\\tagent.send( \\\"temperature\\\", {\\n        \\\"celcius\\\": celcius,\\n        \\\"farenheit\\\": farenheit\\n\\t} );\\n\\t\\n    // Do it again\\n\\timp.wakeup( 5.0, poll );\\n}\\n```\\n\\nThe poll() function will not get called automatically, so the very last line is to call poll() directly. Once inside the function, the imp.wakeup() call will tell the Imp to come back to the poll() function at some interval in the future (centi-second resolution). You can think of this as a setTimeout() call in JavaScript.\\n\\n```\\npoll();\\n```\\n\\n### Agent Code\\n\\nThroughout the device code you will see calls to the \\\"agent\\\" object. The agent represents work that happens on the Electric Imp servers in the cloud. They can do all fashion of robust actions that would otherwise drain your Imp. For example, agents can store persistent, per device, preferences. Agents are where the communication to the Internet actually takes place.\\n\\nTo get from the device to the agent, the device will send the agent an event with some data attached to it. Names for events can be arbitrary, and the value passed along can be as simple as an integer, or as complex as nested data structures. The event my agent will expect from the device for temperature reporting is defined in a constant.\\n\\n```\\n// Events\\n// Parse.com constants removed for blog post\\nconst EVENT_TEMPERATURE = \\\"temperature\\\";\\n```\\n\\nListening for events at the agent takes the form of “device.on()” calls. As you can tell from the device code, the device invokes an event on the agent using the “agent.send()” call. The inverse for both exists as well. The agent can send an event to the device, and the device can listen to events from the server.\\n\\n```\\n// Listener for thermistor sensor events\\ndevice.on( EVENT_TEMPERATURE, function( tableData ) {\\n    ...\\n}\\n```\\n\\nThe Imp API gives you HTTP objects to work with for talking to the Internet. To store an object using the Parse.com REST API, we send an HTTP POST. The POST needs some headers, and some body content encoded as JSON. This is easily accomplished using the Imp API.\\n\\n```\\n// POST data to server\\n// Using Parse.com\\nlocal parse = http.post( \\n    PARSE_COM,\\n    {\\n        \\\"X-Parse-Application-Id\\\": PARSE_APP,\\n        \\\"X-Parse-REST-API-Key\\\": PARSE_KEY,\\n        \\\"Content-Type\\\": \\\"application/json\\\"\\n    },\\n    http.jsonencode( tableData ) \\n).sendasync(\\n    // Log server response\\n    function( response ) {\\n        server.log( response.body );\\n    } \\n);   \\n```\\n\\nThe HTTP POST is made asynchronously, and when the response comes in, another function is invoked. The Parse.com REST API returns JSON in most cases, and this is no different. The Imp API would allow us to parse that JSON, and take additional action based on the result. In this case, I am just logging the response to the Imp IDE so we can see that something actually happened.\\n\\nI want to reiterate the dual nature of the Imp cloud. The agent here can make asynchronous calls, and then more calls based on the response. When dealing with JSON, this would be downright prohibitive to do on many embedded systems. Not to mention the battery drain of processing all those strings. Depending on the size of the response, you would likely also run out of memory. By offloading these functions to the Imp cloud, you dramatically simplify your approach. And we haven't even talked out handing offline scenarios, which you would also have to effectively write yourself without the Imp cloud.\\n\\n### Next Steps\\n\\nIf you are just getting started with IoT, the Imp is a great place to start, but it may be overkill. It is really designed for those heading to full-scale production. The Imp solves many of the very complex problems you have to solve to bring a device to market. As for production, my only nit-pick would be the reliability of the BlinkUp technology.\\n\\nI have really only started to touch upon the abilities of the Imp in this post. I wanted to give you an overview of the system, and some sample code to get going, but there is a lot more you can do with an Imp. You can explore low power modes, setup long-term persistent storage, handle offline operation, send commands from outside the Imp cloud to the Imp agent, and so much more.\\n\"}]],\"sections\":[[10,0]],\"ghostVersion\":\"3.0\"}","html":"<!--kg-card-begin: markdown--><p><em>There are boards a plenty these days for connecting your custom hardware to the Internet. When it comes to development however, like many things, some are better tools for certain jobs than others. Every day this week I will post some details around boards that I have used, along with code on how to accomplish the same task for each. Today's IoT-enabler will be the <a href=\"http://electricimp.com/\">Electric Imp</a>.</em></p>\n<hr>\n<p>Building off a recent <a href=\"http://kevinhoyt.com/blog/2014/11/07/circuit-friday-thermistor.html\">Circuit Friday</a> project, I will be using a thermistor as my example. I like the thermistor for testing because we get a general sense of data input, enough logic to see how the program looks, and it is a cheap and easy first project. By calculating both celcius and farenheit temperatures, we can also send more than one value to our data storage.</p>\n<h3 id=\"parsecom\">Parse.com</h3>\n<p>All this week, I will be using <a href=\"https://parse.com/\">Parse.com</a> as my data storage solution. Why Parse? Perhaps my favorite feature for Parse is a relatively easy to use REST API. They have wrappers for all your assorted needs too. Then, the dashboard for viewing our data is polished and capable, including filtering, access control lists, etc. And finally is the plethora of additional services that Parse offers for growing your business.</p>\n<h3 id=\"electricimp\">Electric Imp</h3>\n<p>For a long time, the Electric Imp was my favorite IoT-enabler. I have even taught a few workshops on how to use it. The Imp is an ARM Cortex M3 processor, running at 3.3V, and it includes wireless connectivity (802.11 b/g/n) using the same chip you will find in an iPhone. The <a href=\"https://www.sparkfun.com/products/12886\">breakout board from SparkFun</a> will give you hookups for battery and USB power, and six (6) GPIO pins.</p>\n<blockquote>\n<p>The GPIO pins can be used in a variety of ways, from simple analog/digital IO to SPI and I2C.</p>\n</blockquote>\n<p>As if the chipset was not robust enough, it is the infrastructure that really shines. The Electric Imp cloud-based IDE lets you develop for your board, and see messages from it in real time. When it comes time to move to production, you can easily update all your deployed units. You can do it all at once, or even a phased roll out. Being able to solve these complex types of problems is one of the big hurdles in going from prototype to production, and the Imp lets you take off a rocket speed.</p>\n<p>Another problem that you will have to solve when deploying a production device is how to set the wireless SSID and password for your user's network. The Imp uses an innovative &quot;BlinkUp&quot; process to accomplish this task. There are two parts to this system. First there is a phototransistor on the Imp. Second is an application for your phone that produces precisely time flashes of light. This effectively establishes a means of communication to the Imp, allowing you to set the SSID of your network.</p>\n<h3 id=\"afewcaveats\">A Few Caveats</h3>\n<p>For all the power and versatility that the Imp brings to the table, you will have make some trade-offs.</p>\n<p>The first trade-off is that the programming language is <a href=\"http://www.squirrel-lang.org/\">Squirrel</a>, which is not exactly a mainstream choice. This means there will be some learning curve in addition to the Imp API. Luckily the language is very similar to JavaScript, which makes it easy to pick up, and the Imp team does a great job on documentation as well.</p>\n<p>Another trade-off might be in having to use the Imp cloud. All your lovely Internet traffic from your devices, will go through the Imp cloud before it ever gets to your servers. This can be a touch subject for some - and an outright show stopper for others. The flip side of the coin however is that awesome roll-out and production unit management feature.</p>\n<p>Then there is the BlinkUp. At the hands-on workshops where I have taught getting started with IoT using the Imp, this was easily the biggest hurdle. If you are using iOS, and can have very tightly controlled screen refreshes, then BlinkUp will be a snap. Android however, was far less likely to complete successfully. As for the Window Phone gang - they were just flat out of luck.</p>\n<blockquote>\n<p>We did eventually find a skunkworks, native desktop Windows application that worked for most devices.</p>\n</blockquote>\n<h3 id=\"circuitdiagram\">Circuit Diagram</h3>\n<p>There is not much difference here than from the Circuit Friday exercise using an Arduino. Since the Imp can assign different behaviors to different pins, I chose a Imp pin pretty much arbitrarily, and then set it in software. Also note that the Imp is running at 3.3V where the Arduino Uno is going to run at 3.3V or 5V depending on which pin you chose.</p>\n<p><img src=\"http://images.kevinhoyt.com/imp.fritzing.thermistor.png\" alt=\"I like using the breadboard gap, but this can be far more compact.\" loading=\"lazy\"></p>\n<h3 id=\"devicecode\">Device Code</h3>\n<p>When it comes to programming the Imp, there are two sides of the house you need to develop. The first is the code that runs on the Imp itself. I will start off by declaring constants for some of the temperature calculation math that needs to happen along the way.</p>\n<pre><code>// Constants\nconst ANALOG_STEPS = 65534.0;\nconst B_THERM = 3977.0;\nconst T_THERM = 298.15;\nconst RESISTOR = 10000.0;\nconst THERMISTOR_OHM = 10000.0;\n</code></pre>\n<p>Up next I define a constant for what I call events. Events are how the Imp device code communicates with the Imp server code (called agents). You do not need to make constants, but I am picky like that.</p>\n<pre><code>// Events\nconst EVENT_TEMPERATURE = &quot;temperature&quot;;\n</code></pre>\n<p>Up next comes the pin assignment - in this case, pin 8 on the breakout will be used for analog input. While I could refer to &quot;hardware.pin8&quot; every time I wanted to access the pin, I have a created a little shortcut and assigned that value to another value called &quot;thermistor&quot;. This lets me change pins without having to crawl my code. I also think it makes things a little easier to read.</p>\n<pre><code>// Configure hardware pin\nthermistor &lt;- hardware.pin8;\nthermistor.configure( ANALOG_IN );\n</code></pre>\n<p>Temperature calculation is up next, and this is pretty much the exact same calculation as used on the Arduino with a couple significant tweaks. First is that since we are using 3.3V, some of the math will change. Also, the Imp gives us 16-bit readings on the analog input, so we have far more steps. And finally, the Imp will actually tell us the real voltage being used, which will make the math more accurate.</p>\n<pre><code>// Calculate celcius temperature\nfunction temperature( analog ) \n{\n    local kelvin = 0.0;\n    local l_therm = 0.0;\n    local r_therm = 0.0;    \n    local v_in = 0.0;\n    local v_out = 0.0;\n\n    // Imp voltage\n    v_in = hardware.voltage();\n\n    // Thermistor calculations\n    v_out = v_in * analog / ANALOG_STEPS;\n    r_therm = ( RESISTOR * v_in / v_out ) - RESISTOR;\n  \n    l_therm = math.log( THERMISTOR_OHM / r_therm );\n    kelvin = ( T_THERM * B_THERM ) / ( B_THERM - T_THERM * l_therm );    \n\n    // Celcius value\n    return kelvin - 273.15;\n}\n</code></pre>\n<p>The Imp does not have an infinite loop like the Arduino - and this is a good thing. In the long run, Imp gives you very fine-grained control over power consumption. I have had an Imp wake up every five minutes, same temperature, log it on the Internet, and then go back to sleep, and last for more than six months on a single 2,500 mAh battery.</p>\n<p>If you do not tell the Imp to loop, it will not - it will execute the code from top to bottom, and then terminate. Since I want to report the temperature every few seconds in this example, the &quot;poll&quot; function will act as the entry point. When put together with the imp.wakeup() function, a loop gets created, but one that only really executes at the pace we want. No faster, no slower, saving us power.</p>\n<pre><code>// Called to poll temperature\n// Sends results to agent for storage\nfunction poll() \n{\n    local analog = 0;\n    local celcius = 0.0;\n    local farenheit = 0.0;\n\n    // Analog reading\n    analog = thermistor.read();\n\n    // Voltage to temperature values\n    celcius = temperature( analog );\n    farenheit = celcius * 9.0 / 5.0 + 32.0;\n\n\t// Log to server\n\t// Send to agent for data storage\n\tserver.log( farenheit );\n\tagent.send( &quot;temperature&quot;, {\n        &quot;celcius&quot;: celcius,\n        &quot;farenheit&quot;: farenheit\n\t} );\n\t\n    // Do it again\n\timp.wakeup( 5.0, poll );\n}\n</code></pre>\n<p>The poll() function will not get called automatically, so the very last line is to call poll() directly. Once inside the function, the imp.wakeup() call will tell the Imp to come back to the poll() function at some interval in the future (centi-second resolution). You can think of this as a setTimeout() call in JavaScript.</p>\n<pre><code>poll();\n</code></pre>\n<h3 id=\"agentcode\">Agent Code</h3>\n<p>Throughout the device code you will see calls to the &quot;agent&quot; object. The agent represents work that happens on the Electric Imp servers in the cloud. They can do all fashion of robust actions that would otherwise drain your Imp. For example, agents can store persistent, per device, preferences. Agents are where the communication to the Internet actually takes place.</p>\n<p>To get from the device to the agent, the device will send the agent an event with some data attached to it. Names for events can be arbitrary, and the value passed along can be as simple as an integer, or as complex as nested data structures. The event my agent will expect from the device for temperature reporting is defined in a constant.</p>\n<pre><code>// Events\n// Parse.com constants removed for blog post\nconst EVENT_TEMPERATURE = &quot;temperature&quot;;\n</code></pre>\n<p>Listening for events at the agent takes the form of “device.on()” calls. As you can tell from the device code, the device invokes an event on the agent using the “agent.send()” call. The inverse for both exists as well. The agent can send an event to the device, and the device can listen to events from the server.</p>\n<pre><code>// Listener for thermistor sensor events\ndevice.on( EVENT_TEMPERATURE, function( tableData ) {\n    ...\n}\n</code></pre>\n<p>The Imp API gives you HTTP objects to work with for talking to the Internet. To store an object using the Parse.com REST API, we send an HTTP POST. The POST needs some headers, and some body content encoded as JSON. This is easily accomplished using the Imp API.</p>\n<pre><code>// POST data to server\n// Using Parse.com\nlocal parse = http.post( \n    PARSE_COM,\n    {\n        &quot;X-Parse-Application-Id&quot;: PARSE_APP,\n        &quot;X-Parse-REST-API-Key&quot;: PARSE_KEY,\n        &quot;Content-Type&quot;: &quot;application/json&quot;\n    },\n    http.jsonencode( tableData ) \n).sendasync(\n    // Log server response\n    function( response ) {\n        server.log( response.body );\n    } \n);   \n</code></pre>\n<p>The HTTP POST is made asynchronously, and when the response comes in, another function is invoked. The Parse.com REST API returns JSON in most cases, and this is no different. The Imp API would allow us to parse that JSON, and take additional action based on the result. In this case, I am just logging the response to the Imp IDE so we can see that something actually happened.</p>\n<p>I want to reiterate the dual nature of the Imp cloud. The agent here can make asynchronous calls, and then more calls based on the response. When dealing with JSON, this would be downright prohibitive to do on many embedded systems. Not to mention the battery drain of processing all those strings. Depending on the size of the response, you would likely also run out of memory. By offloading these functions to the Imp cloud, you dramatically simplify your approach. And we haven't even talked out handing offline scenarios, which you would also have to effectively write yourself without the Imp cloud.</p>\n<h3 id=\"nextsteps\">Next Steps</h3>\n<p>If you are just getting started with IoT, the Imp is a great place to start, but it may be overkill. It is really designed for those heading to full-scale production. The Imp solves many of the very complex problems you have to solve to bring a device to market. As for production, my only nit-pick would be the reliability of the BlinkUp technology.</p>\n<p>I have really only started to touch upon the abilities of the Imp in this post. I wanted to give you an overview of the system, and some sample code to get going, but there is a lot more you can do with an Imp. You can explore low power modes, setup long-term persistent storage, handle offline operation, send commands from outside the Imp cloud to the Imp agent, and so much more.</p>\n<!--kg-card-end: markdown-->","comment_id":"13","plaintext":"There are boards a plenty these days for connecting your custom hardware to the\nInternet. When it comes to development however, like many things, some are\nbetter tools for certain jobs than others. Every day this week I will post some\ndetails around boards that I have used, along with code on how to accomplish the\nsame task for each. Today's IoT-enabler will be the Electric Imp\n[http://electricimp.com/].\n\n\n--------------------------------------------------------------------------------\n\nBuilding off a recent Circuit Friday\n[http://kevinhoyt.com/blog/2014/11/07/circuit-friday-thermistor.html] project, I\nwill be using a thermistor as my example. I like the thermistor for testing\nbecause we get a general sense of data input, enough logic to see how the\nprogram looks, and it is a cheap and easy first project. By calculating both\ncelcius and farenheit temperatures, we can also send more than one value to our\ndata storage.\n\nParse.com\nAll this week, I will be using Parse.com [https://parse.com/] as my data storage\nsolution. Why Parse? Perhaps my favorite feature for Parse is a relatively easy\nto use REST API. They have wrappers for all your assorted needs too. Then, the\ndashboard for viewing our data is polished and capable, including filtering,\naccess control lists, etc. And finally is the plethora of additional services\nthat Parse offers for growing your business.\n\nElectric Imp\nFor a long time, the Electric Imp was my favorite IoT-enabler. I have even\ntaught a few workshops on how to use it. The Imp is an ARM Cortex M3 processor,\nrunning at 3.3V, and it includes wireless connectivity (802.11 b/g/n) using the\nsame chip you will find in an iPhone. The breakout board from SparkFun\n[https://www.sparkfun.com/products/12886] will give you hookups for battery and\nUSB power, and six (6) GPIO pins.\n\n> The GPIO pins can be used in a variety of ways, from simple analog/digital IO to\nSPI and I2C.\n\n\nAs if the chipset was not robust enough, it is the infrastructure that really\nshines. The Electric Imp cloud-based IDE lets you develop for your board, and\nsee messages from it in real time. When it comes time to move to production, you\ncan easily update all your deployed units. You can do it all at once, or even a\nphased roll out. Being able to solve these complex types of problems is one of\nthe big hurdles in going from prototype to production, and the Imp lets you take\noff a rocket speed.\n\nAnother problem that you will have to solve when deploying a production device\nis how to set the wireless SSID and password for your user's network. The Imp\nuses an innovative \"BlinkUp\" process to accomplish this task. There are two\nparts to this system. First there is a phototransistor on the Imp. Second is an\napplication for your phone that produces precisely time flashes of light. This\neffectively establishes a means of communication to the Imp, allowing you to set\nthe SSID of your network.\n\nA Few Caveats\nFor all the power and versatility that the Imp brings to the table, you will\nhave make some trade-offs.\n\nThe first trade-off is that the programming language is Squirrel\n[http://www.squirrel-lang.org/], which is not exactly a mainstream choice. This\nmeans there will be some learning curve in addition to the Imp API. Luckily the\nlanguage is very similar to JavaScript, which makes it easy to pick up, and the\nImp team does a great job on documentation as well.\n\nAnother trade-off might be in having to use the Imp cloud. All your lovely\nInternet traffic from your devices, will go through the Imp cloud before it ever\ngets to your servers. This can be a touch subject for some - and an outright\nshow stopper for others. The flip side of the coin however is that awesome\nroll-out and production unit management feature.\n\nThen there is the BlinkUp. At the hands-on workshops where I have taught getting\nstarted with IoT using the Imp, this was easily the biggest hurdle. If you are\nusing iOS, and can have very tightly controlled screen refreshes, then BlinkUp\nwill be a snap. Android however, was far less likely to complete successfully.\nAs for the Window Phone gang - they were just flat out of luck.\n\n> We did eventually find a skunkworks, native desktop Windows application that\nworked for most devices.\n\n\nCircuit Diagram\nThere is not much difference here than from the Circuit Friday exercise using an\nArduino. Since the Imp can assign different behaviors to different pins, I chose\na Imp pin pretty much arbitrarily, and then set it in software. Also note that\nthe Imp is running at 3.3V where the Arduino Uno is going to run at 3.3V or 5V\ndepending on which pin you chose.\n\n\n\nDevice Code\nWhen it comes to programming the Imp, there are two sides of the house you need\nto develop. The first is the code that runs on the Imp itself. I will start off\nby declaring constants for some of the temperature calculation math that needs\nto happen along the way.\n\n// Constants\nconst ANALOG_STEPS = 65534.0;\nconst B_THERM = 3977.0;\nconst T_THERM = 298.15;\nconst RESISTOR = 10000.0;\nconst THERMISTOR_OHM = 10000.0;\n\n\nUp next I define a constant for what I call events. Events are how the Imp\ndevice code communicates with the Imp server code (called agents). You do not\nneed to make constants, but I am picky like that.\n\n// Events\nconst EVENT_TEMPERATURE = \"temperature\";\n\n\nUp next comes the pin assignment - in this case, pin 8 on the breakout will be\nused for analog input. While I could refer to \"hardware.pin8\" every time I\nwanted to access the pin, I have a created a little shortcut and assigned that\nvalue to another value called \"thermistor\". This lets me change pins without\nhaving to crawl my code. I also think it makes things a little easier to read.\n\n// Configure hardware pin\nthermistor <- hardware.pin8;\nthermistor.configure( ANALOG_IN );\n\n\nTemperature calculation is up next, and this is pretty much the exact same\ncalculation as used on the Arduino with a couple significant tweaks. First is\nthat since we are using 3.3V, some of the math will change. Also, the Imp gives\nus 16-bit readings on the analog input, so we have far more steps. And finally,\nthe Imp will actually tell us the real voltage being used, which will make the\nmath more accurate.\n\n// Calculate celcius temperature\nfunction temperature( analog ) \n{\n    local kelvin = 0.0;\n    local l_therm = 0.0;\n    local r_therm = 0.0;    \n    local v_in = 0.0;\n    local v_out = 0.0;\n\n    // Imp voltage\n    v_in = hardware.voltage();\n\n    // Thermistor calculations\n    v_out = v_in * analog / ANALOG_STEPS;\n    r_therm = ( RESISTOR * v_in / v_out ) - RESISTOR;\n  \n    l_therm = math.log( THERMISTOR_OHM / r_therm );\n    kelvin = ( T_THERM * B_THERM ) / ( B_THERM - T_THERM * l_therm );    \n\n    // Celcius value\n    return kelvin - 273.15;\n}\n\n\nThe Imp does not have an infinite loop like the Arduino - and this is a good\nthing. In the long run, Imp gives you very fine-grained control over power\nconsumption. I have had an Imp wake up every five minutes, same temperature, log\nit on the Internet, and then go back to sleep, and last for more than six months\non a single 2,500 mAh battery.\n\nIf you do not tell the Imp to loop, it will not - it will execute the code from\ntop to bottom, and then terminate. Since I want to report the temperature every\nfew seconds in this example, the \"poll\" function will act as the entry point.\nWhen put together with the imp.wakeup() function, a loop gets created, but one\nthat only really executes at the pace we want. No faster, no slower, saving us\npower.\n\n// Called to poll temperature\n// Sends results to agent for storage\nfunction poll() \n{\n    local analog = 0;\n    local celcius = 0.0;\n    local farenheit = 0.0;\n\n    // Analog reading\n    analog = thermistor.read();\n\n    // Voltage to temperature values\n    celcius = temperature( analog );\n    farenheit = celcius * 9.0 / 5.0 + 32.0;\n\n\t// Log to server\n\t// Send to agent for data storage\n\tserver.log( farenheit );\n\tagent.send( \"temperature\", {\n        \"celcius\": celcius,\n        \"farenheit\": farenheit\n\t} );\n\t\n    // Do it again\n\timp.wakeup( 5.0, poll );\n}\n\n\nThe poll() function will not get called automatically, so the very last line is\nto call poll() directly. Once inside the function, the imp.wakeup() call will\ntell the Imp to come back to the poll() function at some interval in the future\n(centi-second resolution). You can think of this as a setTimeout() call in\nJavaScript.\n\npoll();\n\n\nAgent Code\nThroughout the device code you will see calls to the \"agent\" object. The agent\nrepresents work that happens on the Electric Imp servers in the cloud. They can\ndo all fashion of robust actions that would otherwise drain your Imp. For\nexample, agents can store persistent, per device, preferences. Agents are where\nthe communication to the Internet actually takes place.\n\nTo get from the device to the agent, the device will send the agent an event\nwith some data attached to it. Names for events can be arbitrary, and the value\npassed along can be as simple as an integer, or as complex as nested data\nstructures. The event my agent will expect from the device for temperature\nreporting is defined in a constant.\n\n// Events\n// Parse.com constants removed for blog post\nconst EVENT_TEMPERATURE = \"temperature\";\n\n\nListening for events at the agent takes the form of “device.on()” calls. As you\ncan tell from the device code, the device invokes an event on the agent using\nthe “agent.send()” call. The inverse for both exists as well. The agent can send\nan event to the device, and the device can listen to events from the server.\n\n// Listener for thermistor sensor events\ndevice.on( EVENT_TEMPERATURE, function( tableData ) {\n    ...\n}\n\n\nThe Imp API gives you HTTP objects to work with for talking to the Internet. To\nstore an object using the Parse.com REST API, we send an HTTP POST. The POST\nneeds some headers, and some body content encoded as JSON. This is easily\naccomplished using the Imp API.\n\n// POST data to server\n// Using Parse.com\nlocal parse = http.post( \n    PARSE_COM,\n    {\n        \"X-Parse-Application-Id\": PARSE_APP,\n        \"X-Parse-REST-API-Key\": PARSE_KEY,\n        \"Content-Type\": \"application/json\"\n    },\n    http.jsonencode( tableData ) \n).sendasync(\n    // Log server response\n    function( response ) {\n        server.log( response.body );\n    } \n);   \n\n\nThe HTTP POST is made asynchronously, and when the response comes in, another\nfunction is invoked. The Parse.com REST API returns JSON in most cases, and this\nis no different. The Imp API would allow us to parse that JSON, and take\nadditional action based on the result. In this case, I am just logging the\nresponse to the Imp IDE so we can see that something actually happened.\n\nI want to reiterate the dual nature of the Imp cloud. The agent here can make\nasynchronous calls, and then more calls based on the response. When dealing with\nJSON, this would be downright prohibitive to do on many embedded systems. Not to\nmention the battery drain of processing all those strings. Depending on the size\nof the response, you would likely also run out of memory. By offloading these\nfunctions to the Imp cloud, you dramatically simplify your approach. And we\nhaven't even talked out handing offline scenarios, which you would also have to\neffectively write yourself without the Imp cloud.\n\nNext Steps\nIf you are just getting started with IoT, the Imp is a great place to start, but\nit may be overkill. It is really designed for those heading to full-scale\nproduction. The Imp solves many of the very complex problems you have to solve\nto bring a device to market. As for production, my only nit-pick would be the\nreliability of the BlinkUp technology.\n\nI have really only started to touch upon the abilities of the Imp in this post.\nI wanted to give you an overview of the system, and some sample code to get\ngoing, but there is a lot more you can do with an Imp. You can explore low power\nmodes, setup long-term persistent storage, handle offline operation, send\ncommands from outside the Imp cloud to the Imp agent, and so much more.","feature_image":"http://images.kevinhoyt.com/lightning.jpg","featured":0,"status":"published","locale":null,"visibility":"public","author_id":"1","created_at":"2015-04-21T13:47:58.000Z","updated_at":"2015-04-21T13:57:41.000Z","published_at":"2014-11-17T15:00:00.000Z","custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"type":"post","email_recipient_filter":"none"},{"id":"5adb8922351ffe0018a5771a","uuid":"e19def59-301c-4554-bf28-b5cc8c622a9e","title":"Circuit Friday: DHT22","slug":"circuit-friday-dht22","mobiledoc":"{\"version\":\"0.3.1\",\"markups\":[],\"atoms\":[],\"cards\":[[\"markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\"*Last week for [Circuit Friday](http://kevinhoyt.com/blog/2014/11/07/circuit-friday-thermistor.html) I introduced temperature sensing with a thermistor - effectively a resistor whose resistance changes based on temperature. We were then able to read the variation of current through the circuit, apply some math, and come up with a temperature. At a high level, this is analog input.*\\n\\n*Today I thought I would keep with the temperature sensing theme, but introduce some new concepts. Coming at you this week is the DHT22 package which can measure both temperature and humidity. Along the way we will learn about 1-Wire communications.*\\n\\n---\\n\\nThe DHT22 goes by several names. The other common name is the RHT22. Both are effectively the same, and will set you back $10 USD over at [SparkFun](https://www.sparkfun.com/products/10167). The data sheet says that it will run from 3.3V to 6V which makes it ideal for Arduino application. Temperature range is -40°C to 80°C with 0% to 100% relative humidity.\\n\\n![DHT22 (photo courtesy SparkFun.)](http://images.kevinhoyt.com/dht22.jpg)\\n\\nThe DHT22 package has four (4) pins. One for power, one for ground, one for signal, and one that is effectively null. That signal pin will hook to a digital pin on the Arduino (or other). Wait a second! How can a digital (on/off) pin tell us the analog voltage for temperature readings? And would not there be a pin for humidity analog as well?\\n\\n> 1-Wire communication was designed by Dallas Semiconductor, and can be found in the Java Ring and iButton.\\n\\nThis is where the concept of [1-Wire](http://en.wikipedia.org/wiki/1-Wire) communications comes into play. 1-Wire communication was designed by Dallas Semiconductor, and you can find it in many applications - usually where quick, unique identification is required. Notable products include the Java Ring and iButton, and it is widely used in smart ticket and public transportation applications.\\n\\nYou can think of 1-Wire communication like Morse Code. If you toggle a digital pin high and low at various rates, you can effectively create a system that represents instructions to the desired package (DHT22 in this case). Another analogy might be the letter \\\"A\\\" in a document, that is effectively stored as a series of bits - zero/ones for low/high digital signals.\\n\\nThe problem in the electrical engineering world around this type of communication is that it has effectively become a differentiator. If you can make one type of chip/sensor, you can make many other types. To keep people on your chips then, you tweak the digital communication protocol. In the end, there are several different types of this digital communication out there.\\n\\n### Circuit Diagram\\n\\nSurprisingly, I have seen the DHT22 package wired up in various ways. And to be sure, the resistor value will change depending on the voltage you are using to power the DHT22. At the lower voltage range (3.3V), I have read about people struggling more than with 5V. Sometimes this is because of unreliable sourcing, other times it is line noise from other components.\\n\\nWhat follows is the way that works for me, using a third-party library that works for me. Your milage may vary should you decide to try something a little different.\\n\\n![DHT22 connected to Arduino.](http://images.kevinhoyt.com/fritzing.dht.png)\\n\\nThe wiring is not too different from the thermistor of last week. The DHT22 gets power and ground through dedicated pins. The signal runs from another pin. A pull-down resistor is needed across the power pin and the signal pin to help draw the voltage to zero when set to low. This gives us a clean signal for the DHT22 communication.\\n\\n### Arduino Uno\\n\\nEarlier we talked about the 1-Wire protocol, and how there are various approaches on the market. When it comes to the DHT22, it does not even truly conform to the 1-Wire specification. This is a problem. The data sheet explains how the protocol works, and there are numerous libraries for Arduino that implement it. Some work better than others. For this exercise, I will be using the DHT library by [Rob Tillaart](https://github.com/RobTillaart/Arduino).\\n\\n> If you have the Arduino IDE open, you will need to close it and start it again to get it to pick up new libraries.\\n\\nTo use third-party libraries with the Arduino IDE, you first need to download the code. The Arduino IDE will look in Documents -> Arduino -> libraries on Mac and My Documents -> Arduino -> libraries on Windows for third-party libraries. In this case, take the libraries -> DHTlib from the download, and place it in the appropriate folder for your operating system.\\n\\n```\\n// Third-party library\\n// https://github.com/RobTillaart/Arduino\\n#include <dht.h>\\n\\n// Literals\\n#define DHT_PIN 5\\n\\n// DHT instance\\ndht DHT;\\n\\n// Setup\\nvoid setup()\\n{\\n  // Serial communication\\n  Serial.begin( 9600 );\\n}\\n\\n// Loop\\nvoid loop()\\n{\\n  int check;\\n\\n  // Sensor for reference\\n  Serial.print( \\\"DHT22,\\\" );\\n\\n  // Library version for reference  \\n  Serial.print( DHT_LIB_VERSION );  \\n  Serial.print( \\\",\\\" );  \\n\\n  // Read the DHT data\\n  // Return from call is checksum\\n  // Checks for correct communication\\n  check = DHT.read22( DHT_PIN );\\n\\n  // Check for errors\\n  switch( check )\\n  {\\n    case DHTLIB_OK: \\n      Serial.print( \\\"OK,\\\" ); \\n      break;\\n    case DHTLIB_ERROR_CHECKSUM: \\n      Serial.print( \\\"CHECKSUM,\\\" ); \\n      break;\\n    case DHTLIB_ERROR_TIMEOUT: \\n      Serial.print( \\\"TIME_OUT\\\" ); \\n      break;\\n    default: \\n      Serial.print( \\\"UNKNOWN\\\" ); \\n      break;\\n  }\\n\\n  // Display humidity, celcius, farenheit\\n  Serial.print( DHT.humidity, 1 );\\n  Serial.print( \\\",\\\" );\\n  Serial.print( DHT.temperature, 1 );\\n  Serial.print( \\\",\\\" );\\n  Serial.println( ( DHT.temperature * 1.8 ) + 32, 1 );    \\n\\n  // Wait for next reading\\n  delay( 1000 );\\n}\\n```\\n\\nThe first line of our Arduino code includes the DHT library. The name used corresponds to the name of the library code you placed in the Arduino IDE library folder. We need that one signal pin for use with the library, and then we set up a reference to the DHT object (from the library).\\n\\nOur setup code simply sets up serial communication for the USB port.\\n\\nIn the loop, we use the DHT object to read a value from the DHT22 itself. Various messages can come back from the DHT22, so we need to evaluate what we got back. At this point the DHT instance has the temperature and humidity readings in properties. We can then display these values over the serial port (USB).\\n\\nThat is it! Load the program onto your Arduino, and open the serial monitor to see the temperature and humidity. If you get errors along the way, leave a comment, and I will try to help you out. Usually, problems will arise either from the placement of the library, or the wiring.\\n\\n### Next Steps\\n\\nWhy 1-Wire or similar communication protocol? Eventually, as the complexity of your projects grow, you will likely run out of pins to use. At that point you will be thankful that such techniques actually exist. The approach can also make your circuit easier to wire in the first place. With some implementations you can string together many packages without having the need for additional pins.\\n\\nThe open source nature of Arduino has led to libraries for just about every device you might want to use. If you find problems, I would encourage you to get involved, contribute and keep the community rolling forward. If you implemented something new, then share it!\\n\"}]],\"sections\":[[10,0]],\"ghostVersion\":\"3.0\"}","html":"<!--kg-card-begin: markdown--><p><em>Last week for <a href=\"http://kevinhoyt.com/blog/2014/11/07/circuit-friday-thermistor.html\">Circuit Friday</a> I introduced temperature sensing with a thermistor - effectively a resistor whose resistance changes based on temperature. We were then able to read the variation of current through the circuit, apply some math, and come up with a temperature. At a high level, this is analog input.</em></p>\n<p><em>Today I thought I would keep with the temperature sensing theme, but introduce some new concepts. Coming at you this week is the DHT22 package which can measure both temperature and humidity. Along the way we will learn about 1-Wire communications.</em></p>\n<hr>\n<p>The DHT22 goes by several names. The other common name is the RHT22. Both are effectively the same, and will set you back $10 USD over at <a href=\"https://www.sparkfun.com/products/10167\">SparkFun</a>. The data sheet says that it will run from 3.3V to 6V which makes it ideal for Arduino application. Temperature range is -40°C to 80°C with 0% to 100% relative humidity.</p>\n<p><img src=\"http://images.kevinhoyt.com/dht22.jpg\" alt=\"DHT22 (photo courtesy SparkFun.)\" loading=\"lazy\"></p>\n<p>The DHT22 package has four (4) pins. One for power, one for ground, one for signal, and one that is effectively null. That signal pin will hook to a digital pin on the Arduino (or other). Wait a second! How can a digital (on/off) pin tell us the analog voltage for temperature readings? And would not there be a pin for humidity analog as well?</p>\n<blockquote>\n<p>1-Wire communication was designed by Dallas Semiconductor, and can be found in the Java Ring and iButton.</p>\n</blockquote>\n<p>This is where the concept of <a href=\"http://en.wikipedia.org/wiki/1-Wire\">1-Wire</a> communications comes into play. 1-Wire communication was designed by Dallas Semiconductor, and you can find it in many applications - usually where quick, unique identification is required. Notable products include the Java Ring and iButton, and it is widely used in smart ticket and public transportation applications.</p>\n<p>You can think of 1-Wire communication like Morse Code. If you toggle a digital pin high and low at various rates, you can effectively create a system that represents instructions to the desired package (DHT22 in this case). Another analogy might be the letter &quot;A&quot; in a document, that is effectively stored as a series of bits - zero/ones for low/high digital signals.</p>\n<p>The problem in the electrical engineering world around this type of communication is that it has effectively become a differentiator. If you can make one type of chip/sensor, you can make many other types. To keep people on your chips then, you tweak the digital communication protocol. In the end, there are several different types of this digital communication out there.</p>\n<h3 id=\"circuitdiagram\">Circuit Diagram</h3>\n<p>Surprisingly, I have seen the DHT22 package wired up in various ways. And to be sure, the resistor value will change depending on the voltage you are using to power the DHT22. At the lower voltage range (3.3V), I have read about people struggling more than with 5V. Sometimes this is because of unreliable sourcing, other times it is line noise from other components.</p>\n<p>What follows is the way that works for me, using a third-party library that works for me. Your milage may vary should you decide to try something a little different.</p>\n<p><img src=\"http://images.kevinhoyt.com/fritzing.dht.png\" alt=\"DHT22 connected to Arduino.\" loading=\"lazy\"></p>\n<p>The wiring is not too different from the thermistor of last week. The DHT22 gets power and ground through dedicated pins. The signal runs from another pin. A pull-down resistor is needed across the power pin and the signal pin to help draw the voltage to zero when set to low. This gives us a clean signal for the DHT22 communication.</p>\n<h3 id=\"arduinouno\">Arduino Uno</h3>\n<p>Earlier we talked about the 1-Wire protocol, and how there are various approaches on the market. When it comes to the DHT22, it does not even truly conform to the 1-Wire specification. This is a problem. The data sheet explains how the protocol works, and there are numerous libraries for Arduino that implement it. Some work better than others. For this exercise, I will be using the DHT library by <a href=\"https://github.com/RobTillaart/Arduino\">Rob Tillaart</a>.</p>\n<blockquote>\n<p>If you have the Arduino IDE open, you will need to close it and start it again to get it to pick up new libraries.</p>\n</blockquote>\n<p>To use third-party libraries with the Arduino IDE, you first need to download the code. The Arduino IDE will look in Documents -&gt; Arduino -&gt; libraries on Mac and My Documents -&gt; Arduino -&gt; libraries on Windows for third-party libraries. In this case, take the libraries -&gt; DHTlib from the download, and place it in the appropriate folder for your operating system.</p>\n<pre><code>// Third-party library\n// https://github.com/RobTillaart/Arduino\n#include &lt;dht.h&gt;\n\n// Literals\n#define DHT_PIN 5\n\n// DHT instance\ndht DHT;\n\n// Setup\nvoid setup()\n{\n  // Serial communication\n  Serial.begin( 9600 );\n}\n\n// Loop\nvoid loop()\n{\n  int check;\n\n  // Sensor for reference\n  Serial.print( &quot;DHT22,&quot; );\n\n  // Library version for reference  \n  Serial.print( DHT_LIB_VERSION );  \n  Serial.print( &quot;,&quot; );  \n\n  // Read the DHT data\n  // Return from call is checksum\n  // Checks for correct communication\n  check = DHT.read22( DHT_PIN );\n\n  // Check for errors\n  switch( check )\n  {\n    case DHTLIB_OK: \n      Serial.print( &quot;OK,&quot; ); \n      break;\n    case DHTLIB_ERROR_CHECKSUM: \n      Serial.print( &quot;CHECKSUM,&quot; ); \n      break;\n    case DHTLIB_ERROR_TIMEOUT: \n      Serial.print( &quot;TIME_OUT&quot; ); \n      break;\n    default: \n      Serial.print( &quot;UNKNOWN&quot; ); \n      break;\n  }\n\n  // Display humidity, celcius, farenheit\n  Serial.print( DHT.humidity, 1 );\n  Serial.print( &quot;,&quot; );\n  Serial.print( DHT.temperature, 1 );\n  Serial.print( &quot;,&quot; );\n  Serial.println( ( DHT.temperature * 1.8 ) + 32, 1 );    \n\n  // Wait for next reading\n  delay( 1000 );\n}\n</code></pre>\n<p>The first line of our Arduino code includes the DHT library. The name used corresponds to the name of the library code you placed in the Arduino IDE library folder. We need that one signal pin for use with the library, and then we set up a reference to the DHT object (from the library).</p>\n<p>Our setup code simply sets up serial communication for the USB port.</p>\n<p>In the loop, we use the DHT object to read a value from the DHT22 itself. Various messages can come back from the DHT22, so we need to evaluate what we got back. At this point the DHT instance has the temperature and humidity readings in properties. We can then display these values over the serial port (USB).</p>\n<p>That is it! Load the program onto your Arduino, and open the serial monitor to see the temperature and humidity. If you get errors along the way, leave a comment, and I will try to help you out. Usually, problems will arise either from the placement of the library, or the wiring.</p>\n<h3 id=\"nextsteps\">Next Steps</h3>\n<p>Why 1-Wire or similar communication protocol? Eventually, as the complexity of your projects grow, you will likely run out of pins to use. At that point you will be thankful that such techniques actually exist. The approach can also make your circuit easier to wire in the first place. With some implementations you can string together many packages without having the need for additional pins.</p>\n<p>The open source nature of Arduino has led to libraries for just about every device you might want to use. If you find problems, I would encourage you to get involved, contribute and keep the community rolling forward. If you implemented something new, then share it!</p>\n<!--kg-card-end: markdown-->","comment_id":"14","plaintext":"Last week for Circuit Friday\n[http://kevinhoyt.com/blog/2014/11/07/circuit-friday-thermistor.html] I\nintroduced temperature sensing with a thermistor - effectively a resistor whose\nresistance changes based on temperature. We were then able to read the variation\nof current through the circuit, apply some math, and come up with a temperature.\nAt a high level, this is analog input.\n\nToday I thought I would keep with the temperature sensing theme, but introduce\nsome new concepts. Coming at you this week is the DHT22 package which can\nmeasure both temperature and humidity. Along the way we will learn about 1-Wire\ncommunications.\n\n\n--------------------------------------------------------------------------------\n\nThe DHT22 goes by several names. The other common name is the RHT22. Both are\neffectively the same, and will set you back $10 USD over at SparkFun\n[https://www.sparkfun.com/products/10167]. The data sheet says that it will run\nfrom 3.3V to 6V which makes it ideal for Arduino application. Temperature range\nis -40°C to 80°C with 0% to 100% relative humidity.\n\n\n\nThe DHT22 package has four (4) pins. One for power, one for ground, one for\nsignal, and one that is effectively null. That signal pin will hook to a digital\npin on the Arduino (or other). Wait a second! How can a digital (on/off) pin\ntell us the analog voltage for temperature readings? And would not there be a\npin for humidity analog as well?\n\n> 1-Wire communication was designed by Dallas Semiconductor, and can be found in\nthe Java Ring and iButton.\n\n\nThis is where the concept of 1-Wire [http://en.wikipedia.org/wiki/1-Wire] \ncommunications comes into play. 1-Wire communication was designed by Dallas\nSemiconductor, and you can find it in many applications - usually where quick,\nunique identification is required. Notable products include the Java Ring and\niButton, and it is widely used in smart ticket and public transportation\napplications.\n\nYou can think of 1-Wire communication like Morse Code. If you toggle a digital\npin high and low at various rates, you can effectively create a system that\nrepresents instructions to the desired package (DHT22 in this case). Another\nanalogy might be the letter \"A\" in a document, that is effectively stored as a\nseries of bits - zero/ones for low/high digital signals.\n\nThe problem in the electrical engineering world around this type of\ncommunication is that it has effectively become a differentiator. If you can\nmake one type of chip/sensor, you can make many other types. To keep people on\nyour chips then, you tweak the digital communication protocol. In the end, there\nare several different types of this digital communication out there.\n\nCircuit Diagram\nSurprisingly, I have seen the DHT22 package wired up in various ways. And to be\nsure, the resistor value will change depending on the voltage you are using to\npower the DHT22. At the lower voltage range (3.3V), I have read about people\nstruggling more than with 5V. Sometimes this is because of unreliable sourcing,\nother times it is line noise from other components.\n\nWhat follows is the way that works for me, using a third-party library that\nworks for me. Your milage may vary should you decide to try something a little\ndifferent.\n\n\n\nThe wiring is not too different from the thermistor of last week. The DHT22 gets\npower and ground through dedicated pins. The signal runs from another pin. A\npull-down resistor is needed across the power pin and the signal pin to help\ndraw the voltage to zero when set to low. This gives us a clean signal for the\nDHT22 communication.\n\nArduino Uno\nEarlier we talked about the 1-Wire protocol, and how there are various\napproaches on the market. When it comes to the DHT22, it does not even truly\nconform to the 1-Wire specification. This is a problem. The data sheet explains\nhow the protocol works, and there are numerous libraries for Arduino that\nimplement it. Some work better than others. For this exercise, I will be using\nthe DHT library by Rob Tillaart [https://github.com/RobTillaart/Arduino].\n\n> If you have the Arduino IDE open, you will need to close it and start it again\nto get it to pick up new libraries.\n\n\nTo use third-party libraries with the Arduino IDE, you first need to download\nthe code. The Arduino IDE will look in Documents -> Arduino -> libraries on Mac\nand My Documents -> Arduino -> libraries on Windows for third-party libraries.\nIn this case, take the libraries -> DHTlib from the download, and place it in\nthe appropriate folder for your operating system.\n\n// Third-party library\n// https://github.com/RobTillaart/Arduino\n#include <dht.h>\n\n// Literals\n#define DHT_PIN 5\n\n// DHT instance\ndht DHT;\n\n// Setup\nvoid setup()\n{\n  // Serial communication\n  Serial.begin( 9600 );\n}\n\n// Loop\nvoid loop()\n{\n  int check;\n\n  // Sensor for reference\n  Serial.print( \"DHT22,\" );\n\n  // Library version for reference  \n  Serial.print( DHT_LIB_VERSION );  \n  Serial.print( \",\" );  \n\n  // Read the DHT data\n  // Return from call is checksum\n  // Checks for correct communication\n  check = DHT.read22( DHT_PIN );\n\n  // Check for errors\n  switch( check )\n  {\n    case DHTLIB_OK: \n      Serial.print( \"OK,\" ); \n      break;\n    case DHTLIB_ERROR_CHECKSUM: \n      Serial.print( \"CHECKSUM,\" ); \n      break;\n    case DHTLIB_ERROR_TIMEOUT: \n      Serial.print( \"TIME_OUT\" ); \n      break;\n    default: \n      Serial.print( \"UNKNOWN\" ); \n      break;\n  }\n\n  // Display humidity, celcius, farenheit\n  Serial.print( DHT.humidity, 1 );\n  Serial.print( \",\" );\n  Serial.print( DHT.temperature, 1 );\n  Serial.print( \",\" );\n  Serial.println( ( DHT.temperature * 1.8 ) + 32, 1 );    \n\n  // Wait for next reading\n  delay( 1000 );\n}\n\n\nThe first line of our Arduino code includes the DHT library. The name used\ncorresponds to the name of the library code you placed in the Arduino IDE\nlibrary folder. We need that one signal pin for use with the library, and then\nwe set up a reference to the DHT object (from the library).\n\nOur setup code simply sets up serial communication for the USB port.\n\nIn the loop, we use the DHT object to read a value from the DHT22 itself.\nVarious messages can come back from the DHT22, so we need to evaluate what we\ngot back. At this point the DHT instance has the temperature and humidity\nreadings in properties. We can then display these values over the serial port\n(USB).\n\nThat is it! Load the program onto your Arduino, and open the serial monitor to\nsee the temperature and humidity. If you get errors along the way, leave a\ncomment, and I will try to help you out. Usually, problems will arise either\nfrom the placement of the library, or the wiring.\n\nNext Steps\nWhy 1-Wire or similar communication protocol? Eventually, as the complexity of\nyour projects grow, you will likely run out of pins to use. At that point you\nwill be thankful that such techniques actually exist. The approach can also make\nyour circuit easier to wire in the first place. With some implementations you\ncan string together many packages without having the need for additional pins.\n\nThe open source nature of Arduino has led to libraries for just about every\ndevice you might want to use. If you find problems, I would encourage you to get\ninvolved, contribute and keep the community rolling forward. If you implemented\nsomething new, then share it!","feature_image":"http://images.kevinhoyt.com/wire.rope.jpg","featured":0,"status":"published","locale":null,"visibility":"public","author_id":"1","created_at":"2015-04-21T13:58:28.000Z","updated_at":"2015-04-21T14:04:35.000Z","published_at":"2014-11-14T16:00:00.000Z","custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"type":"post","email_recipient_filter":"none"},{"id":"5adb8922351ffe0018a5771b","uuid":"d53a923a-d308-4d19-9593-e76071dbbe7d","title":"Tic Tac Toe Light","slug":"tic-tac-toe-light","mobiledoc":"{\"version\":\"0.3.1\",\"markups\":[],\"atoms\":[],\"cards\":[[\"markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\"*I had the great privilege of being a speaker at [HTML5 Developer Conference](http://html5devconf.com/) in San Francisco recently.  It was the second HTML5 Dev Conf I have presented at, with the first one being October 2013.  This time, I paired with [Frank Greco](https://twitter.com/frankgreco) to present a session entitled \\\"[WebSockets: Past, Present and Future](http://html5devconf.com/speakers/frank_greco.html#session)\\\".  Frank took the stage for the first half of the session, and I followed up with some hands-on Internet of Things (IoT) demonstrations that were integrated with [Kaazing Gateway](http://kaazing.com/).*\\n\\n---\\n\\n==This is reposted on the Kaazing corporate [blog](http://blog.kaazing.com/2014/06/04/real-time-tic-tac-toe-light/).==\\n\\n### Introducing the Tic Tac Toe Light\\n\\nMy personal favorite demonstration was a project I called the \\\"Tic Tac Toe Light\\\".  I called it this because the custom-built enclosure houses nine (9) [Adafuit NeoPixels](http://www.adafruit.com/products/1312) in a three-by-three (3×3) grid.  The enclosure, made using [foam core board](https://flic.kr/p/nNeFJz) and a hot knife, also contained an Arduino Yun.  I have grown to be a big fan of the [Arduino Yun](http://arduino.cc/en/Main/ArduinoBoardYun) for real-time IoT/web projects.  The board is the same profile as an Arduino Uno, but includes integrated wireless (802.11 b/g/n), an ATmega32u4 (similar to the Arduino Leonardo), and a Linux system on a chip (SoC).\\n\\n![Tic Tac Toe Light in action](http://images.kevinhoyt.com/tic.tac.toe.action.jpg)\\n\\n![Inner-workings of the Tic Tac Toe Light](http://images.kevinhoyt.com/tic.tac.toe.guts.jpg)\\n\\n![Tic Tac Toe Light on the Web](http://images.kevinhoyt.com/tic.tac.toe.web.png)\\n\\nUsing a [web-based user interface](http://tictactoe.kevinhoyt.com/), attendees of the HTM5 Dev Conf session could use their laptop, tablet or smartphone to control each NeoPixel (RGB LED) in the enclosure.  At the same time, the web user interface kept in sync with all the attendees selections – across all screens.  The Arduino Yun was also listening on a real-time connection for color change messages, which is how it knew what lights to change to what colors.\\n\\n### Why Kaazing Gateway\\n\\nI think the bigger question here is \\\"Why real-time?\\\"  Although I do not know the exact count, I would say that the session had nearly 200 attendees.  The ATmega32u4 has a clock speed of 16 MHz with 32 KB of RAM.  If all those attendees were selecting light colors at anywhere near the same time using HTTP, the Arduino would be crushed under the load.  In a real-time scenario however, there is but one connection, and about twenty (20) bytes of data for each color change.  The end result was a far more scalable solution.\\n\\n![Tic Tac Toe Light live on stage.](http://images.kevinhoyt.com/tic.tac.toe.live.png)\\n\\nAnd it had to scale too!  The lights on the Tic Tac Toe box were [blinking wildly](https://vine.co/v/MwaYH6EqUBL) for the duration of the time I had it plugged in (before I had to move on to my next demonstration).\\n\\nCan you imagine the user experience over HTTP, even if the 16 MHz chip could handle the load?  You would select a color, and at some interval later, the color would be set.  That lag however would leave you wondering \\\"Was that my color selection?\\\"  This as compared to an instant response using Kaazing Gateway, even over conference wireless.  Not to mention keeping all the other connected users in sync.  The additional HTTP polling load for that would make the whole project come to a crawl (or just crash).\\n\\n### Next Steps\\n\\nThe 3×3 grid was actually happenstance – I happened to have ten (10) NeoPixels on hand in my component drawer.  I wanted a square, so 3×3 it was.  This led to the name of Tic Tac Toe.  But then I started to wonder.  What if this was the physical manifestation of two players in an actual game of tic-tac-toe?  Or even better yet, maybe [artificial intelligence](http://www.zdnet.com/ibm-to-open-up-watson-to-third-party-developers-7000023194/) (AI) on the server was playing the other side in real-time!\\n\\nThis is where I would like to take the project next.  If you want to see the code for the project, you can hop on over to my [GitHub account](https://github.com/krhoyt/Kaazing/tree/master/tictactoe) where I have posted more details, as well as code itself for the Arduino Yun and the web client.  The fabrication plans are also posted there should you want to take on a project like this yourself.  If you have any questions, feel free to hit me up on [Twitter](https://twitter.com/krhoyt), or drop a comment below.\\n\\n### Credits\\n\\nThanks [Matthias Schroeder](https://twitter.com/IxDesigner) for the [Vine video](https://vine.co/v/MwaYH6EqUBL) of Tic Tac Toe in action during the session.\\n\"}]],\"sections\":[[10,0]],\"ghostVersion\":\"3.0\"}","html":"<!--kg-card-begin: markdown--><p><em>I had the great privilege of being a speaker at <a href=\"http://html5devconf.com/\">HTML5 Developer Conference</a> in San Francisco recently.  It was the second HTML5 Dev Conf I have presented at, with the first one being October 2013.  This time, I paired with <a href=\"https://twitter.com/frankgreco\">Frank Greco</a> to present a session entitled &quot;<a href=\"http://html5devconf.com/speakers/frank_greco.html#session\">WebSockets: Past, Present and Future</a>&quot;.  Frank took the stage for the first half of the session, and I followed up with some hands-on Internet of Things (IoT) demonstrations that were integrated with <a href=\"http://kaazing.com/\">Kaazing Gateway</a>.</em></p>\n<hr>\n<p><mark>This is reposted on the Kaazing corporate <a href=\"http://blog.kaazing.com/2014/06/04/real-time-tic-tac-toe-light/\">blog</a>.</mark></p>\n<h3 id=\"introducingthetictactoelight\">Introducing the Tic Tac Toe Light</h3>\n<p>My personal favorite demonstration was a project I called the &quot;Tic Tac Toe Light&quot;.  I called it this because the custom-built enclosure houses nine (9) <a href=\"http://www.adafruit.com/products/1312\">Adafuit NeoPixels</a> in a three-by-three (3×3) grid.  The enclosure, made using <a href=\"https://flic.kr/p/nNeFJz\">foam core board</a> and a hot knife, also contained an Arduino Yun.  I have grown to be a big fan of the <a href=\"http://arduino.cc/en/Main/ArduinoBoardYun\">Arduino Yun</a> for real-time IoT/web projects.  The board is the same profile as an Arduino Uno, but includes integrated wireless (802.11 b/g/n), an ATmega32u4 (similar to the Arduino Leonardo), and a Linux system on a chip (SoC).</p>\n<p><img src=\"http://images.kevinhoyt.com/tic.tac.toe.action.jpg\" alt=\"Tic Tac Toe Light in action\" loading=\"lazy\"></p>\n<p><img src=\"http://images.kevinhoyt.com/tic.tac.toe.guts.jpg\" alt=\"Inner-workings of the Tic Tac Toe Light\" loading=\"lazy\"></p>\n<p><img src=\"http://images.kevinhoyt.com/tic.tac.toe.web.png\" alt=\"Tic Tac Toe Light on the Web\" loading=\"lazy\"></p>\n<p>Using a <a href=\"http://tictactoe.kevinhoyt.com/\">web-based user interface</a>, attendees of the HTM5 Dev Conf session could use their laptop, tablet or smartphone to control each NeoPixel (RGB LED) in the enclosure.  At the same time, the web user interface kept in sync with all the attendees selections – across all screens.  The Arduino Yun was also listening on a real-time connection for color change messages, which is how it knew what lights to change to what colors.</p>\n<h3 id=\"whykaazinggateway\">Why Kaazing Gateway</h3>\n<p>I think the bigger question here is &quot;Why real-time?&quot;  Although I do not know the exact count, I would say that the session had nearly 200 attendees.  The ATmega32u4 has a clock speed of 16 MHz with 32 KB of RAM.  If all those attendees were selecting light colors at anywhere near the same time using HTTP, the Arduino would be crushed under the load.  In a real-time scenario however, there is but one connection, and about twenty (20) bytes of data for each color change.  The end result was a far more scalable solution.</p>\n<p><img src=\"http://images.kevinhoyt.com/tic.tac.toe.live.png\" alt=\"Tic Tac Toe Light live on stage.\" loading=\"lazy\"></p>\n<p>And it had to scale too!  The lights on the Tic Tac Toe box were <a href=\"https://vine.co/v/MwaYH6EqUBL\">blinking wildly</a> for the duration of the time I had it plugged in (before I had to move on to my next demonstration).</p>\n<p>Can you imagine the user experience over HTTP, even if the 16 MHz chip could handle the load?  You would select a color, and at some interval later, the color would be set.  That lag however would leave you wondering &quot;Was that my color selection?&quot;  This as compared to an instant response using Kaazing Gateway, even over conference wireless.  Not to mention keeping all the other connected users in sync.  The additional HTTP polling load for that would make the whole project come to a crawl (or just crash).</p>\n<h3 id=\"nextsteps\">Next Steps</h3>\n<p>The 3×3 grid was actually happenstance – I happened to have ten (10) NeoPixels on hand in my component drawer.  I wanted a square, so 3×3 it was.  This led to the name of Tic Tac Toe.  But then I started to wonder.  What if this was the physical manifestation of two players in an actual game of tic-tac-toe?  Or even better yet, maybe <a href=\"http://www.zdnet.com/ibm-to-open-up-watson-to-third-party-developers-7000023194/\">artificial intelligence</a> (AI) on the server was playing the other side in real-time!</p>\n<p>This is where I would like to take the project next.  If you want to see the code for the project, you can hop on over to my <a href=\"https://github.com/krhoyt/Kaazing/tree/master/tictactoe\">GitHub account</a> where I have posted more details, as well as code itself for the Arduino Yun and the web client.  The fabrication plans are also posted there should you want to take on a project like this yourself.  If you have any questions, feel free to hit me up on <a href=\"https://twitter.com/krhoyt\">Twitter</a>, or drop a comment below.</p>\n<h3 id=\"credits\">Credits</h3>\n<p>Thanks <a href=\"https://twitter.com/IxDesigner\">Matthias Schroeder</a> for the <a href=\"https://vine.co/v/MwaYH6EqUBL\">Vine video</a> of Tic Tac Toe in action during the session.</p>\n<!--kg-card-end: markdown-->","comment_id":"15","plaintext":"I had the great privilege of being a speaker at HTML5 Developer Conference\n[http://html5devconf.com/] in San Francisco recently. It was the second HTML5\nDev Conf I have presented at, with the first one being October 2013. This time,\nI paired with Frank Greco [https://twitter.com/frankgreco] to present a session\nentitled \"WebSockets: Past, Present and Future\n[http://html5devconf.com/speakers/frank_greco.html#session]\". Frank took the\nstage for the first half of the session, and I followed up with some hands-on\nInternet of Things (IoT) demonstrations that were integrated with Kaazing\nGateway [http://kaazing.com/].\n\n\n--------------------------------------------------------------------------------\n\nThis is reposted on the Kaazing corporate blog\n[http://blog.kaazing.com/2014/06/04/real-time-tic-tac-toe-light/].\n\nIntroducing the Tic Tac Toe Light\nMy personal favorite demonstration was a project I called the \"Tic Tac Toe\nLight\". I called it this because the custom-built enclosure houses nine (9) \nAdafuit NeoPixels [http://www.adafruit.com/products/1312] in a three-by-three\n(3×3) grid. The enclosure, made using foam core board [https://flic.kr/p/nNeFJz] \nand a hot knife, also contained an Arduino Yun. I have grown to be a big fan of\nthe Arduino Yun [http://arduino.cc/en/Main/ArduinoBoardYun] for real-time\nIoT/web projects. The board is the same profile as an Arduino Uno, but includes\nintegrated wireless (802.11 b/g/n), an ATmega32u4 (similar to the Arduino\nLeonardo), and a Linux system on a chip (SoC).\n\n\n\n\n\n\n\nUsing a web-based user interface [http://tictactoe.kevinhoyt.com/], attendees of\nthe HTM5 Dev Conf session could use their laptop, tablet or smartphone to\ncontrol each NeoPixel (RGB LED) in the enclosure. At the same time, the web user\ninterface kept in sync with all the attendees selections – across all screens.\nThe Arduino Yun was also listening on a real-time connection for color change\nmessages, which is how it knew what lights to change to what colors.\n\nWhy Kaazing Gateway\nI think the bigger question here is \"Why real-time?\" Although I do not know the\nexact count, I would say that the session had nearly 200 attendees. The\nATmega32u4 has a clock speed of 16 MHz with 32 KB of RAM. If all those attendees\nwere selecting light colors at anywhere near the same time using HTTP, the\nArduino would be crushed under the load. In a real-time scenario however, there\nis but one connection, and about twenty (20) bytes of data for each color\nchange. The end result was a far more scalable solution.\n\n\n\nAnd it had to scale too! The lights on the Tic Tac Toe box were blinking wildly\n[https://vine.co/v/MwaYH6EqUBL] for the duration of the time I had it plugged in\n(before I had to move on to my next demonstration).\n\nCan you imagine the user experience over HTTP, even if the 16 MHz chip could\nhandle the load? You would select a color, and at some interval later, the color\nwould be set. That lag however would leave you wondering \"Was that my color\nselection?\" This as compared to an instant response using Kaazing Gateway, even\nover conference wireless. Not to mention keeping all the other connected users\nin sync. The additional HTTP polling load for that would make the whole project\ncome to a crawl (or just crash).\n\nNext Steps\nThe 3×3 grid was actually happenstance – I happened to have ten (10) NeoPixels\non hand in my component drawer. I wanted a square, so 3×3 it was. This led to\nthe name of Tic Tac Toe. But then I started to wonder. What if this was the\nphysical manifestation of two players in an actual game of tic-tac-toe? Or even\nbetter yet, maybe artificial intelligence\n[http://www.zdnet.com/ibm-to-open-up-watson-to-third-party-developers-7000023194/] \n(AI) on the server was playing the other side in real-time!\n\nThis is where I would like to take the project next. If you want to see the code\nfor the project, you can hop on over to my GitHub account\n[https://github.com/krhoyt/Kaazing/tree/master/tictactoe] where I have posted\nmore details, as well as code itself for the Arduino Yun and the web client. The\nfabrication plans are also posted there should you want to take on a project\nlike this yourself. If you have any questions, feel free to hit me up on Twitter\n[https://twitter.com/krhoyt], or drop a comment below.\n\nCredits\nThanks Matthias Schroeder [https://twitter.com/IxDesigner] for the Vine video\n[https://vine.co/v/MwaYH6EqUBL] of Tic Tac Toe in action during the session.","feature_image":"__GHOST_URL__/content/images/2020/06/tic.tac.toe.chalk.jpg","featured":0,"status":"published","locale":null,"visibility":"public","author_id":"1","created_at":"2015-04-21T14:38:48.000Z","updated_at":"2020-06-29T17:30:58.000Z","published_at":"2014-06-04T14:00:00.000Z","custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"type":"post","email_recipient_filter":"none"},{"id":"5adb8922351ffe0018a5771c","uuid":"cdb7574f-d35d-4962-94f7-5f976166fa07","title":"Web Chat Application","slug":"building-a-chat-application","mobiledoc":"{\"version\":\"0.3.1\",\"markups\":[],\"atoms\":[],\"cards\":[[\"markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\"*In any given programming language \\\"Hello World\\\" is likely the first thing you will learn to do. The \\\"Hello World\\\" for real-time architectures is a chat application. In this post you will build your very own Web-based chat application. Alternatively, you might also want to check out the Quick Start guide to get a little background.*\\n\\n---\\n\\n==This is reposted on the [Kaazing Open Source Blog](http://kaazing.org/blog/building-a-chat-application/).==\\n\\n### Connecting\\n\\nThe first thing we need to do to get real-time communication for our chat application is to connect to a server. As outlined in the Quick Start Guide, in a publish/subscribe architecture, the server is called a \\\"broker\\\". The broker manages connecting incoming messages, with those wanting to receive messages. More on that later.\\n\\n```\\nvar kaazing = Gateway.connect( null, doConnected );\\n```\\n\\nWhile this may look like a simple call, there is a lot that goes on in the background. Brokers speak various enterprise protocols such as AMQP, XMPP, and MQTT. The browser does not speak broker, or protocols. It does speak WebSocket. And if your browser does not speak WebSocket, it does speak HTTP. Managing that connection, and securely communicating via standard protocols, are among the tasks handled for you.\\n\\n### Subscribing\\n\\nThis call will connect to the broker and establish outgoing and incoming message buckets on the server. At this point, the outgoing bucket (called an exchange) will send messages on to any incoming message buckets (called queues) it has been told about. However, our outgoing bucket has not yet been told about the incoming bucket. To do this, we must subscribe for messages.\\n\\n```\\nfunction doConnected()\\n{\\n  kaazing.on( Gateway.EVENT_MESSAGE, doMessage );\\n  kaazing.subscribe( CHAT_TOPIC );\\n}\\n```\\n\\nOnce our incoming bucket is subscribed to the outgoing bucket, we will begin to receive messages. The underlying protocol we are using for this example is AMQP, which refers to this process as \\\"binding\\\". When the two buckets are bound, messages that match a specific \\\"routing key\\\" are forwarded on from the exchange (outgoing) to the queue (incoming). If you are familiar with other messaging protocols, you can think of the routing key as roughly analogous to a \\\"topic\\\" in this configuration.\\n\\nIf you have an enterprise software background, you may be familiar with these concepts from enterprise messaging.\\n\\nKeep in mind that the routing happens on the server - and that is a good thing. You do not want every message regardless of routing to be sent to the client, for it to figure out how to deal with the content. At scale, the browser would simply be overloaded. Having a broker on the server also gives us features such as guaranteed delivery, flow control (pause/resume), and durable messaging (persistence between restarts).\\n\\n### Publishing\\n\\nWhen somebody in a chat wants to send a message, that is called \\\"publishing\\\". To publish a message, and have it arrive to all the interested parties, you send the content using a specific routing key. Again, you may also think of the routing key as a topic in other messaging protocols.\\n\\n```\\nkaazing.publish( CHAT_TOPIC, message );\\n```\\n\\nWhile AMQP actually sends messages in binary, it is far more commonplace with JavaScript to work with String data types. String content is the expected message content. The AMQP library takes care of resolving the actual binary message. If you want to send complex data structures, you can use JSON.stringify() to get a String representation first. Do not forget to use JSON.parse() with the message arrives.\\n\\n### Shortcut\\n\\nKnowing that chat is the \\\"Hello World\\\" of real-time communication, we have made getting started as easy as possible - there is a shortcut to building your first chat application. To use this, first put \\\"gateway.js\\\" into your HTML document. This loads all the necessary dependencies, and gives you the hooks for chat.\\n\\nIn your HTML document, you will also need two DIV elements. These elements should have CSS class names of \\\"history\\\" and \\\"message\\\". The \\\"history\\\" element will show the chat history. The \\\"message\\\" element is where you type a message to send. Styling these elements is left entirely to your CSS.\\n\\nIf you want some default styling, you can include our example CSS. This is also a good jumping off point to make your own design changes.\\n\\nLast but not least, we need to layer on the chat functionality via the aforementioned hooks. We do not want to do this until the \\\"gateway.js\\\" script is loaded. To wait for the script to load, we can catch the \\\"load\\\" event on the window object. Once we are sure that everything is loaded, we can invoke the chat functionality via a convenience method.\\n\\n![Behold a chat application.](http://images.kevinhoyt.com/hello.chat.screenshot.png)\\n\\nBehold, a [complete chat application](http://kaazing.org/demos/chat/run/)! No, [really](http://kaazing.org/demos/chat/run/)! Copy and paste the following code into your own HTML document to see it in action on your machine - no install required, no registration required, just free flowing real-time goodness.\\n\\n```\\n<html>\\n<head>\\n\\n<title>My Chat Application</title>\\n\\n<link\\n  href=\\\"http://kaazing.org/demos/chat.chat/run/chat.css\\\"\\n  rel=\\\"stylesheet\\\"\\n  type=\\\"text/css\\\">\\n\\n<script\\n  src=\\\"http://kaazing.org/demos/chat.chat/run/gateway.js\\\"\\n  type=\\\"text/javascript\\\">\\n</script>\\n\\n<script type=\\\"text/javascript\\\">\\nwindow.addEventListener( \\\"load\\\", function() {\\n  Gateway.chat(\\n    null,\\n    document.querySelector( \\\".message\\\" ),\\n    document.querySelector( \\\".history\\\" )\\n  );\\n} );\\n</script>\\n\\n</head>\\n<body>\\n\\n<div class=\\\"history\\\"></div>\\n<div class=\\\"message\\\">\\n  Press &lt;Enter&gt; to send\\n</div>\\n\\n</body>\\n</html>\\n```\\n\\n### Next Steps\\n\\nThere are so many features to enterprise messaging, and we are barely even scratching the surface with this example. If you are new to event-driven applications, then you might want to take a look at an [AMQP overview](https://www.rabbitmq.com/tutorials/amqp-concepts.html). If you are coming from a Java Messaging Service (JMS) background, then you might want to see how [AMQP compares](http://www.wmrichards.com/amqp.pdf). And of course Gateway is leveraging [WebSocket](http://www.amazon.com/Definitive-Guide-HTML5-WebSocket-ebook/dp/B00ACC6AZA/ref=sr_1_4?ie=UTF8&qid=1421948356&sr=8-4&keywords=moskovits) where applicable, which is also worth reading up on.\\n\"}]],\"sections\":[[10,0]],\"ghostVersion\":\"3.0\"}","html":"<!--kg-card-begin: markdown--><p><em>In any given programming language &quot;Hello World&quot; is likely the first thing you will learn to do. The &quot;Hello World&quot; for real-time architectures is a chat application. In this post you will build your very own Web-based chat application. Alternatively, you might also want to check out the Quick Start guide to get a little background.</em></p>\n<hr>\n<p><mark>This is reposted on the <a href=\"http://kaazing.org/blog/building-a-chat-application/\">Kaazing Open Source Blog</a>.</mark></p>\n<h3 id=\"connecting\">Connecting</h3>\n<p>The first thing we need to do to get real-time communication for our chat application is to connect to a server. As outlined in the Quick Start Guide, in a publish/subscribe architecture, the server is called a &quot;broker&quot;. The broker manages connecting incoming messages, with those wanting to receive messages. More on that later.</p>\n<pre><code>var kaazing = Gateway.connect( null, doConnected );\n</code></pre>\n<p>While this may look like a simple call, there is a lot that goes on in the background. Brokers speak various enterprise protocols such as AMQP, XMPP, and MQTT. The browser does not speak broker, or protocols. It does speak WebSocket. And if your browser does not speak WebSocket, it does speak HTTP. Managing that connection, and securely communicating via standard protocols, are among the tasks handled for you.</p>\n<h3 id=\"subscribing\">Subscribing</h3>\n<p>This call will connect to the broker and establish outgoing and incoming message buckets on the server. At this point, the outgoing bucket (called an exchange) will send messages on to any incoming message buckets (called queues) it has been told about. However, our outgoing bucket has not yet been told about the incoming bucket. To do this, we must subscribe for messages.</p>\n<pre><code>function doConnected()\n{\n  kaazing.on( Gateway.EVENT_MESSAGE, doMessage );\n  kaazing.subscribe( CHAT_TOPIC );\n}\n</code></pre>\n<p>Once our incoming bucket is subscribed to the outgoing bucket, we will begin to receive messages. The underlying protocol we are using for this example is AMQP, which refers to this process as &quot;binding&quot;. When the two buckets are bound, messages that match a specific &quot;routing key&quot; are forwarded on from the exchange (outgoing) to the queue (incoming). If you are familiar with other messaging protocols, you can think of the routing key as roughly analogous to a &quot;topic&quot; in this configuration.</p>\n<p>If you have an enterprise software background, you may be familiar with these concepts from enterprise messaging.</p>\n<p>Keep in mind that the routing happens on the server - and that is a good thing. You do not want every message regardless of routing to be sent to the client, for it to figure out how to deal with the content. At scale, the browser would simply be overloaded. Having a broker on the server also gives us features such as guaranteed delivery, flow control (pause/resume), and durable messaging (persistence between restarts).</p>\n<h3 id=\"publishing\">Publishing</h3>\n<p>When somebody in a chat wants to send a message, that is called &quot;publishing&quot;. To publish a message, and have it arrive to all the interested parties, you send the content using a specific routing key. Again, you may also think of the routing key as a topic in other messaging protocols.</p>\n<pre><code>kaazing.publish( CHAT_TOPIC, message );\n</code></pre>\n<p>While AMQP actually sends messages in binary, it is far more commonplace with JavaScript to work with String data types. String content is the expected message content. The AMQP library takes care of resolving the actual binary message. If you want to send complex data structures, you can use JSON.stringify() to get a String representation first. Do not forget to use JSON.parse() with the message arrives.</p>\n<h3 id=\"shortcut\">Shortcut</h3>\n<p>Knowing that chat is the &quot;Hello World&quot; of real-time communication, we have made getting started as easy as possible - there is a shortcut to building your first chat application. To use this, first put &quot;gateway.js&quot; into your HTML document. This loads all the necessary dependencies, and gives you the hooks for chat.</p>\n<p>In your HTML document, you will also need two DIV elements. These elements should have CSS class names of &quot;history&quot; and &quot;message&quot;. The &quot;history&quot; element will show the chat history. The &quot;message&quot; element is where you type a message to send. Styling these elements is left entirely to your CSS.</p>\n<p>If you want some default styling, you can include our example CSS. This is also a good jumping off point to make your own design changes.</p>\n<p>Last but not least, we need to layer on the chat functionality via the aforementioned hooks. We do not want to do this until the &quot;gateway.js&quot; script is loaded. To wait for the script to load, we can catch the &quot;load&quot; event on the window object. Once we are sure that everything is loaded, we can invoke the chat functionality via a convenience method.</p>\n<p><img src=\"http://images.kevinhoyt.com/hello.chat.screenshot.png\" alt=\"Behold a chat application.\" loading=\"lazy\"></p>\n<p>Behold, a <a href=\"http://kaazing.org/demos/chat/run/\">complete chat application</a>! No, <a href=\"http://kaazing.org/demos/chat/run/\">really</a>! Copy and paste the following code into your own HTML document to see it in action on your machine - no install required, no registration required, just free flowing real-time goodness.</p>\n<pre><code>&lt;html&gt;\n&lt;head&gt;\n\n&lt;title&gt;My Chat Application&lt;/title&gt;\n\n&lt;link\n  href=&quot;http://kaazing.org/demos/chat.chat/run/chat.css&quot;\n  rel=&quot;stylesheet&quot;\n  type=&quot;text/css&quot;&gt;\n\n&lt;script\n  src=&quot;http://kaazing.org/demos/chat.chat/run/gateway.js&quot;\n  type=&quot;text/javascript&quot;&gt;\n&lt;/script&gt;\n\n&lt;script type=&quot;text/javascript&quot;&gt;\nwindow.addEventListener( &quot;load&quot;, function() {\n  Gateway.chat(\n    null,\n    document.querySelector( &quot;.message&quot; ),\n    document.querySelector( &quot;.history&quot; )\n  );\n} );\n&lt;/script&gt;\n\n&lt;/head&gt;\n&lt;body&gt;\n\n&lt;div class=&quot;history&quot;&gt;&lt;/div&gt;\n&lt;div class=&quot;message&quot;&gt;\n  Press &amp;lt;Enter&amp;gt; to send\n&lt;/div&gt;\n\n&lt;/body&gt;\n&lt;/html&gt;\n</code></pre>\n<h3 id=\"nextsteps\">Next Steps</h3>\n<p>There are so many features to enterprise messaging, and we are barely even scratching the surface with this example. If you are new to event-driven applications, then you might want to take a look at an <a href=\"https://www.rabbitmq.com/tutorials/amqp-concepts.html\">AMQP overview</a>. If you are coming from a Java Messaging Service (JMS) background, then you might want to see how <a href=\"http://www.wmrichards.com/amqp.pdf\">AMQP compares</a>. And of course Gateway is leveraging <a href=\"http://www.amazon.com/Definitive-Guide-HTML5-WebSocket-ebook/dp/B00ACC6AZA/ref=sr_1_4?ie=UTF8&amp;qid=1421948356&amp;sr=8-4&amp;keywords=moskovits\">WebSocket</a> where applicable, which is also worth reading up on.</p>\n<!--kg-card-end: markdown-->","comment_id":"16","plaintext":"In any given programming language \"Hello World\" is likely the first thing you\nwill learn to do. The \"Hello World\" for real-time architectures is a chat\napplication. In this post you will build your very own Web-based chat\napplication. Alternatively, you might also want to check out the Quick Start\nguide to get a little background.\n\n\n--------------------------------------------------------------------------------\n\nThis is reposted on the Kaazing Open Source Blog\n[http://kaazing.org/blog/building-a-chat-application/].\n\nConnecting\nThe first thing we need to do to get real-time communication for our chat\napplication is to connect to a server. As outlined in the Quick Start Guide, in\na publish/subscribe architecture, the server is called a \"broker\". The broker\nmanages connecting incoming messages, with those wanting to receive messages.\nMore on that later.\n\nvar kaazing = Gateway.connect( null, doConnected );\n\n\nWhile this may look like a simple call, there is a lot that goes on in the\nbackground. Brokers speak various enterprise protocols such as AMQP, XMPP, and\nMQTT. The browser does not speak broker, or protocols. It does speak WebSocket.\nAnd if your browser does not speak WebSocket, it does speak HTTP. Managing that\nconnection, and securely communicating via standard protocols, are among the\ntasks handled for you.\n\nSubscribing\nThis call will connect to the broker and establish outgoing and incoming message\nbuckets on the server. At this point, the outgoing bucket (called an exchange)\nwill send messages on to any incoming message buckets (called queues) it has\nbeen told about. However, our outgoing bucket has not yet been told about the\nincoming bucket. To do this, we must subscribe for messages.\n\nfunction doConnected()\n{\n  kaazing.on( Gateway.EVENT_MESSAGE, doMessage );\n  kaazing.subscribe( CHAT_TOPIC );\n}\n\n\nOnce our incoming bucket is subscribed to the outgoing bucket, we will begin to\nreceive messages. The underlying protocol we are using for this example is AMQP,\nwhich refers to this process as \"binding\". When the two buckets are bound,\nmessages that match a specific \"routing key\" are forwarded on from the exchange\n(outgoing) to the queue (incoming). If you are familiar with other messaging\nprotocols, you can think of the routing key as roughly analogous to a \"topic\" in\nthis configuration.\n\nIf you have an enterprise software background, you may be familiar with these\nconcepts from enterprise messaging.\n\nKeep in mind that the routing happens on the server - and that is a good thing.\nYou do not want every message regardless of routing to be sent to the client,\nfor it to figure out how to deal with the content. At scale, the browser would\nsimply be overloaded. Having a broker on the server also gives us features such\nas guaranteed delivery, flow control (pause/resume), and durable messaging\n(persistence between restarts).\n\nPublishing\nWhen somebody in a chat wants to send a message, that is called \"publishing\". To\npublish a message, and have it arrive to all the interested parties, you send\nthe content using a specific routing key. Again, you may also think of the\nrouting key as a topic in other messaging protocols.\n\nkaazing.publish( CHAT_TOPIC, message );\n\n\nWhile AMQP actually sends messages in binary, it is far more commonplace with\nJavaScript to work with String data types. String content is the expected\nmessage content. The AMQP library takes care of resolving the actual binary\nmessage. If you want to send complex data structures, you can use\nJSON.stringify() to get a String representation first. Do not forget to use\nJSON.parse() with the message arrives.\n\nShortcut\nKnowing that chat is the \"Hello World\" of real-time communication, we have made\ngetting started as easy as possible - there is a shortcut to building your first\nchat application. To use this, first put \"gateway.js\" into your HTML document.\nThis loads all the necessary dependencies, and gives you the hooks for chat.\n\nIn your HTML document, you will also need two DIV elements. These elements\nshould have CSS class names of \"history\" and \"message\". The \"history\" element\nwill show the chat history. The \"message\" element is where you type a message to\nsend. Styling these elements is left entirely to your CSS.\n\nIf you want some default styling, you can include our example CSS. This is also\na good jumping off point to make your own design changes.\n\nLast but not least, we need to layer on the chat functionality via the\naforementioned hooks. We do not want to do this until the \"gateway.js\" script is\nloaded. To wait for the script to load, we can catch the \"load\" event on the\nwindow object. Once we are sure that everything is loaded, we can invoke the\nchat functionality via a convenience method.\n\n\n\nBehold, a complete chat application [http://kaazing.org/demos/chat/run/]! No, \nreally [http://kaazing.org/demos/chat/run/]! Copy and paste the following code\ninto your own HTML document to see it in action on your machine - no install\nrequired, no registration required, just free flowing real-time goodness.\n\n<html>\n<head>\n\n<title>My Chat Application</title>\n\n<link\n  href=\"http://kaazing.org/demos/chat.chat/run/chat.css\"\n  rel=\"stylesheet\"\n  type=\"text/css\">\n\n<script\n  src=\"http://kaazing.org/demos/chat.chat/run/gateway.js\"\n  type=\"text/javascript\">\n</script>\n\n<script type=\"text/javascript\">\nwindow.addEventListener( \"load\", function() {\n  Gateway.chat(\n    null,\n    document.querySelector( \".message\" ),\n    document.querySelector( \".history\" )\n  );\n} );\n</script>\n\n</head>\n<body>\n\n<div class=\"history\"></div>\n<div class=\"message\">\n  Press &lt;Enter&gt; to send\n</div>\n\n</body>\n</html>\n\n\nNext Steps\nThere are so many features to enterprise messaging, and we are barely even\nscratching the surface with this example. If you are new to event-driven\napplications, then you might want to take a look at an AMQP overview\n[https://www.rabbitmq.com/tutorials/amqp-concepts.html]. If you are coming from\na Java Messaging Service (JMS) background, then you might want to see how AMQP\ncompares [http://www.wmrichards.com/amqp.pdf]. And of course Gateway is\nleveraging WebSocket\n[http://www.amazon.com/Definitive-Guide-HTML5-WebSocket-ebook/dp/B00ACC6AZA/ref=sr_1_4?ie=UTF8&qid=1421948356&sr=8-4&keywords=moskovits] \nwhere applicable, which is also worth reading up on.","feature_image":"http://images.kevinhoyt.com/talking.fingers.jpg","featured":0,"status":"published","locale":null,"visibility":"public","author_id":"1","created_at":"2015-04-21T14:58:56.000Z","updated_at":"2015-04-21T15:57:23.000Z","published_at":"2015-01-23T17:00:00.000Z","custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"type":"post","email_recipient_filter":"none"},{"id":"5adb8922351ffe0018a5771d","uuid":"424857af-3bc7-42c9-b6cf-743d14d4d0fa","title":"AMQP Breakdown","slug":"amqp-breakdown","mobiledoc":"{\"version\":\"0.3.1\",\"markups\":[],\"atoms\":[],\"cards\":[[\"markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\"*The open source Kaazing Gateway uses Advanced Message Queueing Protocol ([AMQP](http://www.amqp.org/)) to transport your messages from one point to the next. It does this by leveraging the [IETF WebSocket](https://tools.ietf.org/html/rfc6455) protocol. The connection from the browser may use the [W3C WebSocket API](http://www.w3.org/TR/websockets/) if available, but will also fall back to other means if necessary.*\\n\\n*As with any protocol, using AMQP comes with its own set of concepts that you must understand before being able to get the most out of it. In this post we will take a look at an AMQP connection in depth, and understand how that manifests itself through Kaazing Gateway APIs.*\\n\\n---\\n\\n==This is reposted on the Kaazing Open Source [Blog](http://kaazing.org/blog/amqp-breakdown/).==\\n\\n> Note that this article is all about AMQP 0.9.1. AMQP 1.0 is now available, and is a completely different protocol.\\n\\n### A Word on Protocols\\n\\nIf you are new to messaging, or socket communications, then the concept of a protocol may also be new. Undoubtedly, you have run across protocols before, but never really given much thought to them. You type \\\"HTTP\\\" in your browser, and you are off to the races. You use Java Database Connectivity (JDBC) to query data, and never really think about the bytes that are needed to negotiate for your data.\\n\\nAt a high level, protocols are the convention that you are using to communicate between two points on the network. Just as different spoken languages have different semantics, so do protocols care about how the communication happens.\\n\\nA web server for example, speaking/caring about HTTP, will take a series of strings until it finds a double carriage return. At that point it will parse those strings and decide what to do. If it finds the verb \\\"GET\\\" then it will fetch and return the desired document. The web server knows what document is desired because the protocol specifies where to look for the file name within that incoming series of strings.\\n\\nAMQP is a standards-based protocol used by many enterprise systems. As the name implies, it is a protocol designed for routing messages between points on a network.\\n\\n### Connecting\\n\\nAs you might guess, before we can send messages to another point on the network, we need to connect to the network itself. When using the Kaazing Gateway APIs, we establish a connection using the Factory pattern. The factory creates an instance of an AMQP client that we can then use to connect to the desired location on the network.\\n\\n```\\nvar amqp_factory = new AmqpClientFactory();\\n\\nvar amqp_client = amqp_factory.createAmqpClient();\\n\\namqp_client.addEventListener(\\n  \\\"close\\\",\\n  doClientClose\\n);\\n\\namqp_client.connect( {\\n  url: \\\"wss://sandbox.kaazing.net/amqp091\\\",\\n  virtualHost: \\\"/\\\",\\n  credentials: {\\n    username: \\\"guest\\\",\\n    password: \\\"guest\\\"\\n  }\\n}, doClientOpen );\\n```\\n\\nIn order to know where to connect, the AMQP client needs a URL. A virtual host path can also be included. The virtual host path allows you more flexibility on setting up and managing your AMQP broker/network - such as multiple tenancy support. You can even send query string variables. In the example above, we send the credentials to authenticate against the broker.\\n\\n### Channels\\n\\nOnce we are connected to the desired AMQP broker, we can begin building our communication channels. Channels at this point do not differentiate what it is that they will be doing down the road - they are merely conduits for future broker API calls. What types of commands are issued once the channel is connected will in turn dictate what type of activity they will be doing.\\n\\n```\\nvar amqp_publish = amqp_client.openChannel(\\n  doPublishOpen\\n);\\n```\\n\\nYou might notice that the variable name I am giving this channel indicates that I intend to use it to publish messages. What about consuming messages? A message consumer is a second AMQP channel we will talk about momentarily. I am also choosing to cover publish first, because a consumer cannot bind to an exchange that does not exist, and it is the publish process that creates an exchange.\\n\\nBinding? Exchange? Read on!\\n\\n### Publish Exchange\\n\\nIf we decide that we want to use a channel for publishing messages, we declare an exchange. An exchange is a central point on the broker to which messages can be sent. Many clients may be sending messages to any given exchange, so you want the naming to be consistent and predictable.\\n\\n```\\namqp_publish.declareExchange( {\\n  exchange: \\\"my_exchange\\\",\\n  type: \\\"direct\\\"\\n} );\\n```\\n\\nAMQP provides a few different ways to distribute messages from the exchange. If you are coming from a system such as Java Messaging Service (JMS), then you will probably be looking for either the \\\"fanout\\\" or \\\"direct\\\" type. These AMQP exchange types allow for one exchange to distribute messages to many queues.\\n\\nIn the AMQP \\\"fanout\\\" configuration, the name of the exchange dictates how the messages will be routed - effectively to any and all bound queues, regardless of any other qualifier. This leaves it to your code on the client to further review messages and then decide the appropriate course of action.\\n\\nIn the AMQP \\\"direct\\\" configuration, messages are routed by the exchange, based on qualifiers provided by the consumers. This puts the burden on the broker to be able to filter and pass along messages accordingly. For the purposes of this example, we will be using the \\\"direct\\\" exchange type.\\n\\n### Publishing Messages\\n\\nAt this point, our code has not setup any consumers. However that does not mean that we cannot publish messages. There may be other consumers that are not under our control waiting to hear from us. The process of sending a message is called publishing.\\n\\n```\\n// Convert a String to binary\\nfunction stringToArrayBuffer( value )\\n{\\n  var buffer = null;\\n  var view = null;\\n\\n  buffer = new ArrayBuffer( value.length );\\n  view = new Uint8Array( buffer );\\n\\n  for( var i = 0; i < value.length; i++ )\\n  {\\n    view[i] = value.charCodeAt( i );\\n  }\\n\\n  return buffer;\\n}\\n\\n// Example message\\nvar message = {\\n  first_name: \\\"Kevin\\\",\\n  last_name: \\\"Hoyt\\\",\\n  twitter: \\\"krhoyt\\\"\\n};\\n\\n// Binary payload\\nvar body = stringToArrayBuffer(\\n  JSON.stringify( message )\\n);\\n\\n// Publish\\namqp_publish.publishBasic( {\\n  body: body,\\n  properties: null,\\n  exchange: \\\"my_exchange\\\",\\n  routingKey: \\\"my_routing_key\\\"\\n} );\\n```\\n\\nAMQP messages are binary in format. If you are going to be leveraging binary content, then you can publish your data without any further manipulation. In the case of the Web however, it is likely that we will want to leverage JavaScript Object Notation (JSON) to send out content. This means first converting our data structure to a String, and then into binary before sending.\\n\\nYou will also notice the introduction of the term \\\"routing key\\\". If you are familiar with enterprise messaging systems, you can think of the routing key as a topic. Technically, the topic may be the exchange name itself if you are using a \\\"fanout\\\" type, so this analogy is not 100% accurate.\\n\\nPersonally I like to think of the routing key as a hashtag like you might see in social media. Filtering status updates by their hashtag allows me to see all the content I am interested in reading. A hashtag is not a destination, but a means of identifying pieces of information. An exchange (social media network) may have any number of types of information. More on this later.\\n\\n### Consumer Channel\\n\\nWhen we want to subscribe to messages, we need to open a channel, and then use it to invoke broker APIs to differentiate it from a publish channel. Creating a consumer channel is exactly like creating a publish channel, and uses the same client API. It is only the calls we make on that channel that assign it's behavior.\\n\\n```\\nvar amqp_consume = amqp_client.openChannel(\\n  doConsumeOpen\\n);\\n```\\n\\nFor the purposes of differentiation in our code, we will name the consumer variable accordingly. We do however use the same AMQP client, created by the AMQP factory, to open the channel. Depending on your needs, you may have any number of channels open of various types.\\n\\n### Consumer Queue\\n\\nIn AMQP, each consumer channel has a queue. Each queue should have its own unique name. At this point a queue is very similar to an exchange in that it is just a bucket. In this case a queue is a bucket for a specific consumer, whereas an exchange is a bucket for any number of publishers. The buckets know nothing about each other at this point.\\n\\n```\\n// Example events\\namqp_consume.addEventListener(\\n  \\\"declarequeue\\\",\\n  doConsumeDeclare\\n);\\namqp_consume.addEventListener(\\n  \\\"bindqueue\\\",\\n  doConsumeBind\\n);\\namqp_consume.addEventListener(\\n  \\\"message\\\",\\n  doConsumeMessage\\n);\\n\\n// Keep track of the name\\nvar queue_name = \\\"queue\\\" + Date.now();\\n\\n// Declare the queue\\namqp_consume.declareQueue( {\\n  queue: queue_name\\n} );\\n```\\n\\nWhile there are a variety of events on any channel that you can listen for, a consumer channel will emit a \\\"message\\\" event when messages arrive.\\n\\n### Consumer Binding\\n\\nI have used the verb \\\"bind\\\" a few of times in this post. The term \\\"binding\\\" in AMQP is what links a queue (consumer) bucket to an exchange (publish) bucket.\\n\\n```\\namqp_consume.bindQueue( {\\n  queue: queue_name,\\n  exchange: \\\"my_exchange\\\",\\n  routingKey: \\\"my_routing_key\\\"\\n} );\\n```\\n\\nBecause we are using a \\\"direct\\\" exchange type here, we can also specify a \\\"routing key\\\". The routing key is effectively the filter by which the exchange (publish) will decide what messages to put in the bound queues (consumers). You can specify a routing key with a \\\"fanout\\\" exchange, but it will be ignored.\\n\\n### Consuming Messages\\n\\nEven though the exchange and the queue are now bound, and messages are being put into the right buckets, those messages are not necessarily being delivered to the client. In order to get messages delivered from our queue, we need to tell the AMQP broker that we want to receive them.\\n\\n```\\namqp_consume.consumeBasic( {\\n  queue: queue_name,\\n  consumerTag: \\\"my_consumer_tag\\\",\\n  noAck: false\\n} );\\n```\\n\\nFor the purposes of this example, we will be using \\\"push\\\" delivery. That is that we want the queue to deliver messages to the client when they arrive. We could alternatively elect to leave messages in the queue bucket, on the broker, until we check it at some point in the future. This is called \\\"pull\\\" delivery.\\n\\nThere is also a flag here for telling the broker that the client must/not acknowledge that messages have been received. Oddly, this flag is a double-negative, so be careful in setting the right value.\\n\\nIf we set \\\"noAck\\\" to \\\"true\\\" we are telling the broker that the client is not required to acknowledge that the messages have been received. Conversely, setting \\\"noAck\\\" to false means that the client must tell the broker that it has received each message. This obviously means more overhead, however it also means that you can be sure that the client will eventually get every message.\\n\\nAn interesting side effect of setting “noAck” to “false” is that the broker will continue to deliver those messages until they are acknowledged. So while the client may receive the messages, if it does not acknowledge them, the next client that connects will also receive those messages - even if it is the same client. The broker will continue this behavior until all the messages are acknowledged by the client This is called guaranteed delivery.\\n\\n### Message Handling\\n\\nOnce our consumer channel was opened, we registered the \\\"message\\\" event handler. This event handler will be called when messages are delivered from the queue to the client. The message itself will be passed to the event handler. This message comes across in a binary form, so the first thing we need to do is convert it to a String.\\n\\n```\\n// Binary to String\\nfunction arrayBufferToString( buffer )\\n{\\n  return String.fromCharCode.apply(\\n    null,\\n    new Uint8Array( buffer )\\n  );\\n}\\n\\n// Payload as String\\nvar body = arrayBufferToString(\\n  message.getBodyAsArrayBuffer()\\n);\\n\\n// Object from JSON\\nvar data = JSON.parse( body );\\n```\\n\\nThe contents of the message, called the message body, might be any variety of format - that is up to your implementation. As this is a Web example, it makes sense to use JavaScript Object Notation (JSON) to serialize our body data. From there a decision can be made about how best to deal with the data itself.\\n\\n### Message Acknowledgement\\n\\nIf you set \\\"noAck\\\" to \\\"false\\\" then you must additionally inform the broker that you have received the message.\\n\\n```\\nvar config = {\\n  deliveryTag: message.args.deliveryTag,\\n  multiple: true\\n};\\n\\n// Acknowledge is synchronous\\n// Schedule independently\\nsetTimeout( function() {\\n  amqp_consume.ackBasic( config );\\n}, 0 );\\n```\\n\\nIf you have \\\"noAck\\\" set to \\\"false\\\" but do not acknowledge the message, you will not get an error. However the broker will assume that the message from that queue has not been delivered. The next time that consumer declares that queue, it will be sent all the messages that we not acknowledged. If you are not careful with that double-negative, this can cause you a lot of debugging pain in the future.\\n\\nTo acknowledge that the client has received a message, it communicates back to the broker. As acknowledging a message is a synchronous behavior in AMQP, and we do not want to hang the browser thread, we can use a little trick with setTimeout() to schedule the response independently.\\n\\n### Next Steps\\n\\nThe [Quick Start](http://kaazing.org/demos/quick-start/) example wraps all the intimate details of AMQP up into an easy to use wrapper library. Using this approach you have only to think about calling publish and subscribe. If you just want to build a [quick chat application](http://kaazing.org/blog/building-a-chat-application/), there is an [example for that](http://kaazing.org/blog/building-a-chat-application/) too.\\n\\nThere are also features of AMQP, supported by Gateway, that I have not covered in this post. Examples of these advanced topics would be flow control, and other exchange types. To get up close and personal with all your options when using AMQP, check out this [RabbitMQ breakdown](https://www.rabbitmq.com/tutorials/amqp-concepts.html).\"}]],\"sections\":[[10,0]],\"ghostVersion\":\"3.0\"}","html":"<!--kg-card-begin: markdown--><p><em>The open source Kaazing Gateway uses Advanced Message Queueing Protocol (<a href=\"http://www.amqp.org/\">AMQP</a>) to transport your messages from one point to the next. It does this by leveraging the <a href=\"https://tools.ietf.org/html/rfc6455\">IETF WebSocket</a> protocol. The connection from the browser may use the <a href=\"http://www.w3.org/TR/websockets/\">W3C WebSocket API</a> if available, but will also fall back to other means if necessary.</em></p>\n<p><em>As with any protocol, using AMQP comes with its own set of concepts that you must understand before being able to get the most out of it. In this post we will take a look at an AMQP connection in depth, and understand how that manifests itself through Kaazing Gateway APIs.</em></p>\n<hr>\n<p><mark>This is reposted on the Kaazing Open Source <a href=\"http://kaazing.org/blog/amqp-breakdown/\">Blog</a>.</mark></p>\n<blockquote>\n<p>Note that this article is all about AMQP 0.9.1. AMQP 1.0 is now available, and is a completely different protocol.</p>\n</blockquote>\n<h3 id=\"awordonprotocols\">A Word on Protocols</h3>\n<p>If you are new to messaging, or socket communications, then the concept of a protocol may also be new. Undoubtedly, you have run across protocols before, but never really given much thought to them. You type &quot;HTTP&quot; in your browser, and you are off to the races. You use Java Database Connectivity (JDBC) to query data, and never really think about the bytes that are needed to negotiate for your data.</p>\n<p>At a high level, protocols are the convention that you are using to communicate between two points on the network. Just as different spoken languages have different semantics, so do protocols care about how the communication happens.</p>\n<p>A web server for example, speaking/caring about HTTP, will take a series of strings until it finds a double carriage return. At that point it will parse those strings and decide what to do. If it finds the verb &quot;GET&quot; then it will fetch and return the desired document. The web server knows what document is desired because the protocol specifies where to look for the file name within that incoming series of strings.</p>\n<p>AMQP is a standards-based protocol used by many enterprise systems. As the name implies, it is a protocol designed for routing messages between points on a network.</p>\n<h3 id=\"connecting\">Connecting</h3>\n<p>As you might guess, before we can send messages to another point on the network, we need to connect to the network itself. When using the Kaazing Gateway APIs, we establish a connection using the Factory pattern. The factory creates an instance of an AMQP client that we can then use to connect to the desired location on the network.</p>\n<pre><code>var amqp_factory = new AmqpClientFactory();\n\nvar amqp_client = amqp_factory.createAmqpClient();\n\namqp_client.addEventListener(\n  &quot;close&quot;,\n  doClientClose\n);\n\namqp_client.connect( {\n  url: &quot;wss://sandbox.kaazing.net/amqp091&quot;,\n  virtualHost: &quot;/&quot;,\n  credentials: {\n    username: &quot;guest&quot;,\n    password: &quot;guest&quot;\n  }\n}, doClientOpen );\n</code></pre>\n<p>In order to know where to connect, the AMQP client needs a URL. A virtual host path can also be included. The virtual host path allows you more flexibility on setting up and managing your AMQP broker/network - such as multiple tenancy support. You can even send query string variables. In the example above, we send the credentials to authenticate against the broker.</p>\n<h3 id=\"channels\">Channels</h3>\n<p>Once we are connected to the desired AMQP broker, we can begin building our communication channels. Channels at this point do not differentiate what it is that they will be doing down the road - they are merely conduits for future broker API calls. What types of commands are issued once the channel is connected will in turn dictate what type of activity they will be doing.</p>\n<pre><code>var amqp_publish = amqp_client.openChannel(\n  doPublishOpen\n);\n</code></pre>\n<p>You might notice that the variable name I am giving this channel indicates that I intend to use it to publish messages. What about consuming messages? A message consumer is a second AMQP channel we will talk about momentarily. I am also choosing to cover publish first, because a consumer cannot bind to an exchange that does not exist, and it is the publish process that creates an exchange.</p>\n<p>Binding? Exchange? Read on!</p>\n<h3 id=\"publishexchange\">Publish Exchange</h3>\n<p>If we decide that we want to use a channel for publishing messages, we declare an exchange. An exchange is a central point on the broker to which messages can be sent. Many clients may be sending messages to any given exchange, so you want the naming to be consistent and predictable.</p>\n<pre><code>amqp_publish.declareExchange( {\n  exchange: &quot;my_exchange&quot;,\n  type: &quot;direct&quot;\n} );\n</code></pre>\n<p>AMQP provides a few different ways to distribute messages from the exchange. If you are coming from a system such as Java Messaging Service (JMS), then you will probably be looking for either the &quot;fanout&quot; or &quot;direct&quot; type. These AMQP exchange types allow for one exchange to distribute messages to many queues.</p>\n<p>In the AMQP &quot;fanout&quot; configuration, the name of the exchange dictates how the messages will be routed - effectively to any and all bound queues, regardless of any other qualifier. This leaves it to your code on the client to further review messages and then decide the appropriate course of action.</p>\n<p>In the AMQP &quot;direct&quot; configuration, messages are routed by the exchange, based on qualifiers provided by the consumers. This puts the burden on the broker to be able to filter and pass along messages accordingly. For the purposes of this example, we will be using the &quot;direct&quot; exchange type.</p>\n<h3 id=\"publishingmessages\">Publishing Messages</h3>\n<p>At this point, our code has not setup any consumers. However that does not mean that we cannot publish messages. There may be other consumers that are not under our control waiting to hear from us. The process of sending a message is called publishing.</p>\n<pre><code>// Convert a String to binary\nfunction stringToArrayBuffer( value )\n{\n  var buffer = null;\n  var view = null;\n\n  buffer = new ArrayBuffer( value.length );\n  view = new Uint8Array( buffer );\n\n  for( var i = 0; i &lt; value.length; i++ )\n  {\n    view[i] = value.charCodeAt( i );\n  }\n\n  return buffer;\n}\n\n// Example message\nvar message = {\n  first_name: &quot;Kevin&quot;,\n  last_name: &quot;Hoyt&quot;,\n  twitter: &quot;krhoyt&quot;\n};\n\n// Binary payload\nvar body = stringToArrayBuffer(\n  JSON.stringify( message )\n);\n\n// Publish\namqp_publish.publishBasic( {\n  body: body,\n  properties: null,\n  exchange: &quot;my_exchange&quot;,\n  routingKey: &quot;my_routing_key&quot;\n} );\n</code></pre>\n<p>AMQP messages are binary in format. If you are going to be leveraging binary content, then you can publish your data without any further manipulation. In the case of the Web however, it is likely that we will want to leverage JavaScript Object Notation (JSON) to send out content. This means first converting our data structure to a String, and then into binary before sending.</p>\n<p>You will also notice the introduction of the term &quot;routing key&quot;. If you are familiar with enterprise messaging systems, you can think of the routing key as a topic. Technically, the topic may be the exchange name itself if you are using a &quot;fanout&quot; type, so this analogy is not 100% accurate.</p>\n<p>Personally I like to think of the routing key as a hashtag like you might see in social media. Filtering status updates by their hashtag allows me to see all the content I am interested in reading. A hashtag is not a destination, but a means of identifying pieces of information. An exchange (social media network) may have any number of types of information. More on this later.</p>\n<h3 id=\"consumerchannel\">Consumer Channel</h3>\n<p>When we want to subscribe to messages, we need to open a channel, and then use it to invoke broker APIs to differentiate it from a publish channel. Creating a consumer channel is exactly like creating a publish channel, and uses the same client API. It is only the calls we make on that channel that assign it's behavior.</p>\n<pre><code>var amqp_consume = amqp_client.openChannel(\n  doConsumeOpen\n);\n</code></pre>\n<p>For the purposes of differentiation in our code, we will name the consumer variable accordingly. We do however use the same AMQP client, created by the AMQP factory, to open the channel. Depending on your needs, you may have any number of channels open of various types.</p>\n<h3 id=\"consumerqueue\">Consumer Queue</h3>\n<p>In AMQP, each consumer channel has a queue. Each queue should have its own unique name. At this point a queue is very similar to an exchange in that it is just a bucket. In this case a queue is a bucket for a specific consumer, whereas an exchange is a bucket for any number of publishers. The buckets know nothing about each other at this point.</p>\n<pre><code>// Example events\namqp_consume.addEventListener(\n  &quot;declarequeue&quot;,\n  doConsumeDeclare\n);\namqp_consume.addEventListener(\n  &quot;bindqueue&quot;,\n  doConsumeBind\n);\namqp_consume.addEventListener(\n  &quot;message&quot;,\n  doConsumeMessage\n);\n\n// Keep track of the name\nvar queue_name = &quot;queue&quot; + Date.now();\n\n// Declare the queue\namqp_consume.declareQueue( {\n  queue: queue_name\n} );\n</code></pre>\n<p>While there are a variety of events on any channel that you can listen for, a consumer channel will emit a &quot;message&quot; event when messages arrive.</p>\n<h3 id=\"consumerbinding\">Consumer Binding</h3>\n<p>I have used the verb &quot;bind&quot; a few of times in this post. The term &quot;binding&quot; in AMQP is what links a queue (consumer) bucket to an exchange (publish) bucket.</p>\n<pre><code>amqp_consume.bindQueue( {\n  queue: queue_name,\n  exchange: &quot;my_exchange&quot;,\n  routingKey: &quot;my_routing_key&quot;\n} );\n</code></pre>\n<p>Because we are using a &quot;direct&quot; exchange type here, we can also specify a &quot;routing key&quot;. The routing key is effectively the filter by which the exchange (publish) will decide what messages to put in the bound queues (consumers). You can specify a routing key with a &quot;fanout&quot; exchange, but it will be ignored.</p>\n<h3 id=\"consumingmessages\">Consuming Messages</h3>\n<p>Even though the exchange and the queue are now bound, and messages are being put into the right buckets, those messages are not necessarily being delivered to the client. In order to get messages delivered from our queue, we need to tell the AMQP broker that we want to receive them.</p>\n<pre><code>amqp_consume.consumeBasic( {\n  queue: queue_name,\n  consumerTag: &quot;my_consumer_tag&quot;,\n  noAck: false\n} );\n</code></pre>\n<p>For the purposes of this example, we will be using &quot;push&quot; delivery. That is that we want the queue to deliver messages to the client when they arrive. We could alternatively elect to leave messages in the queue bucket, on the broker, until we check it at some point in the future. This is called &quot;pull&quot; delivery.</p>\n<p>There is also a flag here for telling the broker that the client must/not acknowledge that messages have been received. Oddly, this flag is a double-negative, so be careful in setting the right value.</p>\n<p>If we set &quot;noAck&quot; to &quot;true&quot; we are telling the broker that the client is not required to acknowledge that the messages have been received. Conversely, setting &quot;noAck&quot; to false means that the client must tell the broker that it has received each message. This obviously means more overhead, however it also means that you can be sure that the client will eventually get every message.</p>\n<p>An interesting side effect of setting “noAck” to “false” is that the broker will continue to deliver those messages until they are acknowledged. So while the client may receive the messages, if it does not acknowledge them, the next client that connects will also receive those messages - even if it is the same client. The broker will continue this behavior until all the messages are acknowledged by the client This is called guaranteed delivery.</p>\n<h3 id=\"messagehandling\">Message Handling</h3>\n<p>Once our consumer channel was opened, we registered the &quot;message&quot; event handler. This event handler will be called when messages are delivered from the queue to the client. The message itself will be passed to the event handler. This message comes across in a binary form, so the first thing we need to do is convert it to a String.</p>\n<pre><code>// Binary to String\nfunction arrayBufferToString( buffer )\n{\n  return String.fromCharCode.apply(\n    null,\n    new Uint8Array( buffer )\n  );\n}\n\n// Payload as String\nvar body = arrayBufferToString(\n  message.getBodyAsArrayBuffer()\n);\n\n// Object from JSON\nvar data = JSON.parse( body );\n</code></pre>\n<p>The contents of the message, called the message body, might be any variety of format - that is up to your implementation. As this is a Web example, it makes sense to use JavaScript Object Notation (JSON) to serialize our body data. From there a decision can be made about how best to deal with the data itself.</p>\n<h3 id=\"messageacknowledgement\">Message Acknowledgement</h3>\n<p>If you set &quot;noAck&quot; to &quot;false&quot; then you must additionally inform the broker that you have received the message.</p>\n<pre><code>var config = {\n  deliveryTag: message.args.deliveryTag,\n  multiple: true\n};\n\n// Acknowledge is synchronous\n// Schedule independently\nsetTimeout( function() {\n  amqp_consume.ackBasic( config );\n}, 0 );\n</code></pre>\n<p>If you have &quot;noAck&quot; set to &quot;false&quot; but do not acknowledge the message, you will not get an error. However the broker will assume that the message from that queue has not been delivered. The next time that consumer declares that queue, it will be sent all the messages that we not acknowledged. If you are not careful with that double-negative, this can cause you a lot of debugging pain in the future.</p>\n<p>To acknowledge that the client has received a message, it communicates back to the broker. As acknowledging a message is a synchronous behavior in AMQP, and we do not want to hang the browser thread, we can use a little trick with setTimeout() to schedule the response independently.</p>\n<h3 id=\"nextsteps\">Next Steps</h3>\n<p>The <a href=\"http://kaazing.org/demos/quick-start/\">Quick Start</a> example wraps all the intimate details of AMQP up into an easy to use wrapper library. Using this approach you have only to think about calling publish and subscribe. If you just want to build a <a href=\"http://kaazing.org/blog/building-a-chat-application/\">quick chat application</a>, there is an <a href=\"http://kaazing.org/blog/building-a-chat-application/\">example for that</a> too.</p>\n<p>There are also features of AMQP, supported by Gateway, that I have not covered in this post. Examples of these advanced topics would be flow control, and other exchange types. To get up close and personal with all your options when using AMQP, check out this <a href=\"https://www.rabbitmq.com/tutorials/amqp-concepts.html\">RabbitMQ breakdown</a>.</p>\n<!--kg-card-end: markdown-->","comment_id":"17","plaintext":"The open source Kaazing Gateway uses Advanced Message Queueing Protocol (AMQP\n[http://www.amqp.org/]) to transport your messages from one point to the next.\nIt does this by leveraging the IETF WebSocket\n[https://tools.ietf.org/html/rfc6455] protocol. The connection from the browser\nmay use the W3C WebSocket API [http://www.w3.org/TR/websockets/] if available,\nbut will also fall back to other means if necessary.\n\nAs with any protocol, using AMQP comes with its own set of concepts that you\nmust understand before being able to get the most out of it. In this post we\nwill take a look at an AMQP connection in depth, and understand how that\nmanifests itself through Kaazing Gateway APIs.\n\n\n--------------------------------------------------------------------------------\n\nThis is reposted on the Kaazing Open Source Blog\n[http://kaazing.org/blog/amqp-breakdown/].\n\n> Note that this article is all about AMQP 0.9.1. AMQP 1.0 is now available, and\nis a completely different protocol.\n\n\nA Word on Protocols\nIf you are new to messaging, or socket communications, then the concept of a\nprotocol may also be new. Undoubtedly, you have run across protocols before, but\nnever really given much thought to them. You type \"HTTP\" in your browser, and\nyou are off to the races. You use Java Database Connectivity (JDBC) to query\ndata, and never really think about the bytes that are needed to negotiate for\nyour data.\n\nAt a high level, protocols are the convention that you are using to communicate\nbetween two points on the network. Just as different spoken languages have\ndifferent semantics, so do protocols care about how the communication happens.\n\nA web server for example, speaking/caring about HTTP, will take a series of\nstrings until it finds a double carriage return. At that point it will parse\nthose strings and decide what to do. If it finds the verb \"GET\" then it will\nfetch and return the desired document. The web server knows what document is\ndesired because the protocol specifies where to look for the file name within\nthat incoming series of strings.\n\nAMQP is a standards-based protocol used by many enterprise systems. As the name\nimplies, it is a protocol designed for routing messages between points on a\nnetwork.\n\nConnecting\nAs you might guess, before we can send messages to another point on the network,\nwe need to connect to the network itself. When using the Kaazing Gateway APIs,\nwe establish a connection using the Factory pattern. The factory creates an\ninstance of an AMQP client that we can then use to connect to the desired\nlocation on the network.\n\nvar amqp_factory = new AmqpClientFactory();\n\nvar amqp_client = amqp_factory.createAmqpClient();\n\namqp_client.addEventListener(\n  \"close\",\n  doClientClose\n);\n\namqp_client.connect( {\n  url: \"wss://sandbox.kaazing.net/amqp091\",\n  virtualHost: \"/\",\n  credentials: {\n    username: \"guest\",\n    password: \"guest\"\n  }\n}, doClientOpen );\n\n\nIn order to know where to connect, the AMQP client needs a URL. A virtual host\npath can also be included. The virtual host path allows you more flexibility on\nsetting up and managing your AMQP broker/network - such as multiple tenancy\nsupport. You can even send query string variables. In the example above, we send\nthe credentials to authenticate against the broker.\n\nChannels\nOnce we are connected to the desired AMQP broker, we can begin building our\ncommunication channels. Channels at this point do not differentiate what it is\nthat they will be doing down the road - they are merely conduits for future\nbroker API calls. What types of commands are issued once the channel is\nconnected will in turn dictate what type of activity they will be doing.\n\nvar amqp_publish = amqp_client.openChannel(\n  doPublishOpen\n);\n\n\nYou might notice that the variable name I am giving this channel indicates that\nI intend to use it to publish messages. What about consuming messages? A message\nconsumer is a second AMQP channel we will talk about momentarily. I am also\nchoosing to cover publish first, because a consumer cannot bind to an exchange\nthat does not exist, and it is the publish process that creates an exchange.\n\nBinding? Exchange? Read on!\n\nPublish Exchange\nIf we decide that we want to use a channel for publishing messages, we declare\nan exchange. An exchange is a central point on the broker to which messages can\nbe sent. Many clients may be sending messages to any given exchange, so you want\nthe naming to be consistent and predictable.\n\namqp_publish.declareExchange( {\n  exchange: \"my_exchange\",\n  type: \"direct\"\n} );\n\n\nAMQP provides a few different ways to distribute messages from the exchange. If\nyou are coming from a system such as Java Messaging Service (JMS), then you will\nprobably be looking for either the \"fanout\" or \"direct\" type. These AMQP\nexchange types allow for one exchange to distribute messages to many queues.\n\nIn the AMQP \"fanout\" configuration, the name of the exchange dictates how the\nmessages will be routed - effectively to any and all bound queues, regardless of\nany other qualifier. This leaves it to your code on the client to further review\nmessages and then decide the appropriate course of action.\n\nIn the AMQP \"direct\" configuration, messages are routed by the exchange, based\non qualifiers provided by the consumers. This puts the burden on the broker to\nbe able to filter and pass along messages accordingly. For the purposes of this\nexample, we will be using the \"direct\" exchange type.\n\nPublishing Messages\nAt this point, our code has not setup any consumers. However that does not mean\nthat we cannot publish messages. There may be other consumers that are not under\nour control waiting to hear from us. The process of sending a message is called\npublishing.\n\n// Convert a String to binary\nfunction stringToArrayBuffer( value )\n{\n  var buffer = null;\n  var view = null;\n\n  buffer = new ArrayBuffer( value.length );\n  view = new Uint8Array( buffer );\n\n  for( var i = 0; i < value.length; i++ )\n  {\n    view[i] = value.charCodeAt( i );\n  }\n\n  return buffer;\n}\n\n// Example message\nvar message = {\n  first_name: \"Kevin\",\n  last_name: \"Hoyt\",\n  twitter: \"krhoyt\"\n};\n\n// Binary payload\nvar body = stringToArrayBuffer(\n  JSON.stringify( message )\n);\n\n// Publish\namqp_publish.publishBasic( {\n  body: body,\n  properties: null,\n  exchange: \"my_exchange\",\n  routingKey: \"my_routing_key\"\n} );\n\n\nAMQP messages are binary in format. If you are going to be leveraging binary\ncontent, then you can publish your data without any further manipulation. In the\ncase of the Web however, it is likely that we will want to leverage JavaScript\nObject Notation (JSON) to send out content. This means first converting our data\nstructure to a String, and then into binary before sending.\n\nYou will also notice the introduction of the term \"routing key\". If you are\nfamiliar with enterprise messaging systems, you can think of the routing key as\na topic. Technically, the topic may be the exchange name itself if you are using\na \"fanout\" type, so this analogy is not 100% accurate.\n\nPersonally I like to think of the routing key as a hashtag like you might see in\nsocial media. Filtering status updates by their hashtag allows me to see all the\ncontent I am interested in reading. A hashtag is not a destination, but a means\nof identifying pieces of information. An exchange (social media network) may\nhave any number of types of information. More on this later.\n\nConsumer Channel\nWhen we want to subscribe to messages, we need to open a channel, and then use\nit to invoke broker APIs to differentiate it from a publish channel. Creating a\nconsumer channel is exactly like creating a publish channel, and uses the same\nclient API. It is only the calls we make on that channel that assign it's\nbehavior.\n\nvar amqp_consume = amqp_client.openChannel(\n  doConsumeOpen\n);\n\n\nFor the purposes of differentiation in our code, we will name the consumer\nvariable accordingly. We do however use the same AMQP client, created by the\nAMQP factory, to open the channel. Depending on your needs, you may have any\nnumber of channels open of various types.\n\nConsumer Queue\nIn AMQP, each consumer channel has a queue. Each queue should have its own\nunique name. At this point a queue is very similar to an exchange in that it is\njust a bucket. In this case a queue is a bucket for a specific consumer, whereas\nan exchange is a bucket for any number of publishers. The buckets know nothing\nabout each other at this point.\n\n// Example events\namqp_consume.addEventListener(\n  \"declarequeue\",\n  doConsumeDeclare\n);\namqp_consume.addEventListener(\n  \"bindqueue\",\n  doConsumeBind\n);\namqp_consume.addEventListener(\n  \"message\",\n  doConsumeMessage\n);\n\n// Keep track of the name\nvar queue_name = \"queue\" + Date.now();\n\n// Declare the queue\namqp_consume.declareQueue( {\n  queue: queue_name\n} );\n\n\nWhile there are a variety of events on any channel that you can listen for, a\nconsumer channel will emit a \"message\" event when messages arrive.\n\nConsumer Binding\nI have used the verb \"bind\" a few of times in this post. The term \"binding\" in\nAMQP is what links a queue (consumer) bucket to an exchange (publish) bucket.\n\namqp_consume.bindQueue( {\n  queue: queue_name,\n  exchange: \"my_exchange\",\n  routingKey: \"my_routing_key\"\n} );\n\n\nBecause we are using a \"direct\" exchange type here, we can also specify a\n\"routing key\". The routing key is effectively the filter by which the exchange\n(publish) will decide what messages to put in the bound queues (consumers). You\ncan specify a routing key with a \"fanout\" exchange, but it will be ignored.\n\nConsuming Messages\nEven though the exchange and the queue are now bound, and messages are being put\ninto the right buckets, those messages are not necessarily being delivered to\nthe client. In order to get messages delivered from our queue, we need to tell\nthe AMQP broker that we want to receive them.\n\namqp_consume.consumeBasic( {\n  queue: queue_name,\n  consumerTag: \"my_consumer_tag\",\n  noAck: false\n} );\n\n\nFor the purposes of this example, we will be using \"push\" delivery. That is that\nwe want the queue to deliver messages to the client when they arrive. We could\nalternatively elect to leave messages in the queue bucket, on the broker, until\nwe check it at some point in the future. This is called \"pull\" delivery.\n\nThere is also a flag here for telling the broker that the client must/not\nacknowledge that messages have been received. Oddly, this flag is a\ndouble-negative, so be careful in setting the right value.\n\nIf we set \"noAck\" to \"true\" we are telling the broker that the client is not\nrequired to acknowledge that the messages have been received. Conversely,\nsetting \"noAck\" to false means that the client must tell the broker that it has\nreceived each message. This obviously means more overhead, however it also means\nthat you can be sure that the client will eventually get every message.\n\nAn interesting side effect of setting “noAck” to “false” is that the broker will\ncontinue to deliver those messages until they are acknowledged. So while the\nclient may receive the messages, if it does not acknowledge them, the next\nclient that connects will also receive those messages - even if it is the same\nclient. The broker will continue this behavior until all the messages are\nacknowledged by the client This is called guaranteed delivery.\n\nMessage Handling\nOnce our consumer channel was opened, we registered the \"message\" event handler.\nThis event handler will be called when messages are delivered from the queue to\nthe client. The message itself will be passed to the event handler. This message\ncomes across in a binary form, so the first thing we need to do is convert it to\na String.\n\n// Binary to String\nfunction arrayBufferToString( buffer )\n{\n  return String.fromCharCode.apply(\n    null,\n    new Uint8Array( buffer )\n  );\n}\n\n// Payload as String\nvar body = arrayBufferToString(\n  message.getBodyAsArrayBuffer()\n);\n\n// Object from JSON\nvar data = JSON.parse( body );\n\n\nThe contents of the message, called the message body, might be any variety of\nformat - that is up to your implementation. As this is a Web example, it makes\nsense to use JavaScript Object Notation (JSON) to serialize our body data. From\nthere a decision can be made about how best to deal with the data itself.\n\nMessage Acknowledgement\nIf you set \"noAck\" to \"false\" then you must additionally inform the broker that\nyou have received the message.\n\nvar config = {\n  deliveryTag: message.args.deliveryTag,\n  multiple: true\n};\n\n// Acknowledge is synchronous\n// Schedule independently\nsetTimeout( function() {\n  amqp_consume.ackBasic( config );\n}, 0 );\n\n\nIf you have \"noAck\" set to \"false\" but do not acknowledge the message, you will\nnot get an error. However the broker will assume that the message from that\nqueue has not been delivered. The next time that consumer declares that queue,\nit will be sent all the messages that we not acknowledged. If you are not\ncareful with that double-negative, this can cause you a lot of debugging pain in\nthe future.\n\nTo acknowledge that the client has received a message, it communicates back to\nthe broker. As acknowledging a message is a synchronous behavior in AMQP, and we\ndo not want to hang the browser thread, we can use a little trick with\nsetTimeout() to schedule the response independently.\n\nNext Steps\nThe Quick Start [http://kaazing.org/demos/quick-start/] example wraps all the\nintimate details of AMQP up into an easy to use wrapper library. Using this\napproach you have only to think about calling publish and subscribe. If you just\nwant to build a quick chat application\n[http://kaazing.org/blog/building-a-chat-application/], there is an example for\nthat [http://kaazing.org/blog/building-a-chat-application/] too.\n\nThere are also features of AMQP, supported by Gateway, that I have not covered\nin this post. Examples of these advanced topics would be flow control, and other\nexchange types. To get up close and personal with all your options when using\nAMQP, check out this RabbitMQ breakdown\n[https://www.rabbitmq.com/tutorials/amqp-concepts.html].","feature_image":"http://images.kevinhoyt.com/rubiks.cube.disassemble.jpg","featured":0,"status":"published","locale":null,"visibility":"public","author_id":"1","created_at":"2015-04-21T15:09:53.000Z","updated_at":"2015-04-21T15:20:55.000Z","published_at":"2015-02-02T17:00:00.000Z","custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"type":"post","email_recipient_filter":"none"},{"id":"5adb8922351ffe0018a5771e","uuid":"355998ba-5a54-4213-978f-82872a468827","title":"Java Chat Application","slug":"java-chat-application","mobiledoc":"{\"version\":\"0.3.1\",\"markups\":[],\"atoms\":[],\"cards\":[[\"markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\"*If you look around the Kaazing open source web site, you will see a lot of information about delivering real-time solutions on the Web. This is not by accident - we a strong believers in the future of web standards. You can also use Kaazing Gateway with other platforms as well, including Java, Android, and iOS. In this post, we will take a quick look at building a chat application using Java.*\\n\\n---\\n\\n==This is reposted on the Kaazing Open Source [blog](http://kaazing.org/blog/java-chat-application/).==\\n\\n### Desktop\\n\\nAs a cross-platform technology, Java can run in many places. Indeed, Kaazing Gateway itself is built on Java and intended to run on the server. Since we are talking about a chat application here, we will be focusing on the client side library usage. To avoid the complexity of mobile deployment, we will be developing a desktop Java application.\\n\\nThe goal of this Java desktop chat application is to integrate with the web standards chat application from a previous blog post. You can [read the article](http://kaazing.org/blog/building-a-chat-application/) and even use the [live application](http://kaazing.org/demos/chat.chat/run/) (desktop or mobile). This means that Kaazing Gateway brings real-time cross-platform communication to your projects. I will be following this post up by bringing the chat application over to Android, and eventually iOS.\\n\\n### Libraries\\n\\nAs an open source project, you have access to all the client library code you could want. Many of the repositories include build instructions as well. However, if you are just getting started exploring Kaazing Gateway features by building this chat client, then I am guessing that you are not ready to commit code to the project. Our first step then is to get the libraries we need in a form that is ready to drop into our IDE.\\n\\nAs it turns out, the build processes of the open source repositories use [Apache Maven](http://maven.apache.org/) for project management. This means that we can head on over to [The Central Repository](http://search.maven.org/#search%7Cga%7C1%7Cg:%22org.kaazing%22) and pick up the ready-to-use JAR files for our chat client. There are four different libraries that we will need.\\n\\nThe first library we will need is [Gateway client](http://search.maven.org/#search%7Cga%7C1%7Ca:%22gateway.client.java%22) ([GitHub](https://github.com/kaazing/gateway.client.java)). This effectively implements a WebSocket client, which is the baseline for communication to Kaazing Gateway. You will also need [Gateway client transport](http://search.maven.org/#search%7Cga%7C1%7Ca:%22gateway.client.java.transport%22) ([GitHub](https://github.com/kaazing/gateway.client.java.transport)). This library implements the transport layer for Kaazing Gateway WebSocket Java client library. Next up is an [AMQP client](http://search.maven.org/#search%7Cga%7C1%7Ca:%22amqp.client.java%22) ([GitHub](https://github.com/kaazing/amqp.client.java)) implementation. I wrote extensively [about AMQP](http://kaazing.org/blog/amqp-breakdown/) in an earlier post.\\n\\nSince the Web client from the previous post uses JSON (JavaScript Object Notation) to communication messages, we also need the ability to handle JSON in Java. The [Glassfish JSON library](http://search.maven.org/#search%7Cga%7C1%7Ca:%22javax.json%22) is an open source implementation of [JSR-353](https://json-processing-spec.java.net/), which provides a Java API for JSON processing. Paired with the Kaazing libraries, we have everything we need for our chat client.\\n\\n### User Interface\\n\\nBuilding Java desktop user interfaces is something that most people remember either as a painful part of learning Java, or as an empowering art form. I actually fall into the later group, and as such have made the Java desktop client look and behave exactly like the Web client. In the interest of brevity, and perhaps sanity, I will not be covering all the details of JList and custom cell renderers, or other fun Swing internals.\\n\\nYou might now be breathing a sigh of relief, but if you really want to take a look under the covers, the entire project (minus the libraries) is available on my [Kaazing GitHub repository](https://github.com/krhoyt/Kaazing/tree/master/chat).\\n\\n### Connecting\\n\\nThe first thing we need to do, is to establish a connection to Kaazing Gateway. Do not worry if you do not have a build running locally, or on some server in your DMZ, we have a publicly available instance for you to use.\\n\\n```\\n// Establish connection\\nprivate void initConnection()\\n{\\n  // Factory\\n  factory = AmqpClientFactory.createAmqpClientFactory();\\n        \\n  try {\\n    // Client\\n    client = factory.createAmqpClient();\\n            \\n    // Connection listeners\\n    client.addConnectionListener( new ConnectionListener() {\\n            \\n      // Connecting\\n      public void onConnecting( ConnectionEvent ce ) \\n      {\\n        EventQueue.invokeLater( new Runnable() {\\n          public void run()\\n          {\\n            System.out.println( \\\"Connecting...\\\" );\\n          }\\n         } );\\n       }\\n                \\n      // Error\\n      public void onConnectionError( ConnectionEvent ce ) \\n      {\\n        EventQueue.invokeLater( new Runnable() {\\n          public void run()\\n          {\\n            System.out.println( \\\"Connection error.\\\" );\\n          }\\n        } );\\n      }                              \\n                \\n      // Open\\n      public void onConnectionOpen( ConnectionEvent ce ) \\n      {\\n        EventQueue.invokeLater( new Runnable() {\\n          public void run()\\n          {\\n            System.out.println( \\\"Connection open.\\\" );\\n                            \\n            // Setup publisher\\n            doClientOpen();\\n          }\\n        } );\\n      }\\n                \\n      // Close\\n      public void onConnectionClose( ConnectionEvent ce ) \\n      {\\n        EventQueue.invokeLater( new Runnable() {\\n          public void run()\\n          {\\n            System.out.println( \\\"Connection closed.\\\" );\\n          }\\n        } );\\n      }\\n    } );\\n            \\n    // Connect to server\\n    client.connect(\\n      \\\"wss://sandbox.kaazing.net/amqp091\\\", \\n      \\\"/\\\", \\n      \\\"guest\\\", \\n      \\\"guest\\\"\\n    );\\n  } catch( Exception e ) {\\n    e.printStackTrace();\\n  }\\n}\\n```\\n\\nIn connecting, there are many event listeners you can use. For example, you might use the open and close event listeners to show a visual indicator in the user interface. Right now, all we are interested in knowing is when the connection is ready to use which is handled in the \\\"onConnectionOpen\\\" event.\\n\\n### Channels\\n\\nTo publish and consume messages, we will create an AMQP channel for each operation. Similar to opening the connection, there are many event listeners that you can have. We are going to start with the publish channel, which effectively boils down to declaring the exchange. An exchange sits on the message broker (server), and acts as a bucket for incoming messages.\\n\\n```\\nprivate void doClientOpen()\\n{\\n  // Send messages\\n  publish = client.openChannel();\\n        \\n  // Channel listeners\\n  publish.addChannelListener( new ChannelAdapter() {\\n    // Close\\n    public void onClose( ChannelEvent ce ) \\n    {\\n      EventQueue.invokeLater( new Runnable() {\\n        public void run()\\n        {\\n          System.out.println( \\\"Publish closed.\\\" );\\n        }\\n      } );\\n    }\\n            \\n    // Error\\n    public void onError( ChannelEvent ce ) \\n    {\\n      EventQueue.invokeLater( new Runnable() {\\n        public void run()\\n        {\\n          System.out.println( \\\"Publish error.\\\" );\\n        }\\n      } );\\n    }            \\n            \\n    // Declare exchange\\n    public void onDeclareExchange( ChannelEvent ce ) \\n    {\\n      EventQueue.invokeLater( new Runnable() {\\n        public void run()\\n        {\\n          System.out.println( \\\"Exchange declared.\\\" );\\n                        \\n          // Setup consumer\\n          doPublishReady();\\n        }\\n      } );\\n    }            \\n            \\n    // Open\\n    public void onOpen( ChannelEvent ce ) \\n    {\\n      EventQueue.invokeLater( new Runnable() {\\n        public void run()\\n        {\\n          System.out.println( \\\"Publish open.\\\" );\\n                        \\n          // Declare exchange\\n          publish.declareExchange( \\n            \\\"exchange_WLRNhKKM7d\\\", \\n            \\\"direct\\\", \\n            false, \\n            false, \\n            false, \\n            null \\n          );\\n        }\\n      } );\\n    }            \\n  } );\\n}\\n```\\n\\nWith a publish exchange created, we can now move onto creating a consumer queue and binding it to the exchange. The consume channel is the first place your message will arrive in the Java client. AMQP is a binary protocol, so we get the bytes first. Since we are working with JSON from the Web client, we will want the String equivalent of the message payload. We can then process the content however we feel best fit - more on that in the next section.\\n\\n```\\nprivate void doPublishReady()\\n{\\n  // Consume\\n  consume = client.openChannel();\\n        \\n  // Channel listeners\\n  consume.addChannelListener( new ChannelAdapter() {\\n    // Bind queue\\n    public void onBindQueue( ChannelEvent ce )\\n    {\\n      EventQueue.invokeLater( new Runnable() {\\n        public void run()\\n        {\\n          System.out.println( \\\"Queue bound.\\\" );\\n        }\\n      } );\\n    }\\n            \\n    // Close\\n    public void onClose( ChannelEvent ce )\\n    {\\n      EventQueue.invokeLater( new Runnable() {\\n        public void run()\\n        {\\n          System.out.println( \\\"Consume closed.\\\" );\\n        }\\n      } );\\n    }            \\n            \\n    // Consume\\n    public void onConsumeBasic( ChannelEvent ce )\\n    {\\n      EventQueue.invokeLater( new Runnable() {\\n        public void run()\\n        {\\n          System.out.println( \\\"Consuming...\\\" );\\n                        \\n          // Open user interface for sending messages\\n          doConsumeReady();\\n        }\\n      } );\\n    }            \\n            \\n    // Declare queue\\n    public void onDeclareQueue( ChannelEvent ce )\\n    {\\n      EventQueue.invokeLater( new Runnable() {\\n        public void run()\\n        {\\n          System.out.println( \\\"Queue declared.\\\" );\\n        }\\n      } );\\n    }            \\n            \\n    // Flow\\n    public void onFlow( ChannelEvent ce )\\n    {\\n      try {\\n        final boolean isActive = ce.isFlowActive();\\n                    \\n        EventQueue.invokeLater( new Runnable() {\\n          public void run()\\n          {\\n            System.out.println( \\\"Flow is \\\" + ( isActive ? \\\"on\\\" : \\\"off\\\" ) + \\\".\\\" );\\n          }\\n        } );                  \\n      } catch( Exception e ) {\\n        e.printStackTrace();\\n      }\\n    }            \\n            \\n    // Message\\n    public void onMessage( ChannelEvent ce )\\n    {\\n      byte[] bytes;\\n                \\n      bytes = new byte[ce.getBody().remaining()];\\n      ce.getBody().get( bytes );\\n                \\n      final Long    tag = ( Long )ce.getArgument( \\\"deliveryTag\\\" );\\n      final String  value = new String( bytes, Charset.forName( \\\"UTF-8\\\" ) );\\n                \\n      EventQueue.invokeLater( new Runnable() {\\n        public void run()\\n        {\\n          AmqpChannel channel = null;\\n                        \\n          System.out.println( \\\"Message: \\\" + value );\\n                        \\n          // Place in user interface\\n          doMessageArrived( value );\\n                        \\n          // Acknowledge\\n          channel = ce.getChannel();\\n          channel.ackBasic( tag.longValue(), true );\\n        }\\n      } );\\n    }            \\n            \\n    // Open\\n    public void onOpen( ChannelEvent ce )\\n    {\\n      EventQueue.invokeLater( new Runnable() {\\n        public void run()\\n        {\\n          System.out.println( \\\"Consume open.\\\" );\\n                        \\n          // Declare queue\\n          // Bind queue to exchange\\n          // Start consuming\\n          consume.declareQueue( \\n            \\\"queue_AND_123\\\", \\n            false, \\n            false, \\n            false, \\n            false, \\n            false, \\n            null \\n          ).bindQueue( \\n            \\\"queue_AND_123\\\", \\n            \\\"exchange_WLRNhKKM7d\\\", \\n            \\\"chat_topic\\\", \\n            false, \\n            null \\n          ).consumeBasic( \\n            \\\"queue_AND_123\\\", \\n            \\\"start_tag\\\", \\n            false, \\n            false, \\n            false, \\n            false, \\n            null \\n          );\\n        }\\n      } );\\n    }                        \\n  } );\\n}\\n```\\n\\nBecause of the way we have setup our connection, messages must be acknowledged. This is the last bit of code in the message event handler. Without this, the exchange on the broker will effectively hold onto the messages. This is actually a desired behavior for the purposes of message persistence and guaranteed delivery. Long-term however this can mean that your server fills up and runs out of memory. If you are not interested in persistence and guaranteed delivery, you can configure the exchange to not require message acknowledgement.\\n\\n### Parsing JSON\\n\\nAs previously mentioned, messages from the Web client are in JSON format. This means we need to parse the message content into an equivalent Java data type. Parsing JSON in Java reminds me of parsing XML in Java using SAX. The parser effectively rips through the content, while your code looks for specific elements that you are interested in further processing.\\n\\n```\\nprivate void doMessageArrived( String body )\\n{\\n  ChatMessage message = null;\\n  Event        e = null;\\n  InputStream stream = null;\\n  JsonParser  parser = null;\\n        \\n  // String to InputStream\\n  stream = new ByteArrayInputStream( \\n    body.getBytes( StandardCharsets.UTF_8 ) \\n  );\\n  parser = Json.createParser( stream );\\n        \\n  // New chat message\\n  message = new ChatMessage();\\n  message.raw = body;\\n        \\n  // Parse JSON\\n  while( parser.hasNext() )\\n  {\\n    e = parser.next();\\n            \\n    if( e == Event.KEY_NAME )\\n    {\\n      switch( parser.getString() )\\n      {\\n        case \\\"color\\\":\\n          parser.next();\\n          message.color = parseRgb( parser.getString() );\\n          break;\\n                        \\n        case \\\"message\\\":\\n          parser.next();\\n          message.content = parser.getString();\\n          break;\\n                        \\n        case \\\"user\\\":\\n          parser.next();\\n          message.user = parser.getString();\\n          break;\\n      }\\n    }\\n  }\\n        \\n  history.addElement( message );\\n}\\n```\\n\\nIn order to hold the message content, and render it in a JList, I have created a custom data type called ChatMessage. It simply has a few public properties on it to hold the specific pieces of data. Color from the Web client is in CSS format of \\\"rgb( 255, 255, 255 )\\\". This is further parsed into a Java Color object. The JList in turn has a custom cell renderer to show the message in the color provided by the sending client.\\n\\n### Publish\\n\\nPublishing a message that can be consumed by a Web client effectively means encoding our Java data types into their corresponding JSON format. Again, the process is very similar to SAX. The JSR provides for a builder object. Properties are added to the builder. To get the String format of the JSON data, we use the JSR-provided writer object.\\n\\n```\\npublic void keyReleased( KeyEvent ke ) \\n{\\n  AmqpProperties     properties = null;\\n  ByteBuffer         buffer = null;\\n  JsonObject         result = null;\\n  JsonObjectBuilder      builder = null;\\n  StringWriter           sw = null;\\n  Timestamp              stamp = null;\\n        \\n  // There is a message to send\\n  if( ke.getKeyCode() == 10 && field.getText().trim().length() > 0 )\\n  {\\n    // Build JSON object\\n    // Interacting with the web\\n    builder = Json.createObjectBuilder();\\n    builder.add( \\\"message\\\", field.getText().trim() );\\n    builder.add( \\n      \\\"color\\\", \\n      \\\"rgb( \\\" + style.getRed() + \\n      \\\", \\\" + style.getGreen() + \\n      \\\", \\\" + style.getBlue() + \\n      \\\" )\\\" \\n    );\\n    builder.add( \\\"user\\\", \\\"user_\\\" + now );\\n            \\n    result = builder.build();\\n            \\n    // Java JSON object to String\\n    sw = new StringWriter();\\n            \\n    try( JsonWriter writer = Json.createWriter( sw ) ) {\\n      writer.writeObject( result );\\n    }\\n            \\n    // Here is what we are going to send\\n    System.out.println( \\\"Sending: \\\" + sw.toString() );\\n            \\n    // Encode for AMQP\\n    buffer = ByteBuffer.allocate( 512 );\\n    buffer.put( sw.toString().getBytes( Charset.forName( \\\"UTF-8\\\" ) ) );\\n    buffer.flip();\\n            \\n    stamp = new Timestamp( System.currentTimeMillis() );\\n            \\n    // Publish parameters\\n    properties = new AmqpProperties();\\n    properties.setMessageId( \\\"1\\\" );\\n    properties.setCorrelationId( \\\"4\\\" );\\n    properties.setAppId( \\\"java_chat\\\" );\\n    properties.setUserId( \\\"user_\\\" + now );\\n    properties.setContentType( \\\"text/plain\\\" );\\n    properties.setContentEncoding( \\\"UTF-8\\\" );\\n    properties.setPriority( 6 );\\n    properties.setDeliveryMode( 1 );\\n    properties.setTimestamp( stamp );\\n            \\n    // Send\\n    publish.publishBasic( \\n      buffer, \\n      properties, \\n      \\\"exchange_WLRNhKKM7d\\\", \\n      \\\"chat_topic\\\", \\n      false, \\n      false \\n    );\\n            \\n    // Clear text just sent\\n    field.setText( \\\"\\\" );\\n  }\\n}\\n```\\n\\nOnce we have the JSON representation of the outgoing chat message, we create the AMQP message proper. This largely consists of setting various properties the correspond to how the broker should handle the message. After that, we use our previous instantiated publish channel and send the message itself, and the properties, to the exchange on the broker.\\n\\n### Next Steps\\n\\nThe event handlers in the code can seem overwhelming at first - there are so many of them. Remember however that they are predominantly a convenience to provide a better behaving application. Do not let them get in your way of opening your favorite Java IDE and giving the chat client a try.\\n\\n![Screenshot of the Java application and Web application.](http://images.kevinhoyt.com/java.chat.screenshot.jpg)\\n\\nOnce you have the Java client running, you can head over to the Kaazing open source web site and run the [live chat demonstration](http://kaazing.org/demos/chat.chat/run/). The two clients will be able to communicate with one another in real-time. The cross-platform goodness does not stop there either. I will write more in the future on using Kaazing Gateway with Android, iOS, and IoT.\"}]],\"sections\":[[10,0]],\"ghostVersion\":\"3.0\"}","html":"<!--kg-card-begin: markdown--><p><em>If you look around the Kaazing open source web site, you will see a lot of information about delivering real-time solutions on the Web. This is not by accident - we a strong believers in the future of web standards. You can also use Kaazing Gateway with other platforms as well, including Java, Android, and iOS. In this post, we will take a quick look at building a chat application using Java.</em></p>\n<hr>\n<p><mark>This is reposted on the Kaazing Open Source <a href=\"http://kaazing.org/blog/java-chat-application/\">blog</a>.</mark></p>\n<h3 id=\"desktop\">Desktop</h3>\n<p>As a cross-platform technology, Java can run in many places. Indeed, Kaazing Gateway itself is built on Java and intended to run on the server. Since we are talking about a chat application here, we will be focusing on the client side library usage. To avoid the complexity of mobile deployment, we will be developing a desktop Java application.</p>\n<p>The goal of this Java desktop chat application is to integrate with the web standards chat application from a previous blog post. You can <a href=\"http://kaazing.org/blog/building-a-chat-application/\">read the article</a> and even use the <a href=\"http://kaazing.org/demos/chat.chat/run/\">live application</a> (desktop or mobile). This means that Kaazing Gateway brings real-time cross-platform communication to your projects. I will be following this post up by bringing the chat application over to Android, and eventually iOS.</p>\n<h3 id=\"libraries\">Libraries</h3>\n<p>As an open source project, you have access to all the client library code you could want. Many of the repositories include build instructions as well. However, if you are just getting started exploring Kaazing Gateway features by building this chat client, then I am guessing that you are not ready to commit code to the project. Our first step then is to get the libraries we need in a form that is ready to drop into our IDE.</p>\n<p>As it turns out, the build processes of the open source repositories use <a href=\"http://maven.apache.org/\">Apache Maven</a> for project management. This means that we can head on over to <a href=\"http://search.maven.org/#search%7Cga%7C1%7Cg:%22org.kaazing%22\">The Central Repository</a> and pick up the ready-to-use JAR files for our chat client. There are four different libraries that we will need.</p>\n<p>The first library we will need is <a href=\"http://search.maven.org/#search%7Cga%7C1%7Ca:%22gateway.client.java%22\">Gateway client</a> (<a href=\"https://github.com/kaazing/gateway.client.java\">GitHub</a>). This effectively implements a WebSocket client, which is the baseline for communication to Kaazing Gateway. You will also need <a href=\"http://search.maven.org/#search%7Cga%7C1%7Ca:%22gateway.client.java.transport%22\">Gateway client transport</a> (<a href=\"https://github.com/kaazing/gateway.client.java.transport\">GitHub</a>). This library implements the transport layer for Kaazing Gateway WebSocket Java client library. Next up is an <a href=\"http://search.maven.org/#search%7Cga%7C1%7Ca:%22amqp.client.java%22\">AMQP client</a> (<a href=\"https://github.com/kaazing/amqp.client.java\">GitHub</a>) implementation. I wrote extensively <a href=\"http://kaazing.org/blog/amqp-breakdown/\">about AMQP</a> in an earlier post.</p>\n<p>Since the Web client from the previous post uses JSON (JavaScript Object Notation) to communication messages, we also need the ability to handle JSON in Java. The <a href=\"http://search.maven.org/#search%7Cga%7C1%7Ca:%22javax.json%22\">Glassfish JSON library</a> is an open source implementation of <a href=\"https://json-processing-spec.java.net/\">JSR-353</a>, which provides a Java API for JSON processing. Paired with the Kaazing libraries, we have everything we need for our chat client.</p>\n<h3 id=\"userinterface\">User Interface</h3>\n<p>Building Java desktop user interfaces is something that most people remember either as a painful part of learning Java, or as an empowering art form. I actually fall into the later group, and as such have made the Java desktop client look and behave exactly like the Web client. In the interest of brevity, and perhaps sanity, I will not be covering all the details of JList and custom cell renderers, or other fun Swing internals.</p>\n<p>You might now be breathing a sigh of relief, but if you really want to take a look under the covers, the entire project (minus the libraries) is available on my <a href=\"https://github.com/krhoyt/Kaazing/tree/master/chat\">Kaazing GitHub repository</a>.</p>\n<h3 id=\"connecting\">Connecting</h3>\n<p>The first thing we need to do, is to establish a connection to Kaazing Gateway. Do not worry if you do not have a build running locally, or on some server in your DMZ, we have a publicly available instance for you to use.</p>\n<pre><code>// Establish connection\nprivate void initConnection()\n{\n  // Factory\n  factory = AmqpClientFactory.createAmqpClientFactory();\n        \n  try {\n    // Client\n    client = factory.createAmqpClient();\n            \n    // Connection listeners\n    client.addConnectionListener( new ConnectionListener() {\n            \n      // Connecting\n      public void onConnecting( ConnectionEvent ce ) \n      {\n        EventQueue.invokeLater( new Runnable() {\n          public void run()\n          {\n            System.out.println( &quot;Connecting...&quot; );\n          }\n         } );\n       }\n                \n      // Error\n      public void onConnectionError( ConnectionEvent ce ) \n      {\n        EventQueue.invokeLater( new Runnable() {\n          public void run()\n          {\n            System.out.println( &quot;Connection error.&quot; );\n          }\n        } );\n      }                              \n                \n      // Open\n      public void onConnectionOpen( ConnectionEvent ce ) \n      {\n        EventQueue.invokeLater( new Runnable() {\n          public void run()\n          {\n            System.out.println( &quot;Connection open.&quot; );\n                            \n            // Setup publisher\n            doClientOpen();\n          }\n        } );\n      }\n                \n      // Close\n      public void onConnectionClose( ConnectionEvent ce ) \n      {\n        EventQueue.invokeLater( new Runnable() {\n          public void run()\n          {\n            System.out.println( &quot;Connection closed.&quot; );\n          }\n        } );\n      }\n    } );\n            \n    // Connect to server\n    client.connect(\n      &quot;wss://sandbox.kaazing.net/amqp091&quot;, \n      &quot;/&quot;, \n      &quot;guest&quot;, \n      &quot;guest&quot;\n    );\n  } catch( Exception e ) {\n    e.printStackTrace();\n  }\n}\n</code></pre>\n<p>In connecting, there are many event listeners you can use. For example, you might use the open and close event listeners to show a visual indicator in the user interface. Right now, all we are interested in knowing is when the connection is ready to use which is handled in the &quot;onConnectionOpen&quot; event.</p>\n<h3 id=\"channels\">Channels</h3>\n<p>To publish and consume messages, we will create an AMQP channel for each operation. Similar to opening the connection, there are many event listeners that you can have. We are going to start with the publish channel, which effectively boils down to declaring the exchange. An exchange sits on the message broker (server), and acts as a bucket for incoming messages.</p>\n<pre><code>private void doClientOpen()\n{\n  // Send messages\n  publish = client.openChannel();\n        \n  // Channel listeners\n  publish.addChannelListener( new ChannelAdapter() {\n    // Close\n    public void onClose( ChannelEvent ce ) \n    {\n      EventQueue.invokeLater( new Runnable() {\n        public void run()\n        {\n          System.out.println( &quot;Publish closed.&quot; );\n        }\n      } );\n    }\n            \n    // Error\n    public void onError( ChannelEvent ce ) \n    {\n      EventQueue.invokeLater( new Runnable() {\n        public void run()\n        {\n          System.out.println( &quot;Publish error.&quot; );\n        }\n      } );\n    }            \n            \n    // Declare exchange\n    public void onDeclareExchange( ChannelEvent ce ) \n    {\n      EventQueue.invokeLater( new Runnable() {\n        public void run()\n        {\n          System.out.println( &quot;Exchange declared.&quot; );\n                        \n          // Setup consumer\n          doPublishReady();\n        }\n      } );\n    }            \n            \n    // Open\n    public void onOpen( ChannelEvent ce ) \n    {\n      EventQueue.invokeLater( new Runnable() {\n        public void run()\n        {\n          System.out.println( &quot;Publish open.&quot; );\n                        \n          // Declare exchange\n          publish.declareExchange( \n            &quot;exchange_WLRNhKKM7d&quot;, \n            &quot;direct&quot;, \n            false, \n            false, \n            false, \n            null \n          );\n        }\n      } );\n    }            \n  } );\n}\n</code></pre>\n<p>With a publish exchange created, we can now move onto creating a consumer queue and binding it to the exchange. The consume channel is the first place your message will arrive in the Java client. AMQP is a binary protocol, so we get the bytes first. Since we are working with JSON from the Web client, we will want the String equivalent of the message payload. We can then process the content however we feel best fit - more on that in the next section.</p>\n<pre><code>private void doPublishReady()\n{\n  // Consume\n  consume = client.openChannel();\n        \n  // Channel listeners\n  consume.addChannelListener( new ChannelAdapter() {\n    // Bind queue\n    public void onBindQueue( ChannelEvent ce )\n    {\n      EventQueue.invokeLater( new Runnable() {\n        public void run()\n        {\n          System.out.println( &quot;Queue bound.&quot; );\n        }\n      } );\n    }\n            \n    // Close\n    public void onClose( ChannelEvent ce )\n    {\n      EventQueue.invokeLater( new Runnable() {\n        public void run()\n        {\n          System.out.println( &quot;Consume closed.&quot; );\n        }\n      } );\n    }            \n            \n    // Consume\n    public void onConsumeBasic( ChannelEvent ce )\n    {\n      EventQueue.invokeLater( new Runnable() {\n        public void run()\n        {\n          System.out.println( &quot;Consuming...&quot; );\n                        \n          // Open user interface for sending messages\n          doConsumeReady();\n        }\n      } );\n    }            \n            \n    // Declare queue\n    public void onDeclareQueue( ChannelEvent ce )\n    {\n      EventQueue.invokeLater( new Runnable() {\n        public void run()\n        {\n          System.out.println( &quot;Queue declared.&quot; );\n        }\n      } );\n    }            \n            \n    // Flow\n    public void onFlow( ChannelEvent ce )\n    {\n      try {\n        final boolean isActive = ce.isFlowActive();\n                    \n        EventQueue.invokeLater( new Runnable() {\n          public void run()\n          {\n            System.out.println( &quot;Flow is &quot; + ( isActive ? &quot;on&quot; : &quot;off&quot; ) + &quot;.&quot; );\n          }\n        } );                  \n      } catch( Exception e ) {\n        e.printStackTrace();\n      }\n    }            \n            \n    // Message\n    public void onMessage( ChannelEvent ce )\n    {\n      byte[] bytes;\n                \n      bytes = new byte[ce.getBody().remaining()];\n      ce.getBody().get( bytes );\n                \n      final Long    tag = ( Long )ce.getArgument( &quot;deliveryTag&quot; );\n      final String  value = new String( bytes, Charset.forName( &quot;UTF-8&quot; ) );\n                \n      EventQueue.invokeLater( new Runnable() {\n        public void run()\n        {\n          AmqpChannel channel = null;\n                        \n          System.out.println( &quot;Message: &quot; + value );\n                        \n          // Place in user interface\n          doMessageArrived( value );\n                        \n          // Acknowledge\n          channel = ce.getChannel();\n          channel.ackBasic( tag.longValue(), true );\n        }\n      } );\n    }            \n            \n    // Open\n    public void onOpen( ChannelEvent ce )\n    {\n      EventQueue.invokeLater( new Runnable() {\n        public void run()\n        {\n          System.out.println( &quot;Consume open.&quot; );\n                        \n          // Declare queue\n          // Bind queue to exchange\n          // Start consuming\n          consume.declareQueue( \n            &quot;queue_AND_123&quot;, \n            false, \n            false, \n            false, \n            false, \n            false, \n            null \n          ).bindQueue( \n            &quot;queue_AND_123&quot;, \n            &quot;exchange_WLRNhKKM7d&quot;, \n            &quot;chat_topic&quot;, \n            false, \n            null \n          ).consumeBasic( \n            &quot;queue_AND_123&quot;, \n            &quot;start_tag&quot;, \n            false, \n            false, \n            false, \n            false, \n            null \n          );\n        }\n      } );\n    }                        \n  } );\n}\n</code></pre>\n<p>Because of the way we have setup our connection, messages must be acknowledged. This is the last bit of code in the message event handler. Without this, the exchange on the broker will effectively hold onto the messages. This is actually a desired behavior for the purposes of message persistence and guaranteed delivery. Long-term however this can mean that your server fills up and runs out of memory. If you are not interested in persistence and guaranteed delivery, you can configure the exchange to not require message acknowledgement.</p>\n<h3 id=\"parsingjson\">Parsing JSON</h3>\n<p>As previously mentioned, messages from the Web client are in JSON format. This means we need to parse the message content into an equivalent Java data type. Parsing JSON in Java reminds me of parsing XML in Java using SAX. The parser effectively rips through the content, while your code looks for specific elements that you are interested in further processing.</p>\n<pre><code>private void doMessageArrived( String body )\n{\n  ChatMessage message = null;\n  Event        e = null;\n  InputStream stream = null;\n  JsonParser  parser = null;\n        \n  // String to InputStream\n  stream = new ByteArrayInputStream( \n    body.getBytes( StandardCharsets.UTF_8 ) \n  );\n  parser = Json.createParser( stream );\n        \n  // New chat message\n  message = new ChatMessage();\n  message.raw = body;\n        \n  // Parse JSON\n  while( parser.hasNext() )\n  {\n    e = parser.next();\n            \n    if( e == Event.KEY_NAME )\n    {\n      switch( parser.getString() )\n      {\n        case &quot;color&quot;:\n          parser.next();\n          message.color = parseRgb( parser.getString() );\n          break;\n                        \n        case &quot;message&quot;:\n          parser.next();\n          message.content = parser.getString();\n          break;\n                        \n        case &quot;user&quot;:\n          parser.next();\n          message.user = parser.getString();\n          break;\n      }\n    }\n  }\n        \n  history.addElement( message );\n}\n</code></pre>\n<p>In order to hold the message content, and render it in a JList, I have created a custom data type called ChatMessage. It simply has a few public properties on it to hold the specific pieces of data. Color from the Web client is in CSS format of &quot;rgb( 255, 255, 255 )&quot;. This is further parsed into a Java Color object. The JList in turn has a custom cell renderer to show the message in the color provided by the sending client.</p>\n<h3 id=\"publish\">Publish</h3>\n<p>Publishing a message that can be consumed by a Web client effectively means encoding our Java data types into their corresponding JSON format. Again, the process is very similar to SAX. The JSR provides for a builder object. Properties are added to the builder. To get the String format of the JSON data, we use the JSR-provided writer object.</p>\n<pre><code>public void keyReleased( KeyEvent ke ) \n{\n  AmqpProperties     properties = null;\n  ByteBuffer         buffer = null;\n  JsonObject         result = null;\n  JsonObjectBuilder      builder = null;\n  StringWriter           sw = null;\n  Timestamp              stamp = null;\n        \n  // There is a message to send\n  if( ke.getKeyCode() == 10 &amp;&amp; field.getText().trim().length() &gt; 0 )\n  {\n    // Build JSON object\n    // Interacting with the web\n    builder = Json.createObjectBuilder();\n    builder.add( &quot;message&quot;, field.getText().trim() );\n    builder.add( \n      &quot;color&quot;, \n      &quot;rgb( &quot; + style.getRed() + \n      &quot;, &quot; + style.getGreen() + \n      &quot;, &quot; + style.getBlue() + \n      &quot; )&quot; \n    );\n    builder.add( &quot;user&quot;, &quot;user_&quot; + now );\n            \n    result = builder.build();\n            \n    // Java JSON object to String\n    sw = new StringWriter();\n            \n    try( JsonWriter writer = Json.createWriter( sw ) ) {\n      writer.writeObject( result );\n    }\n            \n    // Here is what we are going to send\n    System.out.println( &quot;Sending: &quot; + sw.toString() );\n            \n    // Encode for AMQP\n    buffer = ByteBuffer.allocate( 512 );\n    buffer.put( sw.toString().getBytes( Charset.forName( &quot;UTF-8&quot; ) ) );\n    buffer.flip();\n            \n    stamp = new Timestamp( System.currentTimeMillis() );\n            \n    // Publish parameters\n    properties = new AmqpProperties();\n    properties.setMessageId( &quot;1&quot; );\n    properties.setCorrelationId( &quot;4&quot; );\n    properties.setAppId( &quot;java_chat&quot; );\n    properties.setUserId( &quot;user_&quot; + now );\n    properties.setContentType( &quot;text/plain&quot; );\n    properties.setContentEncoding( &quot;UTF-8&quot; );\n    properties.setPriority( 6 );\n    properties.setDeliveryMode( 1 );\n    properties.setTimestamp( stamp );\n            \n    // Send\n    publish.publishBasic( \n      buffer, \n      properties, \n      &quot;exchange_WLRNhKKM7d&quot;, \n      &quot;chat_topic&quot;, \n      false, \n      false \n    );\n            \n    // Clear text just sent\n    field.setText( &quot;&quot; );\n  }\n}\n</code></pre>\n<p>Once we have the JSON representation of the outgoing chat message, we create the AMQP message proper. This largely consists of setting various properties the correspond to how the broker should handle the message. After that, we use our previous instantiated publish channel and send the message itself, and the properties, to the exchange on the broker.</p>\n<h3 id=\"nextsteps\">Next Steps</h3>\n<p>The event handlers in the code can seem overwhelming at first - there are so many of them. Remember however that they are predominantly a convenience to provide a better behaving application. Do not let them get in your way of opening your favorite Java IDE and giving the chat client a try.</p>\n<p><img src=\"http://images.kevinhoyt.com/java.chat.screenshot.jpg\" alt=\"Screenshot of the Java application and Web application.\" loading=\"lazy\"></p>\n<p>Once you have the Java client running, you can head over to the Kaazing open source web site and run the <a href=\"http://kaazing.org/demos/chat.chat/run/\">live chat demonstration</a>. The two clients will be able to communicate with one another in real-time. The cross-platform goodness does not stop there either. I will write more in the future on using Kaazing Gateway with Android, iOS, and IoT.</p>\n<!--kg-card-end: markdown-->","comment_id":"18","plaintext":"If you look around the Kaazing open source web site, you will see a lot of\ninformation about delivering real-time solutions on the Web. This is not by\naccident - we a strong believers in the future of web standards. You can also\nuse Kaazing Gateway with other platforms as well, including Java, Android, and\niOS. In this post, we will take a quick look at building a chat application\nusing Java.\n\n\n--------------------------------------------------------------------------------\n\nThis is reposted on the Kaazing Open Source blog\n[http://kaazing.org/blog/java-chat-application/].\n\nDesktop\nAs a cross-platform technology, Java can run in many places. Indeed, Kaazing\nGateway itself is built on Java and intended to run on the server. Since we are\ntalking about a chat application here, we will be focusing on the client side\nlibrary usage. To avoid the complexity of mobile deployment, we will be\ndeveloping a desktop Java application.\n\nThe goal of this Java desktop chat application is to integrate with the web\nstandards chat application from a previous blog post. You can read the article\n[http://kaazing.org/blog/building-a-chat-application/] and even use the live\napplication [http://kaazing.org/demos/chat.chat/run/] (desktop or mobile). This\nmeans that Kaazing Gateway brings real-time cross-platform communication to your\nprojects. I will be following this post up by bringing the chat application over\nto Android, and eventually iOS.\n\nLibraries\nAs an open source project, you have access to all the client library code you\ncould want. Many of the repositories include build instructions as well.\nHowever, if you are just getting started exploring Kaazing Gateway features by\nbuilding this chat client, then I am guessing that you are not ready to commit\ncode to the project. Our first step then is to get the libraries we need in a\nform that is ready to drop into our IDE.\n\nAs it turns out, the build processes of the open source repositories use Apache\nMaven [http://maven.apache.org/] for project management. This means that we can\nhead on over to The Central Repository\n[http://search.maven.org/#search%7Cga%7C1%7Cg:%22org.kaazing%22] and pick up the\nready-to-use JAR files for our chat client. There are four different libraries\nthat we will need.\n\nThe first library we will need is Gateway client\n[http://search.maven.org/#search%7Cga%7C1%7Ca:%22gateway.client.java%22] (GitHub\n[https://github.com/kaazing/gateway.client.java]). This effectively implements a\nWebSocket client, which is the baseline for communication to Kaazing Gateway.\nYou will also need Gateway client transport\n[http://search.maven.org/#search%7Cga%7C1%7Ca:%22gateway.client.java.transport%22] \n(GitHub [https://github.com/kaazing/gateway.client.java.transport]). This\nlibrary implements the transport layer for Kaazing Gateway WebSocket Java client\nlibrary. Next up is an AMQP client\n[http://search.maven.org/#search%7Cga%7C1%7Ca:%22amqp.client.java%22] (GitHub\n[https://github.com/kaazing/amqp.client.java]) implementation. I wrote\nextensively about AMQP [http://kaazing.org/blog/amqp-breakdown/] in an earlier\npost.\n\nSince the Web client from the previous post uses JSON (JavaScript Object\nNotation) to communication messages, we also need the ability to handle JSON in\nJava. The Glassfish JSON library\n[http://search.maven.org/#search%7Cga%7C1%7Ca:%22javax.json%22] is an open\nsource implementation of JSR-353 [https://json-processing-spec.java.net/], which\nprovides a Java API for JSON processing. Paired with the Kaazing libraries, we\nhave everything we need for our chat client.\n\nUser Interface\nBuilding Java desktop user interfaces is something that most people remember\neither as a painful part of learning Java, or as an empowering art form. I\nactually fall into the later group, and as such have made the Java desktop\nclient look and behave exactly like the Web client. In the interest of brevity,\nand perhaps sanity, I will not be covering all the details of JList and custom\ncell renderers, or other fun Swing internals.\n\nYou might now be breathing a sigh of relief, but if you really want to take a\nlook under the covers, the entire project (minus the libraries) is available on\nmy Kaazing GitHub repository\n[https://github.com/krhoyt/Kaazing/tree/master/chat].\n\nConnecting\nThe first thing we need to do, is to establish a connection to Kaazing Gateway.\nDo not worry if you do not have a build running locally, or on some server in\nyour DMZ, we have a publicly available instance for you to use.\n\n// Establish connection\nprivate void initConnection()\n{\n  // Factory\n  factory = AmqpClientFactory.createAmqpClientFactory();\n        \n  try {\n    // Client\n    client = factory.createAmqpClient();\n            \n    // Connection listeners\n    client.addConnectionListener( new ConnectionListener() {\n            \n      // Connecting\n      public void onConnecting( ConnectionEvent ce ) \n      {\n        EventQueue.invokeLater( new Runnable() {\n          public void run()\n          {\n            System.out.println( \"Connecting...\" );\n          }\n         } );\n       }\n                \n      // Error\n      public void onConnectionError( ConnectionEvent ce ) \n      {\n        EventQueue.invokeLater( new Runnable() {\n          public void run()\n          {\n            System.out.println( \"Connection error.\" );\n          }\n        } );\n      }                              \n                \n      // Open\n      public void onConnectionOpen( ConnectionEvent ce ) \n      {\n        EventQueue.invokeLater( new Runnable() {\n          public void run()\n          {\n            System.out.println( \"Connection open.\" );\n                            \n            // Setup publisher\n            doClientOpen();\n          }\n        } );\n      }\n                \n      // Close\n      public void onConnectionClose( ConnectionEvent ce ) \n      {\n        EventQueue.invokeLater( new Runnable() {\n          public void run()\n          {\n            System.out.println( \"Connection closed.\" );\n          }\n        } );\n      }\n    } );\n            \n    // Connect to server\n    client.connect(\n      \"wss://sandbox.kaazing.net/amqp091\", \n      \"/\", \n      \"guest\", \n      \"guest\"\n    );\n  } catch( Exception e ) {\n    e.printStackTrace();\n  }\n}\n\n\nIn connecting, there are many event listeners you can use. For example, you\nmight use the open and close event listeners to show a visual indicator in the\nuser interface. Right now, all we are interested in knowing is when the\nconnection is ready to use which is handled in the \"onConnectionOpen\" event.\n\nChannels\nTo publish and consume messages, we will create an AMQP channel for each\noperation. Similar to opening the connection, there are many event listeners\nthat you can have. We are going to start with the publish channel, which\neffectively boils down to declaring the exchange. An exchange sits on the\nmessage broker (server), and acts as a bucket for incoming messages.\n\nprivate void doClientOpen()\n{\n  // Send messages\n  publish = client.openChannel();\n        \n  // Channel listeners\n  publish.addChannelListener( new ChannelAdapter() {\n    // Close\n    public void onClose( ChannelEvent ce ) \n    {\n      EventQueue.invokeLater( new Runnable() {\n        public void run()\n        {\n          System.out.println( \"Publish closed.\" );\n        }\n      } );\n    }\n            \n    // Error\n    public void onError( ChannelEvent ce ) \n    {\n      EventQueue.invokeLater( new Runnable() {\n        public void run()\n        {\n          System.out.println( \"Publish error.\" );\n        }\n      } );\n    }            \n            \n    // Declare exchange\n    public void onDeclareExchange( ChannelEvent ce ) \n    {\n      EventQueue.invokeLater( new Runnable() {\n        public void run()\n        {\n          System.out.println( \"Exchange declared.\" );\n                        \n          // Setup consumer\n          doPublishReady();\n        }\n      } );\n    }            \n            \n    // Open\n    public void onOpen( ChannelEvent ce ) \n    {\n      EventQueue.invokeLater( new Runnable() {\n        public void run()\n        {\n          System.out.println( \"Publish open.\" );\n                        \n          // Declare exchange\n          publish.declareExchange( \n            \"exchange_WLRNhKKM7d\", \n            \"direct\", \n            false, \n            false, \n            false, \n            null \n          );\n        }\n      } );\n    }            \n  } );\n}\n\n\nWith a publish exchange created, we can now move onto creating a consumer queue\nand binding it to the exchange. The consume channel is the first place your\nmessage will arrive in the Java client. AMQP is a binary protocol, so we get the\nbytes first. Since we are working with JSON from the Web client, we will want\nthe String equivalent of the message payload. We can then process the content\nhowever we feel best fit - more on that in the next section.\n\nprivate void doPublishReady()\n{\n  // Consume\n  consume = client.openChannel();\n        \n  // Channel listeners\n  consume.addChannelListener( new ChannelAdapter() {\n    // Bind queue\n    public void onBindQueue( ChannelEvent ce )\n    {\n      EventQueue.invokeLater( new Runnable() {\n        public void run()\n        {\n          System.out.println( \"Queue bound.\" );\n        }\n      } );\n    }\n            \n    // Close\n    public void onClose( ChannelEvent ce )\n    {\n      EventQueue.invokeLater( new Runnable() {\n        public void run()\n        {\n          System.out.println( \"Consume closed.\" );\n        }\n      } );\n    }            \n            \n    // Consume\n    public void onConsumeBasic( ChannelEvent ce )\n    {\n      EventQueue.invokeLater( new Runnable() {\n        public void run()\n        {\n          System.out.println( \"Consuming...\" );\n                        \n          // Open user interface for sending messages\n          doConsumeReady();\n        }\n      } );\n    }            \n            \n    // Declare queue\n    public void onDeclareQueue( ChannelEvent ce )\n    {\n      EventQueue.invokeLater( new Runnable() {\n        public void run()\n        {\n          System.out.println( \"Queue declared.\" );\n        }\n      } );\n    }            \n            \n    // Flow\n    public void onFlow( ChannelEvent ce )\n    {\n      try {\n        final boolean isActive = ce.isFlowActive();\n                    \n        EventQueue.invokeLater( new Runnable() {\n          public void run()\n          {\n            System.out.println( \"Flow is \" + ( isActive ? \"on\" : \"off\" ) + \".\" );\n          }\n        } );                  \n      } catch( Exception e ) {\n        e.printStackTrace();\n      }\n    }            \n            \n    // Message\n    public void onMessage( ChannelEvent ce )\n    {\n      byte[] bytes;\n                \n      bytes = new byte[ce.getBody().remaining()];\n      ce.getBody().get( bytes );\n                \n      final Long    tag = ( Long )ce.getArgument( \"deliveryTag\" );\n      final String  value = new String( bytes, Charset.forName( \"UTF-8\" ) );\n                \n      EventQueue.invokeLater( new Runnable() {\n        public void run()\n        {\n          AmqpChannel channel = null;\n                        \n          System.out.println( \"Message: \" + value );\n                        \n          // Place in user interface\n          doMessageArrived( value );\n                        \n          // Acknowledge\n          channel = ce.getChannel();\n          channel.ackBasic( tag.longValue(), true );\n        }\n      } );\n    }            \n            \n    // Open\n    public void onOpen( ChannelEvent ce )\n    {\n      EventQueue.invokeLater( new Runnable() {\n        public void run()\n        {\n          System.out.println( \"Consume open.\" );\n                        \n          // Declare queue\n          // Bind queue to exchange\n          // Start consuming\n          consume.declareQueue( \n            \"queue_AND_123\", \n            false, \n            false, \n            false, \n            false, \n            false, \n            null \n          ).bindQueue( \n            \"queue_AND_123\", \n            \"exchange_WLRNhKKM7d\", \n            \"chat_topic\", \n            false, \n            null \n          ).consumeBasic( \n            \"queue_AND_123\", \n            \"start_tag\", \n            false, \n            false, \n            false, \n            false, \n            null \n          );\n        }\n      } );\n    }                        \n  } );\n}\n\n\nBecause of the way we have setup our connection, messages must be acknowledged.\nThis is the last bit of code in the message event handler. Without this, the\nexchange on the broker will effectively hold onto the messages. This is actually\na desired behavior for the purposes of message persistence and guaranteed\ndelivery. Long-term however this can mean that your server fills up and runs out\nof memory. If you are not interested in persistence and guaranteed delivery, you\ncan configure the exchange to not require message acknowledgement.\n\nParsing JSON\nAs previously mentioned, messages from the Web client are in JSON format. This\nmeans we need to parse the message content into an equivalent Java data type.\nParsing JSON in Java reminds me of parsing XML in Java using SAX. The parser\neffectively rips through the content, while your code looks for specific\nelements that you are interested in further processing.\n\nprivate void doMessageArrived( String body )\n{\n  ChatMessage message = null;\n  Event        e = null;\n  InputStream stream = null;\n  JsonParser  parser = null;\n        \n  // String to InputStream\n  stream = new ByteArrayInputStream( \n    body.getBytes( StandardCharsets.UTF_8 ) \n  );\n  parser = Json.createParser( stream );\n        \n  // New chat message\n  message = new ChatMessage();\n  message.raw = body;\n        \n  // Parse JSON\n  while( parser.hasNext() )\n  {\n    e = parser.next();\n            \n    if( e == Event.KEY_NAME )\n    {\n      switch( parser.getString() )\n      {\n        case \"color\":\n          parser.next();\n          message.color = parseRgb( parser.getString() );\n          break;\n                        \n        case \"message\":\n          parser.next();\n          message.content = parser.getString();\n          break;\n                        \n        case \"user\":\n          parser.next();\n          message.user = parser.getString();\n          break;\n      }\n    }\n  }\n        \n  history.addElement( message );\n}\n\n\nIn order to hold the message content, and render it in a JList, I have created a\ncustom data type called ChatMessage. It simply has a few public properties on it\nto hold the specific pieces of data. Color from the Web client is in CSS format\nof \"rgb( 255, 255, 255 )\". This is further parsed into a Java Color object. The\nJList in turn has a custom cell renderer to show the message in the color\nprovided by the sending client.\n\nPublish\nPublishing a message that can be consumed by a Web client effectively means\nencoding our Java data types into their corresponding JSON format. Again, the\nprocess is very similar to SAX. The JSR provides for a builder object.\nProperties are added to the builder. To get the String format of the JSON data,\nwe use the JSR-provided writer object.\n\npublic void keyReleased( KeyEvent ke ) \n{\n  AmqpProperties     properties = null;\n  ByteBuffer         buffer = null;\n  JsonObject         result = null;\n  JsonObjectBuilder      builder = null;\n  StringWriter           sw = null;\n  Timestamp              stamp = null;\n        \n  // There is a message to send\n  if( ke.getKeyCode() == 10 && field.getText().trim().length() > 0 )\n  {\n    // Build JSON object\n    // Interacting with the web\n    builder = Json.createObjectBuilder();\n    builder.add( \"message\", field.getText().trim() );\n    builder.add( \n      \"color\", \n      \"rgb( \" + style.getRed() + \n      \", \" + style.getGreen() + \n      \", \" + style.getBlue() + \n      \" )\" \n    );\n    builder.add( \"user\", \"user_\" + now );\n            \n    result = builder.build();\n            \n    // Java JSON object to String\n    sw = new StringWriter();\n            \n    try( JsonWriter writer = Json.createWriter( sw ) ) {\n      writer.writeObject( result );\n    }\n            \n    // Here is what we are going to send\n    System.out.println( \"Sending: \" + sw.toString() );\n            \n    // Encode for AMQP\n    buffer = ByteBuffer.allocate( 512 );\n    buffer.put( sw.toString().getBytes( Charset.forName( \"UTF-8\" ) ) );\n    buffer.flip();\n            \n    stamp = new Timestamp( System.currentTimeMillis() );\n            \n    // Publish parameters\n    properties = new AmqpProperties();\n    properties.setMessageId( \"1\" );\n    properties.setCorrelationId( \"4\" );\n    properties.setAppId( \"java_chat\" );\n    properties.setUserId( \"user_\" + now );\n    properties.setContentType( \"text/plain\" );\n    properties.setContentEncoding( \"UTF-8\" );\n    properties.setPriority( 6 );\n    properties.setDeliveryMode( 1 );\n    properties.setTimestamp( stamp );\n            \n    // Send\n    publish.publishBasic( \n      buffer, \n      properties, \n      \"exchange_WLRNhKKM7d\", \n      \"chat_topic\", \n      false, \n      false \n    );\n            \n    // Clear text just sent\n    field.setText( \"\" );\n  }\n}\n\n\nOnce we have the JSON representation of the outgoing chat message, we create the\nAMQP message proper. This largely consists of setting various properties the\ncorrespond to how the broker should handle the message. After that, we use our\nprevious instantiated publish channel and send the message itself, and the\nproperties, to the exchange on the broker.\n\nNext Steps\nThe event handlers in the code can seem overwhelming at first - there are so\nmany of them. Remember however that they are predominantly a convenience to\nprovide a better behaving application. Do not let them get in your way of\nopening your favorite Java IDE and giving the chat client a try.\n\n\n\nOnce you have the Java client running, you can head over to the Kaazing open\nsource web site and run the live chat demonstration\n[http://kaazing.org/demos/chat.chat/run/]. The two clients will be able to\ncommunicate with one another in real-time. The cross-platform goodness does not\nstop there either. I will write more in the future on using Kaazing Gateway with\nAndroid, iOS, and IoT.","feature_image":"http://images.kevinhoyt.com/coffee.cup.beans.jpg","featured":0,"status":"published","locale":null,"visibility":"public","author_id":"1","created_at":"2015-04-21T15:22:20.000Z","updated_at":"2015-04-21T15:33:34.000Z","published_at":"2015-02-19T17:00:00.000Z","custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"type":"post","email_recipient_filter":"none"},{"id":"5adb8922351ffe0018a5771f","uuid":"dfcfc77d-8614-42d0-b800-033adb0e8cfe","title":"Android Chat Application","slug":"android-chat-application","mobiledoc":"{\"version\":\"0.3.1\",\"markups\":[],\"atoms\":[],\"cards\":[[\"markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\"*In a [previous post](http://kaazing.org/blog/java-chat-application/) I discussed using the open source Kaazing Gateway AMQP Java client library. This approach allows us to build a Java desktop chat client running against our free \\\"sandbox\\\" environment. You might be inclined to think that this lets us build the chat client on Android as well. While the Kaazing libraries do not change, the way we use them on Android does. In this post we will take a look at porting our chat application to Android.*\\n\\n---\\n\\n==This is reposted from the Kaazing Open Source [blog](http://kaazing.org/blog/android-chat-application/).===\\n\\n### A Note on Android\\n\\nI have seen it said before that Android is Java, but that Java is not Android. I think this sums up the Android workflow nicely. Android forgoes many of the tools found in the standard JDK (Java Development Kit). For example, while Android has a user interface to present, it does not use Swing or AWT, but rather it's own layout system, and UI components.\\n\\nIn the case of porting our desktop Java chat client to Android, we will have to migrate the user interface off of Swing, and into the Android workflow. In order to preserve performance, Android then decouples much of the UI operation and rendering into separate threads. It also puts network operations on a separate thread. Putting theses various threads together then requires additional consideration.\\n\\nFinally, there is the tooling. For this post I am using [Android Studio](http://developer.android.com/tools/studio/index.html). Android Studio is produced by IntelliJ, and is a massive leap forward from the Eclipse tooling for Android of old. That being said, it is still Android development, and uses many of the same tooling under the covers. In my experience, this can create unpredictability throughout the development process. When in doubt, restart the emulator and/or Studio processes.\\n\\nIt should be noted that this is not intended to be a tutorial on building Android applications. The focus of this post is predominantly the touch points between Kaazing Gateway libraries and Android development.\\n\\n### Be an Android Maven\\n\\nBefore you get too far into your project, you will want to link in the libraries we will be using. I talk about the libraries in the Java chat application post, and they are the same here, however Android Studio provides direct integration to Maven. You can find the dialog for managing the Maven modules for the project by right-clicking on the project root, and then selecting \\\"Open Module Settings\\\".\\n\\n![Android Studio module settings.](http://images.kevinhoyt.com/android.studio.module.settings.jpg)\\n\\nOnce you have the module dialog window open, you will want to select the \\\"Dependencies\\\" tab. You can then add modules by using the \\\"+\\\" icon at the bottom of the module listing (not the \\\"+\\\" above the \\\"SDK Location\\\" list item). Select the \\\"Library dependency\\\" option from the presented menu. The resulting dialog window will allow you to search the Maven repository for the required libraries.\\n\\nThe libraries you will need for this project are:\\n\\n* amqp.client.java\\n* gateway.client.java\\n* gateway.client.java.transport\\n* javax.json (Glassfish)\\n\\nYou can enter these package names in the provided dialog and click the search icon (magnifying glass) to find them in the Maven repository. The first three on the list are Kaazing Gateway specific. The last one is a JSON (JavaScript Object Notation) library since we will be integrating with a web client as well. That is right, all your clients are belong to us!\\n\\n![Android Studio module dependencies.](http://images.kevinhoyt.com/android.studio.module.dependencies.jpg)\\n\\nOnce you have all of these dependencies added, you can click the \\\"Apply\\\" button at the bottom of the modules dialog. This will kick off a Gradle build process, which may take some time. Once the process is completed, you are ready to start fleshing out the user interface, and wire in some real-time behaviors.\\n\\n### User Interface\\n\\nWhile the process of building an Adroid user interface is beyond the scope of this blog post, there are some differences from the Swing client. The most notable difference is that all Gateway communication has been extracted into its own class. While this makes for cleaner code, it also means that we will need to be able to communicate to and from the user interface.\\n\\n```\\n// Build message\\nchat = new ChatMessage();\\nchat.content = message.getText().toString();\\nchat.user = \\\"user_\\\" + now;\\nchat.color = style;\\n\\n// Publish to other clients\\ngateway.send( chat );\\n```\\n\\nSending a message is a matter of building a ChatMessage object, and using the \\\"send()\\\" method on the Gateway class. The Gateway class will handle translating the properties of the ChatMessage object into JSON prior to sending. When messages arrive however, I decided to use Java callbacks.\\n\\n```\\n// Consume\\npublic void onConsumeBasic( ChannelEvent ce ) {\\n  dispatch.dispatchAsync(new Runnable() {\\n    public void run() {\\n      System.out.println( \\\"Consuming...\\\" );\\n\\n      callback.onAllClear();\\n    }\\n  } );\\n}\\n\\n...\\n\\n// doMessageArrived\\ncallback.onMessage( message );\\n```\\n\\nThe \\\"Gateway\\\" class uses the \\\"ChatCallback\\\" interface, and can emit two calls - onAllClear and onMessage. The \\\"onAllClear\\\" is called when the connection to the server has been established, and both the publish and consume channels have been opened. The \\\"onMessage\\\" is called when a message has arrived - this includes not only other clients, but also the messages sent from this application itself. It sends along with it a \\\"ChatMessage\\\" object.\\n\\n```\\n// Gateway\\ngateway = new Gateway();\\ngateway.callback = new ChatCallback() {\\n  @Override\\n  public void onAllClear() {\\n    Log.i( \\\"Gateway\\\", \\\"Ready for publish and consume.\\\" );\\n  }\\n\\n  @Override\\n  public void onMessage( ChatMessage message ) {\\n    // Add to collection\\n    items.add( message );\\n\\n    // Update list\\n    history.post( new Runnable() {\\n      @Override\\n      public void run() {\\n        adapter.notifyDataSetChanged();\\n      }\\n    } );\\n  }\\n};\\n```\\n\\nThere are a half-dozen different ways to do this type of notification on Android. You might choose to implement a classic Observer pattern along the lines of the Swing EventListener approach. You might choose to use a runnable Handler implementation to post message around the application system. You can use whatever approach you like, but the key here is in keeping the network access on a separate thread from the user interface.\\n\\n### The Missing EventQueue\\n\\nIn the desktop Java application, we kept the network access separate from the user interface using threading using the \\\"EventQueue\\\" class. You might be wondering why I did not use that class here as well? The answer is that \\\"EventQueue\\\" is a Java Swing class, and Android does not include the Swing features of the JDK. Android has it's own solution to managing thread communication - the \\\"HandlerThread\\\" class.\\n\\n```\\nimport android.os.Handler;\\nimport android.os.HandlerThread;\\n\\npublic class DispatchQueue extends HandlerThread {\\n  private Handler handler;\\n\\n  public DispatchQueue( String name ) {\\n    super( name );\\n  }\\n\\n  // Ensure thread ready\\n  public void waitUntilReady() {\\n    handler = new Handler( getLooper() );\\n  }\\n\\n  // Add to message queue\\n  public void dispatchAsync( Runnable task ) {\\n    handler.post( task );\\n  }\\n\\n  public void removePendingJobs() {\\n    handler.removeCallbacksAndMessages( null );\\n  }\\n}\\n\\n...\\n\\n// Channel listeners\\npublish.addChannelListener( new ChannelAdapter() {\\n  // Close\\n  public void onClose( ChannelEvent ce ) {\\n    dispatch.dispatchAsync( new Runnable() {\\n      public void run() {\\n        System.out.println( \\\"Publish closed.\\\" );\\n      }\\n    } );\\n  }\\n  \\n  ...\\n}\\n```\\n\\nThe \\\"Gateway\\\" class, which implements all the chat network communication, uses this technique extensively. While the \\\"Gateway\\\" class uses other classes such as \\\"AmqpChannel\\\" which have their own callback routines, those callbacks are not exposed to the \\\"ChatActivity\\\" in this application. This is an implementation detail you may choose to add in your own project. The \\\"AmqpChannel\\\" and \\\"AmqpClient\\\" classes expose a variety of callbacks around network state. For now, I just dump those calls to the log.\\n\\n### Next Steps\\n\\nSo long as you know where the Android workflow differs from desktop Java, implementing a real-time, cross-platform/device chat using Kaazing Gateway could not be easier. Make sure you leverage the Maven repositories, keep your threads separate, and manage network connectivity efficiently, and you will have no problems. Of course this project is available on [my personal GitHub repository](https://github.com/krhoyt/Kaazing/tree/master/chat/android) as well should you want to download it and have a try yourself.\\n\\n![Android, Java Swing, and Web on iOS chatting together.](http://images.kevinhoyt.com/chatting.it.up.jpg)\\n\\nAs this point you have real-time communication across web standards (desktop and mobile), Java on the desktop, and Android devices. You do not even have to build and manage your own server to get it going thanks to the free \\\"sandbox\\\" server. From here maybe you want an IoT (Internet of Things) example? Maybe all this AMQP talk is a little more than you need for your project, and WebSocket alone will work? If you have specific use-cases you would like to see discussed, please let us know!\\n\"}]],\"sections\":[[10,0]],\"ghostVersion\":\"3.0\"}","html":"<!--kg-card-begin: markdown--><p><em>In a <a href=\"http://kaazing.org/blog/java-chat-application/\">previous post</a> I discussed using the open source Kaazing Gateway AMQP Java client library. This approach allows us to build a Java desktop chat client running against our free &quot;sandbox&quot; environment. You might be inclined to think that this lets us build the chat client on Android as well. While the Kaazing libraries do not change, the way we use them on Android does. In this post we will take a look at porting our chat application to Android.</em></p>\n<hr>\n<p><mark>This is reposted from the Kaazing Open Source <a href=\"http://kaazing.org/blog/android-chat-application/\">blog</a>.</mark>=</p>\n<h3 id=\"anoteonandroid\">A Note on Android</h3>\n<p>I have seen it said before that Android is Java, but that Java is not Android. I think this sums up the Android workflow nicely. Android forgoes many of the tools found in the standard JDK (Java Development Kit). For example, while Android has a user interface to present, it does not use Swing or AWT, but rather it's own layout system, and UI components.</p>\n<p>In the case of porting our desktop Java chat client to Android, we will have to migrate the user interface off of Swing, and into the Android workflow. In order to preserve performance, Android then decouples much of the UI operation and rendering into separate threads. It also puts network operations on a separate thread. Putting theses various threads together then requires additional consideration.</p>\n<p>Finally, there is the tooling. For this post I am using <a href=\"http://developer.android.com/tools/studio/index.html\">Android Studio</a>. Android Studio is produced by IntelliJ, and is a massive leap forward from the Eclipse tooling for Android of old. That being said, it is still Android development, and uses many of the same tooling under the covers. In my experience, this can create unpredictability throughout the development process. When in doubt, restart the emulator and/or Studio processes.</p>\n<p>It should be noted that this is not intended to be a tutorial on building Android applications. The focus of this post is predominantly the touch points between Kaazing Gateway libraries and Android development.</p>\n<h3 id=\"beanandroidmaven\">Be an Android Maven</h3>\n<p>Before you get too far into your project, you will want to link in the libraries we will be using. I talk about the libraries in the Java chat application post, and they are the same here, however Android Studio provides direct integration to Maven. You can find the dialog for managing the Maven modules for the project by right-clicking on the project root, and then selecting &quot;Open Module Settings&quot;.</p>\n<p><img src=\"http://images.kevinhoyt.com/android.studio.module.settings.jpg\" alt=\"Android Studio module settings.\" loading=\"lazy\"></p>\n<p>Once you have the module dialog window open, you will want to select the &quot;Dependencies&quot; tab. You can then add modules by using the &quot;+&quot; icon at the bottom of the module listing (not the &quot;+&quot; above the &quot;SDK Location&quot; list item). Select the &quot;Library dependency&quot; option from the presented menu. The resulting dialog window will allow you to search the Maven repository for the required libraries.</p>\n<p>The libraries you will need for this project are:</p>\n<ul>\n<li>amqp.client.java</li>\n<li>gateway.client.java</li>\n<li>gateway.client.java.transport</li>\n<li>javax.json (Glassfish)</li>\n</ul>\n<p>You can enter these package names in the provided dialog and click the search icon (magnifying glass) to find them in the Maven repository. The first three on the list are Kaazing Gateway specific. The last one is a JSON (JavaScript Object Notation) library since we will be integrating with a web client as well. That is right, all your clients are belong to us!</p>\n<p><img src=\"http://images.kevinhoyt.com/android.studio.module.dependencies.jpg\" alt=\"Android Studio module dependencies.\" loading=\"lazy\"></p>\n<p>Once you have all of these dependencies added, you can click the &quot;Apply&quot; button at the bottom of the modules dialog. This will kick off a Gradle build process, which may take some time. Once the process is completed, you are ready to start fleshing out the user interface, and wire in some real-time behaviors.</p>\n<h3 id=\"userinterface\">User Interface</h3>\n<p>While the process of building an Adroid user interface is beyond the scope of this blog post, there are some differences from the Swing client. The most notable difference is that all Gateway communication has been extracted into its own class. While this makes for cleaner code, it also means that we will need to be able to communicate to and from the user interface.</p>\n<pre><code>// Build message\nchat = new ChatMessage();\nchat.content = message.getText().toString();\nchat.user = &quot;user_&quot; + now;\nchat.color = style;\n\n// Publish to other clients\ngateway.send( chat );\n</code></pre>\n<p>Sending a message is a matter of building a ChatMessage object, and using the &quot;send()&quot; method on the Gateway class. The Gateway class will handle translating the properties of the ChatMessage object into JSON prior to sending. When messages arrive however, I decided to use Java callbacks.</p>\n<pre><code>// Consume\npublic void onConsumeBasic( ChannelEvent ce ) {\n  dispatch.dispatchAsync(new Runnable() {\n    public void run() {\n      System.out.println( &quot;Consuming...&quot; );\n\n      callback.onAllClear();\n    }\n  } );\n}\n\n...\n\n// doMessageArrived\ncallback.onMessage( message );\n</code></pre>\n<p>The &quot;Gateway&quot; class uses the &quot;ChatCallback&quot; interface, and can emit two calls - onAllClear and onMessage. The &quot;onAllClear&quot; is called when the connection to the server has been established, and both the publish and consume channels have been opened. The &quot;onMessage&quot; is called when a message has arrived - this includes not only other clients, but also the messages sent from this application itself. It sends along with it a &quot;ChatMessage&quot; object.</p>\n<pre><code>// Gateway\ngateway = new Gateway();\ngateway.callback = new ChatCallback() {\n  @Override\n  public void onAllClear() {\n    Log.i( &quot;Gateway&quot;, &quot;Ready for publish and consume.&quot; );\n  }\n\n  @Override\n  public void onMessage( ChatMessage message ) {\n    // Add to collection\n    items.add( message );\n\n    // Update list\n    history.post( new Runnable() {\n      @Override\n      public void run() {\n        adapter.notifyDataSetChanged();\n      }\n    } );\n  }\n};\n</code></pre>\n<p>There are a half-dozen different ways to do this type of notification on Android. You might choose to implement a classic Observer pattern along the lines of the Swing EventListener approach. You might choose to use a runnable Handler implementation to post message around the application system. You can use whatever approach you like, but the key here is in keeping the network access on a separate thread from the user interface.</p>\n<h3 id=\"themissingeventqueue\">The Missing EventQueue</h3>\n<p>In the desktop Java application, we kept the network access separate from the user interface using threading using the &quot;EventQueue&quot; class. You might be wondering why I did not use that class here as well? The answer is that &quot;EventQueue&quot; is a Java Swing class, and Android does not include the Swing features of the JDK. Android has it's own solution to managing thread communication - the &quot;HandlerThread&quot; class.</p>\n<pre><code>import android.os.Handler;\nimport android.os.HandlerThread;\n\npublic class DispatchQueue extends HandlerThread {\n  private Handler handler;\n\n  public DispatchQueue( String name ) {\n    super( name );\n  }\n\n  // Ensure thread ready\n  public void waitUntilReady() {\n    handler = new Handler( getLooper() );\n  }\n\n  // Add to message queue\n  public void dispatchAsync( Runnable task ) {\n    handler.post( task );\n  }\n\n  public void removePendingJobs() {\n    handler.removeCallbacksAndMessages( null );\n  }\n}\n\n...\n\n// Channel listeners\npublish.addChannelListener( new ChannelAdapter() {\n  // Close\n  public void onClose( ChannelEvent ce ) {\n    dispatch.dispatchAsync( new Runnable() {\n      public void run() {\n        System.out.println( &quot;Publish closed.&quot; );\n      }\n    } );\n  }\n  \n  ...\n}\n</code></pre>\n<p>The &quot;Gateway&quot; class, which implements all the chat network communication, uses this technique extensively. While the &quot;Gateway&quot; class uses other classes such as &quot;AmqpChannel&quot; which have their own callback routines, those callbacks are not exposed to the &quot;ChatActivity&quot; in this application. This is an implementation detail you may choose to add in your own project. The &quot;AmqpChannel&quot; and &quot;AmqpClient&quot; classes expose a variety of callbacks around network state. For now, I just dump those calls to the log.</p>\n<h3 id=\"nextsteps\">Next Steps</h3>\n<p>So long as you know where the Android workflow differs from desktop Java, implementing a real-time, cross-platform/device chat using Kaazing Gateway could not be easier. Make sure you leverage the Maven repositories, keep your threads separate, and manage network connectivity efficiently, and you will have no problems. Of course this project is available on <a href=\"https://github.com/krhoyt/Kaazing/tree/master/chat/android\">my personal GitHub repository</a> as well should you want to download it and have a try yourself.</p>\n<p><img src=\"http://images.kevinhoyt.com/chatting.it.up.jpg\" alt=\"Android, Java Swing, and Web on iOS chatting together.\" loading=\"lazy\"></p>\n<p>As this point you have real-time communication across web standards (desktop and mobile), Java on the desktop, and Android devices. You do not even have to build and manage your own server to get it going thanks to the free &quot;sandbox&quot; server. From here maybe you want an IoT (Internet of Things) example? Maybe all this AMQP talk is a little more than you need for your project, and WebSocket alone will work? If you have specific use-cases you would like to see discussed, please let us know!</p>\n<!--kg-card-end: markdown-->","comment_id":"19","plaintext":"In a previous post [http://kaazing.org/blog/java-chat-application/] I discussed\nusing the open source Kaazing Gateway AMQP Java client library. This approach\nallows us to build a Java desktop chat client running against our free \"sandbox\"\nenvironment. You might be inclined to think that this lets us build the chat\nclient on Android as well. While the Kaazing libraries do not change, the way we\nuse them on Android does. In this post we will take a look at porting our chat\napplication to Android.\n\n\n--------------------------------------------------------------------------------\n\nThis is reposted from the Kaazing Open Source blog\n[http://kaazing.org/blog/android-chat-application/].=\n\nA Note on Android\nI have seen it said before that Android is Java, but that Java is not Android. I\nthink this sums up the Android workflow nicely. Android forgoes many of the\ntools found in the standard JDK (Java Development Kit). For example, while\nAndroid has a user interface to present, it does not use Swing or AWT, but\nrather it's own layout system, and UI components.\n\nIn the case of porting our desktop Java chat client to Android, we will have to\nmigrate the user interface off of Swing, and into the Android workflow. In order\nto preserve performance, Android then decouples much of the UI operation and\nrendering into separate threads. It also puts network operations on a separate\nthread. Putting theses various threads together then requires additional\nconsideration.\n\nFinally, there is the tooling. For this post I am using Android Studio\n[http://developer.android.com/tools/studio/index.html]. Android Studio is\nproduced by IntelliJ, and is a massive leap forward from the Eclipse tooling for\nAndroid of old. That being said, it is still Android development, and uses many\nof the same tooling under the covers. In my experience, this can create\nunpredictability throughout the development process. When in doubt, restart the\nemulator and/or Studio processes.\n\nIt should be noted that this is not intended to be a tutorial on building\nAndroid applications. The focus of this post is predominantly the touch points\nbetween Kaazing Gateway libraries and Android development.\n\nBe an Android Maven\nBefore you get too far into your project, you will want to link in the libraries\nwe will be using. I talk about the libraries in the Java chat application post,\nand they are the same here, however Android Studio provides direct integration\nto Maven. You can find the dialog for managing the Maven modules for the project\nby right-clicking on the project root, and then selecting \"Open Module\nSettings\".\n\n\n\nOnce you have the module dialog window open, you will want to select the\n\"Dependencies\" tab. You can then add modules by using the \"+\" icon at the bottom\nof the module listing (not the \"+\" above the \"SDK Location\" list item). Select\nthe \"Library dependency\" option from the presented menu. The resulting dialog\nwindow will allow you to search the Maven repository for the required libraries.\n\nThe libraries you will need for this project are:\n\n * amqp.client.java\n * gateway.client.java\n * gateway.client.java.transport\n * javax.json (Glassfish)\n\nYou can enter these package names in the provided dialog and click the search\nicon (magnifying glass) to find them in the Maven repository. The first three on\nthe list are Kaazing Gateway specific. The last one is a JSON (JavaScript Object\nNotation) library since we will be integrating with a web client as well. That\nis right, all your clients are belong to us!\n\n\n\nOnce you have all of these dependencies added, you can click the \"Apply\" button\nat the bottom of the modules dialog. This will kick off a Gradle build process,\nwhich may take some time. Once the process is completed, you are ready to start\nfleshing out the user interface, and wire in some real-time behaviors.\n\nUser Interface\nWhile the process of building an Adroid user interface is beyond the scope of\nthis blog post, there are some differences from the Swing client. The most\nnotable difference is that all Gateway communication has been extracted into its\nown class. While this makes for cleaner code, it also means that we will need to\nbe able to communicate to and from the user interface.\n\n// Build message\nchat = new ChatMessage();\nchat.content = message.getText().toString();\nchat.user = \"user_\" + now;\nchat.color = style;\n\n// Publish to other clients\ngateway.send( chat );\n\n\nSending a message is a matter of building a ChatMessage object, and using the\n\"send()\" method on the Gateway class. The Gateway class will handle translating\nthe properties of the ChatMessage object into JSON prior to sending. When\nmessages arrive however, I decided to use Java callbacks.\n\n// Consume\npublic void onConsumeBasic( ChannelEvent ce ) {\n  dispatch.dispatchAsync(new Runnable() {\n    public void run() {\n      System.out.println( \"Consuming...\" );\n\n      callback.onAllClear();\n    }\n  } );\n}\n\n...\n\n// doMessageArrived\ncallback.onMessage( message );\n\n\nThe \"Gateway\" class uses the \"ChatCallback\" interface, and can emit two calls -\nonAllClear and onMessage. The \"onAllClear\" is called when the connection to the\nserver has been established, and both the publish and consume channels have been\nopened. The \"onMessage\" is called when a message has arrived - this includes not\nonly other clients, but also the messages sent from this application itself. It\nsends along with it a \"ChatMessage\" object.\n\n// Gateway\ngateway = new Gateway();\ngateway.callback = new ChatCallback() {\n  @Override\n  public void onAllClear() {\n    Log.i( \"Gateway\", \"Ready for publish and consume.\" );\n  }\n\n  @Override\n  public void onMessage( ChatMessage message ) {\n    // Add to collection\n    items.add( message );\n\n    // Update list\n    history.post( new Runnable() {\n      @Override\n      public void run() {\n        adapter.notifyDataSetChanged();\n      }\n    } );\n  }\n};\n\n\nThere are a half-dozen different ways to do this type of notification on\nAndroid. You might choose to implement a classic Observer pattern along the\nlines of the Swing EventListener approach. You might choose to use a runnable\nHandler implementation to post message around the application system. You can\nuse whatever approach you like, but the key here is in keeping the network\naccess on a separate thread from the user interface.\n\nThe Missing EventQueue\nIn the desktop Java application, we kept the network access separate from the\nuser interface using threading using the \"EventQueue\" class. You might be\nwondering why I did not use that class here as well? The answer is that\n\"EventQueue\" is a Java Swing class, and Android does not include the Swing\nfeatures of the JDK. Android has it's own solution to managing thread\ncommunication - the \"HandlerThread\" class.\n\nimport android.os.Handler;\nimport android.os.HandlerThread;\n\npublic class DispatchQueue extends HandlerThread {\n  private Handler handler;\n\n  public DispatchQueue( String name ) {\n    super( name );\n  }\n\n  // Ensure thread ready\n  public void waitUntilReady() {\n    handler = new Handler( getLooper() );\n  }\n\n  // Add to message queue\n  public void dispatchAsync( Runnable task ) {\n    handler.post( task );\n  }\n\n  public void removePendingJobs() {\n    handler.removeCallbacksAndMessages( null );\n  }\n}\n\n...\n\n// Channel listeners\npublish.addChannelListener( new ChannelAdapter() {\n  // Close\n  public void onClose( ChannelEvent ce ) {\n    dispatch.dispatchAsync( new Runnable() {\n      public void run() {\n        System.out.println( \"Publish closed.\" );\n      }\n    } );\n  }\n  \n  ...\n}\n\n\nThe \"Gateway\" class, which implements all the chat network communication, uses\nthis technique extensively. While the \"Gateway\" class uses other classes such as\n\"AmqpChannel\" which have their own callback routines, those callbacks are not\nexposed to the \"ChatActivity\" in this application. This is an implementation\ndetail you may choose to add in your own project. The \"AmqpChannel\" and\n\"AmqpClient\" classes expose a variety of callbacks around network state. For\nnow, I just dump those calls to the log.\n\nNext Steps\nSo long as you know where the Android workflow differs from desktop Java,\nimplementing a real-time, cross-platform/device chat using Kaazing Gateway could\nnot be easier. Make sure you leverage the Maven repositories, keep your threads\nseparate, and manage network connectivity efficiently, and you will have no\nproblems. Of course this project is available on my personal GitHub repository\n[https://github.com/krhoyt/Kaazing/tree/master/chat/android] as well should you\nwant to download it and have a try yourself.\n\n\n\nAs this point you have real-time communication across web standards (desktop and\nmobile), Java on the desktop, and Android devices. You do not even have to build\nand manage your own server to get it going thanks to the free \"sandbox\" server.\nFrom here maybe you want an IoT (Internet of Things) example? Maybe all this\nAMQP talk is a little more than you need for your project, and WebSocket alone\nwill work? If you have specific use-cases you would like to see discussed,\nplease let us know!","feature_image":"http://images.kevinhoyt.com/honda.asimo.robot.png","featured":0,"status":"published","locale":null,"visibility":"public","author_id":"1","created_at":"2015-04-21T15:34:47.000Z","updated_at":"2015-04-21T15:45:16.000Z","published_at":"2015-03-04T17:00:00.000Z","custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"type":"post","email_recipient_filter":"none"},{"id":"5adb8922351ffe0018a57720","uuid":"0c21e235-447d-4617-a2d7-f1a9367c57f7","title":"Real-Time Quadcopter Telemetry","slug":"quadcopter-telemetry","mobiledoc":"{\"version\":\"0.3.1\",\"markups\":[],\"atoms\":[],\"cards\":[[\"markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\"*Modern commercial aircraft have what is commonly referred to as a \\\"black box\\\".  The black box is a flight data recorder.  It captures numerous sensor data points across the aircraft, and records them for playback in the case of a catastrophe.*\\n\\n*As we have seen lately however, the onboard data recorder can be turned off, destroyed beyond playback, or altogether lost.  What if all that data was streamed in real-time across a satellite connection and into a data center?  With a DJI Phantom 2, Arduino, a pile of sensors, and an Inmarsat modem, we set out to explore this possibility.*\\n\\n---\\n\\n==This is reposted from the Kaazing [demo listing](http://developer.kaazing.com/portfolio/telemetry/).==\\n\\n\\n###Pieces Parts\\n\\nWhile we could not afford to expense a Boeing 737, we did manage to pick up a [DJI Phantom 2](http://www.dji.com/product/phantom-2).  The Phantom is an remote control quadcopter that is capable of carrying our sensor array.\\n\\nThe sensor array consists of a tupperware bowl filled with an Arduino and a stack of sensors.  The tupperware bowl was then strapped between the landing gear of the Phantom quadcopter.  What is that?  Oh, the actual sensors we used?  Sure, there was quite an array, but we largely attempted to emulate what pilots call the \\\"six pack\\\" - the core set of instruments necessary to fly a plane safely (per the FAA).\\n\\n* Arduino Uno\\n* GPS module on a GPS Shield\\n* Compass\\n* Accelerometer (tilt)\\n* Temperature\\n* Humidity\\n* RGB LED (bi-directional communication)\\n* XBee module on an XBee Shield\\n\\n![Quadcopter sensor array.](http://images.kevinhoyt.com/quadcopter.sensor.array.jpg)\\n\\nWhile we did not have the time or resources to stream the video of the flight, we did mount a GoPro Hero 3 in the tupperware case as well.  It recorded the flights we made independently from the sensor data.  The sensor data was recorded, but also streamed across the web in real-time.  More on that in a moment.  For now, have a look at the project in action.\\n\\n<iframe src=\\\"https://player.vimeo.com/video/93023138\\\" width=\\\"500\\\" height=\\\"281\\\" frameborder=\\\"0\\\" webkitallowfullscreen mozallowfullscreen allowfullscreen></iframe>\\n\\n###Data Communication\\n\\nBecause the Phantom is not quite strong enough to carry a satellite modem, we needed a way to get the sensor data onto the web.  Of course we cannot exactly put a wire to the Phantom either, so the solution needed to be wireless.\\n\\nWe accomplished this using the aforementioned XBee module.  The XBee gave us a range of about one mile, which was plenty for our experiment.  The XBee on the Phantom had a receiver back on the ground, plugged into a computer USB port.  The computer was connected to an Inmarsat satellite modem.  A Java program on the computer, read the data coming from the XBee radion via the USB port, and then sent that data to a Kaazing Gateway as it arrived.  \\n\\n![Inmarsat Explorer.](http://images.kevinhoyt.com/quadcopter.inmarsat.jpg)\\n\\nKeeping in mind that we had to go over XBee, wireless to an Inmarsat modem, up to a satellite in space, back down to a data center connected to the Internet, and then to the various devices through whatever means of connection they had, you might expect some serious lag.  In the end, we reliably measured a latency of **less than one** second from the sensor array, to a mobile device (on a cellular network) tracking the flight.  ==One second!==\\n\\n###Data Recording\\n\\nThanks to Kaazing Gateway, data from the Java program was sent using publish-subscribe.  This meant that data was pushed to any interested clients.  In most cases, this was a mobile phone or tablet monitoring the flight.  However, a Node.js process also listened for sensor messages, and recorded the data to a database.\\n\\n![Web browser tracking the flight in real-time.](http://images.kevinhoyt.com/quadcopter.screenshot.png)\\n\\nBecause the data recording was decoupled from the sensor reporting, it could have been Node.js or any other technology on the other side.  While we as developers tend to think of this as platforms, with publish-subscribe, it could effectively be several entirely different entities.  The aircraft manufacturer could record the data.  The FAA could record the data.  The airline itself could record the data.  All using their own stacks, own database structures, etc. without needing to be a part of the actual telemetry broadcasting itself.\\n\\n###Big Picture\\n\\nInitially, we set out to show that flight data could be recorded in real-time to the cloud.  While our rig is nowhere near what is needed for deployment on actual aircraft, it proves the point that a black box in the cloud is feasible.  But then we started thinking about the downstream implications.\\n\\n![Black box in the cloud.](http://images.kevinhoyt.com/quadcopter.cloud.jpg)\\n\\nThink about how applications track flights today.  Maybe something like a TripIt or FlightTrack that let you know where your flight is, or how soon you can expect grandma to arrive for Christmas.  These application scrape data from various sources to try and give you a decent picture of what is going on.  But imagine if they could simply subscribe to specific flight data for a given client?\\n\\nLeveraging real-time data, and decoupling the parts and pieces using publish-subscribe opens the door to entirely new ways to view and manage aircraft data.  This impacts not only manufacturers and aviation entities such as the FAA, but trickles right on down to you and me in the applications we use.  Modernizing just a fragment of the infrastructure could significantly travel in a positive way.\\n\"}]],\"sections\":[[10,0]],\"ghostVersion\":\"3.0\"}","html":"<!--kg-card-begin: markdown--><p><em>Modern commercial aircraft have what is commonly referred to as a &quot;black box&quot;.  The black box is a flight data recorder.  It captures numerous sensor data points across the aircraft, and records them for playback in the case of a catastrophe.</em></p>\n<p><em>As we have seen lately however, the onboard data recorder can be turned off, destroyed beyond playback, or altogether lost.  What if all that data was streamed in real-time across a satellite connection and into a data center?  With a DJI Phantom 2, Arduino, a pile of sensors, and an Inmarsat modem, we set out to explore this possibility.</em></p>\n<hr>\n<p><mark>This is reposted from the Kaazing <a href=\"http://developer.kaazing.com/portfolio/telemetry/\">demo listing</a>.</mark></p>\n<h3 id=\"piecesparts\">Pieces Parts</h3>\n<p>While we could not afford to expense a Boeing 737, we did manage to pick up a <a href=\"http://www.dji.com/product/phantom-2\">DJI Phantom 2</a>.  The Phantom is an remote control quadcopter that is capable of carrying our sensor array.</p>\n<p>The sensor array consists of a tupperware bowl filled with an Arduino and a stack of sensors.  The tupperware bowl was then strapped between the landing gear of the Phantom quadcopter.  What is that?  Oh, the actual sensors we used?  Sure, there was quite an array, but we largely attempted to emulate what pilots call the &quot;six pack&quot; - the core set of instruments necessary to fly a plane safely (per the FAA).</p>\n<ul>\n<li>Arduino Uno</li>\n<li>GPS module on a GPS Shield</li>\n<li>Compass</li>\n<li>Accelerometer (tilt)</li>\n<li>Temperature</li>\n<li>Humidity</li>\n<li>RGB LED (bi-directional communication)</li>\n<li>XBee module on an XBee Shield</li>\n</ul>\n<p><img src=\"http://images.kevinhoyt.com/quadcopter.sensor.array.jpg\" alt=\"Quadcopter sensor array.\" loading=\"lazy\"></p>\n<p>While we did not have the time or resources to stream the video of the flight, we did mount a GoPro Hero 3 in the tupperware case as well.  It recorded the flights we made independently from the sensor data.  The sensor data was recorded, but also streamed across the web in real-time.  More on that in a moment.  For now, have a look at the project in action.</p>\n<iframe src=\"https://player.vimeo.com/video/93023138\" width=\"500\" height=\"281\" frameborder=\"0\" webkitallowfullscreen mozallowfullscreen allowfullscreen></iframe>\n<h3 id=\"datacommunication\">Data Communication</h3>\n<p>Because the Phantom is not quite strong enough to carry a satellite modem, we needed a way to get the sensor data onto the web.  Of course we cannot exactly put a wire to the Phantom either, so the solution needed to be wireless.</p>\n<p>We accomplished this using the aforementioned XBee module.  The XBee gave us a range of about one mile, which was plenty for our experiment.  The XBee on the Phantom had a receiver back on the ground, plugged into a computer USB port.  The computer was connected to an Inmarsat satellite modem.  A Java program on the computer, read the data coming from the XBee radion via the USB port, and then sent that data to a Kaazing Gateway as it arrived.</p>\n<p><img src=\"http://images.kevinhoyt.com/quadcopter.inmarsat.jpg\" alt=\"Inmarsat Explorer.\" loading=\"lazy\"></p>\n<p>Keeping in mind that we had to go over XBee, wireless to an Inmarsat modem, up to a satellite in space, back down to a data center connected to the Internet, and then to the various devices through whatever means of connection they had, you might expect some serious lag.  In the end, we reliably measured a latency of <strong>less than one</strong> second from the sensor array, to a mobile device (on a cellular network) tracking the flight.  <mark>One second!</mark></p>\n<h3 id=\"datarecording\">Data Recording</h3>\n<p>Thanks to Kaazing Gateway, data from the Java program was sent using publish-subscribe.  This meant that data was pushed to any interested clients.  In most cases, this was a mobile phone or tablet monitoring the flight.  However, a Node.js process also listened for sensor messages, and recorded the data to a database.</p>\n<p><img src=\"http://images.kevinhoyt.com/quadcopter.screenshot.png\" alt=\"Web browser tracking the flight in real-time.\" loading=\"lazy\"></p>\n<p>Because the data recording was decoupled from the sensor reporting, it could have been Node.js or any other technology on the other side.  While we as developers tend to think of this as platforms, with publish-subscribe, it could effectively be several entirely different entities.  The aircraft manufacturer could record the data.  The FAA could record the data.  The airline itself could record the data.  All using their own stacks, own database structures, etc. without needing to be a part of the actual telemetry broadcasting itself.</p>\n<h3 id=\"bigpicture\">Big Picture</h3>\n<p>Initially, we set out to show that flight data could be recorded in real-time to the cloud.  While our rig is nowhere near what is needed for deployment on actual aircraft, it proves the point that a black box in the cloud is feasible.  But then we started thinking about the downstream implications.</p>\n<p><img src=\"http://images.kevinhoyt.com/quadcopter.cloud.jpg\" alt=\"Black box in the cloud.\" loading=\"lazy\"></p>\n<p>Think about how applications track flights today.  Maybe something like a TripIt or FlightTrack that let you know where your flight is, or how soon you can expect grandma to arrive for Christmas.  These application scrape data from various sources to try and give you a decent picture of what is going on.  But imagine if they could simply subscribe to specific flight data for a given client?</p>\n<p>Leveraging real-time data, and decoupling the parts and pieces using publish-subscribe opens the door to entirely new ways to view and manage aircraft data.  This impacts not only manufacturers and aviation entities such as the FAA, but trickles right on down to you and me in the applications we use.  Modernizing just a fragment of the infrastructure could significantly travel in a positive way.</p>\n<!--kg-card-end: markdown-->","comment_id":"20","plaintext":"Modern commercial aircraft have what is commonly referred to as a \"black box\".\nThe black box is a flight data recorder. It captures numerous sensor data points\nacross the aircraft, and records them for playback in the case of a catastrophe.\n\nAs we have seen lately however, the onboard data recorder can be turned off,\ndestroyed beyond playback, or altogether lost. What if all that data was\nstreamed in real-time across a satellite connection and into a data center? With\na DJI Phantom 2, Arduino, a pile of sensors, and an Inmarsat modem, we set out\nto explore this possibility.\n\n\n--------------------------------------------------------------------------------\n\nThis is reposted from the Kaazing demo listing\n[http://developer.kaazing.com/portfolio/telemetry/].\n\nPieces Parts\nWhile we could not afford to expense a Boeing 737, we did manage to pick up a \nDJI Phantom 2 [http://www.dji.com/product/phantom-2]. The Phantom is an remote\ncontrol quadcopter that is capable of carrying our sensor array.\n\nThe sensor array consists of a tupperware bowl filled with an Arduino and a\nstack of sensors. The tupperware bowl was then strapped between the landing gear\nof the Phantom quadcopter. What is that? Oh, the actual sensors we used? Sure,\nthere was quite an array, but we largely attempted to emulate what pilots call\nthe \"six pack\" - the core set of instruments necessary to fly a plane safely\n(per the FAA).\n\n * Arduino Uno\n * GPS module on a GPS Shield\n * Compass\n * Accelerometer (tilt)\n * Temperature\n * Humidity\n * RGB LED (bi-directional communication)\n * XBee module on an XBee Shield\n\n\n\nWhile we did not have the time or resources to stream the video of the flight,\nwe did mount a GoPro Hero 3 in the tupperware case as well. It recorded the\nflights we made independently from the sensor data. The sensor data was\nrecorded, but also streamed across the web in real-time. More on that in a\nmoment. For now, have a look at the project in action.\n\nData Communication\nBecause the Phantom is not quite strong enough to carry a satellite modem, we\nneeded a way to get the sensor data onto the web. Of course we cannot exactly\nput a wire to the Phantom either, so the solution needed to be wireless.\n\nWe accomplished this using the aforementioned XBee module. The XBee gave us a\nrange of about one mile, which was plenty for our experiment. The XBee on the\nPhantom had a receiver back on the ground, plugged into a computer USB port. The\ncomputer was connected to an Inmarsat satellite modem. A Java program on the\ncomputer, read the data coming from the XBee radion via the USB port, and then\nsent that data to a Kaazing Gateway as it arrived.\n\n\n\nKeeping in mind that we had to go over XBee, wireless to an Inmarsat modem, up\nto a satellite in space, back down to a data center connected to the Internet,\nand then to the various devices through whatever means of connection they had,\nyou might expect some serious lag. In the end, we reliably measured a latency of \nless than one second from the sensor array, to a mobile device (on a cellular\nnetwork) tracking the flight. One second!\n\nData Recording\nThanks to Kaazing Gateway, data from the Java program was sent using\npublish-subscribe. This meant that data was pushed to any interested clients. In\nmost cases, this was a mobile phone or tablet monitoring the flight. However, a\nNode.js process also listened for sensor messages, and recorded the data to a\ndatabase.\n\n\n\nBecause the data recording was decoupled from the sensor reporting, it could\nhave been Node.js or any other technology on the other side. While we as\ndevelopers tend to think of this as platforms, with publish-subscribe, it could\neffectively be several entirely different entities. The aircraft manufacturer\ncould record the data. The FAA could record the data. The airline itself could\nrecord the data. All using their own stacks, own database structures, etc.\nwithout needing to be a part of the actual telemetry broadcasting itself.\n\nBig Picture\nInitially, we set out to show that flight data could be recorded in real-time to\nthe cloud. While our rig is nowhere near what is needed for deployment on actual\naircraft, it proves the point that a black box in the cloud is feasible. But\nthen we started thinking about the downstream implications.\n\n\n\nThink about how applications track flights today. Maybe something like a TripIt\nor FlightTrack that let you know where your flight is, or how soon you can\nexpect grandma to arrive for Christmas. These application scrape data from\nvarious sources to try and give you a decent picture of what is going on. But\nimagine if they could simply subscribe to specific flight data for a given\nclient?\n\nLeveraging real-time data, and decoupling the parts and pieces using\npublish-subscribe opens the door to entirely new ways to view and manage\naircraft data. This impacts not only manufacturers and aviation entities such as\nthe FAA, but trickles right on down to you and me in the applications we use.\nModernizing just a fragment of the infrastructure could significantly travel in\na positive way.","feature_image":"http://images.kevinhoyt.com/quadcopter.png","featured":0,"status":"published","locale":null,"visibility":"public","author_id":"1","created_at":"2015-04-22T15:31:01.000Z","updated_at":"2015-05-14T18:29:37.000Z","published_at":"2014-04-26T15:00:00.000Z","custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"type":"post","email_recipient_filter":"none"},{"id":"5adb8922351ffe0018a57721","uuid":"ee92b4c5-a2fd-4419-947a-e23e388fc676","title":"Real-Time Engine Telemetry","slug":"vehicle-engine-telemetry","mobiledoc":"{\"version\":\"0.3.1\",\"markups\":[],\"atoms\":[],\"cards\":[[\"markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\"*[Automatic](https://www.automatic.com/) is a great product passes data from your cars computer to your phone, and onto the servers for analysis.  Insurance companies like [Progressive](https://www.progressive.com/auto/snapshot/) and [eSurance](https://www.esurance.com/drivesense) have also picked up on this to offer their customers discounts for responsible driving.  The same technology has been used extensively in transportation and logistics by companies such as [FedEx](http://tv.adobe.com/watch/adobe-summit-2014/adobe-summit-2014-day-two-keynote-chapter-two/).*\\n\\n---\\n\\n==This is reposted on the [Kaazing Open Source Blog](http://kaazing.org/blog/real-time-vehicle-telemetry/).==\\n\\nOn the insurance and smart driving fronts, these products tend to operate in a batch mode.  They give helpful analytics over the long-term, but are not particularly designed for real-time.  Real-time offerings exist for transportation and logistics, but these systems can be very costly.  With the advances in microcomputing, IoT (Internet of Things), and real-time connectivity, you can now deliver the same experiences on the cheap using off the shelf hardware.\\n\\n###On-Board Diagnostics System\\n\\nPretty much every vehicle manufactured since 1995 has an on-board computer.  These computers offer a number of services to the vehicle itself, but can also be tapped into by third parties through a protocol referred to as OBD (on-board diagnostics).  If you take your car into the shop, they will likely use this interface to take a peek under the hood, without having to manually check every system.\\n\\n![An OBD port in a 2005 Nissan Altima](http://images.kevinhoyt.com/nissan.altima.obd.jpg)\\n\\nThe OBD port is generally located under the driver-side dashboard.  It is a fairly blocky looking thing, that is more narrow on one side than the other.  As it turns out, there are a variety of adapters made to plug into these ports.  The one we chose to use was an ELM-327 unit that communicated via Bluetooth.  These units also come in wi-fi and USB flavors.\\n\\n![ELM327 Bluetooth Mini](http://images.kevinhoyt.com/elm327.bluetooth.mini.jpg)\\n\\n###OBD Protocol\\n\\nTechnically the OBD protocol is an [ISO standard](http://en.wikipedia.org/wiki/ISO_15765-4) but there are many other variations.  You effectively open the serial port to your OBD connector, and then issue a series of commands.  Many of these commands have been well [documented](https://en.wikipedia.org/wiki/OBD-II_PIDs) by the community, and you can find libraries for languages such as [Java](https://github.com/pires/obd-java-api) and [Node.js](https://github.com/EricSmekens/node-bluetooth-obd).  A typical exchange looks something like the following snippet.\\n\\n```\\natz                   // OBD protocol\\n> ELM327 v1.3a\\n\\natrv                  // Vehicle voltage\\n> 12.5V\\n\\natsp0                 // Protocol level\\n> OK\\n\\n0100                  // Current data\\n> 41 00 BF 9F A8 93  \\n\\n010c                  // Engine RPM\\n> 41 0C 0E 96\\n```\\n\\nAs [Kaazing Gateway](http://kaazing.com) provides a Java client, this was our stack for this project.  The Java OBD library expects an InputStream and OutputStream to work.  Serial Java communications is not yet standardized, but we like the [jSSC](https://github.com/scream3r/java-simple-serial-connector/releases) (Java Simple Serial Connector) project.  The problem with jSSC is that it does not expose InputStream or OutputStream projects, but rather works off an event-based approach.  In the end, we rolled our own lightweight, hardly complete, OBD protocol implementation on top of jSSC.\\n\\n> You can find all the code for this project, including our jSSC-based implementation of OBD in my [Kaazing GitHub repository](https://github.com/krhoyt/Kaazing) under \\\"iot/cars/java\\\".\\n\\n###Geolocation\\n\\nMost ECUs (engine control units) have access to on-board GPS systems, but these are generally exposed under vendor-specific OBD commands that are not well documented (if at all).  To address this, we used a third-party [USB GPS module](http://www.amazon.com/gp/product/B000PKX2KA/ref=oh_aui_detailpage_o04_s00?ie=UTF8&psc=1).  GPS units talk using [NMEA](http://www.gpsinformation.org/dale/nmea.htm) (National Marine Electronics Association) data, which is also well documented.  It is effectively a string of comma-separated values.\\n\\n```\\n$GPGGA,123519,4807.038,N,01131.000,E,1,08,0.9,545.4,M,46.9,M,,*47\\n\\nWhich means:\\n- Fix taken at 12:35:19 UTC\\n- Latitude 48 deg 07.038' N\\n- Longitude 11 deg 31.000' E\\n- Fix quality: 1 = GPS fix\\n- 8 satellites being tracked\\n- 0.9 horizontal dilution of position\\n- 545.4,M altitude in meters above mean sea level\\n- 46.9,M height of mean sea level\\n- (empty field) time in seconds since last update\\n- (empty field) station ID number\\n- *47 checksum data\\n```\\n\\nI was initially worried that jSSC would only allow access to a single USB port at a time, but it turns out that it can support multiple ports at once.  Our Java program then pulls OBD data and GPS data, and merges the information.  The merged information is stored locally for playback or batch processing, but also published through Kaazing Gateway using [AMQP](https://www.amqp.org/) (Advanced Message Queing Protocol).\\n\\nBecause we are using a publish-subscribe architecture, any number of systems could be listening for this telemetry data.  This might be an insurance company, the vehicle manufacturer, government highway safety, etc.  This decoupling of data from client is a big step forward over traditional request-response approaches.  For our project, we simply received the information in a web browser.\\n\\n###Web Client\\n\\nKaazing Gateway also provides a JavaScript client for AMQP using WebSocket (pioneered at Kaazing).  This allows us to listen for vehicle data in real-time on the Web.  A splash of Google Maps shows where the vehicle is geographically located.  SVG (Scalable Vector Graphics) in turn, provide for crisp rendering of the engine data itself.  We went with the basic information provided on most vehicle dashboards - fuel level, speed, RPM, and engine coolant temperature.\\n\\n<iframe width=\\\"560\\\" height=\\\"315\\\" src=\\\"https://www.youtube.com/embed/VephhH4buCI\\\" frameborder=\\\"0\\\" allowfullscreen></iframe>\\n\\nThe result is real-time vehicle telemetry across devices and clients - and at a total cost of about $100 USD.  In this case we used a MacBook Pro as our computer, which was then in turn wirelessly connected to a mobile hotspot from a smartphone.  A Raspberry Pi would be equally effective for this purpose, and provide a much small footprint.  If you are looking for something more industrial in nature, Intel makes an [IoT Gateway](http://www.intel.com/content/www/us/en/internet-of-things/gateway-solutions.html) product, which includes a GSM radio.\\n\\n###Next Steps\\n\\nWith modern web standards, there is no reason that your IoT data should be delivered in a batch approach.  Most developers look to batch processing with request-response because that is what is most comfortable to them.  The information loss however, especially in industrial applications, can be significant.  Finding out that a refrigerated freight vehicle has blown it's compressor could mean valuable loss of cargo at an extreme cost to the vendor, customer, and freight companies.\\n\\nIf real-time seems intimidating, I would encourage you to take a look at my Kaazing [chat tutorial](http://blog.kevinhoyt.com/2015/01/23/building-a-chat-application/).\\n\"}]],\"sections\":[[10,0]],\"ghostVersion\":\"3.0\"}","html":"<!--kg-card-begin: markdown--><p><em><a href=\"https://www.automatic.com/\">Automatic</a> is a great product passes data from your cars computer to your phone, and onto the servers for analysis.  Insurance companies like <a href=\"https://www.progressive.com/auto/snapshot/\">Progressive</a> and <a href=\"https://www.esurance.com/drivesense\">eSurance</a> have also picked up on this to offer their customers discounts for responsible driving.  The same technology has been used extensively in transportation and logistics by companies such as <a href=\"http://tv.adobe.com/watch/adobe-summit-2014/adobe-summit-2014-day-two-keynote-chapter-two/\">FedEx</a>.</em></p>\n<hr>\n<p><mark>This is reposted on the <a href=\"http://kaazing.org/blog/real-time-vehicle-telemetry/\">Kaazing Open Source Blog</a>.</mark></p>\n<p>On the insurance and smart driving fronts, these products tend to operate in a batch mode.  They give helpful analytics over the long-term, but are not particularly designed for real-time.  Real-time offerings exist for transportation and logistics, but these systems can be very costly.  With the advances in microcomputing, IoT (Internet of Things), and real-time connectivity, you can now deliver the same experiences on the cheap using off the shelf hardware.</p>\n<h3 id=\"onboarddiagnosticssystem\">On-Board Diagnostics System</h3>\n<p>Pretty much every vehicle manufactured since 1995 has an on-board computer.  These computers offer a number of services to the vehicle itself, but can also be tapped into by third parties through a protocol referred to as OBD (on-board diagnostics).  If you take your car into the shop, they will likely use this interface to take a peek under the hood, without having to manually check every system.</p>\n<p><img src=\"http://images.kevinhoyt.com/nissan.altima.obd.jpg\" alt=\"An OBD port in a 2005 Nissan Altima\" loading=\"lazy\"></p>\n<p>The OBD port is generally located under the driver-side dashboard.  It is a fairly blocky looking thing, that is more narrow on one side than the other.  As it turns out, there are a variety of adapters made to plug into these ports.  The one we chose to use was an ELM-327 unit that communicated via Bluetooth.  These units also come in wi-fi and USB flavors.</p>\n<p><img src=\"http://images.kevinhoyt.com/elm327.bluetooth.mini.jpg\" alt=\"ELM327 Bluetooth Mini\" loading=\"lazy\"></p>\n<h3 id=\"obdprotocol\">OBD Protocol</h3>\n<p>Technically the OBD protocol is an <a href=\"http://en.wikipedia.org/wiki/ISO_15765-4\">ISO standard</a> but there are many other variations.  You effectively open the serial port to your OBD connector, and then issue a series of commands.  Many of these commands have been well <a href=\"https://en.wikipedia.org/wiki/OBD-II_PIDs\">documented</a> by the community, and you can find libraries for languages such as <a href=\"https://github.com/pires/obd-java-api\">Java</a> and <a href=\"https://github.com/EricSmekens/node-bluetooth-obd\">Node.js</a>.  A typical exchange looks something like the following snippet.</p>\n<pre><code>atz                   // OBD protocol\n&gt; ELM327 v1.3a\n\natrv                  // Vehicle voltage\n&gt; 12.5V\n\natsp0                 // Protocol level\n&gt; OK\n\n0100                  // Current data\n&gt; 41 00 BF 9F A8 93  \n\n010c                  // Engine RPM\n&gt; 41 0C 0E 96\n</code></pre>\n<p>As <a href=\"http://kaazing.com\">Kaazing Gateway</a> provides a Java client, this was our stack for this project.  The Java OBD library expects an InputStream and OutputStream to work.  Serial Java communications is not yet standardized, but we like the <a href=\"https://github.com/scream3r/java-simple-serial-connector/releases\">jSSC</a> (Java Simple Serial Connector) project.  The problem with jSSC is that it does not expose InputStream or OutputStream projects, but rather works off an event-based approach.  In the end, we rolled our own lightweight, hardly complete, OBD protocol implementation on top of jSSC.</p>\n<blockquote>\n<p>You can find all the code for this project, including our jSSC-based implementation of OBD in my <a href=\"https://github.com/krhoyt/Kaazing\">Kaazing GitHub repository</a> under &quot;iot/cars/java&quot;.</p>\n</blockquote>\n<h3 id=\"geolocation\">Geolocation</h3>\n<p>Most ECUs (engine control units) have access to on-board GPS systems, but these are generally exposed under vendor-specific OBD commands that are not well documented (if at all).  To address this, we used a third-party <a href=\"http://www.amazon.com/gp/product/B000PKX2KA/ref=oh_aui_detailpage_o04_s00?ie=UTF8&amp;psc=1\">USB GPS module</a>.  GPS units talk using <a href=\"http://www.gpsinformation.org/dale/nmea.htm\">NMEA</a> (National Marine Electronics Association) data, which is also well documented.  It is effectively a string of comma-separated values.</p>\n<pre><code>$GPGGA,123519,4807.038,N,01131.000,E,1,08,0.9,545.4,M,46.9,M,,*47\n\nWhich means:\n- Fix taken at 12:35:19 UTC\n- Latitude 48 deg 07.038' N\n- Longitude 11 deg 31.000' E\n- Fix quality: 1 = GPS fix\n- 8 satellites being tracked\n- 0.9 horizontal dilution of position\n- 545.4,M altitude in meters above mean sea level\n- 46.9,M height of mean sea level\n- (empty field) time in seconds since last update\n- (empty field) station ID number\n- *47 checksum data\n</code></pre>\n<p>I was initially worried that jSSC would only allow access to a single USB port at a time, but it turns out that it can support multiple ports at once.  Our Java program then pulls OBD data and GPS data, and merges the information.  The merged information is stored locally for playback or batch processing, but also published through Kaazing Gateway using <a href=\"https://www.amqp.org/\">AMQP</a> (Advanced Message Queing Protocol).</p>\n<p>Because we are using a publish-subscribe architecture, any number of systems could be listening for this telemetry data.  This might be an insurance company, the vehicle manufacturer, government highway safety, etc.  This decoupling of data from client is a big step forward over traditional request-response approaches.  For our project, we simply received the information in a web browser.</p>\n<h3 id=\"webclient\">Web Client</h3>\n<p>Kaazing Gateway also provides a JavaScript client for AMQP using WebSocket (pioneered at Kaazing).  This allows us to listen for vehicle data in real-time on the Web.  A splash of Google Maps shows where the vehicle is geographically located.  SVG (Scalable Vector Graphics) in turn, provide for crisp rendering of the engine data itself.  We went with the basic information provided on most vehicle dashboards - fuel level, speed, RPM, and engine coolant temperature.</p>\n<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/VephhH4buCI\" frameborder=\"0\" allowfullscreen></iframe>\n<p>The result is real-time vehicle telemetry across devices and clients - and at a total cost of about $100 USD.  In this case we used a MacBook Pro as our computer, which was then in turn wirelessly connected to a mobile hotspot from a smartphone.  A Raspberry Pi would be equally effective for this purpose, and provide a much small footprint.  If you are looking for something more industrial in nature, Intel makes an <a href=\"http://www.intel.com/content/www/us/en/internet-of-things/gateway-solutions.html\">IoT Gateway</a> product, which includes a GSM radio.</p>\n<h3 id=\"nextsteps\">Next Steps</h3>\n<p>With modern web standards, there is no reason that your IoT data should be delivered in a batch approach.  Most developers look to batch processing with request-response because that is what is most comfortable to them.  The information loss however, especially in industrial applications, can be significant.  Finding out that a refrigerated freight vehicle has blown it's compressor could mean valuable loss of cargo at an extreme cost to the vendor, customer, and freight companies.</p>\n<p>If real-time seems intimidating, I would encourage you to take a look at my Kaazing <a href=\"http://blog.kevinhoyt.com/2015/01/23/building-a-chat-application/\">chat tutorial</a>.</p>\n<!--kg-card-end: markdown-->","comment_id":"21","plaintext":"Automatic [https://www.automatic.com/] is a great product passes data from your\ncars computer to your phone, and onto the servers for analysis. Insurance\ncompanies like Progressive [https://www.progressive.com/auto/snapshot/] and \neSurance [https://www.esurance.com/drivesense] have also picked up on this to\noffer their customers discounts for responsible driving. The same technology has\nbeen used extensively in transportation and logistics by companies such as FedEx\n[http://tv.adobe.com/watch/adobe-summit-2014/adobe-summit-2014-day-two-keynote-chapter-two/]\n.\n\n\n--------------------------------------------------------------------------------\n\nThis is reposted on the Kaazing Open Source Blog\n[http://kaazing.org/blog/real-time-vehicle-telemetry/].\n\nOn the insurance and smart driving fronts, these products tend to operate in a\nbatch mode. They give helpful analytics over the long-term, but are not\nparticularly designed for real-time. Real-time offerings exist for\ntransportation and logistics, but these systems can be very costly. With the\nadvances in microcomputing, IoT (Internet of Things), and real-time\nconnectivity, you can now deliver the same experiences on the cheap using off\nthe shelf hardware.\n\nOn-Board Diagnostics System\nPretty much every vehicle manufactured since 1995 has an on-board computer.\nThese computers offer a number of services to the vehicle itself, but can also\nbe tapped into by third parties through a protocol referred to as OBD (on-board\ndiagnostics). If you take your car into the shop, they will likely use this\ninterface to take a peek under the hood, without having to manually check every\nsystem.\n\n\n\nThe OBD port is generally located under the driver-side dashboard. It is a\nfairly blocky looking thing, that is more narrow on one side than the other. As\nit turns out, there are a variety of adapters made to plug into these ports. The\none we chose to use was an ELM-327 unit that communicated via Bluetooth. These\nunits also come in wi-fi and USB flavors.\n\n\n\nOBD Protocol\nTechnically the OBD protocol is an ISO standard\n[http://en.wikipedia.org/wiki/ISO_15765-4] but there are many other variations.\nYou effectively open the serial port to your OBD connector, and then issue a\nseries of commands. Many of these commands have been well documented\n[https://en.wikipedia.org/wiki/OBD-II_PIDs] by the community, and you can find\nlibraries for languages such as Java [https://github.com/pires/obd-java-api] and \nNode.js [https://github.com/EricSmekens/node-bluetooth-obd]. A typical exchange\nlooks something like the following snippet.\n\natz                   // OBD protocol\n> ELM327 v1.3a\n\natrv                  // Vehicle voltage\n> 12.5V\n\natsp0                 // Protocol level\n> OK\n\n0100                  // Current data\n> 41 00 BF 9F A8 93  \n\n010c                  // Engine RPM\n> 41 0C 0E 96\n\n\nAs Kaazing Gateway [http://kaazing.com] provides a Java client, this was our\nstack for this project. The Java OBD library expects an InputStream and\nOutputStream to work. Serial Java communications is not yet standardized, but we\nlike the jSSC\n[https://github.com/scream3r/java-simple-serial-connector/releases] (Java Simple\nSerial Connector) project. The problem with jSSC is that it does not expose\nInputStream or OutputStream projects, but rather works off an event-based\napproach. In the end, we rolled our own lightweight, hardly complete, OBD\nprotocol implementation on top of jSSC.\n\n> You can find all the code for this project, including our jSSC-based\nimplementation of OBD in my Kaazing GitHub repository\n[https://github.com/krhoyt/Kaazing] under \"iot/cars/java\".\n\n\nGeolocation\nMost ECUs (engine control units) have access to on-board GPS systems, but these\nare generally exposed under vendor-specific OBD commands that are not well\ndocumented (if at all). To address this, we used a third-party USB GPS module\n[http://www.amazon.com/gp/product/B000PKX2KA/ref=oh_aui_detailpage_o04_s00?ie=UTF8&psc=1]\n. GPS units talk using NMEA [http://www.gpsinformation.org/dale/nmea.htm] \n(National Marine Electronics Association) data, which is also well documented.\nIt is effectively a string of comma-separated values.\n\n$GPGGA,123519,4807.038,N,01131.000,E,1,08,0.9,545.4,M,46.9,M,,*47\n\nWhich means:\n- Fix taken at 12:35:19 UTC\n- Latitude 48 deg 07.038' N\n- Longitude 11 deg 31.000' E\n- Fix quality: 1 = GPS fix\n- 8 satellites being tracked\n- 0.9 horizontal dilution of position\n- 545.4,M altitude in meters above mean sea level\n- 46.9,M height of mean sea level\n- (empty field) time in seconds since last update\n- (empty field) station ID number\n- *47 checksum data\n\n\nI was initially worried that jSSC would only allow access to a single USB port\nat a time, but it turns out that it can support multiple ports at once. Our Java\nprogram then pulls OBD data and GPS data, and merges the information. The merged\ninformation is stored locally for playback or batch processing, but also\npublished through Kaazing Gateway using AMQP [https://www.amqp.org/] (Advanced\nMessage Queing Protocol).\n\nBecause we are using a publish-subscribe architecture, any number of systems\ncould be listening for this telemetry data. This might be an insurance company,\nthe vehicle manufacturer, government highway safety, etc. This decoupling of\ndata from client is a big step forward over traditional request-response\napproaches. For our project, we simply received the information in a web\nbrowser.\n\nWeb Client\nKaazing Gateway also provides a JavaScript client for AMQP using WebSocket\n(pioneered at Kaazing). This allows us to listen for vehicle data in real-time\non the Web. A splash of Google Maps shows where the vehicle is geographically\nlocated. SVG (Scalable Vector Graphics) in turn, provide for crisp rendering of\nthe engine data itself. We went with the basic information provided on most\nvehicle dashboards - fuel level, speed, RPM, and engine coolant temperature.\n\nThe result is real-time vehicle telemetry across devices and clients - and at a\ntotal cost of about $100 USD. In this case we used a MacBook Pro as our\ncomputer, which was then in turn wirelessly connected to a mobile hotspot from a\nsmartphone. A Raspberry Pi would be equally effective for this purpose, and\nprovide a much small footprint. If you are looking for something more industrial\nin nature, Intel makes an IoT Gateway\n[http://www.intel.com/content/www/us/en/internet-of-things/gateway-solutions.html] \nproduct, which includes a GSM radio.\n\nNext Steps\nWith modern web standards, there is no reason that your IoT data should be\ndelivered in a batch approach. Most developers look to batch processing with\nrequest-response because that is what is most comfortable to them. The\ninformation loss however, especially in industrial applications, can be\nsignificant. Finding out that a refrigerated freight vehicle has blown it's\ncompressor could mean valuable loss of cargo at an extreme cost to the vendor,\ncustomer, and freight companies.\n\nIf real-time seems intimidating, I would encourage you to take a look at my\nKaazing chat tutorial\n[http://blog.kevinhoyt.com/2015/01/23/building-a-chat-application/].","feature_image":"http://images.kevinhoyt.com/safe.driving.jpg","featured":0,"status":"published","locale":null,"visibility":"public","author_id":"1","created_at":"2015-04-23T19:11:04.000Z","updated_at":"2015-06-01T19:06:38.000Z","published_at":"2015-05-14T18:28:05.000Z","custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"type":"post","email_recipient_filter":"none"},{"id":"5adb8922351ffe0018a57722","uuid":"44efeb5a-522a-47d8-9ac2-111858f5834d","title":"Demonstrations","slug":"demos","mobiledoc":"{\"version\":\"0.3.1\",\"markups\":[],\"atoms\":[],\"cards\":[[\"markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\"I have built countless demonstrations over my 15 years (and running) time in sales and marketing.  In a field that blazes forward at unprecedented speed, many of those demonstrations no longer make any sense.  \\n\\nAs an example, I once wrote a version of Allaire HomeSite in Java Swing.  I obsessed over every detail, and it was nearly a pixel perfect match for the native Delphi version - except that it ran across platforms.  It was nearly feature complete as well, including VTML parsing, and even the Query Builder tool.  Who is Allaire?  What is HomeSite?  Everybody knows Java Swing is horrible, right?\\n\\nAnd so technology marches on ...\\n\\nWhat follows is a list of some of the active demonstrations that I have built recently, and show frequently to customers and/or conferences.  They are placed here for easy reference in those situations.  Be advised that demonstrations here today, may not be tomorrow.\\n\\n###[Algorithms](http://algorithms.kevinhoyt.com)\\n\\nIf you interview for Google, you will be asked to solve computational problems from almost every person you meet.  Most of these computational problems are designed to provide the interviewer with better insight as to how you think - not necessarily to be solved.\\n\\nHow does one succinctly communicate a computational problem in the first place?  If you are solving the problem, how do you communicate your approach to a solution?  \\n\\nThere needs to be a common vernacular.  Most algorithms have names (usually for their creator).  Knowing the names of these algorithms, how they work, and ideally, how to write them, is key in establishing a dialog with a Google interviewer.  I would recommend pouring over the book [Algorithm Design Manual](http://goo.gl/5GuCLc).\\n\\nIn my efforts to solidify these algorithms in my head, I practiced implementing them, and visualizing their results.  This page (meant to be viewed in a desktop browser with the console open) is the implementation of a few of those algorithms.\\n\\n![Algorithms](http://images.kevinhoyt.com/algorithms.screenshot.jpg)\\n\\n###[Cars](http://cars.kevinhoyt.com)\\n\\nI started working with physical computing around 2007.  At the time it seemed like a unique new angle to bring to the conference stage.  So many demonstrations are only in the digital realm.  Close the lid of the laptop, and your work disappears.  I wanted something that people could see *and* touch.\\n\\nFrom the very first time I showed the digital and the physical interacting, I was hooked.  What is currently referred to as IoT (Internet of Things) then became a passion of mine, and I have gone on to include some hardware in virtually every demonstration I give.  It has become part of my reputation.\\n\\nIf you see me at a conference, wether presenting or not, you can always stop me and ask to see a hardware demonstration.\\n\\nMy work at Kaazing has been largely focused on real-time data communication.  Bringing such a broadly horizontal technology to life, has often meant applying it to whatever technology discussion is currently trending.  With IoT in mind, I added a Bluetooth OBD (on-board diagnostics) adapter to my car, hooked up a USB GPS, and began broadcasting my driving in real-time - to include actual metrics from the vehicle's ECU (engine control unit).\\n\\n==**Note:** This demonstration does nothing without additional parts being involved.  Want to see it in action?  Hit me up on [Twitter](http://twitter.com/krhoyt), or catch the [YouTube video](https://www.youtube.com/watch?v=VephhH4buCI).==\\n\\n![Cars](http://images.kevinhoyt.com/car.telematics.screenshot.jpg)\\n\\n###[Fingers](http://fingers.kevinhoyt.com)\\n\\nI have worked professionally with a lot of programming languages over the years.  I have ported QBasic games to ActionScript, written WebSocket server implementation in JavaScript (pre-Node.js), managed I2C bus communication using C/C++, and added dynamic elements to web pages using everything from Active Server Pages, to Python.  Just to name a few.\\n\\nThese days I am using mostly Java, C/C++, and web stack technologies such as JavaScript, SVG, HTML, and CSS.\\n\\nKaazing Gateway has real-time client libraries for a number of platforms.  They are designed to be robust and capable for even the most demanding enterprise requirements.  This also makes the learning curve incredibly steep.  In order to lower the barrier to entry, for our most common platform (Web stack), I wrote a JavaScript wrapper for our existing libraries.\\n\\nThe wrapper loads all the necessary JavaScript files, logs usage to a cloud database service, and exposes a handful of functions to the developer (connect, publish, subscribe, events, etc.).  In this case, the underlying protocol for real-time communication is AMQP, so the wrapper abstracts all of that work (acknowledge, queue, exchange, etc.).\\n\\nPut together with a getting started [guide](http://kaazing.org/demos/quick-start/) (written by me), developers can now easily get started using Kaazing Gateway for free without installing or configuring anything.  The demonstration looks like nothing to begin with, but put your mouse (or finger) down on the screen, and you will see an animated circle.  So will every other screen viewing that page (tested thoroughly across devices).\\n\\n==**Note:** I like to put Easter Eggs in my demonstrations.  The Easter Egg here is a hidden chat() function which will setup an entire real-time [chat](http://fingers.kevinhoyt.com/chat.html) for the developer (the \\\"hello world\\\" of real-time).==\\n\\n###[Terry Smash](http://smash.kevinhoyt.com)\\n\\nI worked for Adobe (Allaire, Macromedia, Adobe) for 15 years.  While I am already a fairly creative person, exposure to such world-class talent often led me to develop my own sense of design - even though I have a CIS (Computer Information Systems) degree.  I was a regular, highly rated, speaker at Adobe MAX for nearly ten years.\\n\\nOne such year the marketing department came up with the idea of putting the Developer Evangelists against the Design Evangelists.  A representative from each group would propose and promote an Adobe MAX session using a specific hashtag.  The winner would get their session at the conference, a randomly selected person that used the hashtag would get an all-expenses paid trip to Adobe MAX that year.\\n\\nIt came down to me versus Terry White.  With but a fraction of the social reach, I put up a good fight, but in the end, was not quite able to win.\\n\\nPart of my proposed session (JavaScript Animation Techniques) was to cover the use of physics engines.  My daughter often plays a supporting role in my demonstrations, and at the time she was fond of an iOS game called Tower Smash.  I put the two together and came up with Terry Smash.  In this game you destroy a tower of Terry White (and his session on Adobe Muse) blocks by firing blocks of my photo.\\n\\n![Terry Smash](http://images.kevinhoyt.com/terry.white.tower.smash.jpg)\\n\\n###[Tetris](http://tetris.kevinhoyt.com)\\n\\nI have always been fascinated by game development.  Which is odd considering that I do not like to play video games, and that I have never really built one.  Until now!  Well, okay, Tetris is like the \\\"Hello World\\\" of game development, but I have put my own spin on this version.\\n\\nThe first major spin I put on this version is that it uses the browser DOM - any other version I have seen used HTML5 Canvas.  Not that there is anything wrong with using Canvas.  If you are porting something like Tetris to the Web, then the drawing routines are probably going to match up pretty nicely.  However, getting canvas to render crisply across (high-density) screens is a real chore.\\n\\nThe second major spin I put on this is that it uses a virtual controller.  That is to say that you can open the controller screen on your smartphone, and the game on some other screen (iPad, desktop browser, etc), and control the game on one device from the other device.  The devices do not even have to know anything about one another.  No Bluetooth here - just good old fashion publish/subscribe on the Web, thanks to [Kaazing Gateway](http://kaazing.com).\\n\\nThe blocks are filled using pictures of some of the staff around the Kaazing office at the time this project was being developed.  The other reason I like using the DOM over Canvas, is that it makes it really easy to change the fill of the blocks using a splash of CSS (or SVG).  It could just be solid colors, or faces from presenters at a conference.  Company logos, product logos, or rainbow unicorns.\\n\\n![Tetris game with controller.](http://images.kevinhoyt.com/kaazing.tetris.with.controller.jpg)\\n###[Tic Tac Toe](http://tictactoe.kevinhoyt.com)\\n\\nAlong the way of exploring physical computing and electronics hardware, I discovered the need to have custom enclosures for my projects.  Wires sticking out everywhere just never really did the project justice.  This led me to fabrication techniques including 3D printing, CNC (subtractive processes), and laser cutting.  I have even taught these topics at conferences.\\n\\nBefore fabricating the real thing, using quality materials, I prefer to build a prototype using foam core board, or other materials.  This allows me to catch problems with the design before spending too much money on fabrication materials.  Sometimes the foam core board prototype ends up being as far as some projects make it.\\n\\nBuilding a report with your audience is key to making engaging presentations.  What better way to engage the audience than to let them participate in the presentation directly?  This project used nine (9) [RGB LED](http://www.adafruit.com/products/1312) modules, an [Arduino Yun](http://www.arduino.cc/en/Main/ArduinoBoardYun), and a custom-rolled [STOMP](https://stomp.github.io/) implementation.  The audience could bring up a web page, select a color from the user interface, and that color would appear at the selected place, with that selected color.\\n\\n==**Note:** Even without the hardware aspect, the screens will stay in sync, but it really does not do the demonstration justice.  To see it in action, hit me up on [Twitter](http://twitter.com/krhoyt), or see the [Vine video](https://vine.co/v/MwaYH6EqUBL) recorded at one of my presentations..==\\n\\n![Tic Tac Toe](http://images.kevinhoyt.com/tic.tac.toe.box.jpg)\\n\\n###Addendum\\n\\nWant to see more of my demonstrations?  Would you like to view the slides for a presentation?  Just must get your hands on the code?  Let's talk!  I am on [Twitter](http://twitter.com/krhoyt), [Flickr](https://www.flickr.com/photos/25243531@N04/), and most instant message systems using the handle \\\"parkerkrhoyt\\\".\"}]],\"sections\":[[10,0]],\"ghostVersion\":\"3.0\"}","html":"<!--kg-card-begin: markdown--><p>I have built countless demonstrations over my 15 years (and running) time in sales and marketing.  In a field that blazes forward at unprecedented speed, many of those demonstrations no longer make any sense.</p>\n<p>As an example, I once wrote a version of Allaire HomeSite in Java Swing.  I obsessed over every detail, and it was nearly a pixel perfect match for the native Delphi version - except that it ran across platforms.  It was nearly feature complete as well, including VTML parsing, and even the Query Builder tool.  Who is Allaire?  What is HomeSite?  Everybody knows Java Swing is horrible, right?</p>\n<p>And so technology marches on ...</p>\n<p>What follows is a list of some of the active demonstrations that I have built recently, and show frequently to customers and/or conferences.  They are placed here for easy reference in those situations.  Be advised that demonstrations here today, may not be tomorrow.</p>\n<h3 id=\"algorithms\"><a href=\"http://algorithms.kevinhoyt.com\">Algorithms</a></h3>\n<p>If you interview for Google, you will be asked to solve computational problems from almost every person you meet.  Most of these computational problems are designed to provide the interviewer with better insight as to how you think - not necessarily to be solved.</p>\n<p>How does one succinctly communicate a computational problem in the first place?  If you are solving the problem, how do you communicate your approach to a solution?</p>\n<p>There needs to be a common vernacular.  Most algorithms have names (usually for their creator).  Knowing the names of these algorithms, how they work, and ideally, how to write them, is key in establishing a dialog with a Google interviewer.  I would recommend pouring over the book <a href=\"http://goo.gl/5GuCLc\">Algorithm Design Manual</a>.</p>\n<p>In my efforts to solidify these algorithms in my head, I practiced implementing them, and visualizing their results.  This page (meant to be viewed in a desktop browser with the console open) is the implementation of a few of those algorithms.</p>\n<p><img src=\"http://images.kevinhoyt.com/algorithms.screenshot.jpg\" alt=\"Algorithms\" loading=\"lazy\"></p>\n<h3 id=\"cars\"><a href=\"http://cars.kevinhoyt.com\">Cars</a></h3>\n<p>I started working with physical computing around 2007.  At the time it seemed like a unique new angle to bring to the conference stage.  So many demonstrations are only in the digital realm.  Close the lid of the laptop, and your work disappears.  I wanted something that people could see <em>and</em> touch.</p>\n<p>From the very first time I showed the digital and the physical interacting, I was hooked.  What is currently referred to as IoT (Internet of Things) then became a passion of mine, and I have gone on to include some hardware in virtually every demonstration I give.  It has become part of my reputation.</p>\n<p>If you see me at a conference, wether presenting or not, you can always stop me and ask to see a hardware demonstration.</p>\n<p>My work at Kaazing has been largely focused on real-time data communication.  Bringing such a broadly horizontal technology to life, has often meant applying it to whatever technology discussion is currently trending.  With IoT in mind, I added a Bluetooth OBD (on-board diagnostics) adapter to my car, hooked up a USB GPS, and began broadcasting my driving in real-time - to include actual metrics from the vehicle's ECU (engine control unit).</p>\n<p><mark><strong>Note:</strong> This demonstration does nothing without additional parts being involved.  Want to see it in action?  Hit me up on <a href=\"http://twitter.com/krhoyt\">Twitter</a>, or catch the <a href=\"https://www.youtube.com/watch?v=VephhH4buCI\">YouTube video</a>.</mark></p>\n<p><img src=\"http://images.kevinhoyt.com/car.telematics.screenshot.jpg\" alt=\"Cars\" loading=\"lazy\"></p>\n<h3 id=\"fingers\"><a href=\"http://fingers.kevinhoyt.com\">Fingers</a></h3>\n<p>I have worked professionally with a lot of programming languages over the years.  I have ported QBasic games to ActionScript, written WebSocket server implementation in JavaScript (pre-Node.js), managed I2C bus communication using C/C++, and added dynamic elements to web pages using everything from Active Server Pages, to Python.  Just to name a few.</p>\n<p>These days I am using mostly Java, C/C++, and web stack technologies such as JavaScript, SVG, HTML, and CSS.</p>\n<p>Kaazing Gateway has real-time client libraries for a number of platforms.  They are designed to be robust and capable for even the most demanding enterprise requirements.  This also makes the learning curve incredibly steep.  In order to lower the barrier to entry, for our most common platform (Web stack), I wrote a JavaScript wrapper for our existing libraries.</p>\n<p>The wrapper loads all the necessary JavaScript files, logs usage to a cloud database service, and exposes a handful of functions to the developer (connect, publish, subscribe, events, etc.).  In this case, the underlying protocol for real-time communication is AMQP, so the wrapper abstracts all of that work (acknowledge, queue, exchange, etc.).</p>\n<p>Put together with a getting started <a href=\"http://kaazing.org/demos/quick-start/\">guide</a> (written by me), developers can now easily get started using Kaazing Gateway for free without installing or configuring anything.  The demonstration looks like nothing to begin with, but put your mouse (or finger) down on the screen, and you will see an animated circle.  So will every other screen viewing that page (tested thoroughly across devices).</p>\n<p><mark><strong>Note:</strong> I like to put Easter Eggs in my demonstrations.  The Easter Egg here is a hidden chat() function which will setup an entire real-time <a href=\"http://fingers.kevinhoyt.com/chat.html\">chat</a> for the developer (the &quot;hello world&quot; of real-time).</mark></p>\n<h3 id=\"terrysmash\"><a href=\"http://smash.kevinhoyt.com\">Terry Smash</a></h3>\n<p>I worked for Adobe (Allaire, Macromedia, Adobe) for 15 years.  While I am already a fairly creative person, exposure to such world-class talent often led me to develop my own sense of design - even though I have a CIS (Computer Information Systems) degree.  I was a regular, highly rated, speaker at Adobe MAX for nearly ten years.</p>\n<p>One such year the marketing department came up with the idea of putting the Developer Evangelists against the Design Evangelists.  A representative from each group would propose and promote an Adobe MAX session using a specific hashtag.  The winner would get their session at the conference, a randomly selected person that used the hashtag would get an all-expenses paid trip to Adobe MAX that year.</p>\n<p>It came down to me versus Terry White.  With but a fraction of the social reach, I put up a good fight, but in the end, was not quite able to win.</p>\n<p>Part of my proposed session (JavaScript Animation Techniques) was to cover the use of physics engines.  My daughter often plays a supporting role in my demonstrations, and at the time she was fond of an iOS game called Tower Smash.  I put the two together and came up with Terry Smash.  In this game you destroy a tower of Terry White (and his session on Adobe Muse) blocks by firing blocks of my photo.</p>\n<p><img src=\"http://images.kevinhoyt.com/terry.white.tower.smash.jpg\" alt=\"Terry Smash\" loading=\"lazy\"></p>\n<h3 id=\"tetris\"><a href=\"http://tetris.kevinhoyt.com\">Tetris</a></h3>\n<p>I have always been fascinated by game development.  Which is odd considering that I do not like to play video games, and that I have never really built one.  Until now!  Well, okay, Tetris is like the &quot;Hello World&quot; of game development, but I have put my own spin on this version.</p>\n<p>The first major spin I put on this version is that it uses the browser DOM - any other version I have seen used HTML5 Canvas.  Not that there is anything wrong with using Canvas.  If you are porting something like Tetris to the Web, then the drawing routines are probably going to match up pretty nicely.  However, getting canvas to render crisply across (high-density) screens is a real chore.</p>\n<p>The second major spin I put on this is that it uses a virtual controller.  That is to say that you can open the controller screen on your smartphone, and the game on some other screen (iPad, desktop browser, etc), and control the game on one device from the other device.  The devices do not even have to know anything about one another.  No Bluetooth here - just good old fashion publish/subscribe on the Web, thanks to <a href=\"http://kaazing.com\">Kaazing Gateway</a>.</p>\n<p>The blocks are filled using pictures of some of the staff around the Kaazing office at the time this project was being developed.  The other reason I like using the DOM over Canvas, is that it makes it really easy to change the fill of the blocks using a splash of CSS (or SVG).  It could just be solid colors, or faces from presenters at a conference.  Company logos, product logos, or rainbow unicorns.</p>\n<p><img src=\"http://images.kevinhoyt.com/kaazing.tetris.with.controller.jpg\" alt=\"Tetris game with controller.\" loading=\"lazy\"></p>\n<h3 id=\"tictactoe\"><a href=\"http://tictactoe.kevinhoyt.com\">Tic Tac Toe</a></h3>\n<p>Along the way of exploring physical computing and electronics hardware, I discovered the need to have custom enclosures for my projects.  Wires sticking out everywhere just never really did the project justice.  This led me to fabrication techniques including 3D printing, CNC (subtractive processes), and laser cutting.  I have even taught these topics at conferences.</p>\n<p>Before fabricating the real thing, using quality materials, I prefer to build a prototype using foam core board, or other materials.  This allows me to catch problems with the design before spending too much money on fabrication materials.  Sometimes the foam core board prototype ends up being as far as some projects make it.</p>\n<p>Building a report with your audience is key to making engaging presentations.  What better way to engage the audience than to let them participate in the presentation directly?  This project used nine (9) <a href=\"http://www.adafruit.com/products/1312\">RGB LED</a> modules, an <a href=\"http://www.arduino.cc/en/Main/ArduinoBoardYun\">Arduino Yun</a>, and a custom-rolled <a href=\"https://stomp.github.io/\">STOMP</a> implementation.  The audience could bring up a web page, select a color from the user interface, and that color would appear at the selected place, with that selected color.</p>\n<p><mark><strong>Note:</strong> Even without the hardware aspect, the screens will stay in sync, but it really does not do the demonstration justice.  To see it in action, hit me up on <a href=\"http://twitter.com/krhoyt\">Twitter</a>, or see the <a href=\"https://vine.co/v/MwaYH6EqUBL\">Vine video</a> recorded at one of my presentations..</mark></p>\n<p><img src=\"http://images.kevinhoyt.com/tic.tac.toe.box.jpg\" alt=\"Tic Tac Toe\" loading=\"lazy\"></p>\n<h3 id=\"addendum\">Addendum</h3>\n<p>Want to see more of my demonstrations?  Would you like to view the slides for a presentation?  Just must get your hands on the code?  Let's talk!  I am on <a href=\"http://twitter.com/krhoyt\">Twitter</a>, <a href=\"https://www.flickr.com/photos/25243531@N04/\">Flickr</a>, and most instant message systems using the handle &quot;parkerkrhoyt&quot;.</p>\n<!--kg-card-end: markdown-->","comment_id":"22","plaintext":"I have built countless demonstrations over my 15 years (and running) time in\nsales and marketing. In a field that blazes forward at unprecedented speed, many\nof those demonstrations no longer make any sense.\n\nAs an example, I once wrote a version of Allaire HomeSite in Java Swing. I\nobsessed over every detail, and it was nearly a pixel perfect match for the\nnative Delphi version - except that it ran across platforms. It was nearly\nfeature complete as well, including VTML parsing, and even the Query Builder\ntool. Who is Allaire? What is HomeSite? Everybody knows Java Swing is horrible,\nright?\n\nAnd so technology marches on ...\n\nWhat follows is a list of some of the active demonstrations that I have built\nrecently, and show frequently to customers and/or conferences. They are placed\nhere for easy reference in those situations. Be advised that demonstrations here\ntoday, may not be tomorrow.\n\nAlgorithms [http://algorithms.kevinhoyt.com]\nIf you interview for Google, you will be asked to solve computational problems\nfrom almost every person you meet. Most of these computational problems are\ndesigned to provide the interviewer with better insight as to how you think -\nnot necessarily to be solved.\n\nHow does one succinctly communicate a computational problem in the first place?\nIf you are solving the problem, how do you communicate your approach to a\nsolution?\n\nThere needs to be a common vernacular. Most algorithms have names (usually for\ntheir creator). Knowing the names of these algorithms, how they work, and\nideally, how to write them, is key in establishing a dialog with a Google\ninterviewer. I would recommend pouring over the book Algorithm Design Manual\n[http://goo.gl/5GuCLc].\n\nIn my efforts to solidify these algorithms in my head, I practiced implementing\nthem, and visualizing their results. This page (meant to be viewed in a desktop\nbrowser with the console open) is the implementation of a few of those\nalgorithms.\n\n\n\nCars [http://cars.kevinhoyt.com]\nI started working with physical computing around 2007. At the time it seemed\nlike a unique new angle to bring to the conference stage. So many demonstrations\nare only in the digital realm. Close the lid of the laptop, and your work\ndisappears. I wanted something that people could see and touch.\n\nFrom the very first time I showed the digital and the physical interacting, I\nwas hooked. What is currently referred to as IoT (Internet of Things) then\nbecame a passion of mine, and I have gone on to include some hardware in\nvirtually every demonstration I give. It has become part of my reputation.\n\nIf you see me at a conference, wether presenting or not, you can always stop me\nand ask to see a hardware demonstration.\n\nMy work at Kaazing has been largely focused on real-time data communication.\nBringing such a broadly horizontal technology to life, has often meant applying\nit to whatever technology discussion is currently trending. With IoT in mind, I\nadded a Bluetooth OBD (on-board diagnostics) adapter to my car, hooked up a USB\nGPS, and began broadcasting my driving in real-time - to include actual metrics\nfrom the vehicle's ECU (engine control unit).\n\nNote: This demonstration does nothing without additional parts being involved.\nWant to see it in action? Hit me up on Twitter [http://twitter.com/krhoyt], or\ncatch the YouTube video [https://www.youtube.com/watch?v=VephhH4buCI].\n\n\n\nFingers [http://fingers.kevinhoyt.com]\nI have worked professionally with a lot of programming languages over the years.\nI have ported QBasic games to ActionScript, written WebSocket server\nimplementation in JavaScript (pre-Node.js), managed I2C bus communication using\nC/C++, and added dynamic elements to web pages using everything from Active\nServer Pages, to Python. Just to name a few.\n\nThese days I am using mostly Java, C/C++, and web stack technologies such as\nJavaScript, SVG, HTML, and CSS.\n\nKaazing Gateway has real-time client libraries for a number of platforms. They\nare designed to be robust and capable for even the most demanding enterprise\nrequirements. This also makes the learning curve incredibly steep. In order to\nlower the barrier to entry, for our most common platform (Web stack), I wrote a\nJavaScript wrapper for our existing libraries.\n\nThe wrapper loads all the necessary JavaScript files, logs usage to a cloud\ndatabase service, and exposes a handful of functions to the developer (connect,\npublish, subscribe, events, etc.). In this case, the underlying protocol for\nreal-time communication is AMQP, so the wrapper abstracts all of that work\n(acknowledge, queue, exchange, etc.).\n\nPut together with a getting started guide\n[http://kaazing.org/demos/quick-start/] (written by me), developers can now\neasily get started using Kaazing Gateway for free without installing or\nconfiguring anything. The demonstration looks like nothing to begin with, but\nput your mouse (or finger) down on the screen, and you will see an animated\ncircle. So will every other screen viewing that page (tested thoroughly across\ndevices).\n\nNote: I like to put Easter Eggs in my demonstrations. The Easter Egg here is a\nhidden chat() function which will setup an entire real-time chat\n[http://fingers.kevinhoyt.com/chat.html] for the developer (the \"hello world\" of\nreal-time).\n\nTerry Smash [http://smash.kevinhoyt.com]\nI worked for Adobe (Allaire, Macromedia, Adobe) for 15 years. While I am already\na fairly creative person, exposure to such world-class talent often led me to\ndevelop my own sense of design - even though I have a CIS (Computer Information\nSystems) degree. I was a regular, highly rated, speaker at Adobe MAX for nearly\nten years.\n\nOne such year the marketing department came up with the idea of putting the\nDeveloper Evangelists against the Design Evangelists. A representative from each\ngroup would propose and promote an Adobe MAX session using a specific hashtag.\nThe winner would get their session at the conference, a randomly selected person\nthat used the hashtag would get an all-expenses paid trip to Adobe MAX that\nyear.\n\nIt came down to me versus Terry White. With but a fraction of the social reach,\nI put up a good fight, but in the end, was not quite able to win.\n\nPart of my proposed session (JavaScript Animation Techniques) was to cover the\nuse of physics engines. My daughter often plays a supporting role in my\ndemonstrations, and at the time she was fond of an iOS game called Tower Smash.\nI put the two together and came up with Terry Smash. In this game you destroy a\ntower of Terry White (and his session on Adobe Muse) blocks by firing blocks of\nmy photo.\n\n\n\nTetris [http://tetris.kevinhoyt.com]\nI have always been fascinated by game development. Which is odd considering that\nI do not like to play video games, and that I have never really built one. Until\nnow! Well, okay, Tetris is like the \"Hello World\" of game development, but I\nhave put my own spin on this version.\n\nThe first major spin I put on this version is that it uses the browser DOM - any\nother version I have seen used HTML5 Canvas. Not that there is anything wrong\nwith using Canvas. If you are porting something like Tetris to the Web, then the\ndrawing routines are probably going to match up pretty nicely. However, getting\ncanvas to render crisply across (high-density) screens is a real chore.\n\nThe second major spin I put on this is that it uses a virtual controller. That\nis to say that you can open the controller screen on your smartphone, and the\ngame on some other screen (iPad, desktop browser, etc), and control the game on\none device from the other device. The devices do not even have to know anything\nabout one another. No Bluetooth here - just good old fashion publish/subscribe\non the Web, thanks to Kaazing Gateway [http://kaazing.com].\n\nThe blocks are filled using pictures of some of the staff around the Kaazing\noffice at the time this project was being developed. The other reason I like\nusing the DOM over Canvas, is that it makes it really easy to change the fill of\nthe blocks using a splash of CSS (or SVG). It could just be solid colors, or\nfaces from presenters at a conference. Company logos, product logos, or rainbow\nunicorns.\n\n\n\nTic Tac Toe [http://tictactoe.kevinhoyt.com]\nAlong the way of exploring physical computing and electronics hardware, I\ndiscovered the need to have custom enclosures for my projects. Wires sticking\nout everywhere just never really did the project justice. This led me to\nfabrication techniques including 3D printing, CNC (subtractive processes), and\nlaser cutting. I have even taught these topics at conferences.\n\nBefore fabricating the real thing, using quality materials, I prefer to build a\nprototype using foam core board, or other materials. This allows me to catch\nproblems with the design before spending too much money on fabrication\nmaterials. Sometimes the foam core board prototype ends up being as far as some\nprojects make it.\n\nBuilding a report with your audience is key to making engaging presentations.\nWhat better way to engage the audience than to let them participate in the\npresentation directly? This project used nine (9) RGB LED\n[http://www.adafruit.com/products/1312] modules, an Arduino Yun\n[http://www.arduino.cc/en/Main/ArduinoBoardYun], and a custom-rolled STOMP\n[https://stomp.github.io/] implementation. The audience could bring up a web\npage, select a color from the user interface, and that color would appear at the\nselected place, with that selected color.\n\nNote: Even without the hardware aspect, the screens will stay in sync, but it\nreally does not do the demonstration justice. To see it in action, hit me up on \nTwitter [http://twitter.com/krhoyt], or see the Vine video\n[https://vine.co/v/MwaYH6EqUBL] recorded at one of my presentations..\n\n\n\nAddendum\nWant to see more of my demonstrations? Would you like to view the slides for a\npresentation? Just must get your hands on the code? Let's talk! I am on Twitter\n[http://twitter.com/krhoyt], Flickr\n[https://www.flickr.com/photos/25243531@N04/], and most instant message systems\nusing the handle \"parkerkrhoyt\".","feature_image":"http://images.kevinhoyt.com/reduction.gear.jpg","featured":0,"status":"published","locale":null,"visibility":"public","author_id":"1","created_at":"2015-04-27T13:23:06.000Z","updated_at":"2015-05-29T14:28:08.000Z","published_at":"2015-04-27T13:23:00.000Z","custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"type":"page","email_recipient_filter":"none"},{"id":"5adb8922351ffe0018a57723","uuid":"4cacb144-38f2-4cfc-8fd6-2aec953cf5f5","title":"Real-Time Smart Buildings","slug":"real-time-smart-buildings","mobiledoc":"{\"version\":\"0.3.1\",\"markups\":[],\"atoms\":[],\"cards\":[[\"markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\"*What is your favorite building?  The office?  The coffee shop down the street?  Your home?  Maybe a shed out back?  Whatever the case, there is no shortage of ways to augment your home with IoT (Internet of Things).  Google Nest, Philips Hue, or WeMo give you classic request/response control over your appliances.*\\n\\n---\\n\\n==This is reposted on the [Kaazing Open Source Blog](http://kaazing.org/blog/real-time-smart-buildings/).==\\n\\nYou might even get notifications when appliances fall outside a specific range.  Chances are that this will happen via request/response as well.  Somewhere in the background of the application there will be a routine that occasionally polls the appliance to find out the current conditions.\\n\\n> Have you ever stopped to wonder why request/response is used in these scenarios?\\n\\nI would propose that request/response seems to make sense because it is what we know from Web development.  We have gotten really good at request/response, and have countless frameworks to support it.  While \\\"what I know\\\" is certainly a valid answer, when it comes to IoT, especially on the industrial level, real-time may offer a better solution.\\n\\n##Commercial HVAC\\n\\nMost commercial buildings, whether they be your grocery store, or your office, have massive HVAC (Heating, Venting, Air Conditioning) systems installed.  These system have compressors that manage forcing air (hot or cold) through the building.  Many were installed when the building was built, and this is not equipment that you simply upgrade and replace because you want IoT features.\\n\\nAt the same time, IoT features bring some very real optimizations (read: money saving) to these systems.\\n\\n###Manufacturer\\n\\nFrom a manufacturer perspective, knowing how your equipment runs based on building demand and environmental factors such as the weather, can lead you to build more reliable units that better fit your customer needs (read: improved sales).\\n\\n###Customer\\n\\nThe same type of information that is valuable to the manufacturer can be equally valuable to the customer, but for completely different reasons.  For example, why run the compressor at full capacity if the weekend load does not require it?  Optimizations to be had here save wear and tear, as well as electrical costs to power the compressors (read: savings).\\n\\n###Maintenance\\n\\nCompressors like this are usually sold by a manufacturer, installed by a partner, and maintained by whomever the customer wants to call to have repairs done.  Inevitably, since the customer is not an HVAC technician, the call for maintenance is vague at best.  The result then is that the repair technician will have to spend a lot of time to (a) figure out the problem (b) order the parts and (c) install the parts.  This can take several days; meanwhile building tenants become increasingly disgruntled.\\n\\n> What if the system provided insight to the problem by itself, and was able to notify an HVAC technician with the specifics before they ever left the warehouse? (read: reduced operational costs).\\n\\n###The Request/Response Problem\\n\\nAt first glance, again because it is comfortable, you may be inclined to solve these problems with request/response.  However a request/response solution only gives you a snapshot into the condition of the equipment.  \\n\\nLet us say for example, that we poll the compressor once every five (5) seconds.  The resulting sensor snapshots give us an indication that broken equipment is outside acceptable operational ranges, but it does not give us the whole picture.  In other words, the aforementioned three interested parties will only get glimpses of the data they actually want.\\n\\nNo worries.  We will do what comes naturally and poll more frequently - let us say we drop this to one (1) second intervals.  Now we may be getting more data, but consider the financial impact of this decision.  More frequent polling means more hardware will be needed.  This additional demand will need to be met not only by the web server, but also the database.  And ultimately, if we chose request/response for the compressor in the beginning, it may not even be capable of delivering the information at a faster rate.\\n\\n###The Real-Time Solution\\n\\nWhat if we simply went with a real-time solution from the start?  Sensor data can be continually streamed from the compressor.  To see this you need only fire up an Arduino and Serial.println( data ) as fast as the processor will let you.  Serial connections are how most deployed hardware like these compressors prefer to communicate.\\n\\n> To address commercial IoT, we need to shift our thinking from getting data when we think we need it, to getting all the data all the time, and taking action as conditions exceed operational ranges.\\n\\nThis shift does require a change in how we think about our architecture.  Rather than request/response, a more appropriate solution here would be publish/subscribe.\\n\\nIn a publish/subscribe architecture, commercial equipment can stream data in real-time, all the time.  Secondary applications that are interested in that data then only need to subscribe for those messages.  For example, a database system could capture all the data as it comes in, and either selectively store data incrementally (every five seconds), discarding the information in-between, or batch update with all the captured data.\\n\\nNow that we are operating in real-time, let us take a look at those aforementioned parties.\\n\\n* The manufacturer can receive all sensor data from all deployed units, providing a clear picture as to the behavior of the systems.  Now they really know how to optimize future development.\\n\\n* The customer, armed with real-time data, can now adjust the compressor output on-demand.  As soon as people leave the office to go home, the system can start to wind down - whatever time of day that might be.  *If only weather applications were built on real-time systems too.*\\n\\n* The repair technician now sees how the system is behaving in real-time.  Patterns that indicate specific problems can now be visualized and interpreted for faster resolution.\\n\\n##Real-Time HVAC Example\\n\\nAs the saying goes, seeing is believing.  Based on work Kaazing is doing with partners, we assembled a proof of concept to show the impact of real-time data visualization on a commercial HVAC system.\\n\\n[![Real-time HVAC visualization.](http://images.kevinhoyt.com/kaazing.iot.buildings.web.jpg)](http://temp.kevinhoyt.com/kaazing/iot/buildings/local.html)\\n\\nYou can see the difference yourself, first hand.  When you load the example, the page will be operating the old fashioned request/response way.  To see the latest data, simply refresh the page.  Clicking on the button labeled \\\"5\\\" will move to polling the database every five seconds.  Data will update, but the snapshot is incomplete.  Clicking the button labeled \\\"1\\\" will up the polling rate to every one second.\\n\\nNow click on the Kaazing logo.  Data that flows in real-time immediately surfaces a pattern that has been there all along - a sine wave.  Imagine the repair technician seeing this signal, looking it up in a manual, and showing up at your building before the problem becomes critical.  Additionally, the technician would have all the parts and tools necessary to resolve the situation.\\n\\n###Next Steps\\n\\nExploring real-time IoT does not require that you invest in a commercial HVAC compressor.  Head on over to my [personal GitHub repository](https://github.com/krhoyt/Kaazing/tree/master/iot/buildings), and grab the \\\"iot/buildings\\\" folder.  There is Arduino code in that directory that you can test for yourself.  The accompanying Java code reads the serial data and taps into the open source Kaazing Gateway.  Even if you do not have Kaazing Gateway installed, you can test against our freely available \\\"Sandbox\\\" using that same Java code.\\n\\nIf you're used to using request/response, because it's the solution you know, then perhaps it's time to make the leap to publish/subscribe for your next IoT project.\\n\"}]],\"sections\":[[10,0]],\"ghostVersion\":\"3.0\"}","html":"<!--kg-card-begin: markdown--><p><em>What is your favorite building?  The office?  The coffee shop down the street?  Your home?  Maybe a shed out back?  Whatever the case, there is no shortage of ways to augment your home with IoT (Internet of Things).  Google Nest, Philips Hue, or WeMo give you classic request/response control over your appliances.</em></p>\n<hr>\n<p><mark>This is reposted on the <a href=\"http://kaazing.org/blog/real-time-smart-buildings/\">Kaazing Open Source Blog</a>.</mark></p>\n<p>You might even get notifications when appliances fall outside a specific range.  Chances are that this will happen via request/response as well.  Somewhere in the background of the application there will be a routine that occasionally polls the appliance to find out the current conditions.</p>\n<blockquote>\n<p>Have you ever stopped to wonder why request/response is used in these scenarios?</p>\n</blockquote>\n<p>I would propose that request/response seems to make sense because it is what we know from Web development.  We have gotten really good at request/response, and have countless frameworks to support it.  While &quot;what I know&quot; is certainly a valid answer, when it comes to IoT, especially on the industrial level, real-time may offer a better solution.</p>\n<h2 id=\"commercialhvac\">Commercial HVAC</h2>\n<p>Most commercial buildings, whether they be your grocery store, or your office, have massive HVAC (Heating, Venting, Air Conditioning) systems installed.  These system have compressors that manage forcing air (hot or cold) through the building.  Many were installed when the building was built, and this is not equipment that you simply upgrade and replace because you want IoT features.</p>\n<p>At the same time, IoT features bring some very real optimizations (read: money saving) to these systems.</p>\n<h3 id=\"manufacturer\">Manufacturer</h3>\n<p>From a manufacturer perspective, knowing how your equipment runs based on building demand and environmental factors such as the weather, can lead you to build more reliable units that better fit your customer needs (read: improved sales).</p>\n<h3 id=\"customer\">Customer</h3>\n<p>The same type of information that is valuable to the manufacturer can be equally valuable to the customer, but for completely different reasons.  For example, why run the compressor at full capacity if the weekend load does not require it?  Optimizations to be had here save wear and tear, as well as electrical costs to power the compressors (read: savings).</p>\n<h3 id=\"maintenance\">Maintenance</h3>\n<p>Compressors like this are usually sold by a manufacturer, installed by a partner, and maintained by whomever the customer wants to call to have repairs done.  Inevitably, since the customer is not an HVAC technician, the call for maintenance is vague at best.  The result then is that the repair technician will have to spend a lot of time to (a) figure out the problem (b) order the parts and (c) install the parts.  This can take several days; meanwhile building tenants become increasingly disgruntled.</p>\n<blockquote>\n<p>What if the system provided insight to the problem by itself, and was able to notify an HVAC technician with the specifics before they ever left the warehouse? (read: reduced operational costs).</p>\n</blockquote>\n<h3 id=\"therequestresponseproblem\">The Request/Response Problem</h3>\n<p>At first glance, again because it is comfortable, you may be inclined to solve these problems with request/response.  However a request/response solution only gives you a snapshot into the condition of the equipment.</p>\n<p>Let us say for example, that we poll the compressor once every five (5) seconds.  The resulting sensor snapshots give us an indication that broken equipment is outside acceptable operational ranges, but it does not give us the whole picture.  In other words, the aforementioned three interested parties will only get glimpses of the data they actually want.</p>\n<p>No worries.  We will do what comes naturally and poll more frequently - let us say we drop this to one (1) second intervals.  Now we may be getting more data, but consider the financial impact of this decision.  More frequent polling means more hardware will be needed.  This additional demand will need to be met not only by the web server, but also the database.  And ultimately, if we chose request/response for the compressor in the beginning, it may not even be capable of delivering the information at a faster rate.</p>\n<h3 id=\"therealtimesolution\">The Real-Time Solution</h3>\n<p>What if we simply went with a real-time solution from the start?  Sensor data can be continually streamed from the compressor.  To see this you need only fire up an Arduino and Serial.println( data ) as fast as the processor will let you.  Serial connections are how most deployed hardware like these compressors prefer to communicate.</p>\n<blockquote>\n<p>To address commercial IoT, we need to shift our thinking from getting data when we think we need it, to getting all the data all the time, and taking action as conditions exceed operational ranges.</p>\n</blockquote>\n<p>This shift does require a change in how we think about our architecture.  Rather than request/response, a more appropriate solution here would be publish/subscribe.</p>\n<p>In a publish/subscribe architecture, commercial equipment can stream data in real-time, all the time.  Secondary applications that are interested in that data then only need to subscribe for those messages.  For example, a database system could capture all the data as it comes in, and either selectively store data incrementally (every five seconds), discarding the information in-between, or batch update with all the captured data.</p>\n<p>Now that we are operating in real-time, let us take a look at those aforementioned parties.</p>\n<ul>\n<li>\n<p>The manufacturer can receive all sensor data from all deployed units, providing a clear picture as to the behavior of the systems.  Now they really know how to optimize future development.</p>\n</li>\n<li>\n<p>The customer, armed with real-time data, can now adjust the compressor output on-demand.  As soon as people leave the office to go home, the system can start to wind down - whatever time of day that might be.  <em>If only weather applications were built on real-time systems too.</em></p>\n</li>\n<li>\n<p>The repair technician now sees how the system is behaving in real-time.  Patterns that indicate specific problems can now be visualized and interpreted for faster resolution.</p>\n</li>\n</ul>\n<h2 id=\"realtimehvacexample\">Real-Time HVAC Example</h2>\n<p>As the saying goes, seeing is believing.  Based on work Kaazing is doing with partners, we assembled a proof of concept to show the impact of real-time data visualization on a commercial HVAC system.</p>\n<p><a href=\"http://temp.kevinhoyt.com/kaazing/iot/buildings/local.html\"><img src=\"http://images.kevinhoyt.com/kaazing.iot.buildings.web.jpg\" alt=\"Real-time HVAC visualization.\" loading=\"lazy\"></a></p>\n<p>You can see the difference yourself, first hand.  When you load the example, the page will be operating the old fashioned request/response way.  To see the latest data, simply refresh the page.  Clicking on the button labeled &quot;5&quot; will move to polling the database every five seconds.  Data will update, but the snapshot is incomplete.  Clicking the button labeled &quot;1&quot; will up the polling rate to every one second.</p>\n<p>Now click on the Kaazing logo.  Data that flows in real-time immediately surfaces a pattern that has been there all along - a sine wave.  Imagine the repair technician seeing this signal, looking it up in a manual, and showing up at your building before the problem becomes critical.  Additionally, the technician would have all the parts and tools necessary to resolve the situation.</p>\n<h3 id=\"nextsteps\">Next Steps</h3>\n<p>Exploring real-time IoT does not require that you invest in a commercial HVAC compressor.  Head on over to my <a href=\"https://github.com/krhoyt/Kaazing/tree/master/iot/buildings\">personal GitHub repository</a>, and grab the &quot;iot/buildings&quot; folder.  There is Arduino code in that directory that you can test for yourself.  The accompanying Java code reads the serial data and taps into the open source Kaazing Gateway.  Even if you do not have Kaazing Gateway installed, you can test against our freely available &quot;Sandbox&quot; using that same Java code.</p>\n<p>If you're used to using request/response, because it's the solution you know, then perhaps it's time to make the leap to publish/subscribe for your next IoT project.</p>\n<!--kg-card-end: markdown-->","comment_id":"23","plaintext":"What is your favorite building? The office? The coffee shop down the street?\nYour home? Maybe a shed out back? Whatever the case, there is no shortage of\nways to augment your home with IoT (Internet of Things). Google Nest, Philips\nHue, or WeMo give you classic request/response control over your appliances.\n\n\n--------------------------------------------------------------------------------\n\nThis is reposted on the Kaazing Open Source Blog\n[http://kaazing.org/blog/real-time-smart-buildings/].\n\nYou might even get notifications when appliances fall outside a specific range.\nChances are that this will happen via request/response as well. Somewhere in the\nbackground of the application there will be a routine that occasionally polls\nthe appliance to find out the current conditions.\n\n> Have you ever stopped to wonder why request/response is used in these scenarios?\n\n\nI would propose that request/response seems to make sense because it is what we\nknow from Web development. We have gotten really good at request/response, and\nhave countless frameworks to support it. While \"what I know\" is certainly a\nvalid answer, when it comes to IoT, especially on the industrial level,\nreal-time may offer a better solution.\n\nCommercial HVAC\nMost commercial buildings, whether they be your grocery store, or your office,\nhave massive HVAC (Heating, Venting, Air Conditioning) systems installed. These\nsystem have compressors that manage forcing air (hot or cold) through the\nbuilding. Many were installed when the building was built, and this is not\nequipment that you simply upgrade and replace because you want IoT features.\n\nAt the same time, IoT features bring some very real optimizations (read: money\nsaving) to these systems.\n\nManufacturer\nFrom a manufacturer perspective, knowing how your equipment runs based on\nbuilding demand and environmental factors such as the weather, can lead you to\nbuild more reliable units that better fit your customer needs (read: improved\nsales).\n\nCustomer\nThe same type of information that is valuable to the manufacturer can be equally\nvaluable to the customer, but for completely different reasons. For example, why\nrun the compressor at full capacity if the weekend load does not require it?\nOptimizations to be had here save wear and tear, as well as electrical costs to\npower the compressors (read: savings).\n\nMaintenance\nCompressors like this are usually sold by a manufacturer, installed by a\npartner, and maintained by whomever the customer wants to call to have repairs\ndone. Inevitably, since the customer is not an HVAC technician, the call for\nmaintenance is vague at best. The result then is that the repair technician will\nhave to spend a lot of time to (a) figure out the problem (b) order the parts\nand (c) install the parts. This can take several days; meanwhile building\ntenants become increasingly disgruntled.\n\n> What if the system provided insight to the problem by itself, and was able to\nnotify an HVAC technician with the specifics before they ever left the\nwarehouse? (read: reduced operational costs).\n\n\nThe Request/Response Problem\nAt first glance, again because it is comfortable, you may be inclined to solve\nthese problems with request/response. However a request/response solution only\ngives you a snapshot into the condition of the equipment.\n\nLet us say for example, that we poll the compressor once every five (5) seconds.\nThe resulting sensor snapshots give us an indication that broken equipment is\noutside acceptable operational ranges, but it does not give us the whole\npicture. In other words, the aforementioned three interested parties will only\nget glimpses of the data they actually want.\n\nNo worries. We will do what comes naturally and poll more frequently - let us\nsay we drop this to one (1) second intervals. Now we may be getting more data,\nbut consider the financial impact of this decision. More frequent polling means\nmore hardware will be needed. This additional demand will need to be met not\nonly by the web server, but also the database. And ultimately, if we chose\nrequest/response for the compressor in the beginning, it may not even be capable\nof delivering the information at a faster rate.\n\nThe Real-Time Solution\nWhat if we simply went with a real-time solution from the start? Sensor data can\nbe continually streamed from the compressor. To see this you need only fire up\nan Arduino and Serial.println( data ) as fast as the processor will let you.\nSerial connections are how most deployed hardware like these compressors prefer\nto communicate.\n\n> To address commercial IoT, we need to shift our thinking from getting data when\nwe think we need it, to getting all the data all the time, and taking action as\nconditions exceed operational ranges.\n\n\nThis shift does require a change in how we think about our architecture. Rather\nthan request/response, a more appropriate solution here would be\npublish/subscribe.\n\nIn a publish/subscribe architecture, commercial equipment can stream data in\nreal-time, all the time. Secondary applications that are interested in that data\nthen only need to subscribe for those messages. For example, a database system\ncould capture all the data as it comes in, and either selectively store data\nincrementally (every five seconds), discarding the information in-between, or\nbatch update with all the captured data.\n\nNow that we are operating in real-time, let us take a look at those\naforementioned parties.\n\n * The manufacturer can receive all sensor data from all deployed units,\n   providing a clear picture as to the behavior of the systems. Now they really\n   know how to optimize future development.\n   \n   \n * The customer, armed with real-time data, can now adjust the compressor output\n   on-demand. As soon as people leave the office to go home, the system can\n   start to wind down - whatever time of day that might be. If only weather\n   applications were built on real-time systems too.\n   \n   \n * The repair technician now sees how the system is behaving in real-time.\n   Patterns that indicate specific problems can now be visualized and\n   interpreted for faster resolution.\n   \n   \n\nReal-Time HVAC Example\nAs the saying goes, seeing is believing. Based on work Kaazing is doing with\npartners, we assembled a proof of concept to show the impact of real-time data\nvisualization on a commercial HVAC system.\n\n [http://temp.kevinhoyt.com/kaazing/iot/buildings/local.html]\n\nYou can see the difference yourself, first hand. When you load the example, the\npage will be operating the old fashioned request/response way. To see the latest\ndata, simply refresh the page. Clicking on the button labeled \"5\" will move to\npolling the database every five seconds. Data will update, but the snapshot is\nincomplete. Clicking the button labeled \"1\" will up the polling rate to every\none second.\n\nNow click on the Kaazing logo. Data that flows in real-time immediately surfaces\na pattern that has been there all along - a sine wave. Imagine the repair\ntechnician seeing this signal, looking it up in a manual, and showing up at your\nbuilding before the problem becomes critical. Additionally, the technician would\nhave all the parts and tools necessary to resolve the situation.\n\nNext Steps\nExploring real-time IoT does not require that you invest in a commercial HVAC\ncompressor. Head on over to my personal GitHub repository\n[https://github.com/krhoyt/Kaazing/tree/master/iot/buildings], and grab the\n\"iot/buildings\" folder. There is Arduino code in that directory that you can\ntest for yourself. The accompanying Java code reads the serial data and taps\ninto the open source Kaazing Gateway. Even if you do not have Kaazing Gateway\ninstalled, you can test against our freely available \"Sandbox\" using that same\nJava code.\n\nIf you're used to using request/response, because it's the solution you know,\nthen perhaps it's time to make the leap to publish/subscribe for your next IoT\nproject.","feature_image":"http://images.kevinhoyt.com/hvac.compressor.building.jpg","featured":0,"status":"published","locale":null,"visibility":"public","author_id":"1","created_at":"2015-04-30T18:44:04.000Z","updated_at":"2015-05-27T14:35:09.000Z","published_at":"2015-05-13T14:48:23.000Z","custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"type":"post","email_recipient_filter":"none"},{"id":"5adb8922351ffe0018a57724","uuid":"33e07376-9b3a-42b5-97ef-098c5814ece7","title":"First Steps with Intel Edison","slug":"first-steps-with-intel-edison","mobiledoc":"{\"version\":\"0.3.1\",\"markups\":[],\"atoms\":[],\"cards\":[[\"markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\"*There are a lot of SoC (System on a Chip) devices out there these days.  I have written about the Arduino Yun in the past.  Today I would like to introduce you to the Intel Edison.  At about the quarter of the size of a credit card (35.5mm x 25mm), this little piece of kit packs a lot of punch in a low power footprint.*\\n\\n---\\n\\n==This is reposted on the [Kaazing Open Source Blog](http://kaazing.org/blog/first-steps-with-intel-edison/).==\\n\\nAs the name implies, the Intel Edison includes a dual-core Intel Atom processor, as well as both Wi-Fi and Bluetooth LE radios.  It has a tiny 70-pin connector, which is designed to be mated with other Intel Edison \\\"blocks\\\".  Much in the same fashion as you would stack \\\"shields\\\" on an Arduino.\\n\\n![Intel Edison - photo courtesy SparkFun](http://images.kevinhoyt.com/sparkfun.intel.edison.jpg)\\n\\nHaving mentioned Arduino, many of the Intel Edison tutorials talk about using a \\\"breakout kit\\\", which makes it more compatible with Arduino.  The first steps outlined in this post assume you have the Intel Edison board, and the [SparkFun Base Block](https://www.sparkfun.com/products/13045).  No Arduino compatibility needed here.  We are going to tap directly into the Yocto Linux image on the Intel Edison.\\n\\n![SparkFun Base Block - photo courtesy SparkFun](http://images.kevinhoyt.com/sparkfun.edison.base.block.jpg)\\n\\nWhat follows is an explanation of the commands I use to configure and use the Intel Edison in my projects.  The commands have been taught to others as well, which has led to further refinements.  \\n\\nWhile labeled **\\\"First Steps\\\"** a better title might be **\\\"Build a Real-Time Internet-Connected Barcode Scanner.\\\"**  That is the first project I set out to implement on the Intel Edison, so the vast majority of these notes come from that discovery process.  I will refer to that project throughout, but will not spend much time talking about the code proper.\\n\\n> As with most of my blog posts, you can get the code for an Internet-connected barcode scanner (and much more) from my personal [GitHub repository](https://github.com/krhoyt/Kaazing/tree/master/iot/stores).\\n\\n###Getting to Know You\\n\\nThe main parts to know about the Intel Edison at this point are the two USB-mini ports mounted at the edge.  One is labeled \\\"console\\\" while the other is labeled \\\"OTG\\\".  We will talk more about what that means in a moment, but as you might have guessed, you will in turn need two USB-mini cables to make the most of this tutorial.\\n\\nThe \\\"console\\\" USB port is a TTY port.  This port effectively powers the Intel Edison.  While getting started, this gives you a way to log into Yocto Linux.  Once you have your Edison configured however, you will likely just use this port for power, and SSH into the Edison.  \\n\\n> If you want the Edison to run on its own, without a cable for power, SparkFun makes a [Battery Block](https://www.sparkfun.com/products/13037) as well.\\n\\nThe other USB port is labeled \\\"OTG\\\".  This stands for USB \\\"on the go\\\" which is a formal part of USB.  For the purposes of my barcode scanner project, this effectively translates into \\\"A USB port I can plug devices into, and have them exposed to the Linux OS.\\\"  There is more to OTG, most notably that it can be a hub as well, but that is beyond the scope of first steps.\\n\\n###House Cleaning\\n\\nPlug your two USB micro cables into to two ports on the Intel Edison.  Then plug the cable in the \\\"console\\\" port into the USB port on your computer.  You should see the Edison turn on a blue LED.  Now connect the other cable into another USB port on your computer.  You should see the Edison mount itself as a drive on your system.\\n\\nThe first step here is to get rid of the existing content on the Intel Edison, and the latest Yocto image.  You can download the latest from the [Intel web site](https://software.intel.com/en-us/iot/hardware/edison/downloads).  You will be looking for the \\\"Intel Edison Board Firmware Software\\\" section, and downloading the \\\"Yocto Complete Image\\\".  At the time of this writing, that was Release 2.1.\\n\\nAt first run, you probably will not have any files on the Edison when mounted as a USB drive.  I still like to check, and even run the following commands just to be sure there are no hidden files lying around.  We want this area to be nice and tidy for our next step.\\n\\n```\\ncd /Volumes/Edison\\nrm -rf *\\nrm -rf \\\\.*\\n```\\n\\nNow unzip the the Yocto image you downloaded from the Intel web site into a folder.  Copy the contents of that folder into the root directory of the Intel Edison USB drive.  When we reboot the Intel Edison in a moment, it will then look into that space for a new Yocto image, and use those files.\\n\\nTo reboot the Intel Edison, we first have to connect to the existing Yocto image.  We can do this using the TTY USB cable.  First we need to know how to address the Edison.  Then we will use the \\\"screen\\\" command to log into the Edison.  Then we will tell the Edison to reboot - and we get to watch the screen fill up with information as it goes along.\\n\\n```\\nls /dev/tty.*\\n<My Edison is DA01G6Q8 - yours may be different>\\nscreen /dev/tty.usbserial-DA01G6Q8 115200 -L\\n<Enter again>\\n<Login: root>\\n<There is no password>\\nreboot ota\\n```\\n\\nWhen all the updating is finished, with Release 2.1 you should have a Yocto 1.6.1 image, which should be told to you when you are prompted to log back into the device.  If you see something less, then the process did not complete successfully.  This has randomly happened to me before when not all the image files were copied.  Try the commands again from the top, making sure that everything is copied (or downloaded in the first place).\\n\\n###Configuration\\n\\nBefore disconnecting anything, and while still logged in via the \\\"screen\\\" command from the previous step, you will want to run the Intel Edison configuration utility.  This utility will put a password on the device, give it a host name, a get the board connected to wi-fi among other things.  Complete all the steps.  It is the \\\"other things\\\" you do not see or get prompted for that are important - such as enabling SSH.\\n\\n```\\nconfigure_edison --setup\\n```\\n\\nIf your computer and your Intel Edison are on the same network, you can now check to see that your Edison is ready by loading the hostname in a browser.  For example:\\n\\n```\\nhttp://kedison.local/\\n```\\n\\nWhere I named my Intel Edison \\\"Kedison\\\".  Some browsers are more finicky than others about hostname access, so I suggest entering the full \\\"http\\\" qualifier and the trailing \\\"/\\\" until you know how your browser will react.  If everything has gone correctly, then you will see a web page with information about your Intel Edison.\\n\\n![Intel Edison device information page.](http://images.kevinhoyt.com/intel.edison.device.information.jpg)\\n\\nYou can now type \\\"exit\\\" into your Intel Edison to logout.  Then to exit the screen program type Ctrl+A and then D.  From here on out we will SSH into our Edison.  If you have already accessed your Edison via SSH, then you will need to edit ~/.ssh/known_hosts and remove the Edison line first.\\n\\n```\\n<Remove previous if needed>\\nnano ~/.ssh/known_hosts\\n```\\n\\n```\\nssh root@kedison.local\\n<Password you used during configuration>\\n```\\n\\n==You may also want to head back over the the Edison mounted as a USB drive and remove the image files.  You can do this via a GUI or command line as above.  If you are doing this from a GUI, then do not forget to empty your trash to actually remove the files from the USB drive storage space.==\\n\\nAs you may have noticed from the commands above, I like to use Nano as my editor.  If you have that preference as well, you can use the following command to install Nano on your Intel Edison.\\n\\n```\\nwget http://www.nano-editor.org/dist/v2.2/nano-2.2.6.tar.gz && tar xvf nano-2.2.6.tar.gz && cd nano-2.2.6 && ./configure && make && make install\\n```\\n\\nFinally, to communicate with Kaazing Gateway, I will be using the Java client libraries.  I will also be using Java to access the USB serial port of the Intel Edison, and interact with the barcode scanner.  That means I need to have Java installed.  If you prefer something else, Python is already installed, as is Node.JS version 0.10.35.\\n\\nTo get a JRE (Java Runtime Environment) installed, first head on over to the [Oracle website](http://www.oracle.com/technetwork/java/javase/downloads/index.html) and download the appropriate JRE build.  ==Note that you are specifically looking for a JRE here, not a JDK (Java Development Kit).==  You are looking for Linux x86 in a gzip format.  I downloaded \\\"jre-8u45-linux-i586.tar.gz\\\".  Unfortunately, I cannot provide a direct link since a license agreement must first be accepted.\\n\\nTo copy the compressed JRE from my Mac to the Intel Edison I used SCP.  If you have another tool you prefer, you can use that as well.  This is not the USB drive (as with the Yocto image), but rather moving a file over to a place on the OS.  ==The \\\"/home/root\\\" is the root users home directory.==  Then logged back into the Edison, we can use \\\"tar\\\" to unpack the JRE.\\n\\n```\\nscp ~/Desktop/jre-8u45-linux-i586.gz \\n  root@kedison.local:/home/root/jre-8u45.gz\\ntar xvf jre-8u45.gz\\nln -s jre1.8.0_45/bin/java java\\n```\\n\\nThe last command there creates a symbolic link to the Java binary.  It is not necessary, but makes life a little easier when you are running Java commands from various places on the OS.  From here you might remove the compressed JRE file, as well as the compressed Nano file if you have not already.  Again, not necessary, but keeping a clean house make life easier.\\n\\n###Barcode Scanning\\n\\nAt this point, your Intel Edison is ready to go.  We have the latest image, connected to the Internet, and put some basic tooling in place.  You can now start building the IoT (Internet of Things) project of your dreams.  While not the project of my dreams, I have had a barcode scanner I have been wanting to use in a project for some time.\\n\\nThe [barcode scanner](https://www.sparkfun.com/products/9166) I picked up from SparkFun has a USB cable attached to it.  The scanner also comes with a User's Manual guide which has barcodes all over it.  These barcodes can be scanned using the scanner to configure how it operates.  There are many changes you can make, but I just set it to RS-232, which allows me to access the scanner as a serial port.\\n\\n![Barcode scanner - photo courtesy SparkFun](http://images.kevinhoyt.com/sparkfun.barcode.scanner.jpg)\\n\\nTo configure the scanner, plug it into your computer (not the Intel Edison).  This gives the scanner power.  Then scan the ==\\\"PROGRAM\\\"== barcode at the top of any page.  Next scan the ==\\\"RS-232C\\\"== barcode.  It may take some practice to get your aim down.  On the page labeled ==\\\"RS-232C interface\\\"== scan the ==\\\"Baud rate\\\"== setting to ==9600==.  Then to finish, scan the barcode labeled ==\\\"END\\\"== at the bottom of any page.\\n\\n[![Download the user manual](http://images.kevinhoyt.com/barcode.scanner.user.manual.jpg)](http://images.kevinhoyt.com/barcode.scanner.user.manual.pdf)\\n\\nWith your scanner configured, remove it from your computer.  Since the OTG port on the Intel Edison is of the micro variety, we need an adapter.  I picked up an [USB OTG Cable](https://www.sparkfun.com/products/11604) from SparkFun for this purpose.  Connect the USB-A end of the scanner cable to the matching end of the USB OTG cable.  Then plug the micro end of the USB OTG cable into the OTG port on the Intel Edison.\\n\\n![SparkFun OTG Cable](http://images.kevinhoyt.com/sparkfun.usb.otg.jpg)\\n\\nYocto has a number of mappings for serial ports, but the USB OTG will surface as ==\\\"/dev/ttyACMO\\\"==.  You can use the ==\\\"cat\\\"== command to see if the barcode scanner is attached in RS-232 mode and working correctly.  Pick up a DVD or CD nearby and scan away.  You will see the barcode appear in the Intel Edison console.\\n\\n```\\ncat /dev/ttyACM0 9600\\n```\\n\\nWith my Java-based barcode scanning application bundled up as a JAR file, I copied it over to the Intel Edison using the SCP command from above.  Then using the Java binary I can run the program on the Edison.  This program runs the barcode against Amazon to get a price and image for the item scanned.  That information is then passed to Kaazing Gateway, where any client listening for those messages can pick them up and process them.\\n\\n![Mobile browser shopping cart.](http://images.kevinhoyt.com/kaazing.iot.stores.ios.png)\\n\\nI built a simple web client using the JavaScript library from the Kaazing Open Source blog.  The web client shows the price and image in a shopping cart on the device.  The web client works on the desktop, but also on mobile browsers.  In a future post, I will detail how Java connects to the scanner, retrieves information from Amazon, and sends messages, as well as how the web client works.\\n\\n###Next Steps\\n\\nThe Intel Edison is a really powerful little computer.  It lends itself to many applications where a SoC is preferred (e.g. running applications on an OS).  This post gives you all the details you need to get started and talks about using a barcode scanner.  There are many other USB possibilities to be had though.  I would encourage you to explore the Edison using your own USB device and language of choice.\"}]],\"sections\":[[10,0]],\"ghostVersion\":\"3.0\"}","html":"<!--kg-card-begin: markdown--><p><em>There are a lot of SoC (System on a Chip) devices out there these days.  I have written about the Arduino Yun in the past.  Today I would like to introduce you to the Intel Edison.  At about the quarter of the size of a credit card (35.5mm x 25mm), this little piece of kit packs a lot of punch in a low power footprint.</em></p>\n<hr>\n<p><mark>This is reposted on the <a href=\"http://kaazing.org/blog/first-steps-with-intel-edison/\">Kaazing Open Source Blog</a>.</mark></p>\n<p>As the name implies, the Intel Edison includes a dual-core Intel Atom processor, as well as both Wi-Fi and Bluetooth LE radios.  It has a tiny 70-pin connector, which is designed to be mated with other Intel Edison &quot;blocks&quot;.  Much in the same fashion as you would stack &quot;shields&quot; on an Arduino.</p>\n<p><img src=\"http://images.kevinhoyt.com/sparkfun.intel.edison.jpg\" alt=\"Intel Edison - photo courtesy SparkFun\" loading=\"lazy\"></p>\n<p>Having mentioned Arduino, many of the Intel Edison tutorials talk about using a &quot;breakout kit&quot;, which makes it more compatible with Arduino.  The first steps outlined in this post assume you have the Intel Edison board, and the <a href=\"https://www.sparkfun.com/products/13045\">SparkFun Base Block</a>.  No Arduino compatibility needed here.  We are going to tap directly into the Yocto Linux image on the Intel Edison.</p>\n<p><img src=\"http://images.kevinhoyt.com/sparkfun.edison.base.block.jpg\" alt=\"SparkFun Base Block - photo courtesy SparkFun\" loading=\"lazy\"></p>\n<p>What follows is an explanation of the commands I use to configure and use the Intel Edison in my projects.  The commands have been taught to others as well, which has led to further refinements.</p>\n<p>While labeled <strong>&quot;First Steps&quot;</strong> a better title might be <strong>&quot;Build a Real-Time Internet-Connected Barcode Scanner.&quot;</strong>  That is the first project I set out to implement on the Intel Edison, so the vast majority of these notes come from that discovery process.  I will refer to that project throughout, but will not spend much time talking about the code proper.</p>\n<blockquote>\n<p>As with most of my blog posts, you can get the code for an Internet-connected barcode scanner (and much more) from my personal <a href=\"https://github.com/krhoyt/Kaazing/tree/master/iot/stores\">GitHub repository</a>.</p>\n</blockquote>\n<h3 id=\"gettingtoknowyou\">Getting to Know You</h3>\n<p>The main parts to know about the Intel Edison at this point are the two USB-mini ports mounted at the edge.  One is labeled &quot;console&quot; while the other is labeled &quot;OTG&quot;.  We will talk more about what that means in a moment, but as you might have guessed, you will in turn need two USB-mini cables to make the most of this tutorial.</p>\n<p>The &quot;console&quot; USB port is a TTY port.  This port effectively powers the Intel Edison.  While getting started, this gives you a way to log into Yocto Linux.  Once you have your Edison configured however, you will likely just use this port for power, and SSH into the Edison.</p>\n<blockquote>\n<p>If you want the Edison to run on its own, without a cable for power, SparkFun makes a <a href=\"https://www.sparkfun.com/products/13037\">Battery Block</a> as well.</p>\n</blockquote>\n<p>The other USB port is labeled &quot;OTG&quot;.  This stands for USB &quot;on the go&quot; which is a formal part of USB.  For the purposes of my barcode scanner project, this effectively translates into &quot;A USB port I can plug devices into, and have them exposed to the Linux OS.&quot;  There is more to OTG, most notably that it can be a hub as well, but that is beyond the scope of first steps.</p>\n<h3 id=\"housecleaning\">House Cleaning</h3>\n<p>Plug your two USB micro cables into to two ports on the Intel Edison.  Then plug the cable in the &quot;console&quot; port into the USB port on your computer.  You should see the Edison turn on a blue LED.  Now connect the other cable into another USB port on your computer.  You should see the Edison mount itself as a drive on your system.</p>\n<p>The first step here is to get rid of the existing content on the Intel Edison, and the latest Yocto image.  You can download the latest from the <a href=\"https://software.intel.com/en-us/iot/hardware/edison/downloads\">Intel web site</a>.  You will be looking for the &quot;Intel Edison Board Firmware Software&quot; section, and downloading the &quot;Yocto Complete Image&quot;.  At the time of this writing, that was Release 2.1.</p>\n<p>At first run, you probably will not have any files on the Edison when mounted as a USB drive.  I still like to check, and even run the following commands just to be sure there are no hidden files lying around.  We want this area to be nice and tidy for our next step.</p>\n<pre><code>cd /Volumes/Edison\nrm -rf *\nrm -rf \\.*\n</code></pre>\n<p>Now unzip the the Yocto image you downloaded from the Intel web site into a folder.  Copy the contents of that folder into the root directory of the Intel Edison USB drive.  When we reboot the Intel Edison in a moment, it will then look into that space for a new Yocto image, and use those files.</p>\n<p>To reboot the Intel Edison, we first have to connect to the existing Yocto image.  We can do this using the TTY USB cable.  First we need to know how to address the Edison.  Then we will use the &quot;screen&quot; command to log into the Edison.  Then we will tell the Edison to reboot - and we get to watch the screen fill up with information as it goes along.</p>\n<pre><code>ls /dev/tty.*\n&lt;My Edison is DA01G6Q8 - yours may be different&gt;\nscreen /dev/tty.usbserial-DA01G6Q8 115200 -L\n&lt;Enter again&gt;\n&lt;Login: root&gt;\n&lt;There is no password&gt;\nreboot ota\n</code></pre>\n<p>When all the updating is finished, with Release 2.1 you should have a Yocto 1.6.1 image, which should be told to you when you are prompted to log back into the device.  If you see something less, then the process did not complete successfully.  This has randomly happened to me before when not all the image files were copied.  Try the commands again from the top, making sure that everything is copied (or downloaded in the first place).</p>\n<h3 id=\"configuration\">Configuration</h3>\n<p>Before disconnecting anything, and while still logged in via the &quot;screen&quot; command from the previous step, you will want to run the Intel Edison configuration utility.  This utility will put a password on the device, give it a host name, a get the board connected to wi-fi among other things.  Complete all the steps.  It is the &quot;other things&quot; you do not see or get prompted for that are important - such as enabling SSH.</p>\n<pre><code>configure_edison --setup\n</code></pre>\n<p>If your computer and your Intel Edison are on the same network, you can now check to see that your Edison is ready by loading the hostname in a browser.  For example:</p>\n<pre><code>http://kedison.local/\n</code></pre>\n<p>Where I named my Intel Edison &quot;Kedison&quot;.  Some browsers are more finicky than others about hostname access, so I suggest entering the full &quot;http&quot; qualifier and the trailing &quot;/&quot; until you know how your browser will react.  If everything has gone correctly, then you will see a web page with information about your Intel Edison.</p>\n<p><img src=\"http://images.kevinhoyt.com/intel.edison.device.information.jpg\" alt=\"Intel Edison device information page.\" loading=\"lazy\"></p>\n<p>You can now type &quot;exit&quot; into your Intel Edison to logout.  Then to exit the screen program type Ctrl+A and then D.  From here on out we will SSH into our Edison.  If you have already accessed your Edison via SSH, then you will need to edit ~/.ssh/known_hosts and remove the Edison line first.</p>\n<pre><code>&lt;Remove previous if needed&gt;\nnano ~/.ssh/known_hosts\n</code></pre>\n<pre><code>ssh root@kedison.local\n&lt;Password you used during configuration&gt;\n</code></pre>\n<p><mark>You may also want to head back over the the Edison mounted as a USB drive and remove the image files.  You can do this via a GUI or command line as above.  If you are doing this from a GUI, then do not forget to empty your trash to actually remove the files from the USB drive storage space.</mark></p>\n<p>As you may have noticed from the commands above, I like to use Nano as my editor.  If you have that preference as well, you can use the following command to install Nano on your Intel Edison.</p>\n<pre><code>wget http://www.nano-editor.org/dist/v2.2/nano-2.2.6.tar.gz &amp;&amp; tar xvf nano-2.2.6.tar.gz &amp;&amp; cd nano-2.2.6 &amp;&amp; ./configure &amp;&amp; make &amp;&amp; make install\n</code></pre>\n<p>Finally, to communicate with Kaazing Gateway, I will be using the Java client libraries.  I will also be using Java to access the USB serial port of the Intel Edison, and interact with the barcode scanner.  That means I need to have Java installed.  If you prefer something else, Python is already installed, as is Node.JS version 0.10.35.</p>\n<p>To get a JRE (Java Runtime Environment) installed, first head on over to the <a href=\"http://www.oracle.com/technetwork/java/javase/downloads/index.html\">Oracle website</a> and download the appropriate JRE build.  <mark>Note that you are specifically looking for a JRE here, not a JDK (Java Development Kit).</mark>  You are looking for Linux x86 in a gzip format.  I downloaded &quot;jre-8u45-linux-i586.tar.gz&quot;.  Unfortunately, I cannot provide a direct link since a license agreement must first be accepted.</p>\n<p>To copy the compressed JRE from my Mac to the Intel Edison I used SCP.  If you have another tool you prefer, you can use that as well.  This is not the USB drive (as with the Yocto image), but rather moving a file over to a place on the OS.  <mark>The &quot;/home/root&quot; is the root users home directory.</mark>  Then logged back into the Edison, we can use &quot;tar&quot; to unpack the JRE.</p>\n<pre><code>scp ~/Desktop/jre-8u45-linux-i586.gz \n  root@kedison.local:/home/root/jre-8u45.gz\ntar xvf jre-8u45.gz\nln -s jre1.8.0_45/bin/java java\n</code></pre>\n<p>The last command there creates a symbolic link to the Java binary.  It is not necessary, but makes life a little easier when you are running Java commands from various places on the OS.  From here you might remove the compressed JRE file, as well as the compressed Nano file if you have not already.  Again, not necessary, but keeping a clean house make life easier.</p>\n<h3 id=\"barcodescanning\">Barcode Scanning</h3>\n<p>At this point, your Intel Edison is ready to go.  We have the latest image, connected to the Internet, and put some basic tooling in place.  You can now start building the IoT (Internet of Things) project of your dreams.  While not the project of my dreams, I have had a barcode scanner I have been wanting to use in a project for some time.</p>\n<p>The <a href=\"https://www.sparkfun.com/products/9166\">barcode scanner</a> I picked up from SparkFun has a USB cable attached to it.  The scanner also comes with a User's Manual guide which has barcodes all over it.  These barcodes can be scanned using the scanner to configure how it operates.  There are many changes you can make, but I just set it to RS-232, which allows me to access the scanner as a serial port.</p>\n<p><img src=\"http://images.kevinhoyt.com/sparkfun.barcode.scanner.jpg\" alt=\"Barcode scanner - photo courtesy SparkFun\" loading=\"lazy\"></p>\n<p>To configure the scanner, plug it into your computer (not the Intel Edison).  This gives the scanner power.  Then scan the <mark>&quot;PROGRAM&quot;</mark> barcode at the top of any page.  Next scan the <mark>&quot;RS-232C&quot;</mark> barcode.  It may take some practice to get your aim down.  On the page labeled <mark>&quot;RS-232C interface&quot;</mark> scan the <mark>&quot;Baud rate&quot;</mark> setting to <mark>9600</mark>.  Then to finish, scan the barcode labeled <mark>&quot;END&quot;</mark> at the bottom of any page.</p>\n<p><a href=\"http://images.kevinhoyt.com/barcode.scanner.user.manual.pdf\"><img src=\"http://images.kevinhoyt.com/barcode.scanner.user.manual.jpg\" alt=\"Download the user manual\" loading=\"lazy\"></a></p>\n<p>With your scanner configured, remove it from your computer.  Since the OTG port on the Intel Edison is of the micro variety, we need an adapter.  I picked up an <a href=\"https://www.sparkfun.com/products/11604\">USB OTG Cable</a> from SparkFun for this purpose.  Connect the USB-A end of the scanner cable to the matching end of the USB OTG cable.  Then plug the micro end of the USB OTG cable into the OTG port on the Intel Edison.</p>\n<p><img src=\"http://images.kevinhoyt.com/sparkfun.usb.otg.jpg\" alt=\"SparkFun OTG Cable\" loading=\"lazy\"></p>\n<p>Yocto has a number of mappings for serial ports, but the USB OTG will surface as <mark>&quot;/dev/ttyACMO&quot;</mark>.  You can use the <mark>&quot;cat&quot;</mark> command to see if the barcode scanner is attached in RS-232 mode and working correctly.  Pick up a DVD or CD nearby and scan away.  You will see the barcode appear in the Intel Edison console.</p>\n<pre><code>cat /dev/ttyACM0 9600\n</code></pre>\n<p>With my Java-based barcode scanning application bundled up as a JAR file, I copied it over to the Intel Edison using the SCP command from above.  Then using the Java binary I can run the program on the Edison.  This program runs the barcode against Amazon to get a price and image for the item scanned.  That information is then passed to Kaazing Gateway, where any client listening for those messages can pick them up and process them.</p>\n<p><img src=\"http://images.kevinhoyt.com/kaazing.iot.stores.ios.png\" alt=\"Mobile browser shopping cart.\" loading=\"lazy\"></p>\n<p>I built a simple web client using the JavaScript library from the Kaazing Open Source blog.  The web client shows the price and image in a shopping cart on the device.  The web client works on the desktop, but also on mobile browsers.  In a future post, I will detail how Java connects to the scanner, retrieves information from Amazon, and sends messages, as well as how the web client works.</p>\n<h3 id=\"nextsteps\">Next Steps</h3>\n<p>The Intel Edison is a really powerful little computer.  It lends itself to many applications where a SoC is preferred (e.g. running applications on an OS).  This post gives you all the details you need to get started and talks about using a barcode scanner.  There are many other USB possibilities to be had though.  I would encourage you to explore the Edison using your own USB device and language of choice.</p>\n<!--kg-card-end: markdown-->","comment_id":"24","plaintext":"There are a lot of SoC (System on a Chip) devices out there these days. I have\nwritten about the Arduino Yun in the past. Today I would like to introduce you\nto the Intel Edison. At about the quarter of the size of a credit card (35.5mm x\n25mm), this little piece of kit packs a lot of punch in a low power footprint.\n\n\n--------------------------------------------------------------------------------\n\nThis is reposted on the Kaazing Open Source Blog\n[http://kaazing.org/blog/first-steps-with-intel-edison/].\n\nAs the name implies, the Intel Edison includes a dual-core Intel Atom processor,\nas well as both Wi-Fi and Bluetooth LE radios. It has a tiny 70-pin connector,\nwhich is designed to be mated with other Intel Edison \"blocks\". Much in the same\nfashion as you would stack \"shields\" on an Arduino.\n\n\n\nHaving mentioned Arduino, many of the Intel Edison tutorials talk about using a\n\"breakout kit\", which makes it more compatible with Arduino. The first steps\noutlined in this post assume you have the Intel Edison board, and the SparkFun\nBase Block [https://www.sparkfun.com/products/13045]. No Arduino compatibility\nneeded here. We are going to tap directly into the Yocto Linux image on the\nIntel Edison.\n\n\n\nWhat follows is an explanation of the commands I use to configure and use the\nIntel Edison in my projects. The commands have been taught to others as well,\nwhich has led to further refinements.\n\nWhile labeled \"First Steps\" a better title might be \"Build a Real-Time\nInternet-Connected Barcode Scanner.\" That is the first project I set out to\nimplement on the Intel Edison, so the vast majority of these notes come from\nthat discovery process. I will refer to that project throughout, but will not\nspend much time talking about the code proper.\n\n> As with most of my blog posts, you can get the code for an Internet-connected\nbarcode scanner (and much more) from my personal GitHub repository\n[https://github.com/krhoyt/Kaazing/tree/master/iot/stores].\n\n\nGetting to Know You\nThe main parts to know about the Intel Edison at this point are the two USB-mini\nports mounted at the edge. One is labeled \"console\" while the other is labeled\n\"OTG\". We will talk more about what that means in a moment, but as you might\nhave guessed, you will in turn need two USB-mini cables to make the most of this\ntutorial.\n\nThe \"console\" USB port is a TTY port. This port effectively powers the Intel\nEdison. While getting started, this gives you a way to log into Yocto Linux.\nOnce you have your Edison configured however, you will likely just use this port\nfor power, and SSH into the Edison.\n\n> If you want the Edison to run on its own, without a cable for power, SparkFun\nmakes a Battery Block [https://www.sparkfun.com/products/13037] as well.\n\n\nThe other USB port is labeled \"OTG\". This stands for USB \"on the go\" which is a\nformal part of USB. For the purposes of my barcode scanner project, this\neffectively translates into \"A USB port I can plug devices into, and have them\nexposed to the Linux OS.\" There is more to OTG, most notably that it can be a\nhub as well, but that is beyond the scope of first steps.\n\nHouse Cleaning\nPlug your two USB micro cables into to two ports on the Intel Edison. Then plug\nthe cable in the \"console\" port into the USB port on your computer. You should\nsee the Edison turn on a blue LED. Now connect the other cable into another USB\nport on your computer. You should see the Edison mount itself as a drive on your\nsystem.\n\nThe first step here is to get rid of the existing content on the Intel Edison,\nand the latest Yocto image. You can download the latest from the Intel web site\n[https://software.intel.com/en-us/iot/hardware/edison/downloads]. You will be\nlooking for the \"Intel Edison Board Firmware Software\" section, and downloading\nthe \"Yocto Complete Image\". At the time of this writing, that was Release 2.1.\n\nAt first run, you probably will not have any files on the Edison when mounted as\na USB drive. I still like to check, and even run the following commands just to\nbe sure there are no hidden files lying around. We want this area to be nice and\ntidy for our next step.\n\ncd /Volumes/Edison\nrm -rf *\nrm -rf \\.*\n\n\nNow unzip the the Yocto image you downloaded from the Intel web site into a\nfolder. Copy the contents of that folder into the root directory of the Intel\nEdison USB drive. When we reboot the Intel Edison in a moment, it will then look\ninto that space for a new Yocto image, and use those files.\n\nTo reboot the Intel Edison, we first have to connect to the existing Yocto\nimage. We can do this using the TTY USB cable. First we need to know how to\naddress the Edison. Then we will use the \"screen\" command to log into the\nEdison. Then we will tell the Edison to reboot - and we get to watch the screen\nfill up with information as it goes along.\n\nls /dev/tty.*\n<My Edison is DA01G6Q8 - yours may be different>\nscreen /dev/tty.usbserial-DA01G6Q8 115200 -L\n<Enter again>\n<Login: root>\n<There is no password>\nreboot ota\n\n\nWhen all the updating is finished, with Release 2.1 you should have a Yocto\n1.6.1 image, which should be told to you when you are prompted to log back into\nthe device. If you see something less, then the process did not complete\nsuccessfully. This has randomly happened to me before when not all the image\nfiles were copied. Try the commands again from the top, making sure that\neverything is copied (or downloaded in the first place).\n\nConfiguration\nBefore disconnecting anything, and while still logged in via the \"screen\"\ncommand from the previous step, you will want to run the Intel Edison\nconfiguration utility. This utility will put a password on the device, give it a\nhost name, a get the board connected to wi-fi among other things. Complete all\nthe steps. It is the \"other things\" you do not see or get prompted for that are\nimportant - such as enabling SSH.\n\nconfigure_edison --setup\n\n\nIf your computer and your Intel Edison are on the same network, you can now\ncheck to see that your Edison is ready by loading the hostname in a browser. For\nexample:\n\nhttp://kedison.local/\n\n\nWhere I named my Intel Edison \"Kedison\". Some browsers are more finicky than\nothers about hostname access, so I suggest entering the full \"http\" qualifier\nand the trailing \"/\" until you know how your browser will react. If everything\nhas gone correctly, then you will see a web page with information about your\nIntel Edison.\n\n\n\nYou can now type \"exit\" into your Intel Edison to logout. Then to exit the\nscreen program type Ctrl+A and then D. From here on out we will SSH into our\nEdison. If you have already accessed your Edison via SSH, then you will need to\nedit ~/.ssh/known_hosts and remove the Edison line first.\n\n<Remove previous if needed>\nnano ~/.ssh/known_hosts\n\n\nssh root@kedison.local\n<Password you used during configuration>\n\n\nYou may also want to head back over the the Edison mounted as a USB drive and\nremove the image files. You can do this via a GUI or command line as above. If\nyou are doing this from a GUI, then do not forget to empty your trash to\nactually remove the files from the USB drive storage space.\n\nAs you may have noticed from the commands above, I like to use Nano as my\neditor. If you have that preference as well, you can use the following command\nto install Nano on your Intel Edison.\n\nwget http://www.nano-editor.org/dist/v2.2/nano-2.2.6.tar.gz && tar xvf nano-2.2.6.tar.gz && cd nano-2.2.6 && ./configure && make && make install\n\n\nFinally, to communicate with Kaazing Gateway, I will be using the Java client\nlibraries. I will also be using Java to access the USB serial port of the Intel\nEdison, and interact with the barcode scanner. That means I need to have Java\ninstalled. If you prefer something else, Python is already installed, as is\nNode.JS version 0.10.35.\n\nTo get a JRE (Java Runtime Environment) installed, first head on over to the \nOracle website\n[http://www.oracle.com/technetwork/java/javase/downloads/index.html] and\ndownload the appropriate JRE build. Note that you are specifically looking for a\nJRE here, not a JDK (Java Development Kit). You are looking for Linux x86 in a\ngzip format. I downloaded \"jre-8u45-linux-i586.tar.gz\". Unfortunately, I cannot\nprovide a direct link since a license agreement must first be accepted.\n\nTo copy the compressed JRE from my Mac to the Intel Edison I used SCP. If you\nhave another tool you prefer, you can use that as well. This is not the USB\ndrive (as with the Yocto image), but rather moving a file over to a place on the\nOS. The \"/home/root\" is the root users home directory. Then logged back into the\nEdison, we can use \"tar\" to unpack the JRE.\n\nscp ~/Desktop/jre-8u45-linux-i586.gz \n  root@kedison.local:/home/root/jre-8u45.gz\ntar xvf jre-8u45.gz\nln -s jre1.8.0_45/bin/java java\n\n\nThe last command there creates a symbolic link to the Java binary. It is not\nnecessary, but makes life a little easier when you are running Java commands\nfrom various places on the OS. From here you might remove the compressed JRE\nfile, as well as the compressed Nano file if you have not already. Again, not\nnecessary, but keeping a clean house make life easier.\n\nBarcode Scanning\nAt this point, your Intel Edison is ready to go. We have the latest image,\nconnected to the Internet, and put some basic tooling in place. You can now\nstart building the IoT (Internet of Things) project of your dreams. While not\nthe project of my dreams, I have had a barcode scanner I have been wanting to\nuse in a project for some time.\n\nThe barcode scanner [https://www.sparkfun.com/products/9166] I picked up from\nSparkFun has a USB cable attached to it. The scanner also comes with a User's\nManual guide which has barcodes all over it. These barcodes can be scanned using\nthe scanner to configure how it operates. There are many changes you can make,\nbut I just set it to RS-232, which allows me to access the scanner as a serial\nport.\n\n\n\nTo configure the scanner, plug it into your computer (not the Intel Edison).\nThis gives the scanner power. Then scan the \"PROGRAM\" barcode at the top of any\npage. Next scan the \"RS-232C\" barcode. It may take some practice to get your aim\ndown. On the page labeled \"RS-232C interface\" scan the \"Baud rate\" setting to \n9600. Then to finish, scan the barcode labeled \"END\" at the bottom of any page.\n\n [http://images.kevinhoyt.com/barcode.scanner.user.manual.pdf]\n\nWith your scanner configured, remove it from your computer. Since the OTG port\non the Intel Edison is of the micro variety, we need an adapter. I picked up an \nUSB OTG Cable [https://www.sparkfun.com/products/11604] from SparkFun for this\npurpose. Connect the USB-A end of the scanner cable to the matching end of the\nUSB OTG cable. Then plug the micro end of the USB OTG cable into the OTG port on\nthe Intel Edison.\n\n\n\nYocto has a number of mappings for serial ports, but the USB OTG will surface as \n\"/dev/ttyACMO\". You can use the \"cat\" command to see if the barcode scanner is\nattached in RS-232 mode and working correctly. Pick up a DVD or CD nearby and\nscan away. You will see the barcode appear in the Intel Edison console.\n\ncat /dev/ttyACM0 9600\n\n\nWith my Java-based barcode scanning application bundled up as a JAR file, I\ncopied it over to the Intel Edison using the SCP command from above. Then using\nthe Java binary I can run the program on the Edison. This program runs the\nbarcode against Amazon to get a price and image for the item scanned. That\ninformation is then passed to Kaazing Gateway, where any client listening for\nthose messages can pick them up and process them.\n\n\n\nI built a simple web client using the JavaScript library from the Kaazing Open\nSource blog. The web client shows the price and image in a shopping cart on the\ndevice. The web client works on the desktop, but also on mobile browsers. In a\nfuture post, I will detail how Java connects to the scanner, retrieves\ninformation from Amazon, and sends messages, as well as how the web client\nworks.\n\nNext Steps\nThe Intel Edison is a really powerful little computer. It lends itself to many\napplications where a SoC is preferred (e.g. running applications on an OS). This\npost gives you all the details you need to get started and talks about using a\nbarcode scanner. There are many other USB possibilities to be had though. I\nwould encourage you to explore the Edison using your own USB device and language\nof choice.","feature_image":"http://images.kevinhoyt.com/auto.sorting.packages.jpg","featured":0,"status":"published","locale":null,"visibility":"public","author_id":"1","created_at":"2015-05-12T18:18:53.000Z","updated_at":"2015-06-05T16:34:49.000Z","published_at":"2015-05-12T22:26:15.000Z","custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"type":"post","email_recipient_filter":"none"},{"id":"5adb8922351ffe0018a57725","uuid":"511575de-0a7c-42a9-ba30-e04af7350503","title":"Real-Time Barcode Scanner","slug":"real-time-barcode-scanner","mobiledoc":"{\"version\":\"0.3.1\",\"markups\":[],\"atoms\":[],\"cards\":[[\"markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\"*In a previous post I talked about getting started with Intel Edison, and elaborated a little on a barcode scanner project I used with it.  In this post I will discuss the specific architecture and crawl some code.*\\n\\n---\\n\\n==This is reposted on the [Kaazing Open Source Blog](http://kaazing.org/blog/real-time-barcode-scanner/).==\\n\\nThe barcode scanner project is interesting because the [Intel Edison](https://www.sparkfun.com/products/13024) has wi-fi built in.  This means I can run the code to communicate with the scanner, lookup UPC (Universal Product Code) data, and then pass the data along the message bus, entirely from the Edison.  This is different than many other of my examples where something like an Arduino passes the data over USB to a computer, that in turn puts the data on the message bus.\\n\\n###Barcode Scanner\\n\\nThe [barcode scanner](https://www.sparkfun.com/products/9166) I picked up from SparkFun has an RS-232 serial communication feature.  Keyboard wedge is the other common means of using a barcode scanner.  The scanner comes with a cable that is a male RJ45 on one side and USB-A on the other.  The Intel Edison [Base Block](https://www.sparkfun.com/products/13045) from SparkFun has a USB Micro port.  To mate the two together, I used an [USB OTG](https://www.sparkfun.com/products/11604) cable with a female USB-A port on one side, and USB Micro on the other.\\n\\n![Scanner to OTG to Intel Edison, then Edison to USB.](http://images.kevinhoyt.com/real.time.barcode.wiring.jpg)\\n\\nYou will need to set the scanner into RS-232 mode by using the provided User's Manual.  The process starts by scanning the ==\\\"PROGRAM\\\"== barcode at the top of any page.  Then scan the ==\\\"RS-232C\\\"== option on the ==\\\"Interface Selection\\\"== section.  The default baud is 9600 with 8 stop bits, no parity, and one data bit (8-N-1).\\n\\nWhen connected to the Intel Edison, the scanner will be attached to the serial port at ==/dev/ttyACM0== by the Yocto Linux OS.  There is no easy way to discover this.  Yocto shows a long list of TTY endpoints on the file system.  I was able to get it by scouring the Intel documentation, and then trial and error against a handful of endpoints using the ==\\\"cat\\\"== command.\\n\\n```\\ncat /dev/ttyACM0 9600\\n<Scan something>\\n```\\n\\n###Java\\n\\nWhile the Intel Edison runs Node.js and Python straight out of the box, I chose Java for this project.  The open source Kaazing Gateway  includes Java client libraries.  I am also already familiar with serial communication in Java.  Java then made an ideal selection for this project.\\n\\n> I had reservations as to if the Edison would run Java.  In the end, the Edison handled this type of Java load with ease.\\n\\n###Serial Communication\\n\\n[jSSC](https://github.com/scream3r/java-simple-serial-connector) (Java Simple Serial Connector) is a library for communicating from Java to serial ports via JNI (Java Native Interface).  The native code for communicating with serial ports is included in the JAR (Java Archive).  This makes deployment and configuration a snap.  So long as the jSSC JAR is in your project, or on the Java path, native serial access is yours!\\n\\n> There is a Java specification for native serial port access, but it has not been implemented in core Java yet.  The jSSC library is also used by Arduino, making the library reliable and robust.\\n\\nSerial port access consists of two parts.  The first part is to open the port itself.  With the port open, you then need to add an event listener for when data arrives (the barcode in this case).  jSSC requires that a specific interface be implemented in order to handle events.  The respective code blocks look something like the following.\\n\\n```\\nserial = new SerialPort( port );\\n\\ntry {\\n  // Open serial port\\n  // Listen for data\\n  serial.openPort();\\n  serial.setParams( \\n    SerialPort.BAUDRATE_9600,\\n    SerialPort.DATABITS_8,\\n    SerialPort.STOPBITS_1,\\n    SerialPort.PARITY_NONE\\n  );\\n  serial.addEventListener( this );\\t\\t\\t\\n} catch( SerialPortException spe ) {\\n  spe.printStackTrace();\\n}\\t\\n```\\n\\n```\\n// Incoming serial data\\n@Override\\npublic void serialEvent( SerialPortEvent event ) {\\n  byte[] buffer = null;        \\t\\n    \\t\\n  // Receiving\\n  if( event.isRXCHAR() ) {\\n    try {\\n      // Latest bytes\\n      buffer = serial.readBytes( event.getEventValue() );\\n                \\n      for( byte b:buffer ) {\\n        // Look for record end\\n        if( b == SERIAL_END ) {\\n          amazon.search( builder.toString().trim() );                    \\t\\n          builder.setLength( 0 );\\n        } else {\\n          // Keep adding until complete record\\n          builder.append( ( char )b );\\n        }\\n      }                        \\n    } catch( SerialPortException spe ) {\\n      spe.printStackTrace();\\n    }\\n  }\\n}\\n```\\n\\nProcessing the incoming data really is just a matter a reading the bytes and appending those bits as character data (in this case) to a StringBuilder object.  I look for a carriage return at the end of the barcode line, and then head off to Amazon to process the UPC.\\n\\n###Amazon\\n\\nUnless you have been living under a rock for the past decade, chances are that you know Amazon provides a massive variety of developer services.  One of those services is the ability to lookup products by a number of criteria - including UPC.  This service used to be part of AWS (Amazon Web Services).  It has since been moved under the Amazon Affiliate Program.\\n\\n> My usage of the service in this manner is not technically covered by the EULA.  The service is designed to be used in conjunction with selling Amazon products on affiliate web sites, and is licensed as such.\\n\\nSince this type of usage of the service is not part of the expected usage, there was a little digging around the documentation needed to figure out how to lookup information.  Oddly enough, I found a snippet of code doing exactly that, in Java, from an Amazon employee, in the support forum.  I encapsulated this code in [my own wrapper](https://github.com/krhoyt/Kaazing/blob/master/iot/stores/java/src/Amazon.java) for this project.\\n\\n###JSON\\n\\nAt this point we have gotten a UPC from the scanner, and looked up the product information from Amazon.  Once this data is placed on the message bus, it will be headed to a web client for display.  Since the web client can readily leverage JSON (JavaScript Object Notation), we will want to massage the data in Java first.\\n\\n```\\n// Process serial port data\\n// Send to gateway\\n// Send as JSON\\nprivate void process( AmazonResult scan ) {\\n  JsonObject\\t\\t\\tresult;\\n  JsonObjectBuilder\\tbuilder;\\n  StringWriter\\t\\tsw;\\n\\t\\t\\t\\t\\n  // Build JSON structure\\n  builder = Json.createObjectBuilder();\\n  builder.add( KEY_ACTION, ACTION_SHOW );\\n  builder.add( KEY_UPC, scan.getUpc() );\\n  builder.add( KEY_TITLE, scan.getTitle() );\\n  builder.add( KEY_IMAGE, scan.getImage() );\\n  builder.add( KEY_PRICE, scan.getPrice() );\\t\\t\\n\\t\\t\\n  // Encode\\n  result = builder.build();\\n\\t\\t\\n  // Stringify\\n  sw = new StringWriter();\\n\\t\\t\\n  try( JsonWriter writer = Json.createWriter( sw ) ) {\\n    writer.writeObject( result );\\n  }\\n\\t\\t\\n  // Publish message\\n  // May not be connected yet\\n  if( gateway.isConnected() ) {\\n    gateway.publish( TOPIC, sw.toString() );\\n  }\\n}\\t\\n```\\n\\n###Messaging Bus\\n\\nThis project used the **open source** Kaazing Gateway running for **free** in the cloud - a little feature we call the \\\"Sandbox\\\".  You can use it to test our your own ideas.  It uses the [Qpid](https://qpid.apache.org/) message broker.  I wrote a wrapper for [accessing Sandbox in JavaScript](https://github.com/krhoyt/Kaazing/blob/master/iot/stores/web/gateway.js) and wrote about it in a previous post.  For this example, I ported the [wrapper over to Java](https://github.com/krhoyt/Kaazing/blob/master/iot/stores/java/src/Gateway.java).  The relevant code to use the wrapper looks like the following.\\n\\n```\\n// Instantiate\\ngateway = new Gateway();\\n\\n// Debugging\\ngateway.setVerbose( false );\\n\\n// Event handlers\\ngateway.callback = new GatewayListener() {\\n\\n  ...\\t\\t\\n\\n  @Override\\n  public void onMessage( String body ) {\\t\\n    Event\\t\\te = null;\\n    InputStream\\tstream = null;\\n    JsonParser\\tparser = null;\\n\\t\\t\\n    // String to InputStream\\n    stream = new ByteArrayInputStream( \\n      body.getBytes( StandardCharsets.UTF_8 ) \\n    );\\n    parser = Json.createParser( stream );\\t\\t\\t\\t\\n\\t\\t\\n    // Iterate through map keys\\n    while( parser.hasNext() ) {\\n      e = parser.next();\\n\\t\\t\\t\\n      if( e == Event.KEY_NAME ) {\\n        switch( parser.getString() ) {\\n          case KEY_ACTION:\\n            parser.next();\\n            break;\\n        }\\n      }\\n    }\\n  }\\n\\n  ...\\n\\n};\\n\\n// Connect to gateway\\ngateway.connect( KAAZING_ID );\\t\\t\\n\\n// Publish message\\nif( gateway.isConnected() ) {\\n  gateway.publish( TOPIC, sw.toString() );\\n}\\n```\\n\\nAll of the action so far happens on the Intel Edison - and it happens pretty quickly.  The code is packaged up into a JAR file and dropped on the Edison via SCP.  It is then run using a modern JRE for Linux.  We can now merrily scan barcodes all day long and send the details out over the message bus.  \\n\\nWhat is listening for those messages is up to you.  That is part of the beauty of the decoupling that using a message bus brings to the table.  In this case, I wanted to display the messages on a web client that would act as my shopping cart.\\n\\n###Web Client\\n\\nThe web client is probably the simplest part of the entire architecture.  Getting started with Intel Edison can take some time.  Learning serial port access also takes some practice (especially going into a device).  JSON is pretty common, but you might be new to messaging.  \\n\\nBy comparison, the web client waits for a message, in JSON format, and adds an element to the DOM.  Handling the messaging in JavaScript uses the aforementioned JavaScript wrapper, making it easy to use and get started.  Here is what handling the message in JavaScript looks like.\\n\\n```\\n// Connect to Gateway\\nkaazing = Gateway.connect( KAAZING_ID, doGatewayConnect ); \\n\\n// Called when connected to Gateway\\n// Subscribe to topic\\nfunction doGatewayConnect()\\n{\\n  console.log( 'Client connected.' );\\n  \\n  // Subscribe\\n  kaazing.on( Gateway.EVENT_MESSAGE, doGatewayMessage );\\n  kaazing.subscribe( TOPIC );    \\n}\\n\\n// Called when message arrives\\nfunction doGatewayMessage( message )\\n{\\n  var data = null;\\n  \\n  // Parse JSON\\n  data = JSON.parse( message );\\n  \\n  // Decision tree for incoming action\\n  // Display actual scanner values\\n  if( data.action == ACTION_SHOW )\\n  {\\n    // Add to cart\\n    cart.push( data );\\n    \\n    // Update user interface\\n    line();\\n    total();\\n    \\n    // Debug\\n    console.log( data.upc );\\n    console.log( data.title );\\n    console.log( data.price );    \\n    console.log( data.image );\\n  } else if( data.action == ACTION_REMOVE ) {\\n    remove( data.upc );  \\n  }\\n}\\n```\\n\\n==That is it!==  Now you have yourself a real-time [IoT] barcode scanner.\\n\\n<iframe width=\\\"640\\\" height=\\\"390\\\" src=\\\"https://www.youtube.com/embed/l31fECTlus0\\\" frameborder=\\\"0\\\" allowfullscreen></iframe>\\n\\n###Next Steps\\n\\nFrom here I supposed you might wire the web client back into the Amazon Affiliate program to make it a legitimate use of the UPC lookup service per the license.\\n\\nI originally envisioned the results of this project in two parts.\\n\\nThe first and most obvious part is modernization of PoS (Point of Sale) systems.  The days of needing large scale PoS deployments (usually through the likes of IBM) are quickly coming to a close.  There is also a cost savings as the security of Kaazing Gateway would allow retailers to use the open Internet instead of costly dedicated private lines.\\n\\nThe second less obvious result is a real-time supply chain.  At this point the retailers internal systems can track items being sold in real-time.  They can then in turn load up trucks accordingly.  Or perhaps they offer specials for high demand items in real-time with digital signage.  Even further might be rolling this into a frequency program to better understand consumer purchasing behaviors.\\n\\nI have already started building a Java Android application that scans UPC barcodes and pushes messages into this same system.  A unified PoS where the shoppers are the cashiers might make for an interesting application as well.  Where will you go with it?  Let me  know on [Twitter](http://twitter.com/krhoyt).\\n\"}]],\"sections\":[[10,0]],\"ghostVersion\":\"3.0\"}","html":"<!--kg-card-begin: markdown--><p><em>In a previous post I talked about getting started with Intel Edison, and elaborated a little on a barcode scanner project I used with it.  In this post I will discuss the specific architecture and crawl some code.</em></p>\n<hr>\n<p><mark>This is reposted on the <a href=\"http://kaazing.org/blog/real-time-barcode-scanner/\">Kaazing Open Source Blog</a>.</mark></p>\n<p>The barcode scanner project is interesting because the <a href=\"https://www.sparkfun.com/products/13024\">Intel Edison</a> has wi-fi built in.  This means I can run the code to communicate with the scanner, lookup UPC (Universal Product Code) data, and then pass the data along the message bus, entirely from the Edison.  This is different than many other of my examples where something like an Arduino passes the data over USB to a computer, that in turn puts the data on the message bus.</p>\n<h3 id=\"barcodescanner\">Barcode Scanner</h3>\n<p>The <a href=\"https://www.sparkfun.com/products/9166\">barcode scanner</a> I picked up from SparkFun has an RS-232 serial communication feature.  Keyboard wedge is the other common means of using a barcode scanner.  The scanner comes with a cable that is a male RJ45 on one side and USB-A on the other.  The Intel Edison <a href=\"https://www.sparkfun.com/products/13045\">Base Block</a> from SparkFun has a USB Micro port.  To mate the two together, I used an <a href=\"https://www.sparkfun.com/products/11604\">USB OTG</a> cable with a female USB-A port on one side, and USB Micro on the other.</p>\n<p><img src=\"http://images.kevinhoyt.com/real.time.barcode.wiring.jpg\" alt=\"Scanner to OTG to Intel Edison, then Edison to USB.\" loading=\"lazy\"></p>\n<p>You will need to set the scanner into RS-232 mode by using the provided User's Manual.  The process starts by scanning the <mark>&quot;PROGRAM&quot;</mark> barcode at the top of any page.  Then scan the <mark>&quot;RS-232C&quot;</mark> option on the <mark>&quot;Interface Selection&quot;</mark> section.  The default baud is 9600 with 8 stop bits, no parity, and one data bit (8-N-1).</p>\n<p>When connected to the Intel Edison, the scanner will be attached to the serial port at <mark>/dev/ttyACM0</mark> by the Yocto Linux OS.  There is no easy way to discover this.  Yocto shows a long list of TTY endpoints on the file system.  I was able to get it by scouring the Intel documentation, and then trial and error against a handful of endpoints using the <mark>&quot;cat&quot;</mark> command.</p>\n<pre><code>cat /dev/ttyACM0 9600\n&lt;Scan something&gt;\n</code></pre>\n<h3 id=\"java\">Java</h3>\n<p>While the Intel Edison runs Node.js and Python straight out of the box, I chose Java for this project.  The open source Kaazing Gateway  includes Java client libraries.  I am also already familiar with serial communication in Java.  Java then made an ideal selection for this project.</p>\n<blockquote>\n<p>I had reservations as to if the Edison would run Java.  In the end, the Edison handled this type of Java load with ease.</p>\n</blockquote>\n<h3 id=\"serialcommunication\">Serial Communication</h3>\n<p><a href=\"https://github.com/scream3r/java-simple-serial-connector\">jSSC</a> (Java Simple Serial Connector) is a library for communicating from Java to serial ports via JNI (Java Native Interface).  The native code for communicating with serial ports is included in the JAR (Java Archive).  This makes deployment and configuration a snap.  So long as the jSSC JAR is in your project, or on the Java path, native serial access is yours!</p>\n<blockquote>\n<p>There is a Java specification for native serial port access, but it has not been implemented in core Java yet.  The jSSC library is also used by Arduino, making the library reliable and robust.</p>\n</blockquote>\n<p>Serial port access consists of two parts.  The first part is to open the port itself.  With the port open, you then need to add an event listener for when data arrives (the barcode in this case).  jSSC requires that a specific interface be implemented in order to handle events.  The respective code blocks look something like the following.</p>\n<pre><code>serial = new SerialPort( port );\n\ntry {\n  // Open serial port\n  // Listen for data\n  serial.openPort();\n  serial.setParams( \n    SerialPort.BAUDRATE_9600,\n    SerialPort.DATABITS_8,\n    SerialPort.STOPBITS_1,\n    SerialPort.PARITY_NONE\n  );\n  serial.addEventListener( this );\t\t\t\n} catch( SerialPortException spe ) {\n  spe.printStackTrace();\n}\t\n</code></pre>\n<pre><code>// Incoming serial data\n@Override\npublic void serialEvent( SerialPortEvent event ) {\n  byte[] buffer = null;        \t\n    \t\n  // Receiving\n  if( event.isRXCHAR() ) {\n    try {\n      // Latest bytes\n      buffer = serial.readBytes( event.getEventValue() );\n                \n      for( byte b:buffer ) {\n        // Look for record end\n        if( b == SERIAL_END ) {\n          amazon.search( builder.toString().trim() );                    \t\n          builder.setLength( 0 );\n        } else {\n          // Keep adding until complete record\n          builder.append( ( char )b );\n        }\n      }                        \n    } catch( SerialPortException spe ) {\n      spe.printStackTrace();\n    }\n  }\n}\n</code></pre>\n<p>Processing the incoming data really is just a matter a reading the bytes and appending those bits as character data (in this case) to a StringBuilder object.  I look for a carriage return at the end of the barcode line, and then head off to Amazon to process the UPC.</p>\n<h3 id=\"amazon\">Amazon</h3>\n<p>Unless you have been living under a rock for the past decade, chances are that you know Amazon provides a massive variety of developer services.  One of those services is the ability to lookup products by a number of criteria - including UPC.  This service used to be part of AWS (Amazon Web Services).  It has since been moved under the Amazon Affiliate Program.</p>\n<blockquote>\n<p>My usage of the service in this manner is not technically covered by the EULA.  The service is designed to be used in conjunction with selling Amazon products on affiliate web sites, and is licensed as such.</p>\n</blockquote>\n<p>Since this type of usage of the service is not part of the expected usage, there was a little digging around the documentation needed to figure out how to lookup information.  Oddly enough, I found a snippet of code doing exactly that, in Java, from an Amazon employee, in the support forum.  I encapsulated this code in <a href=\"https://github.com/krhoyt/Kaazing/blob/master/iot/stores/java/src/Amazon.java\">my own wrapper</a> for this project.</p>\n<h3 id=\"json\">JSON</h3>\n<p>At this point we have gotten a UPC from the scanner, and looked up the product information from Amazon.  Once this data is placed on the message bus, it will be headed to a web client for display.  Since the web client can readily leverage JSON (JavaScript Object Notation), we will want to massage the data in Java first.</p>\n<pre><code>// Process serial port data\n// Send to gateway\n// Send as JSON\nprivate void process( AmazonResult scan ) {\n  JsonObject\t\t\tresult;\n  JsonObjectBuilder\tbuilder;\n  StringWriter\t\tsw;\n\t\t\t\t\n  // Build JSON structure\n  builder = Json.createObjectBuilder();\n  builder.add( KEY_ACTION, ACTION_SHOW );\n  builder.add( KEY_UPC, scan.getUpc() );\n  builder.add( KEY_TITLE, scan.getTitle() );\n  builder.add( KEY_IMAGE, scan.getImage() );\n  builder.add( KEY_PRICE, scan.getPrice() );\t\t\n\t\t\n  // Encode\n  result = builder.build();\n\t\t\n  // Stringify\n  sw = new StringWriter();\n\t\t\n  try( JsonWriter writer = Json.createWriter( sw ) ) {\n    writer.writeObject( result );\n  }\n\t\t\n  // Publish message\n  // May not be connected yet\n  if( gateway.isConnected() ) {\n    gateway.publish( TOPIC, sw.toString() );\n  }\n}\t\n</code></pre>\n<h3 id=\"messagingbus\">Messaging Bus</h3>\n<p>This project used the <strong>open source</strong> Kaazing Gateway running for <strong>free</strong> in the cloud - a little feature we call the &quot;Sandbox&quot;.  You can use it to test our your own ideas.  It uses the <a href=\"https://qpid.apache.org/\">Qpid</a> message broker.  I wrote a wrapper for <a href=\"https://github.com/krhoyt/Kaazing/blob/master/iot/stores/web/gateway.js\">accessing Sandbox in JavaScript</a> and wrote about it in a previous post.  For this example, I ported the <a href=\"https://github.com/krhoyt/Kaazing/blob/master/iot/stores/java/src/Gateway.java\">wrapper over to Java</a>.  The relevant code to use the wrapper looks like the following.</p>\n<pre><code>// Instantiate\ngateway = new Gateway();\n\n// Debugging\ngateway.setVerbose( false );\n\n// Event handlers\ngateway.callback = new GatewayListener() {\n\n  ...\t\t\n\n  @Override\n  public void onMessage( String body ) {\t\n    Event\t\te = null;\n    InputStream\tstream = null;\n    JsonParser\tparser = null;\n\t\t\n    // String to InputStream\n    stream = new ByteArrayInputStream( \n      body.getBytes( StandardCharsets.UTF_8 ) \n    );\n    parser = Json.createParser( stream );\t\t\t\t\n\t\t\n    // Iterate through map keys\n    while( parser.hasNext() ) {\n      e = parser.next();\n\t\t\t\n      if( e == Event.KEY_NAME ) {\n        switch( parser.getString() ) {\n          case KEY_ACTION:\n            parser.next();\n            break;\n        }\n      }\n    }\n  }\n\n  ...\n\n};\n\n// Connect to gateway\ngateway.connect( KAAZING_ID );\t\t\n\n// Publish message\nif( gateway.isConnected() ) {\n  gateway.publish( TOPIC, sw.toString() );\n}\n</code></pre>\n<p>All of the action so far happens on the Intel Edison - and it happens pretty quickly.  The code is packaged up into a JAR file and dropped on the Edison via SCP.  It is then run using a modern JRE for Linux.  We can now merrily scan barcodes all day long and send the details out over the message bus.</p>\n<p>What is listening for those messages is up to you.  That is part of the beauty of the decoupling that using a message bus brings to the table.  In this case, I wanted to display the messages on a web client that would act as my shopping cart.</p>\n<h3 id=\"webclient\">Web Client</h3>\n<p>The web client is probably the simplest part of the entire architecture.  Getting started with Intel Edison can take some time.  Learning serial port access also takes some practice (especially going into a device).  JSON is pretty common, but you might be new to messaging.</p>\n<p>By comparison, the web client waits for a message, in JSON format, and adds an element to the DOM.  Handling the messaging in JavaScript uses the aforementioned JavaScript wrapper, making it easy to use and get started.  Here is what handling the message in JavaScript looks like.</p>\n<pre><code>// Connect to Gateway\nkaazing = Gateway.connect( KAAZING_ID, doGatewayConnect ); \n\n// Called when connected to Gateway\n// Subscribe to topic\nfunction doGatewayConnect()\n{\n  console.log( 'Client connected.' );\n  \n  // Subscribe\n  kaazing.on( Gateway.EVENT_MESSAGE, doGatewayMessage );\n  kaazing.subscribe( TOPIC );    \n}\n\n// Called when message arrives\nfunction doGatewayMessage( message )\n{\n  var data = null;\n  \n  // Parse JSON\n  data = JSON.parse( message );\n  \n  // Decision tree for incoming action\n  // Display actual scanner values\n  if( data.action == ACTION_SHOW )\n  {\n    // Add to cart\n    cart.push( data );\n    \n    // Update user interface\n    line();\n    total();\n    \n    // Debug\n    console.log( data.upc );\n    console.log( data.title );\n    console.log( data.price );    \n    console.log( data.image );\n  } else if( data.action == ACTION_REMOVE ) {\n    remove( data.upc );  \n  }\n}\n</code></pre>\n<p><mark>That is it!</mark>  Now you have yourself a real-time [IoT] barcode scanner.</p>\n<iframe width=\"640\" height=\"390\" src=\"https://www.youtube.com/embed/l31fECTlus0\" frameborder=\"0\" allowfullscreen></iframe>\n<h3 id=\"nextsteps\">Next Steps</h3>\n<p>From here I supposed you might wire the web client back into the Amazon Affiliate program to make it a legitimate use of the UPC lookup service per the license.</p>\n<p>I originally envisioned the results of this project in two parts.</p>\n<p>The first and most obvious part is modernization of PoS (Point of Sale) systems.  The days of needing large scale PoS deployments (usually through the likes of IBM) are quickly coming to a close.  There is also a cost savings as the security of Kaazing Gateway would allow retailers to use the open Internet instead of costly dedicated private lines.</p>\n<p>The second less obvious result is a real-time supply chain.  At this point the retailers internal systems can track items being sold in real-time.  They can then in turn load up trucks accordingly.  Or perhaps they offer specials for high demand items in real-time with digital signage.  Even further might be rolling this into a frequency program to better understand consumer purchasing behaviors.</p>\n<p>I have already started building a Java Android application that scans UPC barcodes and pushes messages into this same system.  A unified PoS where the shoppers are the cashiers might make for an interesting application as well.  Where will you go with it?  Let me  know on <a href=\"http://twitter.com/krhoyt\">Twitter</a>.</p>\n<!--kg-card-end: markdown-->","comment_id":"25","plaintext":"In a previous post I talked about getting started with Intel Edison, and\nelaborated a little on a barcode scanner project I used with it. In this post I\nwill discuss the specific architecture and crawl some code.\n\n\n--------------------------------------------------------------------------------\n\nThis is reposted on the Kaazing Open Source Blog\n[http://kaazing.org/blog/real-time-barcode-scanner/].\n\nThe barcode scanner project is interesting because the Intel Edison\n[https://www.sparkfun.com/products/13024] has wi-fi built in. This means I can\nrun the code to communicate with the scanner, lookup UPC (Universal Product\nCode) data, and then pass the data along the message bus, entirely from the\nEdison. This is different than many other of my examples where something like an\nArduino passes the data over USB to a computer, that in turn puts the data on\nthe message bus.\n\nBarcode Scanner\nThe barcode scanner [https://www.sparkfun.com/products/9166] I picked up from\nSparkFun has an RS-232 serial communication feature. Keyboard wedge is the other\ncommon means of using a barcode scanner. The scanner comes with a cable that is\na male RJ45 on one side and USB-A on the other. The Intel Edison Base Block\n[https://www.sparkfun.com/products/13045] from SparkFun has a USB Micro port. To\nmate the two together, I used an USB OTG\n[https://www.sparkfun.com/products/11604] cable with a female USB-A port on one\nside, and USB Micro on the other.\n\n\n\nYou will need to set the scanner into RS-232 mode by using the provided User's\nManual. The process starts by scanning the \"PROGRAM\" barcode at the top of any\npage. Then scan the \"RS-232C\" option on the \"Interface Selection\" section. The\ndefault baud is 9600 with 8 stop bits, no parity, and one data bit (8-N-1).\n\nWhen connected to the Intel Edison, the scanner will be attached to the serial\nport at /dev/ttyACM0 by the Yocto Linux OS. There is no easy way to discover\nthis. Yocto shows a long list of TTY endpoints on the file system. I was able to\nget it by scouring the Intel documentation, and then trial and error against a\nhandful of endpoints using the \"cat\" command.\n\ncat /dev/ttyACM0 9600\n<Scan something>\n\n\nJava\nWhile the Intel Edison runs Node.js and Python straight out of the box, I chose\nJava for this project. The open source Kaazing Gateway includes Java client\nlibraries. I am also already familiar with serial communication in Java. Java\nthen made an ideal selection for this project.\n\n> I had reservations as to if the Edison would run Java. In the end, the Edison\nhandled this type of Java load with ease.\n\n\nSerial Communication\njSSC [https://github.com/scream3r/java-simple-serial-connector] (Java Simple\nSerial Connector) is a library for communicating from Java to serial ports via\nJNI (Java Native Interface). The native code for communicating with serial ports\nis included in the JAR (Java Archive). This makes deployment and configuration a\nsnap. So long as the jSSC JAR is in your project, or on the Java path, native\nserial access is yours!\n\n> There is a Java specification for native serial port access, but it has not been\nimplemented in core Java yet. The jSSC library is also used by Arduino, making\nthe library reliable and robust.\n\n\nSerial port access consists of two parts. The first part is to open the port\nitself. With the port open, you then need to add an event listener for when data\narrives (the barcode in this case). jSSC requires that a specific interface be\nimplemented in order to handle events. The respective code blocks look something\nlike the following.\n\nserial = new SerialPort( port );\n\ntry {\n  // Open serial port\n  // Listen for data\n  serial.openPort();\n  serial.setParams( \n    SerialPort.BAUDRATE_9600,\n    SerialPort.DATABITS_8,\n    SerialPort.STOPBITS_1,\n    SerialPort.PARITY_NONE\n  );\n  serial.addEventListener( this );\t\t\t\n} catch( SerialPortException spe ) {\n  spe.printStackTrace();\n}\t\n\n\n// Incoming serial data\n@Override\npublic void serialEvent( SerialPortEvent event ) {\n  byte[] buffer = null;        \t\n    \t\n  // Receiving\n  if( event.isRXCHAR() ) {\n    try {\n      // Latest bytes\n      buffer = serial.readBytes( event.getEventValue() );\n                \n      for( byte b:buffer ) {\n        // Look for record end\n        if( b == SERIAL_END ) {\n          amazon.search( builder.toString().trim() );                    \t\n          builder.setLength( 0 );\n        } else {\n          // Keep adding until complete record\n          builder.append( ( char )b );\n        }\n      }                        \n    } catch( SerialPortException spe ) {\n      spe.printStackTrace();\n    }\n  }\n}\n\n\nProcessing the incoming data really is just a matter a reading the bytes and\nappending those bits as character data (in this case) to a StringBuilder object.\nI look for a carriage return at the end of the barcode line, and then head off\nto Amazon to process the UPC.\n\nAmazon\nUnless you have been living under a rock for the past decade, chances are that\nyou know Amazon provides a massive variety of developer services. One of those\nservices is the ability to lookup products by a number of criteria - including\nUPC. This service used to be part of AWS (Amazon Web Services). It has since\nbeen moved under the Amazon Affiliate Program.\n\n> My usage of the service in this manner is not technically covered by the EULA.\nThe service is designed to be used in conjunction with selling Amazon products\non affiliate web sites, and is licensed as such.\n\n\nSince this type of usage of the service is not part of the expected usage, there\nwas a little digging around the documentation needed to figure out how to lookup\ninformation. Oddly enough, I found a snippet of code doing exactly that, in\nJava, from an Amazon employee, in the support forum. I encapsulated this code in \nmy own wrapper\n[https://github.com/krhoyt/Kaazing/blob/master/iot/stores/java/src/Amazon.java] \nfor this project.\n\nJSON\nAt this point we have gotten a UPC from the scanner, and looked up the product\ninformation from Amazon. Once this data is placed on the message bus, it will be\nheaded to a web client for display. Since the web client can readily leverage\nJSON (JavaScript Object Notation), we will want to massage the data in Java\nfirst.\n\n// Process serial port data\n// Send to gateway\n// Send as JSON\nprivate void process( AmazonResult scan ) {\n  JsonObject\t\t\tresult;\n  JsonObjectBuilder\tbuilder;\n  StringWriter\t\tsw;\n\t\t\t\t\n  // Build JSON structure\n  builder = Json.createObjectBuilder();\n  builder.add( KEY_ACTION, ACTION_SHOW );\n  builder.add( KEY_UPC, scan.getUpc() );\n  builder.add( KEY_TITLE, scan.getTitle() );\n  builder.add( KEY_IMAGE, scan.getImage() );\n  builder.add( KEY_PRICE, scan.getPrice() );\t\t\n\t\t\n  // Encode\n  result = builder.build();\n\t\t\n  // Stringify\n  sw = new StringWriter();\n\t\t\n  try( JsonWriter writer = Json.createWriter( sw ) ) {\n    writer.writeObject( result );\n  }\n\t\t\n  // Publish message\n  // May not be connected yet\n  if( gateway.isConnected() ) {\n    gateway.publish( TOPIC, sw.toString() );\n  }\n}\t\n\n\nMessaging Bus\nThis project used the open source Kaazing Gateway running for free in the cloud\n- a little feature we call the \"Sandbox\". You can use it to test our your own\nideas. It uses the Qpid [https://qpid.apache.org/] message broker. I wrote a\nwrapper for accessing Sandbox in JavaScript\n[https://github.com/krhoyt/Kaazing/blob/master/iot/stores/web/gateway.js] and\nwrote about it in a previous post. For this example, I ported the wrapper over\nto Java\n[https://github.com/krhoyt/Kaazing/blob/master/iot/stores/java/src/Gateway.java]\n. The relevant code to use the wrapper looks like the following.\n\n// Instantiate\ngateway = new Gateway();\n\n// Debugging\ngateway.setVerbose( false );\n\n// Event handlers\ngateway.callback = new GatewayListener() {\n\n  ...\t\t\n\n  @Override\n  public void onMessage( String body ) {\t\n    Event\t\te = null;\n    InputStream\tstream = null;\n    JsonParser\tparser = null;\n\t\t\n    // String to InputStream\n    stream = new ByteArrayInputStream( \n      body.getBytes( StandardCharsets.UTF_8 ) \n    );\n    parser = Json.createParser( stream );\t\t\t\t\n\t\t\n    // Iterate through map keys\n    while( parser.hasNext() ) {\n      e = parser.next();\n\t\t\t\n      if( e == Event.KEY_NAME ) {\n        switch( parser.getString() ) {\n          case KEY_ACTION:\n            parser.next();\n            break;\n        }\n      }\n    }\n  }\n\n  ...\n\n};\n\n// Connect to gateway\ngateway.connect( KAAZING_ID );\t\t\n\n// Publish message\nif( gateway.isConnected() ) {\n  gateway.publish( TOPIC, sw.toString() );\n}\n\n\nAll of the action so far happens on the Intel Edison - and it happens pretty\nquickly. The code is packaged up into a JAR file and dropped on the Edison via\nSCP. It is then run using a modern JRE for Linux. We can now merrily scan\nbarcodes all day long and send the details out over the message bus.\n\nWhat is listening for those messages is up to you. That is part of the beauty of\nthe decoupling that using a message bus brings to the table. In this case, I\nwanted to display the messages on a web client that would act as my shopping\ncart.\n\nWeb Client\nThe web client is probably the simplest part of the entire architecture. Getting\nstarted with Intel Edison can take some time. Learning serial port access also\ntakes some practice (especially going into a device). JSON is pretty common, but\nyou might be new to messaging.\n\nBy comparison, the web client waits for a message, in JSON format, and adds an\nelement to the DOM. Handling the messaging in JavaScript uses the aforementioned\nJavaScript wrapper, making it easy to use and get started. Here is what handling\nthe message in JavaScript looks like.\n\n// Connect to Gateway\nkaazing = Gateway.connect( KAAZING_ID, doGatewayConnect ); \n\n// Called when connected to Gateway\n// Subscribe to topic\nfunction doGatewayConnect()\n{\n  console.log( 'Client connected.' );\n  \n  // Subscribe\n  kaazing.on( Gateway.EVENT_MESSAGE, doGatewayMessage );\n  kaazing.subscribe( TOPIC );    \n}\n\n// Called when message arrives\nfunction doGatewayMessage( message )\n{\n  var data = null;\n  \n  // Parse JSON\n  data = JSON.parse( message );\n  \n  // Decision tree for incoming action\n  // Display actual scanner values\n  if( data.action == ACTION_SHOW )\n  {\n    // Add to cart\n    cart.push( data );\n    \n    // Update user interface\n    line();\n    total();\n    \n    // Debug\n    console.log( data.upc );\n    console.log( data.title );\n    console.log( data.price );    \n    console.log( data.image );\n  } else if( data.action == ACTION_REMOVE ) {\n    remove( data.upc );  \n  }\n}\n\n\nThat is it! Now you have yourself a real-time [IoT] barcode scanner.\n\nNext Steps\nFrom here I supposed you might wire the web client back into the Amazon\nAffiliate program to make it a legitimate use of the UPC lookup service per the\nlicense.\n\nI originally envisioned the results of this project in two parts.\n\nThe first and most obvious part is modernization of PoS (Point of Sale) systems.\nThe days of needing large scale PoS deployments (usually through the likes of\nIBM) are quickly coming to a close. There is also a cost savings as the security\nof Kaazing Gateway would allow retailers to use the open Internet instead of\ncostly dedicated private lines.\n\nThe second less obvious result is a real-time supply chain. At this point the\nretailers internal systems can track items being sold in real-time. They can\nthen in turn load up trucks accordingly. Or perhaps they offer specials for high\ndemand items in real-time with digital signage. Even further might be rolling\nthis into a frequency program to better understand consumer purchasing\nbehaviors.\n\nI have already started building a Java Android application that scans UPC\nbarcodes and pushes messages into this same system. A unified PoS where the\nshoppers are the cashiers might make for an interesting application as well.\nWhere will you go with it? Let me know on Twitter [http://twitter.com/krhoyt].","feature_image":"http://images.kevinhoyt.com/shopping.carts.jpg","featured":0,"status":"published","locale":null,"visibility":"public","author_id":"1","created_at":"2015-05-13T18:49:02.000Z","updated_at":"2015-06-08T00:09:01.000Z","published_at":"2015-05-15T16:58:52.000Z","custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"type":"post","email_recipient_filter":"none"},{"id":"5adb8922351ffe0018a57726","uuid":"56e3387b-8129-453d-8cf2-25bf8ae61fd6","title":"Android Barcode Scanner","slug":"android-barcode-scanner","mobiledoc":"{\"version\":\"0.3.1\",\"markups\":[],\"atoms\":[],\"cards\":[[\"markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\"*In previous posts I talked about a barcode scanner project.  The first post was from a [hardware perspective](http://blog.kevinhoyt.com/2015/05/12/first-steps-with-intel-edison/), looking at the Intel Edison.  The second post was centered around the [software](http://blog.kevinhoyt.com/2015/05/15/real-time-barcode-scanner/) for integrating with the hardware, and presenting a Web-based interface.  In this post, we continue with the concept, but using an Android camera, and native barcode scanning.*\\n\\n---\\n\\n==This is reposted on the [Kaazing Open Source Blog](http://kaazing.org/blog/android-barcode-scanner/).==\\n\\n###Pieces Parts\\n\\nTo dive into that a little bit more, here is a run-down of the architecture for this project.\\n\\nIn place of a dedicated barcode scanner that uses a laser, we will use an image processing technique using a smartphone camera.  In this case, the smartphone is an Android device.  Barcode scanning through image processing is pretty common these days, and there are many libraries available (both commercial and open source).  This project will use the [Zebra Crossing](https://github.com/zxing/zxing) project from Google.\\n\\n> Note that the Zebra Crossing project is licensed under [Apache 2.0](https://github.com/zxing/zxing/blob/master/COPYING).  Embedding in commercial software should be reviewed prior to deployment.\\n\\nSince this is not a commercial project, I will be embedding the Zebra Crossing (ZXing) scanner directly into my application.  The alternative is to use the Android Intent model, and defer the work of scanning to another (licensed) application.  For embedding the scanner, I am using the excellent [ZXing wrapper](https://github.com/journeyapps/zxing-android-embedded) from [Journey Apps](http://journeyapps.com/).\\n\\nOnce a UPC value has been obtained from the scanner, it is run through the [Amazon Associates](https://affiliate-program.amazon.com/) Product API.  While I have seen many tutorials that do this type of processing on the UI thread, I have written a Task subclass to handle the lookup on a separate thread.\\n\\nWith all the product details in hand, a message is constructed, and sent to a [Kaazing Gateway](http://kaazing.org) instance in the cloud we refer to as Sandbox.  Sandbox is a free instance of Kaazing Gateway for developers to use prior to deployment.  The message is in turn routed to whatever clients may be interested.\\n\\nIn this case, both the web client from the previous post, and this native client are interested in the message - and the content will show up in both places.  In fact, the two have functional parity, and will remain in sync throughout the shopping experience.  ==The power of publish/subscribe!==\\n\\n> Imagine walking up to a point of sale system that already knows what you have in your cart.  Just select your method of payment on your device and walk out.\\n\\n###Embedding ZXing\\n\\nThere are good instructions on the GitHub repository for this embeddable version of ZXing, but I am going to cover it here again for good measure.\\n\\nIf you are using [Android Studio](http://developer.android.com/tools/studio/index.html) for Android development (and you should be), your first stop will be the [Gradle](https://gradle.org/) files for your application - specifically for your application, not the top-level project.  You will be able to tell the difference because the comments in the file will tell you \\\"not this file\\\" and also because the tab in Android Studio will be labeled \\\"app\\\" not \\\"android\\\".\\n\\nYou will want to add three lines to your Gradle file.  The first goes in the \\\"repositories\\\" section, and the other two go in the \\\"dependencies\\\" section.  All said and done, it should look something like the following.  Note that you may have other entries here.  These are just the lines for the barcode scanner.\\n\\n```\\nrepositories {\\n    jcenter()\\n}\\n\\ndependencies {\\n    compile 'com.journeyapps:zxing-android-embedded:3.0.0@aar'\\n    compile 'com.google.zxing:core:3.2.0'\\n}\\n```\\n\\nTo use the barcode scanner, you will generally have some user interface in your application that triggers the scanning operation.  In my case, this manifests itself on the ActionBar menu.  A button would work fine as well.  Once the UI event has been triggered, you launch the barcode scanner as follows.  This will turn on the camera, and present the scanning Activity (user interface).\\n\\n```\\nintegrator = new IntentIntegrator( StoresActivity.this );\\nintegrator.initiateScan();\\n```\\n\\nThat is all there is to it - two lines of code.  From here the user may scan a barcode, or cancel the operation.  When completed, the scanning Activity will be dismissed, and your application UI will be presented again.\\n\\n```\\n// Scan complete\\n// Lookup details from Amazon\\n// Send message over Kaazing Gateway\\npublic void onActivityResult( int requestCode, int resultCode, Intent intent ) {\\n  AmazonTask      amazonTask;\\n  IntentResult    scanResult;\\n  String          resultContents;\\n\\n  // Results\\n  scanResult = IntentIntegrator.parseActivityResult( requestCode, resultCode, intent );\\n\\n  // Make sure something was scanned\\n  if( scanResult != null ) {\\n\\n    // Get the UPC from the scan results\\n    resultContents = scanResult.getContents();\\n\\n    if( resultContents == null ) {\\n      Log.i( \\\"SCAN\\\", \\\"Scanning cancelled.\\\" );\\n    } else {\\n      Log.i( \\\"SCAN\\\", resultContents );\\n    }\\n  }\\n}\\n```\\n\\nIf you think about all the image processing that goes behind the scene to get you a UPC value, this is an amazingly small amount of code.  You do not have to think about camera focus (or lack thereof), the angle of the camera, or decoding the lines.  Having written a barcode scanner [from scratch](https://github.com/krhoyt/ActionScript/tree/master/Barcode) before (ActionScript), I can tell you that this is no small feat.\\n\\n###Amazon Product Lookup\\n\\nAs mentioned earlier, network request for data should happen in their own thread.  ==It is poor practice to put network processing on the UI thread.==  Threading in Android surfaces in a variety of forms, but for this type of action, a Task subclass will handle the lookup.\\n\\n> If you are interested in how my Task subclass actually works, you can find the code on my [personal GitHub repository](https://github.com/krhoyt/Kaazing/blob/master/iot/stores/amazon/app/src/main/java/org/kaazing/amazon/AmazonTask.java).  \\n\\nMost of the code in my Task subclass (called AmazonTask) is actually lifted from the [Amazon Product Advertising API](http://docs.aws.amazon.com/AWSECommerceService/latest/DG/AuthJavaSampleSig2.html).  Rather than focus on how that code works, I will cover how to use my AmazonTask wrapper.\\n\\n```\\n// Asynchronous Amazon search\\namazonTask = new AmazonTask();\\namazonTask.callback = new AmazonListener() {\\n\\n  // Called with Amazon search result\\n  // Process and send to Kaazing Gateway\\n  @Override\\n  public void onSearchResult( AmazonResult result ) {\\n    process( result );\\n  }\\n\\n};\\namazonTask.execute( resultContents );\\n```\\n\\nCreation of the Task (a thread) starts by simply instantiating an instance of the class.  The class contains a public \\\"callback\\\" property.  That callback property is a reference to an \\\"AmazonListener\\\" interface.  You must then provide your own implementation and assign it to the callback.  \\n\\nThe interface is simple, containing only one method to be implemented.  The \\\"onSearchResult\\\" method will get a String of the response from Amazon.  That String is in JSON (JavaScript Object Notation) format, and will need to be parsed.  There is a lot of data returned, but I pick off a few key fields for display in the shopping cart.\\n\\nOnce the Task is complete, the thread is killed and the memory returned the operating system.  To do another lookup, simply instantiate the Task again, and execute it by passing the UPC value returned from the scanner.  Trying to use the same Task instance again will result in an exception. \\n\\n###Kaazing Gateway\\n\\nAt this point I certainly could just push the results into a List for display, but I want to keep the Web client (or any other client for that matter) in sync with the native application.  To accomplish this, I turn to the power of publish/subscribe as exposed by Kaazing Gateway.\\n\\nBecause a persistent connection is established from my native application to Kaazing Gateway, it is again important to not run this functionality on the UI thread.  We cannot use a Task here because this is a long-running process, which will be used throughout the life of the application.  The Android documentation suggests using a Handler subclass (also a Thread) for this type of operation.\\n\\nUsing threads in this fashion leads to some interesting challenges.  The main consideration here is how to get data from the network thread to the UI thread.  As it turns out, the Android documentation suggests using a publish/subscribe pattern.\\n\\nWhen the network thread has information it wants to pass onto other application threads it uses the operating system MessageQueue.  You can send a message to the queue, and the UI thread can pick that message up, and put the data into the respective user interface.\\n\\n> As a Kaazing employee, somebody who champions the use of publish/subscribe, having the Android documentation recommend the same pattern was an awesome discovery.  I will cover this in more detail in a future post.\\n\\nThere is a lot going on in communicating to Kaazing Gateway, so to make this process easier, I wrote a Handler subclass to wrap the functionality.  To use this subclass you must in turn subclass it in your application, and implement the \\\"handleMessage\\\" method.\\n\\n```\\n// Kaazing Gateway for messaging\\ngateway = new GatewayHandler();\\ngateway.setVerbose( true );\\ngateway.connect( KAAZING_ID );\\n\\n// Publish message\\n// May not be connected yet\\nif( gateway.isConnected() ) {\\n  gateway.publish( TOPIC, sw.toString() );\\n}\\n\\n// Inner class for Kaazing Gateway thread\\nprivate class GatewayHandler extends Gateway {\\n\\n  // Override message handler\\n  // Evaluate inter-processes message\\n  public void handleMessage( Message message ) {\\n    Bundle  bundle;\\n    String  body;\\n\\n    // Get message content\\n    bundle = message.getData();\\n\\n    // Do something with the resulting data\\n    // Evaluate action key in message data\\n    switch( bundle.getString( KEY_ACTION ) ) {\\n\\n      // Your code here\\n\\n    }\\n}\\n```\\n\\nThe first step is to instantiate the Handler subclass, which I have called \\\"Gateway\\\".  To facilitate getting the data to the UI components, I use an inner class that subclasses the \\\"Gateway\\\" thread.  That inner class is called \\\"GatewayHandler\\\" here.  To send a message to Kaazing Gateway, simply call the \\\"publish\\\" method, passing the topic name you want to use, and the data to send (a JSON string in this case).\\n\\nWhen events happen from the Kaazing Gateway, the \\\"Gateway\\\" super class will send a message to the Android operating system.  To handle the message, implement the \\\"handleMessage\\\" method.  From there you can decide what actions you want to take.  In the case of this shopping cart application, I populate the user interface with the data.\\n\\n<iframe width=\\\"640\\\" height=\\\"390\\\" src=\\\"https://www.youtube.com/embed/l31fECTlus0\\\" frameborder=\\\"0\\\" allowfullscreen></iframe>\\n\\n###Next Steps\\n\\nIt is important to note that this application sends the data to Kaazing Gateway (and other interested clients) before populating the shopping cart list.  This means that the Web client also receives this message, and populates the Web-based shopping cart at the same time.\\n\\nBecause the two applications, running on completely different technology stacks, are kept in sync, whatever happens to one, happens to the other.  If you remove a shopping cart item from the Web-based application, the item will also be removed from the native application.\\n\\n*The two applications (Web and native) are completely decoupled, and know nothing about one another.  They can be developed completely independent of one another, yet at the same time, take advantage of the data regardless of where it originates.  This is the power of the publish/subscribe pattern and Kaazing Gateway.*\\n\\nIt occurred to me while developing this application, that there could in turn be yet another application, written on yet another completely different technology stack, to manage product pricing.\\n\\nAs an example, a back-office application, managing the supply chain logistics, might see a spike in the purchase of a certain product, and raise a notification.  At that point, the vendor may choose to lower the price of an in-demand product.  This would send another message, and all the shopping cart applications, native or otherwise, would change instantly to show the new price.\\n\\nWhen we stop thinking in terms of the request/response pattern, and move to a publish/subscribe pattern, a whole new world of possibilities emerge.  I like to think of this as why have some of the data some of the time, when you can have all the data, all of the time?\\n\"}]],\"sections\":[[10,0]],\"ghostVersion\":\"3.0\"}","html":"<!--kg-card-begin: markdown--><p><em>In previous posts I talked about a barcode scanner project.  The first post was from a <a href=\"http://blog.kevinhoyt.com/2015/05/12/first-steps-with-intel-edison/\">hardware perspective</a>, looking at the Intel Edison.  The second post was centered around the <a href=\"http://blog.kevinhoyt.com/2015/05/15/real-time-barcode-scanner/\">software</a> for integrating with the hardware, and presenting a Web-based interface.  In this post, we continue with the concept, but using an Android camera, and native barcode scanning.</em></p>\n<hr>\n<p><mark>This is reposted on the <a href=\"http://kaazing.org/blog/android-barcode-scanner/\">Kaazing Open Source Blog</a>.</mark></p>\n<h3 id=\"piecesparts\">Pieces Parts</h3>\n<p>To dive into that a little bit more, here is a run-down of the architecture for this project.</p>\n<p>In place of a dedicated barcode scanner that uses a laser, we will use an image processing technique using a smartphone camera.  In this case, the smartphone is an Android device.  Barcode scanning through image processing is pretty common these days, and there are many libraries available (both commercial and open source).  This project will use the <a href=\"https://github.com/zxing/zxing\">Zebra Crossing</a> project from Google.</p>\n<blockquote>\n<p>Note that the Zebra Crossing project is licensed under <a href=\"https://github.com/zxing/zxing/blob/master/COPYING\">Apache 2.0</a>.  Embedding in commercial software should be reviewed prior to deployment.</p>\n</blockquote>\n<p>Since this is not a commercial project, I will be embedding the Zebra Crossing (ZXing) scanner directly into my application.  The alternative is to use the Android Intent model, and defer the work of scanning to another (licensed) application.  For embedding the scanner, I am using the excellent <a href=\"https://github.com/journeyapps/zxing-android-embedded\">ZXing wrapper</a> from <a href=\"http://journeyapps.com/\">Journey Apps</a>.</p>\n<p>Once a UPC value has been obtained from the scanner, it is run through the <a href=\"https://affiliate-program.amazon.com/\">Amazon Associates</a> Product API.  While I have seen many tutorials that do this type of processing on the UI thread, I have written a Task subclass to handle the lookup on a separate thread.</p>\n<p>With all the product details in hand, a message is constructed, and sent to a <a href=\"http://kaazing.org\">Kaazing Gateway</a> instance in the cloud we refer to as Sandbox.  Sandbox is a free instance of Kaazing Gateway for developers to use prior to deployment.  The message is in turn routed to whatever clients may be interested.</p>\n<p>In this case, both the web client from the previous post, and this native client are interested in the message - and the content will show up in both places.  In fact, the two have functional parity, and will remain in sync throughout the shopping experience.  <mark>The power of publish/subscribe!</mark></p>\n<blockquote>\n<p>Imagine walking up to a point of sale system that already knows what you have in your cart.  Just select your method of payment on your device and walk out.</p>\n</blockquote>\n<h3 id=\"embeddingzxing\">Embedding ZXing</h3>\n<p>There are good instructions on the GitHub repository for this embeddable version of ZXing, but I am going to cover it here again for good measure.</p>\n<p>If you are using <a href=\"http://developer.android.com/tools/studio/index.html\">Android Studio</a> for Android development (and you should be), your first stop will be the <a href=\"https://gradle.org/\">Gradle</a> files for your application - specifically for your application, not the top-level project.  You will be able to tell the difference because the comments in the file will tell you &quot;not this file&quot; and also because the tab in Android Studio will be labeled &quot;app&quot; not &quot;android&quot;.</p>\n<p>You will want to add three lines to your Gradle file.  The first goes in the &quot;repositories&quot; section, and the other two go in the &quot;dependencies&quot; section.  All said and done, it should look something like the following.  Note that you may have other entries here.  These are just the lines for the barcode scanner.</p>\n<pre><code>repositories {\n    jcenter()\n}\n\ndependencies {\n    compile 'com.journeyapps:zxing-android-embedded:3.0.0@aar'\n    compile 'com.google.zxing:core:3.2.0'\n}\n</code></pre>\n<p>To use the barcode scanner, you will generally have some user interface in your application that triggers the scanning operation.  In my case, this manifests itself on the ActionBar menu.  A button would work fine as well.  Once the UI event has been triggered, you launch the barcode scanner as follows.  This will turn on the camera, and present the scanning Activity (user interface).</p>\n<pre><code>integrator = new IntentIntegrator( StoresActivity.this );\nintegrator.initiateScan();\n</code></pre>\n<p>That is all there is to it - two lines of code.  From here the user may scan a barcode, or cancel the operation.  When completed, the scanning Activity will be dismissed, and your application UI will be presented again.</p>\n<pre><code>// Scan complete\n// Lookup details from Amazon\n// Send message over Kaazing Gateway\npublic void onActivityResult( int requestCode, int resultCode, Intent intent ) {\n  AmazonTask      amazonTask;\n  IntentResult    scanResult;\n  String          resultContents;\n\n  // Results\n  scanResult = IntentIntegrator.parseActivityResult( requestCode, resultCode, intent );\n\n  // Make sure something was scanned\n  if( scanResult != null ) {\n\n    // Get the UPC from the scan results\n    resultContents = scanResult.getContents();\n\n    if( resultContents == null ) {\n      Log.i( &quot;SCAN&quot;, &quot;Scanning cancelled.&quot; );\n    } else {\n      Log.i( &quot;SCAN&quot;, resultContents );\n    }\n  }\n}\n</code></pre>\n<p>If you think about all the image processing that goes behind the scene to get you a UPC value, this is an amazingly small amount of code.  You do not have to think about camera focus (or lack thereof), the angle of the camera, or decoding the lines.  Having written a barcode scanner <a href=\"https://github.com/krhoyt/ActionScript/tree/master/Barcode\">from scratch</a> before (ActionScript), I can tell you that this is no small feat.</p>\n<h3 id=\"amazonproductlookup\">Amazon Product Lookup</h3>\n<p>As mentioned earlier, network request for data should happen in their own thread.  <mark>It is poor practice to put network processing on the UI thread.</mark>  Threading in Android surfaces in a variety of forms, but for this type of action, a Task subclass will handle the lookup.</p>\n<blockquote>\n<p>If you are interested in how my Task subclass actually works, you can find the code on my <a href=\"https://github.com/krhoyt/Kaazing/blob/master/iot/stores/amazon/app/src/main/java/org/kaazing/amazon/AmazonTask.java\">personal GitHub repository</a>.</p>\n</blockquote>\n<p>Most of the code in my Task subclass (called AmazonTask) is actually lifted from the <a href=\"http://docs.aws.amazon.com/AWSECommerceService/latest/DG/AuthJavaSampleSig2.html\">Amazon Product Advertising API</a>.  Rather than focus on how that code works, I will cover how to use my AmazonTask wrapper.</p>\n<pre><code>// Asynchronous Amazon search\namazonTask = new AmazonTask();\namazonTask.callback = new AmazonListener() {\n\n  // Called with Amazon search result\n  // Process and send to Kaazing Gateway\n  @Override\n  public void onSearchResult( AmazonResult result ) {\n    process( result );\n  }\n\n};\namazonTask.execute( resultContents );\n</code></pre>\n<p>Creation of the Task (a thread) starts by simply instantiating an instance of the class.  The class contains a public &quot;callback&quot; property.  That callback property is a reference to an &quot;AmazonListener&quot; interface.  You must then provide your own implementation and assign it to the callback.</p>\n<p>The interface is simple, containing only one method to be implemented.  The &quot;onSearchResult&quot; method will get a String of the response from Amazon.  That String is in JSON (JavaScript Object Notation) format, and will need to be parsed.  There is a lot of data returned, but I pick off a few key fields for display in the shopping cart.</p>\n<p>Once the Task is complete, the thread is killed and the memory returned the operating system.  To do another lookup, simply instantiate the Task again, and execute it by passing the UPC value returned from the scanner.  Trying to use the same Task instance again will result in an exception.</p>\n<h3 id=\"kaazinggateway\">Kaazing Gateway</h3>\n<p>At this point I certainly could just push the results into a List for display, but I want to keep the Web client (or any other client for that matter) in sync with the native application.  To accomplish this, I turn to the power of publish/subscribe as exposed by Kaazing Gateway.</p>\n<p>Because a persistent connection is established from my native application to Kaazing Gateway, it is again important to not run this functionality on the UI thread.  We cannot use a Task here because this is a long-running process, which will be used throughout the life of the application.  The Android documentation suggests using a Handler subclass (also a Thread) for this type of operation.</p>\n<p>Using threads in this fashion leads to some interesting challenges.  The main consideration here is how to get data from the network thread to the UI thread.  As it turns out, the Android documentation suggests using a publish/subscribe pattern.</p>\n<p>When the network thread has information it wants to pass onto other application threads it uses the operating system MessageQueue.  You can send a message to the queue, and the UI thread can pick that message up, and put the data into the respective user interface.</p>\n<blockquote>\n<p>As a Kaazing employee, somebody who champions the use of publish/subscribe, having the Android documentation recommend the same pattern was an awesome discovery.  I will cover this in more detail in a future post.</p>\n</blockquote>\n<p>There is a lot going on in communicating to Kaazing Gateway, so to make this process easier, I wrote a Handler subclass to wrap the functionality.  To use this subclass you must in turn subclass it in your application, and implement the &quot;handleMessage&quot; method.</p>\n<pre><code>// Kaazing Gateway for messaging\ngateway = new GatewayHandler();\ngateway.setVerbose( true );\ngateway.connect( KAAZING_ID );\n\n// Publish message\n// May not be connected yet\nif( gateway.isConnected() ) {\n  gateway.publish( TOPIC, sw.toString() );\n}\n\n// Inner class for Kaazing Gateway thread\nprivate class GatewayHandler extends Gateway {\n\n  // Override message handler\n  // Evaluate inter-processes message\n  public void handleMessage( Message message ) {\n    Bundle  bundle;\n    String  body;\n\n    // Get message content\n    bundle = message.getData();\n\n    // Do something with the resulting data\n    // Evaluate action key in message data\n    switch( bundle.getString( KEY_ACTION ) ) {\n\n      // Your code here\n\n    }\n}\n</code></pre>\n<p>The first step is to instantiate the Handler subclass, which I have called &quot;Gateway&quot;.  To facilitate getting the data to the UI components, I use an inner class that subclasses the &quot;Gateway&quot; thread.  That inner class is called &quot;GatewayHandler&quot; here.  To send a message to Kaazing Gateway, simply call the &quot;publish&quot; method, passing the topic name you want to use, and the data to send (a JSON string in this case).</p>\n<p>When events happen from the Kaazing Gateway, the &quot;Gateway&quot; super class will send a message to the Android operating system.  To handle the message, implement the &quot;handleMessage&quot; method.  From there you can decide what actions you want to take.  In the case of this shopping cart application, I populate the user interface with the data.</p>\n<iframe width=\"640\" height=\"390\" src=\"https://www.youtube.com/embed/l31fECTlus0\" frameborder=\"0\" allowfullscreen></iframe>\n<h3 id=\"nextsteps\">Next Steps</h3>\n<p>It is important to note that this application sends the data to Kaazing Gateway (and other interested clients) before populating the shopping cart list.  This means that the Web client also receives this message, and populates the Web-based shopping cart at the same time.</p>\n<p>Because the two applications, running on completely different technology stacks, are kept in sync, whatever happens to one, happens to the other.  If you remove a shopping cart item from the Web-based application, the item will also be removed from the native application.</p>\n<p><em>The two applications (Web and native) are completely decoupled, and know nothing about one another.  They can be developed completely independent of one another, yet at the same time, take advantage of the data regardless of where it originates.  This is the power of the publish/subscribe pattern and Kaazing Gateway.</em></p>\n<p>It occurred to me while developing this application, that there could in turn be yet another application, written on yet another completely different technology stack, to manage product pricing.</p>\n<p>As an example, a back-office application, managing the supply chain logistics, might see a spike in the purchase of a certain product, and raise a notification.  At that point, the vendor may choose to lower the price of an in-demand product.  This would send another message, and all the shopping cart applications, native or otherwise, would change instantly to show the new price.</p>\n<p>When we stop thinking in terms of the request/response pattern, and move to a publish/subscribe pattern, a whole new world of possibilities emerge.  I like to think of this as why have some of the data some of the time, when you can have all the data, all of the time?</p>\n<!--kg-card-end: markdown-->","comment_id":"27","plaintext":"In previous posts I talked about a barcode scanner project. The first post was\nfrom a hardware perspective\n[http://blog.kevinhoyt.com/2015/05/12/first-steps-with-intel-edison/], looking\nat the Intel Edison. The second post was centered around the software\n[http://blog.kevinhoyt.com/2015/05/15/real-time-barcode-scanner/] for\nintegrating with the hardware, and presenting a Web-based interface. In this\npost, we continue with the concept, but using an Android camera, and native\nbarcode scanning.\n\n\n--------------------------------------------------------------------------------\n\nThis is reposted on the Kaazing Open Source Blog\n[http://kaazing.org/blog/android-barcode-scanner/].\n\nPieces Parts\nTo dive into that a little bit more, here is a run-down of the architecture for\nthis project.\n\nIn place of a dedicated barcode scanner that uses a laser, we will use an image\nprocessing technique using a smartphone camera. In this case, the smartphone is\nan Android device. Barcode scanning through image processing is pretty common\nthese days, and there are many libraries available (both commercial and open\nsource). This project will use the Zebra Crossing\n[https://github.com/zxing/zxing] project from Google.\n\n> Note that the Zebra Crossing project is licensed under Apache 2.0\n[https://github.com/zxing/zxing/blob/master/COPYING]. Embedding in commercial\nsoftware should be reviewed prior to deployment.\n\n\nSince this is not a commercial project, I will be embedding the Zebra Crossing\n(ZXing) scanner directly into my application. The alternative is to use the\nAndroid Intent model, and defer the work of scanning to another (licensed)\napplication. For embedding the scanner, I am using the excellent ZXing wrapper\n[https://github.com/journeyapps/zxing-android-embedded] from Journey Apps\n[http://journeyapps.com/].\n\nOnce a UPC value has been obtained from the scanner, it is run through the \nAmazon Associates [https://affiliate-program.amazon.com/] Product API. While I\nhave seen many tutorials that do this type of processing on the UI thread, I\nhave written a Task subclass to handle the lookup on a separate thread.\n\nWith all the product details in hand, a message is constructed, and sent to a \nKaazing Gateway [http://kaazing.org] instance in the cloud we refer to as\nSandbox. Sandbox is a free instance of Kaazing Gateway for developers to use\nprior to deployment. The message is in turn routed to whatever clients may be\ninterested.\n\nIn this case, both the web client from the previous post, and this native client\nare interested in the message - and the content will show up in both places. In\nfact, the two have functional parity, and will remain in sync throughout the\nshopping experience. The power of publish/subscribe!\n\n> Imagine walking up to a point of sale system that already knows what you have in\nyour cart. Just select your method of payment on your device and walk out.\n\n\nEmbedding ZXing\nThere are good instructions on the GitHub repository for this embeddable version\nof ZXing, but I am going to cover it here again for good measure.\n\nIf you are using Android Studio\n[http://developer.android.com/tools/studio/index.html] for Android development\n(and you should be), your first stop will be the Gradle [https://gradle.org/] \nfiles for your application - specifically for your application, not the\ntop-level project. You will be able to tell the difference because the comments\nin the file will tell you \"not this file\" and also because the tab in Android\nStudio will be labeled \"app\" not \"android\".\n\nYou will want to add three lines to your Gradle file. The first goes in the\n\"repositories\" section, and the other two go in the \"dependencies\" section. All\nsaid and done, it should look something like the following. Note that you may\nhave other entries here. These are just the lines for the barcode scanner.\n\nrepositories {\n    jcenter()\n}\n\ndependencies {\n    compile 'com.journeyapps:zxing-android-embedded:3.0.0@aar'\n    compile 'com.google.zxing:core:3.2.0'\n}\n\n\nTo use the barcode scanner, you will generally have some user interface in your\napplication that triggers the scanning operation. In my case, this manifests\nitself on the ActionBar menu. A button would work fine as well. Once the UI\nevent has been triggered, you launch the barcode scanner as follows. This will\nturn on the camera, and present the scanning Activity (user interface).\n\nintegrator = new IntentIntegrator( StoresActivity.this );\nintegrator.initiateScan();\n\n\nThat is all there is to it - two lines of code. From here the user may scan a\nbarcode, or cancel the operation. When completed, the scanning Activity will be\ndismissed, and your application UI will be presented again.\n\n// Scan complete\n// Lookup details from Amazon\n// Send message over Kaazing Gateway\npublic void onActivityResult( int requestCode, int resultCode, Intent intent ) {\n  AmazonTask      amazonTask;\n  IntentResult    scanResult;\n  String          resultContents;\n\n  // Results\n  scanResult = IntentIntegrator.parseActivityResult( requestCode, resultCode, intent );\n\n  // Make sure something was scanned\n  if( scanResult != null ) {\n\n    // Get the UPC from the scan results\n    resultContents = scanResult.getContents();\n\n    if( resultContents == null ) {\n      Log.i( \"SCAN\", \"Scanning cancelled.\" );\n    } else {\n      Log.i( \"SCAN\", resultContents );\n    }\n  }\n}\n\n\nIf you think about all the image processing that goes behind the scene to get\nyou a UPC value, this is an amazingly small amount of code. You do not have to\nthink about camera focus (or lack thereof), the angle of the camera, or decoding\nthe lines. Having written a barcode scanner from scratch\n[https://github.com/krhoyt/ActionScript/tree/master/Barcode] before\n(ActionScript), I can tell you that this is no small feat.\n\nAmazon Product Lookup\nAs mentioned earlier, network request for data should happen in their own\nthread. It is poor practice to put network processing on the UI thread. \nThreading in Android surfaces in a variety of forms, but for this type of\naction, a Task subclass will handle the lookup.\n\n> If you are interested in how my Task subclass actually works, you can find the\ncode on my personal GitHub repository\n[https://github.com/krhoyt/Kaazing/blob/master/iot/stores/amazon/app/src/main/java/org/kaazing/amazon/AmazonTask.java]\n.\n\n\nMost of the code in my Task subclass (called AmazonTask) is actually lifted from\nthe Amazon Product Advertising API\n[http://docs.aws.amazon.com/AWSECommerceService/latest/DG/AuthJavaSampleSig2.html]\n. Rather than focus on how that code works, I will cover how to use my\nAmazonTask wrapper.\n\n// Asynchronous Amazon search\namazonTask = new AmazonTask();\namazonTask.callback = new AmazonListener() {\n\n  // Called with Amazon search result\n  // Process and send to Kaazing Gateway\n  @Override\n  public void onSearchResult( AmazonResult result ) {\n    process( result );\n  }\n\n};\namazonTask.execute( resultContents );\n\n\nCreation of the Task (a thread) starts by simply instantiating an instance of\nthe class. The class contains a public \"callback\" property. That callback\nproperty is a reference to an \"AmazonListener\" interface. You must then provide\nyour own implementation and assign it to the callback.\n\nThe interface is simple, containing only one method to be implemented. The\n\"onSearchResult\" method will get a String of the response from Amazon. That\nString is in JSON (JavaScript Object Notation) format, and will need to be\nparsed. There is a lot of data returned, but I pick off a few key fields for\ndisplay in the shopping cart.\n\nOnce the Task is complete, the thread is killed and the memory returned the\noperating system. To do another lookup, simply instantiate the Task again, and\nexecute it by passing the UPC value returned from the scanner. Trying to use the\nsame Task instance again will result in an exception.\n\nKaazing Gateway\nAt this point I certainly could just push the results into a List for display,\nbut I want to keep the Web client (or any other client for that matter) in sync\nwith the native application. To accomplish this, I turn to the power of\npublish/subscribe as exposed by Kaazing Gateway.\n\nBecause a persistent connection is established from my native application to\nKaazing Gateway, it is again important to not run this functionality on the UI\nthread. We cannot use a Task here because this is a long-running process, which\nwill be used throughout the life of the application. The Android documentation\nsuggests using a Handler subclass (also a Thread) for this type of operation.\n\nUsing threads in this fashion leads to some interesting challenges. The main\nconsideration here is how to get data from the network thread to the UI thread.\nAs it turns out, the Android documentation suggests using a publish/subscribe\npattern.\n\nWhen the network thread has information it wants to pass onto other application\nthreads it uses the operating system MessageQueue. You can send a message to the\nqueue, and the UI thread can pick that message up, and put the data into the\nrespective user interface.\n\n> As a Kaazing employee, somebody who champions the use of publish/subscribe,\nhaving the Android documentation recommend the same pattern was an awesome\ndiscovery. I will cover this in more detail in a future post.\n\n\nThere is a lot going on in communicating to Kaazing Gateway, so to make this\nprocess easier, I wrote a Handler subclass to wrap the functionality. To use\nthis subclass you must in turn subclass it in your application, and implement\nthe \"handleMessage\" method.\n\n// Kaazing Gateway for messaging\ngateway = new GatewayHandler();\ngateway.setVerbose( true );\ngateway.connect( KAAZING_ID );\n\n// Publish message\n// May not be connected yet\nif( gateway.isConnected() ) {\n  gateway.publish( TOPIC, sw.toString() );\n}\n\n// Inner class for Kaazing Gateway thread\nprivate class GatewayHandler extends Gateway {\n\n  // Override message handler\n  // Evaluate inter-processes message\n  public void handleMessage( Message message ) {\n    Bundle  bundle;\n    String  body;\n\n    // Get message content\n    bundle = message.getData();\n\n    // Do something with the resulting data\n    // Evaluate action key in message data\n    switch( bundle.getString( KEY_ACTION ) ) {\n\n      // Your code here\n\n    }\n}\n\n\nThe first step is to instantiate the Handler subclass, which I have called\n\"Gateway\". To facilitate getting the data to the UI components, I use an inner\nclass that subclasses the \"Gateway\" thread. That inner class is called\n\"GatewayHandler\" here. To send a message to Kaazing Gateway, simply call the\n\"publish\" method, passing the topic name you want to use, and the data to send\n(a JSON string in this case).\n\nWhen events happen from the Kaazing Gateway, the \"Gateway\" super class will send\na message to the Android operating system. To handle the message, implement the\n\"handleMessage\" method. From there you can decide what actions you want to take.\nIn the case of this shopping cart application, I populate the user interface\nwith the data.\n\nNext Steps\nIt is important to note that this application sends the data to Kaazing Gateway\n(and other interested clients) before populating the shopping cart list. This\nmeans that the Web client also receives this message, and populates the\nWeb-based shopping cart at the same time.\n\nBecause the two applications, running on completely different technology stacks,\nare kept in sync, whatever happens to one, happens to the other. If you remove a\nshopping cart item from the Web-based application, the item will also be removed\nfrom the native application.\n\nThe two applications (Web and native) are completely decoupled, and know nothing\nabout one another. They can be developed completely independent of one another,\nyet at the same time, take advantage of the data regardless of where it\noriginates. This is the power of the publish/subscribe pattern and Kaazing\nGateway.\n\nIt occurred to me while developing this application, that there could in turn be\nyet another application, written on yet another completely different technology\nstack, to manage product pricing.\n\nAs an example, a back-office application, managing the supply chain logistics,\nmight see a spike in the purchase of a certain product, and raise a\nnotification. At that point, the vendor may choose to lower the price of an\nin-demand product. This would send another message, and all the shopping cart\napplications, native or otherwise, would change instantly to show the new price.\n\nWhen we stop thinking in terms of the request/response pattern, and move to a\npublish/subscribe pattern, a whole new world of possibilities emerge. I like to\nthink of this as why have some of the data some of the time, when you can have\nall the data, all of the time?","feature_image":"http://images.kevinhoyt.com/newspapers.in.paris.jpg","featured":0,"status":"published","locale":null,"visibility":"public","author_id":"1","created_at":"2015-05-28T18:10:55.000Z","updated_at":"2015-06-09T23:15:27.000Z","published_at":"2015-06-05T16:31:48.000Z","custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"type":"post","email_recipient_filter":"none"},{"id":"5adb8922351ffe0018a57727","uuid":"99c290dd-38d3-4e79-8987-728d3d7aeae9","title":"Friday Diversion: Dirty Dishes","slug":"friday-diversion-dirty-dishes","mobiledoc":"{\"version\":\"0.3.1\",\"markups\":[],\"atoms\":[],\"cards\":[[\"markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\"*The never-ending chore of dishes. ==(somber pause)==  What more needs to be said?  My daughter actually prefers to clean out the cat liter boxes nightly over having to do dishes.  Every now and again you pass the pile accumulating in the sink, and think to yourself that you should put those in the dishwasher.  Is load in the dishwasher dirty, or clean?  This project aims to answer that question.*\\n\\n---\\n\\n==This post was featured in the [Thinking Closet's](http://www.thinkingcloset.com) *Winter and Spring Reader Showcase*! Check out [the post](http://www.thinkingcloset.com/2015/06/27/reader-showcase-a-look-back-at-winter-spring-2015/) for more inspirational projects.==\\n\\nWe recently moved.  In our old house, the dishwasher had a light on the front that indicated that the dishes were clean.  You knew with one glance.  In our new house, the dishwasher has a similar light, but it is on top of the door, tucked under the counter.  So you can never tell if the dishes in the washer are dirty or clean.\\n\\n###Inspiration\\n\\nI have not fabricated anything in a while, so this seemed like something I could solve.  I could put something with a magnet on the dishwasher to show if the dishes inside were dirty or clean.  It could not have a magnet on both sides though - that would cover up the sign.\\n\\nA little Pinterest for inspiration led me to [this post](http://www.thinkingcloset.com/2014/05/27/dishwasher-magnets-free-cut-file-printable/) on The Thinking Closet.  Lauren had encountered a similar problem, and crafted up a slick little solution.  The answer to my problem of not wanting to put magnets on both sides of the sign, was to make the sign a circle!\\n\\n[![Thinking Closet Dishwasher Magnet](http://images.kevinhoyt.com/thinking.closet.dishes.sign.jpg)](http://www.thinkingcloset.com/2014/05/27/dishwasher-magnets-free-cut-file-printable/)\\n\\n###Fabrication\\n\\nLauren offers a free download of the cut file and printable.  That is great if you are using a Silhouette cutter, but I am not.  The Silhouette Studio software does not allow exporting to other formats either.  Looks like I will have to recreate this bad boy myself.  Thankfully, Lauren goes into enough detail in the post to know where to get the fonts and the clip art.\\n\\nAfter downloading the assets, I recreated (roughly) Lauren's work in Adobe Illustrator.  The \\\"fork, knife and spoon\\\" graphic Lauren used from [Sweet Clip Art](http://sweetclipart.com/fork-knife-and-spoon-823), while drawn using vector tooling, is offered as a bitmap PNG file.  I used Illustrator's \\\"Image Trace\\\" to get it back to a vector format.  ==Why vector?  Because I am going to laser cut this bad boy!==\\n\\n![My Adobe Illustrator version](http://images.kevinhoyt.com/dirty.clean.dishes.vector.svg)\\n\\nWhen laser cutting, you have a wide breadth of materials to choose from.  I have used acrylic (plastic), birch plywood, particle board, and even foam core board.  Being in the kitchen however reminded me of the bamboo utensils.  When etching acrylic, the results are pretty noticeable.  When etching birch, the burnt wood really pops.  I was not sure how bamboo would show up.\\n\\n> I used a 2.4mm thick sheet of bamboo.  Thick enough to grab and pull off the dishwasher.\\n\\nTo address this, I made two versions of the design.  One with the background etched out, and one with the wording and graphics etched out.  I decided on a four inch diameter circle for the design (I think Lauren used 3.25 inches).  After placing the design on a [Ponoko](http://www.ponoko.com/) template, I sent it off and waited eagerly for the result.\\n\\n###The Result\\n\\nBamboo turned out to be an interesting material to laser cut.  The circle cut itself left the typical burn discoloration on the outer edge.  The etching however was apparently not hot enough to leave and mark.  Or perhaps Ponoko chose to put the design in a CNC machine?  Here you can see the two side-by-side.\\n\\n![The two results side-by-side](http://images.kevinhoyt.com/dirty.clean.dishes.finished.jpg)\\n\\nThere is a nice grain in the bamboo that I really like.  If I were to do this again, I would choose a deeper etch for just a little more depth and pop of the design.\\n\\nOn the back of the bamboo, I used a [thin magnetic sheet](http://shop.hobbylobby.com/products/5-x-8-flexible-adhesive-magnetic-sheet-495796/).  To accomplish this, I traced the circle on the magnet sheet, and then cut about a quarter inch inside of that circle.  Pull off the adhesive sheet, and slap it onto the back of the wood.\\n\\nThe finished product has been in place for about a week now, and is working really well.  Thanks to Lauren for sharing her creative process.  If you want to laser cut your own \\\"dirty dishes sign\\\" you can [download the Ponoko-ready EPS file](http://images.kevinhoyt.com/dirty.clean.dishes.ponoko.eps).\"}]],\"sections\":[[10,0]],\"ghostVersion\":\"3.0\"}","html":"<!--kg-card-begin: markdown--><p><em>The never-ending chore of dishes. <mark>(somber pause)</mark>  What more needs to be said?  My daughter actually prefers to clean out the cat liter boxes nightly over having to do dishes.  Every now and again you pass the pile accumulating in the sink, and think to yourself that you should put those in the dishwasher.  Is load in the dishwasher dirty, or clean?  This project aims to answer that question.</em></p>\n<hr>\n<p><mark>This post was featured in the <a href=\"http://www.thinkingcloset.com\">Thinking Closet's</a> <em>Winter and Spring Reader Showcase</em>! Check out <a href=\"http://www.thinkingcloset.com/2015/06/27/reader-showcase-a-look-back-at-winter-spring-2015/\">the post</a> for more inspirational projects.</mark></p>\n<p>We recently moved.  In our old house, the dishwasher had a light on the front that indicated that the dishes were clean.  You knew with one glance.  In our new house, the dishwasher has a similar light, but it is on top of the door, tucked under the counter.  So you can never tell if the dishes in the washer are dirty or clean.</p>\n<h3 id=\"inspiration\">Inspiration</h3>\n<p>I have not fabricated anything in a while, so this seemed like something I could solve.  I could put something with a magnet on the dishwasher to show if the dishes inside were dirty or clean.  It could not have a magnet on both sides though - that would cover up the sign.</p>\n<p>A little Pinterest for inspiration led me to <a href=\"http://www.thinkingcloset.com/2014/05/27/dishwasher-magnets-free-cut-file-printable/\">this post</a> on The Thinking Closet.  Lauren had encountered a similar problem, and crafted up a slick little solution.  The answer to my problem of not wanting to put magnets on both sides of the sign, was to make the sign a circle!</p>\n<p><a href=\"http://www.thinkingcloset.com/2014/05/27/dishwasher-magnets-free-cut-file-printable/\"><img src=\"http://images.kevinhoyt.com/thinking.closet.dishes.sign.jpg\" alt=\"Thinking Closet Dishwasher Magnet\" loading=\"lazy\"></a></p>\n<h3 id=\"fabrication\">Fabrication</h3>\n<p>Lauren offers a free download of the cut file and printable.  That is great if you are using a Silhouette cutter, but I am not.  The Silhouette Studio software does not allow exporting to other formats either.  Looks like I will have to recreate this bad boy myself.  Thankfully, Lauren goes into enough detail in the post to know where to get the fonts and the clip art.</p>\n<p>After downloading the assets, I recreated (roughly) Lauren's work in Adobe Illustrator.  The &quot;fork, knife and spoon&quot; graphic Lauren used from <a href=\"http://sweetclipart.com/fork-knife-and-spoon-823\">Sweet Clip Art</a>, while drawn using vector tooling, is offered as a bitmap PNG file.  I used Illustrator's &quot;Image Trace&quot; to get it back to a vector format.  <mark>Why vector?  Because I am going to laser cut this bad boy!</mark></p>\n<p><img src=\"http://images.kevinhoyt.com/dirty.clean.dishes.vector.svg\" alt=\"My Adobe Illustrator version\" loading=\"lazy\"></p>\n<p>When laser cutting, you have a wide breadth of materials to choose from.  I have used acrylic (plastic), birch plywood, particle board, and even foam core board.  Being in the kitchen however reminded me of the bamboo utensils.  When etching acrylic, the results are pretty noticeable.  When etching birch, the burnt wood really pops.  I was not sure how bamboo would show up.</p>\n<blockquote>\n<p>I used a 2.4mm thick sheet of bamboo.  Thick enough to grab and pull off the dishwasher.</p>\n</blockquote>\n<p>To address this, I made two versions of the design.  One with the background etched out, and one with the wording and graphics etched out.  I decided on a four inch diameter circle for the design (I think Lauren used 3.25 inches).  After placing the design on a <a href=\"http://www.ponoko.com/\">Ponoko</a> template, I sent it off and waited eagerly for the result.</p>\n<h3 id=\"theresult\">The Result</h3>\n<p>Bamboo turned out to be an interesting material to laser cut.  The circle cut itself left the typical burn discoloration on the outer edge.  The etching however was apparently not hot enough to leave and mark.  Or perhaps Ponoko chose to put the design in a CNC machine?  Here you can see the two side-by-side.</p>\n<p><img src=\"http://images.kevinhoyt.com/dirty.clean.dishes.finished.jpg\" alt=\"The two results side-by-side\" loading=\"lazy\"></p>\n<p>There is a nice grain in the bamboo that I really like.  If I were to do this again, I would choose a deeper etch for just a little more depth and pop of the design.</p>\n<p>On the back of the bamboo, I used a <a href=\"http://shop.hobbylobby.com/products/5-x-8-flexible-adhesive-magnetic-sheet-495796/\">thin magnetic sheet</a>.  To accomplish this, I traced the circle on the magnet sheet, and then cut about a quarter inch inside of that circle.  Pull off the adhesive sheet, and slap it onto the back of the wood.</p>\n<p>The finished product has been in place for about a week now, and is working really well.  Thanks to Lauren for sharing her creative process.  If you want to laser cut your own &quot;dirty dishes sign&quot; you can <a href=\"http://images.kevinhoyt.com/dirty.clean.dishes.ponoko.eps\">download the Ponoko-ready EPS file</a>.</p>\n<!--kg-card-end: markdown-->","comment_id":"28","plaintext":"The never-ending chore of dishes. (somber pause) What more needs to be said? My\ndaughter actually prefers to clean out the cat liter boxes nightly over having\nto do dishes. Every now and again you pass the pile accumulating in the sink,\nand think to yourself that you should put those in the dishwasher. Is load in\nthe dishwasher dirty, or clean? This project aims to answer that question.\n\n\n--------------------------------------------------------------------------------\n\nThis post was featured in the Thinking Closet's [http://www.thinkingcloset.com] \nWinter and Spring Reader Showcase! Check out the post\n[http://www.thinkingcloset.com/2015/06/27/reader-showcase-a-look-back-at-winter-spring-2015/] \nfor more inspirational projects.\n\nWe recently moved. In our old house, the dishwasher had a light on the front\nthat indicated that the dishes were clean. You knew with one glance. In our new\nhouse, the dishwasher has a similar light, but it is on top of the door, tucked\nunder the counter. So you can never tell if the dishes in the washer are dirty\nor clean.\n\nInspiration\nI have not fabricated anything in a while, so this seemed like something I could\nsolve. I could put something with a magnet on the dishwasher to show if the\ndishes inside were dirty or clean. It could not have a magnet on both sides\nthough - that would cover up the sign.\n\nA little Pinterest for inspiration led me to this post\n[http://www.thinkingcloset.com/2014/05/27/dishwasher-magnets-free-cut-file-printable/] \non The Thinking Closet. Lauren had encountered a similar problem, and crafted up\na slick little solution. The answer to my problem of not wanting to put magnets\non both sides of the sign, was to make the sign a circle!\n\n \n[http://www.thinkingcloset.com/2014/05/27/dishwasher-magnets-free-cut-file-printable/]\n\nFabrication\nLauren offers a free download of the cut file and printable. That is great if\nyou are using a Silhouette cutter, but I am not. The Silhouette Studio software\ndoes not allow exporting to other formats either. Looks like I will have to\nrecreate this bad boy myself. Thankfully, Lauren goes into enough detail in the\npost to know where to get the fonts and the clip art.\n\nAfter downloading the assets, I recreated (roughly) Lauren's work in Adobe\nIllustrator. The \"fork, knife and spoon\" graphic Lauren used from Sweet Clip Art\n[http://sweetclipart.com/fork-knife-and-spoon-823], while drawn using vector\ntooling, is offered as a bitmap PNG file. I used Illustrator's \"Image Trace\" to\nget it back to a vector format. Why vector? Because I am going to laser cut this\nbad boy!\n\n\n\nWhen laser cutting, you have a wide breadth of materials to choose from. I have\nused acrylic (plastic), birch plywood, particle board, and even foam core board.\nBeing in the kitchen however reminded me of the bamboo utensils. When etching\nacrylic, the results are pretty noticeable. When etching birch, the burnt wood\nreally pops. I was not sure how bamboo would show up.\n\n> I used a 2.4mm thick sheet of bamboo. Thick enough to grab and pull off the\ndishwasher.\n\n\nTo address this, I made two versions of the design. One with the background\netched out, and one with the wording and graphics etched out. I decided on a\nfour inch diameter circle for the design (I think Lauren used 3.25 inches).\nAfter placing the design on a Ponoko [http://www.ponoko.com/] template, I sent\nit off and waited eagerly for the result.\n\nThe Result\nBamboo turned out to be an interesting material to laser cut. The circle cut\nitself left the typical burn discoloration on the outer edge. The etching\nhowever was apparently not hot enough to leave and mark. Or perhaps Ponoko chose\nto put the design in a CNC machine? Here you can see the two side-by-side.\n\n\n\nThere is a nice grain in the bamboo that I really like. If I were to do this\nagain, I would choose a deeper etch for just a little more depth and pop of the\ndesign.\n\nOn the back of the bamboo, I used a thin magnetic sheet\n[http://shop.hobbylobby.com/products/5-x-8-flexible-adhesive-magnetic-sheet-495796/]\n. To accomplish this, I traced the circle on the magnet sheet, and then cut\nabout a quarter inch inside of that circle. Pull off the adhesive sheet, and\nslap it onto the back of the wood.\n\nThe finished product has been in place for about a week now, and is working\nreally well. Thanks to Lauren for sharing her creative process. If you want to\nlaser cut your own \"dirty dishes sign\" you can download the Ponoko-ready EPS\nfile [http://images.kevinhoyt.com/dirty.clean.dishes.ponoko.eps].","feature_image":"http://images.kevinhoyt.com/dirty.clean.dishes.header.jpg","featured":0,"status":"published","locale":null,"visibility":"public","author_id":"1","created_at":"2015-05-29T14:32:45.000Z","updated_at":"2015-07-01T02:30:01.000Z","published_at":"2015-05-29T16:07:58.000Z","custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"type":"post","email_recipient_filter":"none"},{"id":"5adb8922351ffe0018a57728","uuid":"664b75db-8b48-47af-b177-30d84476a68d","title":"Tetris with Virtual Controller","slug":"tetris-with-virtual-controller","mobiledoc":"{\"version\":\"0.3.1\",\"markups\":[],\"atoms\":[],\"cards\":[[\"markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\"*As somebody educated in Information Technology, I have always had an admiration of game developers.  Despite the interactive experience and immersive graphics, there is just some black magic in the architecture that leaves me in awe.  While I have had my hand at small games in the past, I decided that I would have a crack at [Tetris](http://en.wikipedia.org/wiki/Tetris), using a real-time virtual controller.*\\n\\n---\\n\\n==This is reposted on the [Kaazing Open Source Blog](http://kaazing.org/blog/tetris-with-virtual-controller/).==\\n\\n###Research\\n\\nWhile I wanted to solve the algorithms on my own, it never hurts to take a look at see what has been done already.  The first port I found with reasonably compact code, was done in [Python](http://inventwithpython.com/pygame/chapter7.html).  A little more searching led me to one ported to [JavaScript](http://codeincomplete.com/posts/2011/10/10/javascript_tetris/).\\n\\nThe problem (for me) with both of these is that they used traditional graphics routines for the drawing.  While I suspect this is how the original Tetris is written, I wanted to make the game using the DOM for crisp rendering across platforms.\\n\\n> Covering in detail how Tetris works is beyond the scope of this blog post.  Instead I will focus predominantly on tips I learned along the way, and implementing a virtual controller.\\n\\nI also felt that using the DOM would provide me with a game that was easier to style (using CSS, SVG).  This is important for my game because I wanted to be able to easily change the graphics to reflect holidays, conferences, and other special events.  Imagine animated GIFs for the color fills of the falling blocks.\\n\\nAll the ports I found (without too much digging) also used the keyboard to control the game.  This would be fine if you are sitting at the same machine, but decoupling the game from the controller would bring new possibilities to the game.  I could for example use an Arduino or other hardware for my own custom controller. \\n\\n###Game Board\\n\\nMy approach resulted in two parts - the game board, and the game controller.  The game board would display all the visuals you might normally encounter when playing Tetris.  The game controller would be the buttons on a traditional physical controller that you might push to move the visual pieces around.\\n\\nThe traditional game board is ~20 cells high and ~10 cells wide (depending on who you ask).  For my game board, I decided to stick with the 20 cells tall, but make the entire screen width the width of the game board.  ==Depending on your device, this could be upwards of 1,000 individual DOM elements to represent the board.==\\n\\nAt first I tried using ==querySelector()== to get the cells as they needed rendering.  This turned out to be far too slow.  ==Instead, I stored references to the cells as I created them to fit the device display.==  The result is most certainly a high memory requirement, but also a much faster access to drawing the board (by styling the cells).\\n\\nI also originally went with ==setInterval()== for my game clock.  In order to keep a reasonable frame rate during game play, I set a very low interval rate.  This really put a load on the CPU (fans spinning).  ==In the end, I went with requestAnimationFrame() which kept frame rates high and CPU usage much lower (no more spinning fans).==\\n\\nThe game board itself has two representations.  There is the visual one you see while playing, but also a logical one that tracks where blocks have been placed.  ==Rather than create another 1,000 element array, I use data attributes on the cell reference array I already had.==  Using getAttribute() and setAttribute() had no noticeable impact on game performance.\\n\\n![Tetris game board in action.](http://images.kevinhoyt.com/tetris.game.board.jpg)\\n\\n###Game Controller\\n\\nThe hardest part of building the game controller was in managing the device orientation.  I was using my iPhone during development.  Full screen support for Web applications is flat out [not supported on iOS](http://caniuse.com/#search=full) - **way to be Web-friendly Apple**.  Also on Safari iOS8 the toolbars appear and disappear at what seems like their own timing.\\n\\nIn the end, I decided on a height-to-width ratio that would fit regardless of the toolbars being present or not.  This still left me with plenty of room for game controls.  I did not put too much effort into styling, and instead played the game repeatedly until I had sizing that felt reliable and comfortable for my hands.\\n\\n> Having some tactile feedback would really help with Web-based games.\\n\\nThe game controller is designed such that it does not have to be a separate device.  The game can be played with two desktop browsers for example.  This means I need to listen for touch events, and react accordingly.  The controller also cares about when a button is pressed, and when it is released.\\n\\n![Tetris virtual controller with and without Safari interference.](http://images.kevinhoyt.com/tetris.virtual.controller.jpg)\\n\\n###Linking Board and Controller\\n\\nOnce I had performance of the game board ironed out, and a placement of the game controller dialed in, the next part was to link the two devices.  This would normally be pretty tricky.  ==In fact, many native iOS games only allow for this over local wireless or Bluetooth== - where the devices can search each other out.\\n\\nThanks to the power of Kaazing Gateway, and the publish/subscribe pattern, the only thing the two devices need to communicate is a common messaging topic.  \\n\\nWhen the game board loads, a [QR code](http://davidshimjs.github.io/qrcodejs/) is presented.  The contents of that QR code is a URL to the game controller.  That URL also has a query string appended to it that uniquely identifies that specific game.  This query string effectively represents the messaging topic.\\n\\nIf the game controller device is a smartphone, a QR code scanner application can retrieve the URL (including the query string) from the game board.  This will in turn launch a browser on that device, and load that URL.  The game controller picks off the query string, and is then ready to publish to that specific message topic.\\n\\nIn fact, once the controller has loaded, a message is sent to the game board to tell it to hide the QR code, and start the game.  As buttons are pressed and released, those events are passed as messages to Kaazing Gateway, which are in turn sent to that specific game board.  [Game on!](http://tetris.kevinhoyt.com)\\n\\n###Next Steps\\n\\nIt still very much surprises me how many native games (iOS or otherwise) still use local lookup mechanisms to establish a connection between two devices, and to communicate game state.  The **Real-Time Web** is here, and ==Kaazing Gateway== would allow these games to ==securely== use the open Internet without sacrificing communication ==performance==.\\n\\n> Kaazing Gateway also works across platforms, so you get a similar API across whatever technology stack you might choose to use.\\n\\nThere are two directions I would like to take this project:\\n\\n* Introduce a network of decoupled hardware sensors, placed on various points on the body.  Want to move a piece left? Lift your left leg.  Want to rotate the piece clockwise?  Spin your left arm around.\\n\\n* Another fun development would be head-to-head play.  Show two boards on the same screen at the same time.  Then let two virtual controllers join.  Rows removed from one player's board, end up on the other player's board.\\n\\nI have posted the source code for the game to my [personal GitHub account](https://github.com/krhoyt/Kaazing/tree/master/tetris).  Because Kaazing offers a free \\\"Sandbox\\\" instance for you to get started using, you can also have your hand at my port of Tetris [right now](http://tetris.kevinhoyt.com).  Do not let request/response keep your applications limited, join the Real-Time Web today by checking out the [getting started guide](http://kaazing.org/demos/quick-start/).\"}]],\"sections\":[[10,0]],\"ghostVersion\":\"3.0\"}","html":"<!--kg-card-begin: markdown--><p><em>As somebody educated in Information Technology, I have always had an admiration of game developers.  Despite the interactive experience and immersive graphics, there is just some black magic in the architecture that leaves me in awe.  While I have had my hand at small games in the past, I decided that I would have a crack at <a href=\"http://en.wikipedia.org/wiki/Tetris\">Tetris</a>, using a real-time virtual controller.</em></p>\n<hr>\n<p><mark>This is reposted on the <a href=\"http://kaazing.org/blog/tetris-with-virtual-controller/\">Kaazing Open Source Blog</a>.</mark></p>\n<h3 id=\"research\">Research</h3>\n<p>While I wanted to solve the algorithms on my own, it never hurts to take a look at see what has been done already.  The first port I found with reasonably compact code, was done in <a href=\"http://inventwithpython.com/pygame/chapter7.html\">Python</a>.  A little more searching led me to one ported to <a href=\"http://codeincomplete.com/posts/2011/10/10/javascript_tetris/\">JavaScript</a>.</p>\n<p>The problem (for me) with both of these is that they used traditional graphics routines for the drawing.  While I suspect this is how the original Tetris is written, I wanted to make the game using the DOM for crisp rendering across platforms.</p>\n<blockquote>\n<p>Covering in detail how Tetris works is beyond the scope of this blog post.  Instead I will focus predominantly on tips I learned along the way, and implementing a virtual controller.</p>\n</blockquote>\n<p>I also felt that using the DOM would provide me with a game that was easier to style (using CSS, SVG).  This is important for my game because I wanted to be able to easily change the graphics to reflect holidays, conferences, and other special events.  Imagine animated GIFs for the color fills of the falling blocks.</p>\n<p>All the ports I found (without too much digging) also used the keyboard to control the game.  This would be fine if you are sitting at the same machine, but decoupling the game from the controller would bring new possibilities to the game.  I could for example use an Arduino or other hardware for my own custom controller.</p>\n<h3 id=\"gameboard\">Game Board</h3>\n<p>My approach resulted in two parts - the game board, and the game controller.  The game board would display all the visuals you might normally encounter when playing Tetris.  The game controller would be the buttons on a traditional physical controller that you might push to move the visual pieces around.</p>\n<p>The traditional game board is ~20 cells high and ~10 cells wide (depending on who you ask).  For my game board, I decided to stick with the 20 cells tall, but make the entire screen width the width of the game board.  <mark>Depending on your device, this could be upwards of 1,000 individual DOM elements to represent the board.</mark></p>\n<p>At first I tried using <mark>querySelector()</mark> to get the cells as they needed rendering.  This turned out to be far too slow.  <mark>Instead, I stored references to the cells as I created them to fit the device display.</mark>  The result is most certainly a high memory requirement, but also a much faster access to drawing the board (by styling the cells).</p>\n<p>I also originally went with <mark>setInterval()</mark> for my game clock.  In order to keep a reasonable frame rate during game play, I set a very low interval rate.  This really put a load on the CPU (fans spinning).  <mark>In the end, I went with requestAnimationFrame() which kept frame rates high and CPU usage much lower (no more spinning fans).</mark></p>\n<p>The game board itself has two representations.  There is the visual one you see while playing, but also a logical one that tracks where blocks have been placed.  <mark>Rather than create another 1,000 element array, I use data attributes on the cell reference array I already had.</mark>  Using getAttribute() and setAttribute() had no noticeable impact on game performance.</p>\n<p><img src=\"http://images.kevinhoyt.com/tetris.game.board.jpg\" alt=\"Tetris game board in action.\" loading=\"lazy\"></p>\n<h3 id=\"gamecontroller\">Game Controller</h3>\n<p>The hardest part of building the game controller was in managing the device orientation.  I was using my iPhone during development.  Full screen support for Web applications is flat out <a href=\"http://caniuse.com/#search=full\">not supported on iOS</a> - <strong>way to be Web-friendly Apple</strong>.  Also on Safari iOS8 the toolbars appear and disappear at what seems like their own timing.</p>\n<p>In the end, I decided on a height-to-width ratio that would fit regardless of the toolbars being present or not.  This still left me with plenty of room for game controls.  I did not put too much effort into styling, and instead played the game repeatedly until I had sizing that felt reliable and comfortable for my hands.</p>\n<blockquote>\n<p>Having some tactile feedback would really help with Web-based games.</p>\n</blockquote>\n<p>The game controller is designed such that it does not have to be a separate device.  The game can be played with two desktop browsers for example.  This means I need to listen for touch events, and react accordingly.  The controller also cares about when a button is pressed, and when it is released.</p>\n<p><img src=\"http://images.kevinhoyt.com/tetris.virtual.controller.jpg\" alt=\"Tetris virtual controller with and without Safari interference.\" loading=\"lazy\"></p>\n<h3 id=\"linkingboardandcontroller\">Linking Board and Controller</h3>\n<p>Once I had performance of the game board ironed out, and a placement of the game controller dialed in, the next part was to link the two devices.  This would normally be pretty tricky.  <mark>In fact, many native iOS games only allow for this over local wireless or Bluetooth</mark> - where the devices can search each other out.</p>\n<p>Thanks to the power of Kaazing Gateway, and the publish/subscribe pattern, the only thing the two devices need to communicate is a common messaging topic.</p>\n<p>When the game board loads, a <a href=\"http://davidshimjs.github.io/qrcodejs/\">QR code</a> is presented.  The contents of that QR code is a URL to the game controller.  That URL also has a query string appended to it that uniquely identifies that specific game.  This query string effectively represents the messaging topic.</p>\n<p>If the game controller device is a smartphone, a QR code scanner application can retrieve the URL (including the query string) from the game board.  This will in turn launch a browser on that device, and load that URL.  The game controller picks off the query string, and is then ready to publish to that specific message topic.</p>\n<p>In fact, once the controller has loaded, a message is sent to the game board to tell it to hide the QR code, and start the game.  As buttons are pressed and released, those events are passed as messages to Kaazing Gateway, which are in turn sent to that specific game board.  <a href=\"http://tetris.kevinhoyt.com\">Game on!</a></p>\n<h3 id=\"nextsteps\">Next Steps</h3>\n<p>It still very much surprises me how many native games (iOS or otherwise) still use local lookup mechanisms to establish a connection between two devices, and to communicate game state.  The <strong>Real-Time Web</strong> is here, and <mark>Kaazing Gateway</mark> would allow these games to <mark>securely</mark> use the open Internet without sacrificing communication <mark>performance</mark>.</p>\n<blockquote>\n<p>Kaazing Gateway also works across platforms, so you get a similar API across whatever technology stack you might choose to use.</p>\n</blockquote>\n<p>There are two directions I would like to take this project:</p>\n<ul>\n<li>\n<p>Introduce a network of decoupled hardware sensors, placed on various points on the body.  Want to move a piece left? Lift your left leg.  Want to rotate the piece clockwise?  Spin your left arm around.</p>\n</li>\n<li>\n<p>Another fun development would be head-to-head play.  Show two boards on the same screen at the same time.  Then let two virtual controllers join.  Rows removed from one player's board, end up on the other player's board.</p>\n</li>\n</ul>\n<p>I have posted the source code for the game to my <a href=\"https://github.com/krhoyt/Kaazing/tree/master/tetris\">personal GitHub account</a>.  Because Kaazing offers a free &quot;Sandbox&quot; instance for you to get started using, you can also have your hand at my port of Tetris <a href=\"http://tetris.kevinhoyt.com\">right now</a>.  Do not let request/response keep your applications limited, join the Real-Time Web today by checking out the <a href=\"http://kaazing.org/demos/quick-start/\">getting started guide</a>.</p>\n<!--kg-card-end: markdown-->","comment_id":"29","plaintext":"As somebody educated in Information Technology, I have always had an admiration\nof game developers. Despite the interactive experience and immersive graphics,\nthere is just some black magic in the architecture that leaves me in awe. While\nI have had my hand at small games in the past, I decided that I would have a\ncrack at Tetris [http://en.wikipedia.org/wiki/Tetris], using a real-time virtual\ncontroller.\n\n\n--------------------------------------------------------------------------------\n\nThis is reposted on the Kaazing Open Source Blog\n[http://kaazing.org/blog/tetris-with-virtual-controller/].\n\nResearch\nWhile I wanted to solve the algorithms on my own, it never hurts to take a look\nat see what has been done already. The first port I found with reasonably\ncompact code, was done in Python\n[http://inventwithpython.com/pygame/chapter7.html]. A little more searching led\nme to one ported to JavaScript\n[http://codeincomplete.com/posts/2011/10/10/javascript_tetris/].\n\nThe problem (for me) with both of these is that they used traditional graphics\nroutines for the drawing. While I suspect this is how the original Tetris is\nwritten, I wanted to make the game using the DOM for crisp rendering across\nplatforms.\n\n> Covering in detail how Tetris works is beyond the scope of this blog post.\nInstead I will focus predominantly on tips I learned along the way, and\nimplementing a virtual controller.\n\n\nI also felt that using the DOM would provide me with a game that was easier to\nstyle (using CSS, SVG). This is important for my game because I wanted to be\nable to easily change the graphics to reflect holidays, conferences, and other\nspecial events. Imagine animated GIFs for the color fills of the falling blocks.\n\nAll the ports I found (without too much digging) also used the keyboard to\ncontrol the game. This would be fine if you are sitting at the same machine, but\ndecoupling the game from the controller would bring new possibilities to the\ngame. I could for example use an Arduino or other hardware for my own custom\ncontroller.\n\nGame Board\nMy approach resulted in two parts - the game board, and the game controller. The\ngame board would display all the visuals you might normally encounter when\nplaying Tetris. The game controller would be the buttons on a traditional\nphysical controller that you might push to move the visual pieces around.\n\nThe traditional game board is ~20 cells high and ~10 cells wide (depending on\nwho you ask). For my game board, I decided to stick with the 20 cells tall, but\nmake the entire screen width the width of the game board. Depending on your\ndevice, this could be upwards of 1,000 individual DOM elements to represent the\nboard.\n\nAt first I tried using querySelector() to get the cells as they needed\nrendering. This turned out to be far too slow. Instead, I stored references to\nthe cells as I created them to fit the device display. The result is most\ncertainly a high memory requirement, but also a much faster access to drawing\nthe board (by styling the cells).\n\nI also originally went with setInterval() for my game clock. In order to keep a\nreasonable frame rate during game play, I set a very low interval rate. This\nreally put a load on the CPU (fans spinning). In the end, I went with\nrequestAnimationFrame() which kept frame rates high and CPU usage much lower (no\nmore spinning fans).\n\nThe game board itself has two representations. There is the visual one you see\nwhile playing, but also a logical one that tracks where blocks have been placed. \nRather than create another 1,000 element array, I use data attributes on the\ncell reference array I already had. Using getAttribute() and setAttribute() had\nno noticeable impact on game performance.\n\n\n\nGame Controller\nThe hardest part of building the game controller was in managing the device\norientation. I was using my iPhone during development. Full screen support for\nWeb applications is flat out not supported on iOS\n[http://caniuse.com/#search=full] - way to be Web-friendly Apple. Also on Safari\niOS8 the toolbars appear and disappear at what seems like their own timing.\n\nIn the end, I decided on a height-to-width ratio that would fit regardless of\nthe toolbars being present or not. This still left me with plenty of room for\ngame controls. I did not put too much effort into styling, and instead played\nthe game repeatedly until I had sizing that felt reliable and comfortable for my\nhands.\n\n> Having some tactile feedback would really help with Web-based games.\n\n\nThe game controller is designed such that it does not have to be a separate\ndevice. The game can be played with two desktop browsers for example. This means\nI need to listen for touch events, and react accordingly. The controller also\ncares about when a button is pressed, and when it is released.\n\n\n\nLinking Board and Controller\nOnce I had performance of the game board ironed out, and a placement of the game\ncontroller dialed in, the next part was to link the two devices. This would\nnormally be pretty tricky. In fact, many native iOS games only allow for this\nover local wireless or Bluetooth - where the devices can search each other out.\n\nThanks to the power of Kaazing Gateway, and the publish/subscribe pattern, the\nonly thing the two devices need to communicate is a common messaging topic.\n\nWhen the game board loads, a QR code [http://davidshimjs.github.io/qrcodejs/] is\npresented. The contents of that QR code is a URL to the game controller. That\nURL also has a query string appended to it that uniquely identifies that\nspecific game. This query string effectively represents the messaging topic.\n\nIf the game controller device is a smartphone, a QR code scanner application can\nretrieve the URL (including the query string) from the game board. This will in\nturn launch a browser on that device, and load that URL. The game controller\npicks off the query string, and is then ready to publish to that specific\nmessage topic.\n\nIn fact, once the controller has loaded, a message is sent to the game board to\ntell it to hide the QR code, and start the game. As buttons are pressed and\nreleased, those events are passed as messages to Kaazing Gateway, which are in\nturn sent to that specific game board. Game on! [http://tetris.kevinhoyt.com]\n\nNext Steps\nIt still very much surprises me how many native games (iOS or otherwise) still\nuse local lookup mechanisms to establish a connection between two devices, and\nto communicate game state. The Real-Time Web is here, and Kaazing Gateway would\nallow these games to securely use the open Internet without sacrificing\ncommunication performance.\n\n> Kaazing Gateway also works across platforms, so you get a similar API across\nwhatever technology stack you might choose to use.\n\n\nThere are two directions I would like to take this project:\n\n * Introduce a network of decoupled hardware sensors, placed on various points\n   on the body. Want to move a piece left? Lift your left leg. Want to rotate\n   the piece clockwise? Spin your left arm around.\n   \n   \n * Another fun development would be head-to-head play. Show two boards on the\n   same screen at the same time. Then let two virtual controllers join. Rows\n   removed from one player's board, end up on the other player's board.\n   \n   \n\nI have posted the source code for the game to my personal GitHub account\n[https://github.com/krhoyt/Kaazing/tree/master/tetris]. Because Kaazing offers a\nfree \"Sandbox\" instance for you to get started using, you can also have your\nhand at my port of Tetris right now [http://tetris.kevinhoyt.com]. Do not let\nrequest/response keep your applications limited, join the Real-Time Web today by\nchecking out the getting started guide [http://kaazing.org/demos/quick-start/].","feature_image":"__GHOST_URL__/content/images/2020/06/brick.wall.close.up.jpg","featured":0,"status":"published","locale":null,"visibility":"public","author_id":"1","created_at":"2015-06-01T19:14:06.000Z","updated_at":"2020-06-29T17:40:47.000Z","published_at":"2015-06-09T15:54:47.000Z","custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"type":"post","email_recipient_filter":"none"},{"id":"5adb8922351ffe0018a57729","uuid":"a90255f1-d935-4119-9093-df6cb0feb6e0","title":"Publish-Subscribe Everywhere","slug":"publish-subscribe-everywhere","mobiledoc":"{\"version\":\"0.3.1\",\"markups\":[],\"atoms\":[],\"cards\":[[\"markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\"*Kaazing Gateway brings many awesome new superpowers to your stack.  Among those is the ever-sexy enterprise WebSocket (pioneered at Kaazing) feature.  This feature allows you to tap into enterprise messaging systems, and leverage the [publish-subscribe pattern](http://en.wikipedia.org/wiki/Publish%E2%80%93subscribe_pattern) in your applications.  Publish-subscribe is catching on everywhere these days.  Here are a few examples.*\\n\\n---\\n\\n==This is reposted on the [Kaazing Open Source Blog](http://kaazing.org/blog/publish-subscribe-everywhere/).==\\n\\n###Android\\n\\nI recently posted a series of articles around a [barcode scanner project](http://blog.kevinhoyt.com/2015/05/15/real-time-barcode-scanner/) I was working on.  I started off with an actual physical barcode scanner, like you would find at a grocery store.  Later in the series I used the camera on an Android phone.\\n\\nWhen architecting the camera-based barcode scanner on Android, I found that Android really does not like you dealing with the network on the user interface thread.  It has a large stable of ways to help you decouple the UI thread from everything else.  This ranges from Tasks, Handlers, Intents and more.\\n\\nFor a long-running network operation - such as an always open WebSocket connection - the Android documentation suggests using the Handler class.  The question then becomes, how to get data from the Handler thread, back to the UI thread.  One of the answers is to use publish-subscribe in the form of the [Android MessageQueue](http://developer.android.com/reference/android/os/MessageQueue.html) class.\\n\\n==In short, publish-subscribe is the preferred means to communicate across processes in Android==.\\n\\n###Internet of Things\\n\\nThe Internet of Things is rapidly approaching in a big way.  You can read various [analyst reports](http://www.forbes.com/sites/gilpress/2014/08/22/internet-of-things-by-the-numbers-market-estimates-and-forecasts/) that suggest how many devices will be connected to the Internet in the next few years.  With SoC (system on a chip) controllers like the [Intel Edison](http://blog.kevinhoyt.com/2015/05/12/first-steps-with-intel-edison/) hitting the market, it is only a matter of time before we move on from simplistic applications such as thermostats, and move onto what is being called the Industrial Internet of Things.\\n\\nAt an industrial level, having silos of information in the form of single standalone devices, is simply unacceptable.  The possibilities only really start to open up when IoT devices can readily communicate with one another.  I [gave a talk](https://www.youtube.com/watch?v=wQ5r4iTNppw) along these lines at IoTa Conference in October of 2014.\\n\\nHow then are we supposed to let one device communicate with another?  Especially in a network topology where the devices do not know about one another?  Maybe even from different vendors?  ==The answer is to apply a pattern designed for decoupled communication - publish-subscribe.==\\n\\nIndeed, we see publish-subscribe starting to form the baseline for how IoT devices communicate with one another (over request/response) in the form of [MQTT](http://mqtt.org/).\\n\\n###Containers\\n\\nThe complicated mess that is the current state of affairs around deploying an application has given rise to many new technologies.  One of the more projects to surface on this front is [Docker](https://www.docker.com/).  Docker allows you to encapsulate application functionality into what is termed \\\"containers\\\".\\n\\n> If you have not played with Docker yet, it is definitely worth your time.  Might I suggest that you start with our very own [Kaazing Gateway on Docker](https://registry.hub.docker.com/u/kaazing/gateway/?utm_content=buffer03ffb&utm_medium=social&utm_source=twitter.com&utm_campaign=buffer)?\\n\\nContainers allow you to build isolated application functionality.  It is much like a virtual machine, without the machine dependency.  In order to be flexible, it is recommended that containers represent atomic functionality.  Two containers should know nothing about one another.\\n\\nDecoupling application functionality in this manner offers amazing benefits, but sometimes you need two containers to talk to one another.  ==The answer here is to create a container with a message broker, and then to use publish-subscribe to communicate between the two.==\\n\\n###Microservices\\n\\nA close cousin to containers is [microservices](http://en.wikipedia.org/wiki/Microservices).  Microservices are an application-level architecture that encourages the development of small, highly-focused, functionality.  Like containers, microservices should be designed completely independent of one-another, which communicate through language-agnostic APIs.\\n\\nWhat exactly is a language-agnostic API for decoupling application functionality?  ==It is no surprise here the publish-subscribe is once again the best answer.==\\n\\n###Reactive\\n\\nNot to be confused with [React.js](https://facebook.github.io/react/) (an excellent application framework), reactive application development means developing flexible, loosely-coupled, and scalable, applications.  There is a whole school of thought around how this can be, and should be, accomplished.  I would refer you to the [Reactive Manifesto](http://www.reactivemanifesto.org/) to get the entire picture.\\n\\nIt should come as no surprise however that reactive systems are suggested to be message driven.  ==Developing reactive systems depends on publish-subscribe.==  Reactive systems are not just theory.  Companies like the [New York Times](http://www.techrepublic.com/article/how-the-new-york-times-uses-reactive-programming-tools-like-scala-to-scale/) use the approach to scale their infrastructure.\\n\\n###Big Data\\n\\nIf you start pairing consumer and industry trends, it will not be long before you stumble across the problem of [big data](http://en.wikipedia.org/wiki/Big_data).  Industrial sensors are everywhere (planes, trains, and automobiles).  Modern phones are powerful computing platforms.  Vendors are lining up wearable computing offerings, and expansive verticals such a health care a lining up for a crack at the data.\\n\\nBefore long you reach a point where request-response simply does not scale.  Rather than dealing with some of the data some of the time, the most efficient solution actually means handling all the data, all the time, and then raising evens when something interesting happens.\\n\\nRaising an event from one system, written on one technology stack, to be handled by another system, perhaps written on an entirely different technology stack means adopting the publish-subscribe pattern.  Your enterprise should never have to settle with an occasional snapshot of some of the data available to it.  ==Reconsidering the incumbent request-response for the more scalable and nimble publish-subscribe should be at the top of your list when tackling big data problems.==\\n\\n###Next Steps\\n\\nWhen I joined Kaazing just over a year ago, it seemed that publish-subscribe was something that interested developers, but that few were actively embracing.  Fast-forward a year and we see publish-subscribe being adopted everywhere.  ==Kaazing Gateway provides enterprise-grade WebSocket built with publish-subscribe in mind.==\\n\\n> While the underpinning of Kaazing Gateway is WebSocket, that does not mean it is limited only to Web applications.  With Kaazing Gateway, you can use publish-subscribe everywhere to conquer all your needs from IoT to big data.\\n\\nIf you have not yet started digging into implementing a publish-subscribe system, you can take a look at our getting [started tutorial](http://kaazing.org/demos/quick-start/).  This will have you up and running with the basic concepts in no time at all.  From there, our awesome [developer documentation](http://developer.kaazing.com/documentation/5.0/index.html) will take you the rest of the way.  If you looking for examples, be sure to check out my [personal GitHub repository](https://github.com/krhoyt/Kaazing) as well. \\n\"}]],\"sections\":[[10,0]],\"ghostVersion\":\"3.0\"}","html":"<!--kg-card-begin: markdown--><p><em>Kaazing Gateway brings many awesome new superpowers to your stack.  Among those is the ever-sexy enterprise WebSocket (pioneered at Kaazing) feature.  This feature allows you to tap into enterprise messaging systems, and leverage the <a href=\"http://en.wikipedia.org/wiki/Publish%E2%80%93subscribe_pattern\">publish-subscribe pattern</a> in your applications.  Publish-subscribe is catching on everywhere these days.  Here are a few examples.</em></p>\n<hr>\n<p><mark>This is reposted on the <a href=\"http://kaazing.org/blog/publish-subscribe-everywhere/\">Kaazing Open Source Blog</a>.</mark></p>\n<h3 id=\"android\">Android</h3>\n<p>I recently posted a series of articles around a <a href=\"http://blog.kevinhoyt.com/2015/05/15/real-time-barcode-scanner/\">barcode scanner project</a> I was working on.  I started off with an actual physical barcode scanner, like you would find at a grocery store.  Later in the series I used the camera on an Android phone.</p>\n<p>When architecting the camera-based barcode scanner on Android, I found that Android really does not like you dealing with the network on the user interface thread.  It has a large stable of ways to help you decouple the UI thread from everything else.  This ranges from Tasks, Handlers, Intents and more.</p>\n<p>For a long-running network operation - such as an always open WebSocket connection - the Android documentation suggests using the Handler class.  The question then becomes, how to get data from the Handler thread, back to the UI thread.  One of the answers is to use publish-subscribe in the form of the <a href=\"http://developer.android.com/reference/android/os/MessageQueue.html\">Android MessageQueue</a> class.</p>\n<p><mark>In short, publish-subscribe is the preferred means to communicate across processes in Android</mark>.</p>\n<h3 id=\"internetofthings\">Internet of Things</h3>\n<p>The Internet of Things is rapidly approaching in a big way.  You can read various <a href=\"http://www.forbes.com/sites/gilpress/2014/08/22/internet-of-things-by-the-numbers-market-estimates-and-forecasts/\">analyst reports</a> that suggest how many devices will be connected to the Internet in the next few years.  With SoC (system on a chip) controllers like the <a href=\"http://blog.kevinhoyt.com/2015/05/12/first-steps-with-intel-edison/\">Intel Edison</a> hitting the market, it is only a matter of time before we move on from simplistic applications such as thermostats, and move onto what is being called the Industrial Internet of Things.</p>\n<p>At an industrial level, having silos of information in the form of single standalone devices, is simply unacceptable.  The possibilities only really start to open up when IoT devices can readily communicate with one another.  I <a href=\"https://www.youtube.com/watch?v=wQ5r4iTNppw\">gave a talk</a> along these lines at IoTa Conference in October of 2014.</p>\n<p>How then are we supposed to let one device communicate with another?  Especially in a network topology where the devices do not know about one another?  Maybe even from different vendors?  <mark>The answer is to apply a pattern designed for decoupled communication - publish-subscribe.</mark></p>\n<p>Indeed, we see publish-subscribe starting to form the baseline for how IoT devices communicate with one another (over request/response) in the form of <a href=\"http://mqtt.org/\">MQTT</a>.</p>\n<h3 id=\"containers\">Containers</h3>\n<p>The complicated mess that is the current state of affairs around deploying an application has given rise to many new technologies.  One of the more projects to surface on this front is <a href=\"https://www.docker.com/\">Docker</a>.  Docker allows you to encapsulate application functionality into what is termed &quot;containers&quot;.</p>\n<blockquote>\n<p>If you have not played with Docker yet, it is definitely worth your time.  Might I suggest that you start with our very own <a href=\"https://registry.hub.docker.com/u/kaazing/gateway/?utm_content=buffer03ffb&amp;utm_medium=social&amp;utm_source=twitter.com&amp;utm_campaign=buffer\">Kaazing Gateway on Docker</a>?</p>\n</blockquote>\n<p>Containers allow you to build isolated application functionality.  It is much like a virtual machine, without the machine dependency.  In order to be flexible, it is recommended that containers represent atomic functionality.  Two containers should know nothing about one another.</p>\n<p>Decoupling application functionality in this manner offers amazing benefits, but sometimes you need two containers to talk to one another.  <mark>The answer here is to create a container with a message broker, and then to use publish-subscribe to communicate between the two.</mark></p>\n<h3 id=\"microservices\">Microservices</h3>\n<p>A close cousin to containers is <a href=\"http://en.wikipedia.org/wiki/Microservices\">microservices</a>.  Microservices are an application-level architecture that encourages the development of small, highly-focused, functionality.  Like containers, microservices should be designed completely independent of one-another, which communicate through language-agnostic APIs.</p>\n<p>What exactly is a language-agnostic API for decoupling application functionality?  <mark>It is no surprise here the publish-subscribe is once again the best answer.</mark></p>\n<h3 id=\"reactive\">Reactive</h3>\n<p>Not to be confused with <a href=\"https://facebook.github.io/react/\">React.js</a> (an excellent application framework), reactive application development means developing flexible, loosely-coupled, and scalable, applications.  There is a whole school of thought around how this can be, and should be, accomplished.  I would refer you to the <a href=\"http://www.reactivemanifesto.org/\">Reactive Manifesto</a> to get the entire picture.</p>\n<p>It should come as no surprise however that reactive systems are suggested to be message driven.  <mark>Developing reactive systems depends on publish-subscribe.</mark>  Reactive systems are not just theory.  Companies like the <a href=\"http://www.techrepublic.com/article/how-the-new-york-times-uses-reactive-programming-tools-like-scala-to-scale/\">New York Times</a> use the approach to scale their infrastructure.</p>\n<h3 id=\"bigdata\">Big Data</h3>\n<p>If you start pairing consumer and industry trends, it will not be long before you stumble across the problem of <a href=\"http://en.wikipedia.org/wiki/Big_data\">big data</a>.  Industrial sensors are everywhere (planes, trains, and automobiles).  Modern phones are powerful computing platforms.  Vendors are lining up wearable computing offerings, and expansive verticals such a health care a lining up for a crack at the data.</p>\n<p>Before long you reach a point where request-response simply does not scale.  Rather than dealing with some of the data some of the time, the most efficient solution actually means handling all the data, all the time, and then raising evens when something interesting happens.</p>\n<p>Raising an event from one system, written on one technology stack, to be handled by another system, perhaps written on an entirely different technology stack means adopting the publish-subscribe pattern.  Your enterprise should never have to settle with an occasional snapshot of some of the data available to it.  <mark>Reconsidering the incumbent request-response for the more scalable and nimble publish-subscribe should be at the top of your list when tackling big data problems.</mark></p>\n<h3 id=\"nextsteps\">Next Steps</h3>\n<p>When I joined Kaazing just over a year ago, it seemed that publish-subscribe was something that interested developers, but that few were actively embracing.  Fast-forward a year and we see publish-subscribe being adopted everywhere.  <mark>Kaazing Gateway provides enterprise-grade WebSocket built with publish-subscribe in mind.</mark></p>\n<blockquote>\n<p>While the underpinning of Kaazing Gateway is WebSocket, that does not mean it is limited only to Web applications.  With Kaazing Gateway, you can use publish-subscribe everywhere to conquer all your needs from IoT to big data.</p>\n</blockquote>\n<p>If you have not yet started digging into implementing a publish-subscribe system, you can take a look at our getting <a href=\"http://kaazing.org/demos/quick-start/\">started tutorial</a>.  This will have you up and running with the basic concepts in no time at all.  From there, our awesome <a href=\"http://developer.kaazing.com/documentation/5.0/index.html\">developer documentation</a> will take you the rest of the way.  If you looking for examples, be sure to check out my <a href=\"https://github.com/krhoyt/Kaazing\">personal GitHub repository</a> as well.</p>\n<!--kg-card-end: markdown-->","comment_id":"30","plaintext":"Kaazing Gateway brings many awesome new superpowers to your stack. Among those\nis the ever-sexy enterprise WebSocket (pioneered at Kaazing) feature. This\nfeature allows you to tap into enterprise messaging systems, and leverage the \npublish-subscribe pattern\n[http://en.wikipedia.org/wiki/Publish%E2%80%93subscribe_pattern] in your\napplications. Publish-subscribe is catching on everywhere these days. Here are a\nfew examples.\n\n\n--------------------------------------------------------------------------------\n\nThis is reposted on the Kaazing Open Source Blog\n[http://kaazing.org/blog/publish-subscribe-everywhere/].\n\nAndroid\nI recently posted a series of articles around a barcode scanner project\n[http://blog.kevinhoyt.com/2015/05/15/real-time-barcode-scanner/] I was working\non. I started off with an actual physical barcode scanner, like you would find\nat a grocery store. Later in the series I used the camera on an Android phone.\n\nWhen architecting the camera-based barcode scanner on Android, I found that\nAndroid really does not like you dealing with the network on the user interface\nthread. It has a large stable of ways to help you decouple the UI thread from\neverything else. This ranges from Tasks, Handlers, Intents and more.\n\nFor a long-running network operation - such as an always open WebSocket\nconnection - the Android documentation suggests using the Handler class. The\nquestion then becomes, how to get data from the Handler thread, back to the UI\nthread. One of the answers is to use publish-subscribe in the form of the \nAndroid MessageQueue\n[http://developer.android.com/reference/android/os/MessageQueue.html] class.\n\nIn short, publish-subscribe is the preferred means to communicate across\nprocesses in Android.\n\nInternet of Things\nThe Internet of Things is rapidly approaching in a big way. You can read various \nanalyst reports\n[http://www.forbes.com/sites/gilpress/2014/08/22/internet-of-things-by-the-numbers-market-estimates-and-forecasts/] \nthat suggest how many devices will be connected to the Internet in the next few\nyears. With SoC (system on a chip) controllers like the Intel Edison\n[http://blog.kevinhoyt.com/2015/05/12/first-steps-with-intel-edison/] hitting\nthe market, it is only a matter of time before we move on from simplistic\napplications such as thermostats, and move onto what is being called the\nIndustrial Internet of Things.\n\nAt an industrial level, having silos of information in the form of single\nstandalone devices, is simply unacceptable. The possibilities only really start\nto open up when IoT devices can readily communicate with one another. I gave a\ntalk [https://www.youtube.com/watch?v=wQ5r4iTNppw] along these lines at IoTa\nConference in October of 2014.\n\nHow then are we supposed to let one device communicate with another? Especially\nin a network topology where the devices do not know about one another? Maybe\neven from different vendors? The answer is to apply a pattern designed for\ndecoupled communication - publish-subscribe.\n\nIndeed, we see publish-subscribe starting to form the baseline for how IoT\ndevices communicate with one another (over request/response) in the form of MQTT\n[http://mqtt.org/].\n\nContainers\nThe complicated mess that is the current state of affairs around deploying an\napplication has given rise to many new technologies. One of the more projects to\nsurface on this front is Docker [https://www.docker.com/]. Docker allows you to\nencapsulate application functionality into what is termed \"containers\".\n\n> If you have not played with Docker yet, it is definitely worth your time. Might\nI suggest that you start with our very own Kaazing Gateway on Docker\n[https://registry.hub.docker.com/u/kaazing/gateway/?utm_content=buffer03ffb&utm_medium=social&utm_source=twitter.com&utm_campaign=buffer]\n?\n\n\nContainers allow you to build isolated application functionality. It is much\nlike a virtual machine, without the machine dependency. In order to be flexible,\nit is recommended that containers represent atomic functionality. Two containers\nshould know nothing about one another.\n\nDecoupling application functionality in this manner offers amazing benefits, but\nsometimes you need two containers to talk to one another. The answer here is to\ncreate a container with a message broker, and then to use publish-subscribe to\ncommunicate between the two.\n\nMicroservices\nA close cousin to containers is microservices\n[http://en.wikipedia.org/wiki/Microservices]. Microservices are an\napplication-level architecture that encourages the development of small,\nhighly-focused, functionality. Like containers, microservices should be designed\ncompletely independent of one-another, which communicate through\nlanguage-agnostic APIs.\n\nWhat exactly is a language-agnostic API for decoupling application\nfunctionality? It is no surprise here the publish-subscribe is once again the\nbest answer.\n\nReactive\nNot to be confused with React.js [https://facebook.github.io/react/] (an\nexcellent application framework), reactive application development means\ndeveloping flexible, loosely-coupled, and scalable, applications. There is a\nwhole school of thought around how this can be, and should be, accomplished. I\nwould refer you to the Reactive Manifesto [http://www.reactivemanifesto.org/] to\nget the entire picture.\n\nIt should come as no surprise however that reactive systems are suggested to be\nmessage driven. Developing reactive systems depends on publish-subscribe. \nReactive systems are not just theory. Companies like the New York Times\n[http://www.techrepublic.com/article/how-the-new-york-times-uses-reactive-programming-tools-like-scala-to-scale/] \nuse the approach to scale their infrastructure.\n\nBig Data\nIf you start pairing consumer and industry trends, it will not be long before\nyou stumble across the problem of big data\n[http://en.wikipedia.org/wiki/Big_data]. Industrial sensors are everywhere\n(planes, trains, and automobiles). Modern phones are powerful computing\nplatforms. Vendors are lining up wearable computing offerings, and expansive\nverticals such a health care a lining up for a crack at the data.\n\nBefore long you reach a point where request-response simply does not scale.\nRather than dealing with some of the data some of the time, the most efficient\nsolution actually means handling all the data, all the time, and then raising\nevens when something interesting happens.\n\nRaising an event from one system, written on one technology stack, to be handled\nby another system, perhaps written on an entirely different technology stack\nmeans adopting the publish-subscribe pattern. Your enterprise should never have\nto settle with an occasional snapshot of some of the data available to it. \nReconsidering the incumbent request-response for the more scalable and nimble\npublish-subscribe should be at the top of your list when tackling big data\nproblems.\n\nNext Steps\nWhen I joined Kaazing just over a year ago, it seemed that publish-subscribe was\nsomething that interested developers, but that few were actively embracing.\nFast-forward a year and we see publish-subscribe being adopted everywhere. \nKaazing Gateway provides enterprise-grade WebSocket built with publish-subscribe\nin mind.\n\n> While the underpinning of Kaazing Gateway is WebSocket, that does not mean it is\nlimited only to Web applications. With Kaazing Gateway, you can use\npublish-subscribe everywhere to conquer all your needs from IoT to big data.\n\n\nIf you have not yet started digging into implementing a publish-subscribe\nsystem, you can take a look at our getting started tutorial\n[http://kaazing.org/demos/quick-start/]. This will have you up and running with\nthe basic concepts in no time at all. From there, our awesome developer\ndocumentation [http://developer.kaazing.com/documentation/5.0/index.html] will\ntake you the rest of the way. If you looking for examples, be sure to check out\nmy personal GitHub repository [https://github.com/krhoyt/Kaazing] as well.","feature_image":"http://images.kevinhoyt.com/publish.subscribe.everywhere.jpg","featured":0,"status":"published","locale":null,"visibility":"public","author_id":"1","created_at":"2015-06-08T18:40:36.000Z","updated_at":"2015-06-18T23:23:02.000Z","published_at":"2015-06-12T14:05:35.000Z","custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"type":"post","email_recipient_filter":"none"},{"id":"5adb8922351ffe0018a5772a","uuid":"2145157f-28a4-48c0-9b03-d3c647eb52f7","title":"Particle Photon","slug":"particle-photon","mobiledoc":"{\"version\":\"0.3.1\",\"markups\":[],\"atoms\":[],\"cards\":[[\"markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\"*The company Spark, which became popular with their Core product, has recently changed the name of their company to Particle.  The successor to the Spark Core then is the [Particle Photon](https://www.particle.io/prototype#photon).  I have received two Photon units and have been putting them through their paces over the past week.*\\n\\n---\\n\\n![Particle Photon unboxing](http://images.kevinhoyt.com/particle.photon.unboxing.jpg)\\n\\nThe biggest change from the Core to the Photon, for most practical purposes, is the move to a new wireless chip - the ==Broadcom BCM43362==.  That may seem like an insignificant change, but if you consider that the whole purpose of the Photon is to connect to the Web, then this is effectively open heart surgery.\\n\\nI had two main complaints about the Spark Core.\\n\\n* The CC3000 wireless chip came with numerous limitations, not the least of which was no 802.11n support.\\n* The mechanism for pairing was a clumsy back door on the 802.11b/g  which is unlike most other devices.\\n\\n###Wireless Support\\n\\nThe new Particle Photon supports ==802.11b/g/n== networks.  Since I run my home on 802.11n, this means that I do not have to switch over to older standards to put the Photon on my network.  \\n\\nI have also found that connecting to the network happens much faster.  This is a big deal when you are developing a project.\\n\\nNew firmware is delivered to the Photon via the Particle Cloud (just like the Core before it).  The Photon receives the new firmware, flashes it, and then reboots.  That reboot process takes the device off the network (true for both the Photon and the Core).  The device then needs to reconnect to the network.\\n\\nIf you are compiling new code frequently, the time it takes to reboot and reconnect can really hamper your momentum.  With the Photon however, the process of uploading new firmware, and reconnecting to the Internet, is is ==almost as fast as uploading locally to an Arduino==.\\n\\n###Pairing\\n\\nThis is easily my favorite new feature of the Photon.  It seems like a simple thing, but when your device has no screen or keyboard, making sure this goes right is of the utmost importance for customer success.  If your customer cannot get their fancy new device on the Internet, and they have just bought a brick, you are not going to hear the end of it.\\n\\nWhen you first plug in the Photon, it creates its own wireless network.  This is a technique known as Soft AP for soft access point.    From here you connect to the Photon's network using your smartphone.  Using the Particle application, you can then tell the Photon about your network.  From there, the Photon will connect to your network, and you can access it in the Build tool.\\n\\n> You can also tell the Photon about the network settings using the Particle CLI.\\n\\nThis is the type of configuration that the Google Nest uses.  Or even if you have used an Arduino Yun.  The broad precedent helps to ensure that your customer knows how to connect their shiny new device, or can easily find support to get there.\\n\\nThe one drawback to this approach right now, for Particle, is the lack of production SDKs.  \\n\\nThat is to say that you do not want to tell your end customer to use the Particle application to configure their network settings - you want your end customer to use your own application.  This means embedding an SDK to configure your Photon device, into your application.  Right now the only option is iOS, which is beta at that.\\n\\n![Particle Tinker application](http://images.kevinhoyt.com/particle.tinker.application.png)\\n\\nTheoretically, any computing device with a browser should be enough to configure network settings if Soft AP was done thoroughly.  Being able to use a browser desktop for example would give your customers a much greater rate of success.  You could embed videos, detailed image stills, and more textual descriptions with the extra screen realty.\\n\\nTo be fair, the Spark Core did not do this either.\\n\\n###Moving Forward\\n\\nAt this point, I would pretty much always recommend ==starting new Arduino-esque projects with the Particle Photon==.  The horsepower is there.  The connectivity the Arduino never had properly done.  SparkFun even offers shields for the Photon ranging from [weather](https://www.sparkfun.com/products/13630), to [battery](https://www.sparkfun.com/products/13626), to [micro LED](https://www.sparkfun.com/products/13628), and more.\\n\\n>The Photon has also been shrunk down into SMD form-factors called the ==[P0 and the P1](https://www.particle.io/prototype#p0-and-p1)==.  This makes it very easy to move from project to product.\\n\\nAt ==$19.00== for a Photon with headers, you are actually $5.00 less than the Arduino Uno right out of the gate.  On top of that you get wireless, which could run you as little as $8.00 or as much as $75.00 depending on the amount of work you want to put into it.  Pile on cloud deployment, a rich firmware API, as extensive library for sensors, and more, and there is just no beating the Photon for getting started with IoT.\\n\"}]],\"sections\":[[10,0]],\"ghostVersion\":\"3.0\"}","html":"<!--kg-card-begin: markdown--><p><em>The company Spark, which became popular with their Core product, has recently changed the name of their company to Particle.  The successor to the Spark Core then is the <a href=\"https://www.particle.io/prototype#photon\">Particle Photon</a>.  I have received two Photon units and have been putting them through their paces over the past week.</em></p>\n<hr>\n<p><img src=\"http://images.kevinhoyt.com/particle.photon.unboxing.jpg\" alt=\"Particle Photon unboxing\" loading=\"lazy\"></p>\n<p>The biggest change from the Core to the Photon, for most practical purposes, is the move to a new wireless chip - the <mark>Broadcom BCM43362</mark>.  That may seem like an insignificant change, but if you consider that the whole purpose of the Photon is to connect to the Web, then this is effectively open heart surgery.</p>\n<p>I had two main complaints about the Spark Core.</p>\n<ul>\n<li>The CC3000 wireless chip came with numerous limitations, not the least of which was no 802.11n support.</li>\n<li>The mechanism for pairing was a clumsy back door on the 802.11b/g  which is unlike most other devices.</li>\n</ul>\n<h3 id=\"wirelesssupport\">Wireless Support</h3>\n<p>The new Particle Photon supports <mark>802.11b/g/n</mark> networks.  Since I run my home on 802.11n, this means that I do not have to switch over to older standards to put the Photon on my network.</p>\n<p>I have also found that connecting to the network happens much faster.  This is a big deal when you are developing a project.</p>\n<p>New firmware is delivered to the Photon via the Particle Cloud (just like the Core before it).  The Photon receives the new firmware, flashes it, and then reboots.  That reboot process takes the device off the network (true for both the Photon and the Core).  The device then needs to reconnect to the network.</p>\n<p>If you are compiling new code frequently, the time it takes to reboot and reconnect can really hamper your momentum.  With the Photon however, the process of uploading new firmware, and reconnecting to the Internet, is is <mark>almost as fast as uploading locally to an Arduino</mark>.</p>\n<h3 id=\"pairing\">Pairing</h3>\n<p>This is easily my favorite new feature of the Photon.  It seems like a simple thing, but when your device has no screen or keyboard, making sure this goes right is of the utmost importance for customer success.  If your customer cannot get their fancy new device on the Internet, and they have just bought a brick, you are not going to hear the end of it.</p>\n<p>When you first plug in the Photon, it creates its own wireless network.  This is a technique known as Soft AP for soft access point.    From here you connect to the Photon's network using your smartphone.  Using the Particle application, you can then tell the Photon about your network.  From there, the Photon will connect to your network, and you can access it in the Build tool.</p>\n<blockquote>\n<p>You can also tell the Photon about the network settings using the Particle CLI.</p>\n</blockquote>\n<p>This is the type of configuration that the Google Nest uses.  Or even if you have used an Arduino Yun.  The broad precedent helps to ensure that your customer knows how to connect their shiny new device, or can easily find support to get there.</p>\n<p>The one drawback to this approach right now, for Particle, is the lack of production SDKs.</p>\n<p>That is to say that you do not want to tell your end customer to use the Particle application to configure their network settings - you want your end customer to use your own application.  This means embedding an SDK to configure your Photon device, into your application.  Right now the only option is iOS, which is beta at that.</p>\n<p><img src=\"http://images.kevinhoyt.com/particle.tinker.application.png\" alt=\"Particle Tinker application\" loading=\"lazy\"></p>\n<p>Theoretically, any computing device with a browser should be enough to configure network settings if Soft AP was done thoroughly.  Being able to use a browser desktop for example would give your customers a much greater rate of success.  You could embed videos, detailed image stills, and more textual descriptions with the extra screen realty.</p>\n<p>To be fair, the Spark Core did not do this either.</p>\n<h3 id=\"movingforward\">Moving Forward</h3>\n<p>At this point, I would pretty much always recommend <mark>starting new Arduino-esque projects with the Particle Photon</mark>.  The horsepower is there.  The connectivity the Arduino never had properly done.  SparkFun even offers shields for the Photon ranging from <a href=\"https://www.sparkfun.com/products/13630\">weather</a>, to <a href=\"https://www.sparkfun.com/products/13626\">battery</a>, to <a href=\"https://www.sparkfun.com/products/13628\">micro LED</a>, and more.</p>\n<blockquote>\n<p>The Photon has also been shrunk down into SMD form-factors called the <mark><a href=\"https://www.particle.io/prototype#p0-and-p1\">P0 and the P1</a></mark>.  This makes it very easy to move from project to product.</p>\n</blockquote>\n<p>At <mark>$19.00</mark> for a Photon with headers, you are actually $5.00 less than the Arduino Uno right out of the gate.  On top of that you get wireless, which could run you as little as $8.00 or as much as $75.00 depending on the amount of work you want to put into it.  Pile on cloud deployment, a rich firmware API, as extensive library for sensors, and more, and there is just no beating the Photon for getting started with IoT.</p>\n<!--kg-card-end: markdown-->","comment_id":"31","plaintext":"The company Spark, which became popular with their Core product, has recently\nchanged the name of their company to Particle. The successor to the Spark Core\nthen is the Particle Photon [https://www.particle.io/prototype#photon]. I have\nreceived two Photon units and have been putting them through their paces over\nthe past week.\n\n\n--------------------------------------------------------------------------------\n\n\n\nThe biggest change from the Core to the Photon, for most practical purposes, is\nthe move to a new wireless chip - the Broadcom BCM43362. That may seem like an\ninsignificant change, but if you consider that the whole purpose of the Photon\nis to connect to the Web, then this is effectively open heart surgery.\n\nI had two main complaints about the Spark Core.\n\n * The CC3000 wireless chip came with numerous limitations, not the least of\n   which was no 802.11n support.\n * The mechanism for pairing was a clumsy back door on the 802.11b/g which is\n   unlike most other devices.\n\nWireless Support\nThe new Particle Photon supports 802.11b/g/n networks. Since I run my home on\n802.11n, this means that I do not have to switch over to older standards to put\nthe Photon on my network.\n\nI have also found that connecting to the network happens much faster. This is a\nbig deal when you are developing a project.\n\nNew firmware is delivered to the Photon via the Particle Cloud (just like the\nCore before it). The Photon receives the new firmware, flashes it, and then\nreboots. That reboot process takes the device off the network (true for both the\nPhoton and the Core). The device then needs to reconnect to the network.\n\nIf you are compiling new code frequently, the time it takes to reboot and\nreconnect can really hamper your momentum. With the Photon however, the process\nof uploading new firmware, and reconnecting to the Internet, is is almost as\nfast as uploading locally to an Arduino.\n\nPairing\nThis is easily my favorite new feature of the Photon. It seems like a simple\nthing, but when your device has no screen or keyboard, making sure this goes\nright is of the utmost importance for customer success. If your customer cannot\nget their fancy new device on the Internet, and they have just bought a brick,\nyou are not going to hear the end of it.\n\nWhen you first plug in the Photon, it creates its own wireless network. This is\na technique known as Soft AP for soft access point. From here you connect to the\nPhoton's network using your smartphone. Using the Particle application, you can\nthen tell the Photon about your network. From there, the Photon will connect to\nyour network, and you can access it in the Build tool.\n\n> You can also tell the Photon about the network settings using the Particle CLI.\n\n\nThis is the type of configuration that the Google Nest uses. Or even if you have\nused an Arduino Yun. The broad precedent helps to ensure that your customer\nknows how to connect their shiny new device, or can easily find support to get\nthere.\n\nThe one drawback to this approach right now, for Particle, is the lack of\nproduction SDKs.\n\nThat is to say that you do not want to tell your end customer to use the\nParticle application to configure their network settings - you want your end\ncustomer to use your own application. This means embedding an SDK to configure\nyour Photon device, into your application. Right now the only option is iOS,\nwhich is beta at that.\n\n\n\nTheoretically, any computing device with a browser should be enough to configure\nnetwork settings if Soft AP was done thoroughly. Being able to use a browser\ndesktop for example would give your customers a much greater rate of success.\nYou could embed videos, detailed image stills, and more textual descriptions\nwith the extra screen realty.\n\nTo be fair, the Spark Core did not do this either.\n\nMoving Forward\nAt this point, I would pretty much always recommend starting new Arduino-esque\nprojects with the Particle Photon. The horsepower is there. The connectivity the\nArduino never had properly done. SparkFun even offers shields for the Photon\nranging from weather [https://www.sparkfun.com/products/13630], to battery\n[https://www.sparkfun.com/products/13626], to micro LED\n[https://www.sparkfun.com/products/13628], and more.\n\n> The Photon has also been shrunk down into SMD form-factors called the P0 and\nthe\nP1 [https://www.particle.io/prototype#p0-and-p1]. This makes it very easy to\nmove from project to product.\n\n\nAt $19.00 for a Photon with headers, you are actually $5.00 less than the\nArduino Uno right out of the gate. On top of that you get wireless, which could\nrun you as little as $8.00 or as much as $75.00 depending on the amount of work\nyou want to put into it. Pile on cloud deployment, a rich firmware API, as\nextensive library for sensors, and more, and there is just no beating the Photon\nfor getting started with IoT.","feature_image":"http://images.kevinhoyt.com/photon.dance.zwopper.jpg","featured":0,"status":"published","locale":null,"visibility":"public","author_id":"1","created_at":"2015-06-09T18:14:51.000Z","updated_at":"2015-06-15T22:31:02.000Z","published_at":"2015-06-15T22:28:55.000Z","custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"type":"post","email_recipient_filter":"none"},{"id":"5adb8922351ffe0018a5772b","uuid":"ec6a094c-5a3b-4798-a090-52e5cf46d812","title":"Upload a Web Camera Image","slug":"webcam-to-php-upload","mobiledoc":"{\"version\":\"0.3.1\",\"markups\":[],\"atoms\":[],\"cards\":[[\"markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\"*Long ago, in a galaxy far, far away, Flash Player gave developers access to the user's web camera.  This 2002 feature is [just starting](http://caniuse.com/#search=getUserMedia) to become available to the Web.  At the same time, HTML5 Canvas brings pixel level manipulation of content to the table.  Throw in a dash of server processing (in this case PHP), and you have the ability to upload, save, and retrieve images from a web camera.*\\n\\n---\\n\\n###Video Element\\n\\nBefore we get into scripting web camera access, we will need a place to display the video feed.  This happens using the video element in your HTML.  You can give it an ID or class to query it later, or if you are only going to have one video element, just leave it be and query by the tag name.\\n\\n```\\n<!-- Video stream -->\\n<video>Video stream not available.</video>  \\n```\\n\\nYou will notice that there is no size specified.  There are many variables to consider here, not the least of which is the resolution of the web camera itself, and the browser support for those resolutions.  ==In my experience, 640x480 was the most common resulting resolution.==\\n\\nRather than specify the dimensions now, we will come back to how to detect and size the video element in a moment.  For now, wire up a global reference to the video element in JavaScript.  In the following code, the reference I use is called \\\"video\\\".\\n\\n```\\nvar video = document.querySelector( 'video' );\\n```\\n\\n###Access the Web Camera\\n\\nThe main approach to accessing a web camera from JavaScript is through a method named ==getUserMedia()==.  Depending on the browser, this may be on the ==Navigator== object (old), or the ==MediaDevices== object.  Regardless, you probably want a little polyfill work to get started.\\n\\nFor my testing, I went with:\\n\\n```\\n// Polyfill browser differences\\nnavigator.getMedia = ( \\n  navigator.getUserMedia ||\\n  navigator.webkitGetUserMedia ||\\n  navigator.mozGetUserMedia ||\\n  navigator.msGetUserMedia \\n);  \\n  \\n// Start video stream  \\nnavigator.getMedia( \\n  {\\n    video: true,\\n    audio: false\\n  },\\n  doMediaStream,\\n  doMediaError\\n);\\n```\\n\\nIn my code, the method ==doMediaStream()== is called when the user has given access to the web camera.  That means we can get a reference to the video stream, and start playing it.\\n\\n```\\n// Called when media stream is successful\\n// Starts watch for video element sizing\\n// Starts stream to video element\\nfunction doMediaStream( stream )\\n{  \\n  var url = null;\\n\\n  // Debug\\n  console.log( 'Media stream linked.' );  \\n  \\n  // Watch for video dimensions to be set\\n  interval = setInterval( doMediaPlay, 100 );\\n  \\n  // Polyfill browser differences\\n  if( navigator.mozGetUserMedia ) \\n  {\\n    video.mozSrcObject = stream;\\n  } else {\\n    url = window.URL || window.webkitURL;\\n    video.src = url.createObjectURL( stream );\\n  }\\n  \\n  // Start the stream\\n  video.play();  \\n}\\n```\\n\\nMost of this should be pretty logical, but one line should stick out - the call to ==setInterval()==.\\n\\nWhile there are a myriad of events for handling streaming video playback, it appears that the browsers disagree as to when the video actually starts, and what events to raise.  This means display dimensions are equally scattered.  To get around this, we start an interval that watches for the ==video.videoWidth== and ==video.videoHeight== properties to be a value other than zero (0).\\n\\n```\\n// Called when the video starts\\n// Size image processing canvas\\nfunction doMediaPlay()\\n{\\n  // Video stream playing and sized by browser\\n  if( video.videoWidth > 0 && video.videoHeight > 0 )\\n  {\\n    clearInterval( interval );  \\n    interval = null;\\n  } else {\\n    return;\\n  }\\n  \\n  // Debug\\n  console.log( 'Video stream playing.' );\\n  \\n  // Size image processing canvas\\n  // Match original video stream size\\n  // Use attributes for sizing over styles\\n  // Attributes do not stretch canvas like styles\\n  canvas.setAttribute( \\n    'width', \\n    video.videoWidth \\n  );\\n  canvas.setAttribute( \\n    'height', \\n    video.videoHeight \\n  );\\n} \\n```\\n\\nYou might have noticed that the last lines from the above code snippet reference a canvas object.  While the video element displays the web camera, the canvas element will be responsible for accessing the pixels of that feed, and turning them into an image.\\n\\n###Adding a Canvas\\n\\nCanvas (specifically the canvas context) has an interesting method called ==drawImage()==.  Typically you see this when you want to draw images into a canvas.  The places you can pull those images from however include portions of the canvas itself, images from the HTML or loaded via JavaScript, and even the video element.\\n\\n> There was a time when drawImage could actually draw from any HTML element - even the document body.  This was considered a security risk, and pared down to a few select elements.\\n\\nThis means that you will need a canvas element in your HTML.  You can hide it with CSS using the ==visibility== property.  Do not use ==display: none;== as in some browsers this means nothing will get drawn to the canvas.\\n\\n```\\nvar canvas = document.querySelector( 'canvas' );\\nvar context = canvas.getContext( '2d' );\\n```\\n\\nYou also do not want to size the canvas element using CSS.  Sizing using CSS will actually cause the contents of the canvas to be stretched.  By using ==setAttribute()== we can control the physical size of the canvas.\\n\\nIn the code snippet above, I size the canvas to match the dimensions of the video feed.  \\n\\nThe video element will default to whatever size the browser wants to display the feed.  If you have other plans, I suggest using ==setAttribute()== there as well.  How the video displays, and what is considered the video's bounding box, are two different topics you will have to explore to get right.\\n\\n###Drawing to Canvas\\n\\nI will assume that you are wiring up some button or other event to trigger when an image is captured from the video feed.  When that event is triggered, you are now set to draw the video feed onto the canvas.\\n\\n```\\n// Draw video to canvas\\ncontext.drawImage( \\n  video, \\n  0, \\n  0, \\n  canvas.clientWidth, \\n  canvas.clientHeight \\n);\\n```\\n\\nThere is that ==drawImage()== method we talked about earlier.  We are telling the canvas context to capture the pixels from the video feed, and place them at the upper left corner.  Along the way we want context to fit the pixels of the video into the canvas (which should be the same size).\\n\\n> You might alternatively consider ==video.pause()== when you take a picture to let the user see what it is that was captured.\\n\\nYou can technically tell context to size the image being drawn to whatever dimensions you want.  Context will in turn scale or stretch the image being drawn to match.  This is a technique to consider if you want to generate a thumbnail of the image you captured.\\n\\n###Uploading the Image\\n\\nTo upload the image to the server, we will use the ==XMLHttpRequest== object.  While I will be using PHP on the server for this example, the same concept apply to whatever language it is you prefer to use on the server.\\n\\n> It is interesting to note that Web standards are almost mature enough to where you do not have to upload to a server at all, but could actually save the image directly to the user's disk.  Check out the ==[FileWriter](http://caniuse.com/#search=FileWriter)== API.\\n\\nWhen we send an image to the server, we will be sending a lot of data.  The HTTP POST method lends itself to uploading this quantity of data.  If you have other data you want to send along, you can tack it onto the image, or in this case, I send a custom header that uniquely identifies the client session.\\n\\n```\\n// Send image data to server\\nxhr = new XMLHttpRequest();\\nxhr.addEventListener( 'load', doPhotoLoad );\\nxhr.open( 'POST', SERVER_PATH, true );\\nxhr.setRequestHeader( 'X-Client-ID', uuid );    \\nxhr.send( canvas.toDataURL( 'image/png' ) );\\n```\\n\\nThe magic here is really the ==toDataURL()== method.  This method takes the pixels from the canvas, and encodes them for a given image type (==image/png==) using Base64.  The result is a really long string, that we can send as the raw HTTP POST content.\\n\\n###Saving the Image\\n\\nOur work on the client is pretty much complete at this point.  Now we turn our focus to the server, and handing the incoming HTTP POST of image data.  Again, I will be using PHP here, but the concepts should translate across to your preferred language easily enough.\\n\\n```\\n// Get raw HTTP content\\n$body = file_get_contents( 'php://input' );\\n\\n// Trim off encoding string\\n$prefix = strpos( $body, ',' ) + strlen( ',' );\\n$data = substr( $body, $prefix );\\n\\n// Decode into binary image\\n$image = base64_decode( $data );\\n\\n// Write image to file\\nfile_put_contents( $PATH . $filename, $image );\\n\\n// Tell the client where to find the file\\necho $filename;\\n```\\n\\nThe first step here is to get the HTTP POST content.  If you are familiar with the HTTP specification, this is effectively a long block of content that follows the HTTP headers.  The HTTP headers themselves state the length of the actual content.  This is different from form fields you may have posted in other applications.  It is just raw content.\\n\\nWhen canvas turns the pixels into an image, and Base64 encodes the result into a string, it will prefix that string with information about the image type (JPG, PNG), and also the method used to encode the pixels.  That little string looks like this:\\n\\n```\\ndata:image/png;base64,\\n```\\n\\nThis string is helpful information, but it is not part of the image data itself.  This means that before we decode the image back into a binary representation of an image format, that we first need to remove this string.  I do this by looking for that comma and grabbing everything after it.\\n\\nNow that we have the Base64 encoded representation of an image file, we need to turn it from text, back into binary before saving the image to disk.  In this code snippet I use ==base64\\\\_decode()== to get the binary representation of the file.  Then I use ==file\\\\_put\\\\_contents()== to write the file to disk.\\n\\nDepending on your needs, you may want to tell the client where it can find the image on the server.  Of course, the client already has the pixels on a canvas, and can put them into an ==Image== element directly by setting the ==src== attribute.  I will leave the decision of what to return to you.\\n\\n###Next Steps\\n\\nOnce you have the image on the server, there is a lot of processing that you can do.  Maybe you want to keep a pointer to that file in a database.  Maybe you want to perform some additional image processing on it, and send it in an MMS message or email.  Up next for me was applying face recognition.\\n\\nI have posted the complete version of this application (face recognition and all) on my [GitHub](https://github.com/krhoyt/Personal/tree/master/Alchemy) account.  Feel free to comment below with questions, or send me a [tweet](http://twitter.com/krhoyt).\\n\"}]],\"sections\":[[10,0]],\"ghostVersion\":\"3.0\"}","html":"<!--kg-card-begin: markdown--><p><em>Long ago, in a galaxy far, far away, Flash Player gave developers access to the user's web camera.  This 2002 feature is <a href=\"http://caniuse.com/#search=getUserMedia\">just starting</a> to become available to the Web.  At the same time, HTML5 Canvas brings pixel level manipulation of content to the table.  Throw in a dash of server processing (in this case PHP), and you have the ability to upload, save, and retrieve images from a web camera.</em></p>\n<hr>\n<h3 id=\"videoelement\">Video Element</h3>\n<p>Before we get into scripting web camera access, we will need a place to display the video feed.  This happens using the video element in your HTML.  You can give it an ID or class to query it later, or if you are only going to have one video element, just leave it be and query by the tag name.</p>\n<pre><code>&lt;!-- Video stream --&gt;\n&lt;video&gt;Video stream not available.&lt;/video&gt;  \n</code></pre>\n<p>You will notice that there is no size specified.  There are many variables to consider here, not the least of which is the resolution of the web camera itself, and the browser support for those resolutions.  <mark>In my experience, 640x480 was the most common resulting resolution.</mark></p>\n<p>Rather than specify the dimensions now, we will come back to how to detect and size the video element in a moment.  For now, wire up a global reference to the video element in JavaScript.  In the following code, the reference I use is called &quot;video&quot;.</p>\n<pre><code>var video = document.querySelector( 'video' );\n</code></pre>\n<h3 id=\"accessthewebcamera\">Access the Web Camera</h3>\n<p>The main approach to accessing a web camera from JavaScript is through a method named <mark>getUserMedia()</mark>.  Depending on the browser, this may be on the <mark>Navigator</mark> object (old), or the <mark>MediaDevices</mark> object.  Regardless, you probably want a little polyfill work to get started.</p>\n<p>For my testing, I went with:</p>\n<pre><code>// Polyfill browser differences\nnavigator.getMedia = ( \n  navigator.getUserMedia ||\n  navigator.webkitGetUserMedia ||\n  navigator.mozGetUserMedia ||\n  navigator.msGetUserMedia \n);  \n  \n// Start video stream  \nnavigator.getMedia( \n  {\n    video: true,\n    audio: false\n  },\n  doMediaStream,\n  doMediaError\n);\n</code></pre>\n<p>In my code, the method <mark>doMediaStream()</mark> is called when the user has given access to the web camera.  That means we can get a reference to the video stream, and start playing it.</p>\n<pre><code>// Called when media stream is successful\n// Starts watch for video element sizing\n// Starts stream to video element\nfunction doMediaStream( stream )\n{  \n  var url = null;\n\n  // Debug\n  console.log( 'Media stream linked.' );  \n  \n  // Watch for video dimensions to be set\n  interval = setInterval( doMediaPlay, 100 );\n  \n  // Polyfill browser differences\n  if( navigator.mozGetUserMedia ) \n  {\n    video.mozSrcObject = stream;\n  } else {\n    url = window.URL || window.webkitURL;\n    video.src = url.createObjectURL( stream );\n  }\n  \n  // Start the stream\n  video.play();  \n}\n</code></pre>\n<p>Most of this should be pretty logical, but one line should stick out - the call to <mark>setInterval()</mark>.</p>\n<p>While there are a myriad of events for handling streaming video playback, it appears that the browsers disagree as to when the video actually starts, and what events to raise.  This means display dimensions are equally scattered.  To get around this, we start an interval that watches for the <mark>video.videoWidth</mark> and <mark>video.videoHeight</mark> properties to be a value other than zero (0).</p>\n<pre><code>// Called when the video starts\n// Size image processing canvas\nfunction doMediaPlay()\n{\n  // Video stream playing and sized by browser\n  if( video.videoWidth &gt; 0 &amp;&amp; video.videoHeight &gt; 0 )\n  {\n    clearInterval( interval );  \n    interval = null;\n  } else {\n    return;\n  }\n  \n  // Debug\n  console.log( 'Video stream playing.' );\n  \n  // Size image processing canvas\n  // Match original video stream size\n  // Use attributes for sizing over styles\n  // Attributes do not stretch canvas like styles\n  canvas.setAttribute( \n    'width', \n    video.videoWidth \n  );\n  canvas.setAttribute( \n    'height', \n    video.videoHeight \n  );\n} \n</code></pre>\n<p>You might have noticed that the last lines from the above code snippet reference a canvas object.  While the video element displays the web camera, the canvas element will be responsible for accessing the pixels of that feed, and turning them into an image.</p>\n<h3 id=\"addingacanvas\">Adding a Canvas</h3>\n<p>Canvas (specifically the canvas context) has an interesting method called <mark>drawImage()</mark>.  Typically you see this when you want to draw images into a canvas.  The places you can pull those images from however include portions of the canvas itself, images from the HTML or loaded via JavaScript, and even the video element.</p>\n<blockquote>\n<p>There was a time when drawImage could actually draw from any HTML element - even the document body.  This was considered a security risk, and pared down to a few select elements.</p>\n</blockquote>\n<p>This means that you will need a canvas element in your HTML.  You can hide it with CSS using the <mark>visibility</mark> property.  Do not use <mark>display: none;</mark> as in some browsers this means nothing will get drawn to the canvas.</p>\n<pre><code>var canvas = document.querySelector( 'canvas' );\nvar context = canvas.getContext( '2d' );\n</code></pre>\n<p>You also do not want to size the canvas element using CSS.  Sizing using CSS will actually cause the contents of the canvas to be stretched.  By using <mark>setAttribute()</mark> we can control the physical size of the canvas.</p>\n<p>In the code snippet above, I size the canvas to match the dimensions of the video feed.</p>\n<p>The video element will default to whatever size the browser wants to display the feed.  If you have other plans, I suggest using <mark>setAttribute()</mark> there as well.  How the video displays, and what is considered the video's bounding box, are two different topics you will have to explore to get right.</p>\n<h3 id=\"drawingtocanvas\">Drawing to Canvas</h3>\n<p>I will assume that you are wiring up some button or other event to trigger when an image is captured from the video feed.  When that event is triggered, you are now set to draw the video feed onto the canvas.</p>\n<pre><code>// Draw video to canvas\ncontext.drawImage( \n  video, \n  0, \n  0, \n  canvas.clientWidth, \n  canvas.clientHeight \n);\n</code></pre>\n<p>There is that <mark>drawImage()</mark> method we talked about earlier.  We are telling the canvas context to capture the pixels from the video feed, and place them at the upper left corner.  Along the way we want context to fit the pixels of the video into the canvas (which should be the same size).</p>\n<blockquote>\n<p>You might alternatively consider <mark>video.pause()</mark> when you take a picture to let the user see what it is that was captured.</p>\n</blockquote>\n<p>You can technically tell context to size the image being drawn to whatever dimensions you want.  Context will in turn scale or stretch the image being drawn to match.  This is a technique to consider if you want to generate a thumbnail of the image you captured.</p>\n<h3 id=\"uploadingtheimage\">Uploading the Image</h3>\n<p>To upload the image to the server, we will use the <mark>XMLHttpRequest</mark> object.  While I will be using PHP on the server for this example, the same concept apply to whatever language it is you prefer to use on the server.</p>\n<blockquote>\n<p>It is interesting to note that Web standards are almost mature enough to where you do not have to upload to a server at all, but could actually save the image directly to the user's disk.  Check out the <mark><a href=\"http://caniuse.com/#search=FileWriter\">FileWriter</a></mark> API.</p>\n</blockquote>\n<p>When we send an image to the server, we will be sending a lot of data.  The HTTP POST method lends itself to uploading this quantity of data.  If you have other data you want to send along, you can tack it onto the image, or in this case, I send a custom header that uniquely identifies the client session.</p>\n<pre><code>// Send image data to server\nxhr = new XMLHttpRequest();\nxhr.addEventListener( 'load', doPhotoLoad );\nxhr.open( 'POST', SERVER_PATH, true );\nxhr.setRequestHeader( 'X-Client-ID', uuid );    \nxhr.send( canvas.toDataURL( 'image/png' ) );\n</code></pre>\n<p>The magic here is really the <mark>toDataURL()</mark> method.  This method takes the pixels from the canvas, and encodes them for a given image type (<mark>image/png</mark>) using Base64.  The result is a really long string, that we can send as the raw HTTP POST content.</p>\n<h3 id=\"savingtheimage\">Saving the Image</h3>\n<p>Our work on the client is pretty much complete at this point.  Now we turn our focus to the server, and handing the incoming HTTP POST of image data.  Again, I will be using PHP here, but the concepts should translate across to your preferred language easily enough.</p>\n<pre><code>// Get raw HTTP content\n$body = file_get_contents( 'php://input' );\n\n// Trim off encoding string\n$prefix = strpos( $body, ',' ) + strlen( ',' );\n$data = substr( $body, $prefix );\n\n// Decode into binary image\n$image = base64_decode( $data );\n\n// Write image to file\nfile_put_contents( $PATH . $filename, $image );\n\n// Tell the client where to find the file\necho $filename;\n</code></pre>\n<p>The first step here is to get the HTTP POST content.  If you are familiar with the HTTP specification, this is effectively a long block of content that follows the HTTP headers.  The HTTP headers themselves state the length of the actual content.  This is different from form fields you may have posted in other applications.  It is just raw content.</p>\n<p>When canvas turns the pixels into an image, and Base64 encodes the result into a string, it will prefix that string with information about the image type (JPG, PNG), and also the method used to encode the pixels.  That little string looks like this:</p>\n<pre><code>data:image/png;base64,\n</code></pre>\n<p>This string is helpful information, but it is not part of the image data itself.  This means that before we decode the image back into a binary representation of an image format, that we first need to remove this string.  I do this by looking for that comma and grabbing everything after it.</p>\n<p>Now that we have the Base64 encoded representation of an image file, we need to turn it from text, back into binary before saving the image to disk.  In this code snippet I use <mark>base64_decode()</mark> to get the binary representation of the file.  Then I use <mark>file_put_contents()</mark> to write the file to disk.</p>\n<p>Depending on your needs, you may want to tell the client where it can find the image on the server.  Of course, the client already has the pixels on a canvas, and can put them into an <mark>Image</mark> element directly by setting the <mark>src</mark> attribute.  I will leave the decision of what to return to you.</p>\n<h3 id=\"nextsteps\">Next Steps</h3>\n<p>Once you have the image on the server, there is a lot of processing that you can do.  Maybe you want to keep a pointer to that file in a database.  Maybe you want to perform some additional image processing on it, and send it in an MMS message or email.  Up next for me was applying face recognition.</p>\n<p>I have posted the complete version of this application (face recognition and all) on my <a href=\"https://github.com/krhoyt/Personal/tree/master/Alchemy\">GitHub</a> account.  Feel free to comment below with questions, or send me a <a href=\"http://twitter.com/krhoyt\">tweet</a>.</p>\n<!--kg-card-end: markdown-->","comment_id":"32","plaintext":"Long ago, in a galaxy far, far away, Flash Player gave developers access to the\nuser's web camera. This 2002 feature is just starting\n[http://caniuse.com/#search=getUserMedia] to become available to the Web. At the\nsame time, HTML5 Canvas brings pixel level manipulation of content to the table.\nThrow in a dash of server processing (in this case PHP), and you have the\nability to upload, save, and retrieve images from a web camera.\n\n\n--------------------------------------------------------------------------------\n\nVideo Element\nBefore we get into scripting web camera access, we will need a place to display\nthe video feed. This happens using the video element in your HTML. You can give\nit an ID or class to query it later, or if you are only going to have one video\nelement, just leave it be and query by the tag name.\n\n<!-- Video stream -->\n<video>Video stream not available.</video>  \n\n\nYou will notice that there is no size specified. There are many variables to\nconsider here, not the least of which is the resolution of the web camera\nitself, and the browser support for those resolutions. In my experience, 640x480\nwas the most common resulting resolution.\n\nRather than specify the dimensions now, we will come back to how to detect and\nsize the video element in a moment. For now, wire up a global reference to the\nvideo element in JavaScript. In the following code, the reference I use is\ncalled \"video\".\n\nvar video = document.querySelector( 'video' );\n\n\nAccess the Web Camera\nThe main approach to accessing a web camera from JavaScript is through a method\nnamed getUserMedia(). Depending on the browser, this may be on the Navigator \nobject (old), or the MediaDevices object. Regardless, you probably want a little\npolyfill work to get started.\n\nFor my testing, I went with:\n\n// Polyfill browser differences\nnavigator.getMedia = ( \n  navigator.getUserMedia ||\n  navigator.webkitGetUserMedia ||\n  navigator.mozGetUserMedia ||\n  navigator.msGetUserMedia \n);  \n  \n// Start video stream  \nnavigator.getMedia( \n  {\n    video: true,\n    audio: false\n  },\n  doMediaStream,\n  doMediaError\n);\n\n\nIn my code, the method doMediaStream() is called when the user has given access\nto the web camera. That means we can get a reference to the video stream, and\nstart playing it.\n\n// Called when media stream is successful\n// Starts watch for video element sizing\n// Starts stream to video element\nfunction doMediaStream( stream )\n{  \n  var url = null;\n\n  // Debug\n  console.log( 'Media stream linked.' );  \n  \n  // Watch for video dimensions to be set\n  interval = setInterval( doMediaPlay, 100 );\n  \n  // Polyfill browser differences\n  if( navigator.mozGetUserMedia ) \n  {\n    video.mozSrcObject = stream;\n  } else {\n    url = window.URL || window.webkitURL;\n    video.src = url.createObjectURL( stream );\n  }\n  \n  // Start the stream\n  video.play();  \n}\n\n\nMost of this should be pretty logical, but one line should stick out - the call\nto setInterval().\n\nWhile there are a myriad of events for handling streaming video playback, it\nappears that the browsers disagree as to when the video actually starts, and\nwhat events to raise. This means display dimensions are equally scattered. To\nget around this, we start an interval that watches for the video.videoWidth and \nvideo.videoHeight properties to be a value other than zero (0).\n\n// Called when the video starts\n// Size image processing canvas\nfunction doMediaPlay()\n{\n  // Video stream playing and sized by browser\n  if( video.videoWidth > 0 && video.videoHeight > 0 )\n  {\n    clearInterval( interval );  \n    interval = null;\n  } else {\n    return;\n  }\n  \n  // Debug\n  console.log( 'Video stream playing.' );\n  \n  // Size image processing canvas\n  // Match original video stream size\n  // Use attributes for sizing over styles\n  // Attributes do not stretch canvas like styles\n  canvas.setAttribute( \n    'width', \n    video.videoWidth \n  );\n  canvas.setAttribute( \n    'height', \n    video.videoHeight \n  );\n} \n\n\nYou might have noticed that the last lines from the above code snippet reference\na canvas object. While the video element displays the web camera, the canvas\nelement will be responsible for accessing the pixels of that feed, and turning\nthem into an image.\n\nAdding a Canvas\nCanvas (specifically the canvas context) has an interesting method called \ndrawImage(). Typically you see this when you want to draw images into a canvas.\nThe places you can pull those images from however include portions of the canvas\nitself, images from the HTML or loaded via JavaScript, and even the video\nelement.\n\n> There was a time when drawImage could actually draw from any HTML element - even\nthe document body. This was considered a security risk, and pared down to a few\nselect elements.\n\n\nThis means that you will need a canvas element in your HTML. You can hide it\nwith CSS using the visibility property. Do not use display: none; as in some\nbrowsers this means nothing will get drawn to the canvas.\n\nvar canvas = document.querySelector( 'canvas' );\nvar context = canvas.getContext( '2d' );\n\n\nYou also do not want to size the canvas element using CSS. Sizing using CSS will\nactually cause the contents of the canvas to be stretched. By using \nsetAttribute() we can control the physical size of the canvas.\n\nIn the code snippet above, I size the canvas to match the dimensions of the\nvideo feed.\n\nThe video element will default to whatever size the browser wants to display the\nfeed. If you have other plans, I suggest using setAttribute() there as well. How\nthe video displays, and what is considered the video's bounding box, are two\ndifferent topics you will have to explore to get right.\n\nDrawing to Canvas\nI will assume that you are wiring up some button or other event to trigger when\nan image is captured from the video feed. When that event is triggered, you are\nnow set to draw the video feed onto the canvas.\n\n// Draw video to canvas\ncontext.drawImage( \n  video, \n  0, \n  0, \n  canvas.clientWidth, \n  canvas.clientHeight \n);\n\n\nThere is that drawImage() method we talked about earlier. We are telling the\ncanvas context to capture the pixels from the video feed, and place them at the\nupper left corner. Along the way we want context to fit the pixels of the video\ninto the canvas (which should be the same size).\n\n> You might alternatively consider video.pause() when you take a picture to let\nthe user see what it is that was captured.\n\n\nYou can technically tell context to size the image being drawn to whatever\ndimensions you want. Context will in turn scale or stretch the image being drawn\nto match. This is a technique to consider if you want to generate a thumbnail of\nthe image you captured.\n\nUploading the Image\nTo upload the image to the server, we will use the XMLHttpRequest object. While\nI will be using PHP on the server for this example, the same concept apply to\nwhatever language it is you prefer to use on the server.\n\n> It is interesting to note that Web standards are almost mature enough to where\nyou do not have to upload to a server at all, but could actually save the image\ndirectly to the user's disk. Check out the FileWriter\n[http://caniuse.com/#search=FileWriter] API.\n\n\nWhen we send an image to the server, we will be sending a lot of data. The HTTP\nPOST method lends itself to uploading this quantity of data. If you have other\ndata you want to send along, you can tack it onto the image, or in this case, I\nsend a custom header that uniquely identifies the client session.\n\n// Send image data to server\nxhr = new XMLHttpRequest();\nxhr.addEventListener( 'load', doPhotoLoad );\nxhr.open( 'POST', SERVER_PATH, true );\nxhr.setRequestHeader( 'X-Client-ID', uuid );    \nxhr.send( canvas.toDataURL( 'image/png' ) );\n\n\nThe magic here is really the toDataURL() method. This method takes the pixels\nfrom the canvas, and encodes them for a given image type (image/png) using\nBase64. The result is a really long string, that we can send as the raw HTTP\nPOST content.\n\nSaving the Image\nOur work on the client is pretty much complete at this point. Now we turn our\nfocus to the server, and handing the incoming HTTP POST of image data. Again, I\nwill be using PHP here, but the concepts should translate across to your\npreferred language easily enough.\n\n// Get raw HTTP content\n$body = file_get_contents( 'php://input' );\n\n// Trim off encoding string\n$prefix = strpos( $body, ',' ) + strlen( ',' );\n$data = substr( $body, $prefix );\n\n// Decode into binary image\n$image = base64_decode( $data );\n\n// Write image to file\nfile_put_contents( $PATH . $filename, $image );\n\n// Tell the client where to find the file\necho $filename;\n\n\nThe first step here is to get the HTTP POST content. If you are familiar with\nthe HTTP specification, this is effectively a long block of content that follows\nthe HTTP headers. The HTTP headers themselves state the length of the actual\ncontent. This is different from form fields you may have posted in other\napplications. It is just raw content.\n\nWhen canvas turns the pixels into an image, and Base64 encodes the result into a\nstring, it will prefix that string with information about the image type (JPG,\nPNG), and also the method used to encode the pixels. That little string looks\nlike this:\n\ndata:image/png;base64,\n\n\nThis string is helpful information, but it is not part of the image data itself.\nThis means that before we decode the image back into a binary representation of\nan image format, that we first need to remove this string. I do this by looking\nfor that comma and grabbing everything after it.\n\nNow that we have the Base64 encoded representation of an image file, we need to\nturn it from text, back into binary before saving the image to disk. In this\ncode snippet I use base64_decode() to get the binary representation of the file.\nThen I use file_put_contents() to write the file to disk.\n\nDepending on your needs, you may want to tell the client where it can find the\nimage on the server. Of course, the client already has the pixels on a canvas,\nand can put them into an Image element directly by setting the src attribute. I\nwill leave the decision of what to return to you.\n\nNext Steps\nOnce you have the image on the server, there is a lot of processing that you can\ndo. Maybe you want to keep a pointer to that file in a database. Maybe you want\nto perform some additional image processing on it, and send it in an MMS message\nor email. Up next for me was applying face recognition.\n\nI have posted the complete version of this application (face recognition and\nall) on my GitHub [https://github.com/krhoyt/Personal/tree/master/Alchemy] \naccount. Feel free to comment below with questions, or send me a tweet\n[http://twitter.com/krhoyt].","feature_image":"http://images.kevinhoyt.com/camera.lens.jpg","featured":0,"status":"published","locale":null,"visibility":"public","author_id":"1","created_at":"2015-06-18T13:44:24.000Z","updated_at":"2015-06-18T15:10:23.000Z","published_at":"2015-06-18T15:07:58.000Z","custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"type":"post","email_recipient_filter":"none"},{"id":"5adb8922351ffe0018a5772c","uuid":"63abdd2e-c183-4399-bd56-ccf6bdc9a24c","title":"Events","slug":"events","mobiledoc":"{\"version\":\"0.3.1\",\"atoms\":[],\"cards\":[[\"markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\"###TL;DR\\n\\n- DevNexus 2019 (session)\\n- Heartland Developer Conference 2018 (workshop, session)\\n- Index Conference 2018 (track manager)\\n- Heartland Developer Conference 2017 (workshop, session)\\n- FullStack London 2017 (session)\\n- Jfokus 2017 (keynote)\\n- Desert Code Camp 2016 (session)\\n- LibertyJS 2016 (workshop)\\n- Heartland Developer Conference 2016 (workshop, session)\\n- WindyCityThings 2016 (session)\\n- WebVisions Portland 2016 (workshop, session)\\n- Heartland Developer Conference 2015 (workshop, session, keynote)\\n\\n###By Date (Newest to Oldest)\\n\\n**[DevNexus 2019](https://devnexus.com)**<br>\\n*March 6 - 8, 2019*\\n\\n**Session:** [Computer Vision vs. Machine Learning](https://devnexus.com/presentations/3150)\\n\\nSet against the backdrop of the capabilities of the browser, this interactive session compares and contrasts both approaches through two different applications. In the first, you will learn image processing and feature detection techniques to track, and solve, a Rubik’s Cube. In the second, you will learn to apply those techniques to build an object classifier for facial recognition. Sit in the front row at your own risk!\\n\\n**[Heartland Developer Conference 2018](http://careerlink.com/hdc)**<br>\\n*September 5 - 7, 2018*\\n\\n**Workshop:** [Computer Vision and Machine Learning in the Browser](http://sched.co/FEvn)\\n\\nThe browser, on both the desktop and mobile, are capable of much more than we often give them credit. In this workshop we will push the CPU to 100% as we audit several image processing libraries used for computer vision and machine learning.\\n\\nLearn computer vision with real-time facial detection, optical character recognition, color distance calculations, object tracking, and even reading barcodes and augmented reality markers. From there, we will turn our focus to machine learning where you will learn about developing models for neural networks. \\n\\n**Session:** [Computer Vision vs. Machine Learning](http://sched.co/FEzB)\\n\\nComputer vision seeks to acquire, process, and analyze digital images based on dimensional data to produce numerical decisions. Machine learning applies convolutional neural networks to classify, identify, and detect symbolic information from image data. The easiest way to decipher all of this and understand when to apply which technique to your application, is to attend this session. \\n\\nSet against the backdrop of the capabilities of the browser, this interactive session compares and contrasts both approaches through two different applications. In the first, you will learn image processing and feature detection techniques to track, and solve, a Rubik’s Cube. In the second, you will learn to apply those techniques to build an object classifier for facial recognition. Sit in the front row at your own risk! \\n\\n**[Index Conference 2018](http://indexconf.com)**<br>\\n*February 20 - 22, 2018*\\n\\n**Track Manager:** Geek Shop and the Future\\n\\nIn the fall of 2017, I was asked to manage a track centered around future technology for a new conference, Index. I had managed the web standards track at Adobe MAX years ago, and thoroughly enjoyed the collaboration. These days I tend to be more focused on emerging technology, so Index seemed like a great fit.\\n\\nAt first, as a passionate practitioner of desktop fabrication, and the maker movement, I really wanted to focus on the \\\"geek shop\\\" side of things. Most of the makers in my professional network were busy however, and the submitted abstracts pushed the track into the \\\"futures\\\" topic. Coverage included quantum computing, blockchain, augmented reality, and even \\\"wetware\\\" - the physical integration of man and machine.\\n\\n**[Heartland Developer Conference 2017](http://careerlink.com/hdc/)**<br>\\n*September 6 - 8, 2017*\\n\\n**Workshop:** [Understanding Blockchain](http://sched.co/B98S)\\n\\nThe cryptocurrency Bitcoin is well known even outside of technology spheres of influence. Underlying Bitcoin however is Blockchain - a distributed ledger with some very special properties. These properties make Blockchain an ideal data store for all variety of applications, not just financial records. This workshop, half lecture and half interactive hands-on, is designed to help you understand blockchain, and how to use it in your business.\\n\\nLecture topics include:\\n\\n- Where does blockchain fit in your application infrastructure?\\n- What are the differences between ledgers and databases?\\n- How does chaining data work, and why does it matter?\\n- What is decentralized consensus, and why does it matter?\\n- What patterns identify an application as a fit for blockchain?\\n- What are smart contracts, and how are they developed?\\n\\nHands-on activities include:\\n\\n- Model the business domain of a blockchain application\\n- Write transaction functions using JavaScript\\n- Test a blockchain application\\n- Interact with blockchain via REST\\n- Trade physical assets with other attendees via blockchain\\n\\n**Session:** [Indoor Location with Beacons](http://sched.co/B9DA)\\n\\nHaving a highly capable and accurate GPS in everybody's pocket has fundamentally changed the way we live, with broader impacts like self-driving cars, yet to come. The moment you step inside the HDC conference center however, all that technology is virtually useless. In order to solve this problem, various vendors, from Apple to Google, and many others, have been implementing indoor location technologies, predominantly via beacons.\\n\\nIn this session learn how beacons function at the physical layer, how that gets interpreted at the network layer, how to use the APIs at the presentation layer, and how to make sense of it all at the application layer. This will be accomplished as IBM Developer Advocate, Kevin Hoyt, configures and deploys a set of beacons in the conference room, live, during the session. We will also explore how various industries are using indoor location from shopping malls to hotels, and airports to grocery stores.\\n\\n**[FullStack London 2017](https://skillsmatter.com/conferences/8264-fullstack-2017-the-conference-on-javascript-node-and-internet-of-things)**<br>\\n*July 12 - 14, 2017*\\n\\n**Session:** [Understanding Blockchain](https://skillsmatter.com/conferences/8264-fullstack-2017-the-conference-on-javascript-node-and-internet-of-things#program)\\n\\nThe cryptocurrency Bitcoin is well known even outside of technology spheres of influence. Underlying Bitcoin however is Blockchain - a distributed ledger with some very special properties. These properties make Blockchain an ideal data store for all variety of applications, not just financial records. In this session, join IBM Developer Advocate, Kevin Hoyt, to understand blockchain terminology and unlock the potential for your business.\\n\\nCentered around a web-based to-do list application, built on the Polymer 2 RC, topics covered will include spinning up your own Blockchain (Hyperledger Fabric 1.0) node, writing and deploying a \\\"smart contract\\\" to perform CRUD operations, and integration using Web standards. With proficiency obtained, you will also take a look at blockchain scaffolding tools. You will leave understanding where and how to use Blockchain technolgies, with the knowledge needed to get started.\\n\\n**[Jfokus 2017](https://www.jfokus.se/jfokus/)**<br>\\n*February 6 - 8, 2017*\\n\\n**Keynote:** [Crossing the IoT Chasm](https://www.jfokus.se/jfokus/talks.jsp#CrossingtheIoTChasm)\\n\\nI gave my first IoT presentation over a decade ago. The response was overwhelming, and I've been waiting for the IoT tidal wave to land ever since. Strangely, it has not. Why not? Fast-forward through those ten years, and I have learned countless lessons. Join me as I review the strengths and weakness of hardware platforms designed to get you to market, examine how Big Data may just hold the key to IoT success, and glimpse at an AI-powered future.\\n\\n**[Desert Code Camp 2016](http://oct2016.desertcodecamp.com/)**<br>\\n*October 8, 2016*\\n\\n**Session:** [Explore Machine Learning with IBM Watson](http://oct2016.desertcodecamp.com/session/1313)\\n\\nIt has been suggested that in the future, developers will not program computers, as much as train them. This has largely to do with the rapid growth of machine learning. But what is machine learning? What can it do for developers today? And how does one leverage it in their application? \\n\\nIn this session, join IBM Developer Advocate, Kevin Hoyt, and IBM Watson, to untangle the details. We will start with a little artificial intelligence history. From there we will move on to defining some of the more common machine learning features by implementing them in various applications (Node.js, iOS, JavaScript).\\n\\n**[LibertyJS 2016](http://www.libertyjs.com/)**<br>\\n*September 30 - October 1, 2016*\\n\\n**Workshop:** [Machine Learning for JavaScript Developers](http://www.libertyjs.com/speakers/)\\n\\nIt has been suggested that in the future, developers will not so much program computers, as they will train them. Machine learning is having a broad impact across all industries, but what does it mean, and how does one use it in an application? \\n\\nJoin IBM Developer Advocate, Kevin Hoyt, and IBM Watson, to learn a brief history of machine learning, common features of machine learning, and how to leverage those features in the browser, on the server, and on IoT devices, all with JavaScript. \\n\\nCoverage will include:\\n\\n- Speech-To-Text\\n- Text-To-Speech\\n- Conversational Dialog\\n- Visual Recognition\\n- Translation\\n- Concept Extraction\\n- Tone Analysis\\n- Personality Insight\\n\\nGet ahead of the curve with what venture capitalist firm [Andreessen Horowitz](http://a16z.com/2016/06/10/ai-deep-learning-machines/) calls a fundamental technique that is expected to be in products.\\n\\n**[Heartland Developer Conference 2016](http://careerlink.com/hdc/)**<br>\\n*September 7 - 9, 2016*\\n\\n**Session:** Node-RED for Faster Development\\n\\nIf you have been programming long enough, you know that certain patterns emerge, and that work often gets wrapped in boilerplate code. An example would be CRUD (create, read, update, delete) operations on a database, that you might expose via an API. Sending an email or text message. What if you could develop, design, and orchestrate business logic using visual tooling?\\n\\nNode-RED is an open source visual tool for wiring together business logic, built on Node.js. Expose API endpoints via drag-and-drop. Consume APIs in the same way. Access databases, email server, Watson cognitive services, Twitter feeds, and more in a browser-based tool that allows you to visual control the flow of logic. IBM Developer Advocate, Kevin Hoyt, takes you on a tour of the tool that may just revolutionize how you work, with increased productivity, and faster time to market.\\n\\n**Workshop:** [Introduction to Deep Learning with IBM Watson](http://careerlink.com/hdc/master-schedule)\\n\\nWhat is the price of not knowing? Every day we generate 2.5 quintillion bytes of data, and 90% of the data in the world has been generated in the past two years alone. There are 7.3 billion people connected to the Internet around the world, creating 1.7 megabytes of data every minute. Unstructured data - dark data - accounts for 80% of all data generated today. Like dark matter, which cannot be seen, we know dark data exists by its impact (or lack thereof) on our industries.\\n\\nThis is the dawn of the cognitive era - an inflection point that occurs once about every 40 years - that lends itself to machine learning. In this workshop, join IBM Developer Advocate, Kevin Hoyt, and go hands on with IBM Watson. Learn about APIs such as text-to-speech, and speech-to-text, translation, image recognition, natural language processing (NLP), concept and personality insights, and more. Leave with a new toolkit to help you leverage deep learning, and to solve the problems your industry faces.\\n\\n**[Windy City Things 2016](https://windycitythings.com/)**<br>\\n*June 23 - 24, 2016*\\n\\n**Session:** [IoT All The Things (An Audit)](https://windycitythings.com/schedule/)\\n\\nThere are no shortage of IoT platforms to launch you next product, but which one is the right one for you? In this session join IBM Developer Advocate, Kevin Hoyt, on an audit of some of the more popular chipsets on the market. This will include overviews of Electric Imp, Photon Particle and Photon Electron, Tessel 2 from Technical Machines, and Intel Edison. Explore GPIO options, wireless pairing, development tooling, physical size and technical specifications (processor, battery, etc), set against a backdrop of real-time data delivery.\\n\\n**[WebVisions Portland 2016](http://www.webvisionsevent.com/portland/)**<br>\\n*May 18 - 20, 2016*\\n\\n**Workshop:** [On the Verge of Genius](http://www.webvisionsevent.com/portland/workshop/how-to-build-a-crowdsourced-pollution-monitoring-device/?loc=portland-2016)\\n\\nCross a WebVisions workshop with Bill Nye the Science Guy (or Mr. Wizard depending on your generation), and you will get IBM's Kevin Hoyt, leading you through an interactive, hands-on, exploration of the increasingly connected world of cities, farms, and you.  \\n\\nOn this three-hour tour, the weather may just get rough, but smart cities with vast arrays of connected sensors will keep us on course and on time. Leaving the city behind, we will discover that data is the new fertilizer for the green acres of smart agriculture. The next stop on this fantastic voyage is inner space as we seek to leverage smart healthcare to unlock the secrets of heart disease and asthma.\\n\\nThis workshop is packed with live demonstrations of a large number of scientific sensors in action. The PH of your drinking water. The air quality of the conference center. The galvanic skin response (sweating) of the presenter. And many more. Having established the possibilities, you will have the option to spend an hour with your very own Internet-connected hardware. Solving the world's problems is hard work, but together we can achieve genius.\\n\\n**Session:** [The Business of Beacons](http://www.webvisionsevent.com/session/the-business-of-beacons/)\\n\\nBuilt on BLE (Bluetooth Low Energy) beacons get a lot of press, and have seen increasing deployment around the globe, adding value to business and consumers. In this session, join IBM Developer Advocate, Kevin Hoyt, on a journey through beacon opportunities you may not have considered. Practical, real-world, references will be highlighted as we examine innovative use-cases for beacons in your business.\\n\\n**[Heartland Developer Conference 2015](http://www.heartlanddc.com)**<br>\\n*September 9 - 11, 2015*\\n\\n**Workshop:** [Hands-On IoT](http://www.heartlanddc.com/a3)\\n\\nThis workshop will introduce you to IoT using the Particle (formerly Spark) Photon. The Particle Photon offers a suite of hardware and software tools to help you prototype, scale, and manage your IoT products. Based on Broadcom’s WICED architecture (used in Nest, Amazon Dash), combined with a powerful ARM Cortex M3, the Photon offer wi-fi for everything. During this workshop you will pair the Photon with a wireless network, and then learn to control the digital and analog features, both in firmware, and from your own web application.\\n\\n**Session:** [Unlocking Your Application with WebSocket](http://www.heartlanddc.com/session-201-2)\\n\\nWebSocket has been available to browsers (both mobile and desktop) for years, yet the potential remains largely untapped. In this session, get an introduction to using WebSocket in your applications. You will learn about the potential for WebSocket and real-time communication, and explore the usage of the publish/subscribe pattern over the commonplace request/response pattern. Packed with live, real-world demonstrations, IBM Developer Advocate, Kevin Hoyt, will show you how to unlock the potential latent in your applications. Other concepts that will be introduced include micro-services, container-based deployments (Docker), and the Internet of Things.\\n\\n**Keynote:** [Welcome Makers](http://www.heartlanddc.com/session-k3)\\n\\nIn 1916, engineering student James Wilkins proposed spending $100 million to build a suspension bridge the world would eventually come to know as the Golden Gate Bridge.  Fast-forward 100 years, and we are rapidly throwing away our ability to innovate.  In this keynote address, join IBM Developer Adovcate, Kevin Hoyt, in taking a sneak peak at the next industrial revolution - the Internet of Things - and learn how it puts you, the developer, in the driver seat of opportunity.\\n\"}]],\"markups\":[],\"sections\":[[10,0]],\"ghostVersion\":\"3.0\"}","html":"<!--kg-card-begin: markdown--><h3 id=\"tldr\">TL;DR</h3>\n<ul>\n<li>DevNexus 2019 (session)</li>\n<li>Heartland Developer Conference 2018 (workshop, session)</li>\n<li>Index Conference 2018 (track manager)</li>\n<li>Heartland Developer Conference 2017 (workshop, session)</li>\n<li>FullStack London 2017 (session)</li>\n<li>Jfokus 2017 (keynote)</li>\n<li>Desert Code Camp 2016 (session)</li>\n<li>LibertyJS 2016 (workshop)</li>\n<li>Heartland Developer Conference 2016 (workshop, session)</li>\n<li>WindyCityThings 2016 (session)</li>\n<li>WebVisions Portland 2016 (workshop, session)</li>\n<li>Heartland Developer Conference 2015 (workshop, session, keynote)</li>\n</ul>\n<h3 id=\"bydatenewesttooldest\">By Date (Newest to Oldest)</h3>\n<p><strong><a href=\"https://devnexus.com\">DevNexus 2019</a></strong><br><br>\n<em>March 6 - 8, 2019</em></p>\n<p><strong>Session:</strong> <a href=\"https://devnexus.com/presentations/3150\">Computer Vision vs. Machine Learning</a></p>\n<p>Set against the backdrop of the capabilities of the browser, this interactive session compares and contrasts both approaches through two different applications. In the first, you will learn image processing and feature detection techniques to track, and solve, a Rubik’s Cube. In the second, you will learn to apply those techniques to build an object classifier for facial recognition. Sit in the front row at your own risk!</p>\n<p><strong><a href=\"http://careerlink.com/hdc\">Heartland Developer Conference 2018</a></strong><br><br>\n<em>September 5 - 7, 2018</em></p>\n<p><strong>Workshop:</strong> <a href=\"http://sched.co/FEvn\">Computer Vision and Machine Learning in the Browser</a></p>\n<p>The browser, on both the desktop and mobile, are capable of much more than we often give them credit. In this workshop we will push the CPU to 100% as we audit several image processing libraries used for computer vision and machine learning.</p>\n<p>Learn computer vision with real-time facial detection, optical character recognition, color distance calculations, object tracking, and even reading barcodes and augmented reality markers. From there, we will turn our focus to machine learning where you will learn about developing models for neural networks.</p>\n<p><strong>Session:</strong> <a href=\"http://sched.co/FEzB\">Computer Vision vs. Machine Learning</a></p>\n<p>Computer vision seeks to acquire, process, and analyze digital images based on dimensional data to produce numerical decisions. Machine learning applies convolutional neural networks to classify, identify, and detect symbolic information from image data. The easiest way to decipher all of this and understand when to apply which technique to your application, is to attend this session.</p>\n<p>Set against the backdrop of the capabilities of the browser, this interactive session compares and contrasts both approaches through two different applications. In the first, you will learn image processing and feature detection techniques to track, and solve, a Rubik’s Cube. In the second, you will learn to apply those techniques to build an object classifier for facial recognition. Sit in the front row at your own risk!</p>\n<p><strong><a href=\"http://indexconf.com\">Index Conference 2018</a></strong><br><br>\n<em>February 20 - 22, 2018</em></p>\n<p><strong>Track Manager:</strong> Geek Shop and the Future</p>\n<p>In the fall of 2017, I was asked to manage a track centered around future technology for a new conference, Index. I had managed the web standards track at Adobe MAX years ago, and thoroughly enjoyed the collaboration. These days I tend to be more focused on emerging technology, so Index seemed like a great fit.</p>\n<p>At first, as a passionate practitioner of desktop fabrication, and the maker movement, I really wanted to focus on the &quot;geek shop&quot; side of things. Most of the makers in my professional network were busy however, and the submitted abstracts pushed the track into the &quot;futures&quot; topic. Coverage included quantum computing, blockchain, augmented reality, and even &quot;wetware&quot; - the physical integration of man and machine.</p>\n<p><strong><a href=\"http://careerlink.com/hdc/\">Heartland Developer Conference 2017</a></strong><br><br>\n<em>September 6 - 8, 2017</em></p>\n<p><strong>Workshop:</strong> <a href=\"http://sched.co/B98S\">Understanding Blockchain</a></p>\n<p>The cryptocurrency Bitcoin is well known even outside of technology spheres of influence. Underlying Bitcoin however is Blockchain - a distributed ledger with some very special properties. These properties make Blockchain an ideal data store for all variety of applications, not just financial records. This workshop, half lecture and half interactive hands-on, is designed to help you understand blockchain, and how to use it in your business.</p>\n<p>Lecture topics include:</p>\n<ul>\n<li>Where does blockchain fit in your application infrastructure?</li>\n<li>What are the differences between ledgers and databases?</li>\n<li>How does chaining data work, and why does it matter?</li>\n<li>What is decentralized consensus, and why does it matter?</li>\n<li>What patterns identify an application as a fit for blockchain?</li>\n<li>What are smart contracts, and how are they developed?</li>\n</ul>\n<p>Hands-on activities include:</p>\n<ul>\n<li>Model the business domain of a blockchain application</li>\n<li>Write transaction functions using JavaScript</li>\n<li>Test a blockchain application</li>\n<li>Interact with blockchain via REST</li>\n<li>Trade physical assets with other attendees via blockchain</li>\n</ul>\n<p><strong>Session:</strong> <a href=\"http://sched.co/B9DA\">Indoor Location with Beacons</a></p>\n<p>Having a highly capable and accurate GPS in everybody's pocket has fundamentally changed the way we live, with broader impacts like self-driving cars, yet to come. The moment you step inside the HDC conference center however, all that technology is virtually useless. In order to solve this problem, various vendors, from Apple to Google, and many others, have been implementing indoor location technologies, predominantly via beacons.</p>\n<p>In this session learn how beacons function at the physical layer, how that gets interpreted at the network layer, how to use the APIs at the presentation layer, and how to make sense of it all at the application layer. This will be accomplished as IBM Developer Advocate, Kevin Hoyt, configures and deploys a set of beacons in the conference room, live, during the session. We will also explore how various industries are using indoor location from shopping malls to hotels, and airports to grocery stores.</p>\n<p><strong><a href=\"https://skillsmatter.com/conferences/8264-fullstack-2017-the-conference-on-javascript-node-and-internet-of-things\">FullStack London 2017</a></strong><br><br>\n<em>July 12 - 14, 2017</em></p>\n<p><strong>Session:</strong> <a href=\"https://skillsmatter.com/conferences/8264-fullstack-2017-the-conference-on-javascript-node-and-internet-of-things#program\">Understanding Blockchain</a></p>\n<p>The cryptocurrency Bitcoin is well known even outside of technology spheres of influence. Underlying Bitcoin however is Blockchain - a distributed ledger with some very special properties. These properties make Blockchain an ideal data store for all variety of applications, not just financial records. In this session, join IBM Developer Advocate, Kevin Hoyt, to understand blockchain terminology and unlock the potential for your business.</p>\n<p>Centered around a web-based to-do list application, built on the Polymer 2 RC, topics covered will include spinning up your own Blockchain (Hyperledger Fabric 1.0) node, writing and deploying a &quot;smart contract&quot; to perform CRUD operations, and integration using Web standards. With proficiency obtained, you will also take a look at blockchain scaffolding tools. You will leave understanding where and how to use Blockchain technolgies, with the knowledge needed to get started.</p>\n<p><strong><a href=\"https://www.jfokus.se/jfokus/\">Jfokus 2017</a></strong><br><br>\n<em>February 6 - 8, 2017</em></p>\n<p><strong>Keynote:</strong> <a href=\"https://www.jfokus.se/jfokus/talks.jsp#CrossingtheIoTChasm\">Crossing the IoT Chasm</a></p>\n<p>I gave my first IoT presentation over a decade ago. The response was overwhelming, and I've been waiting for the IoT tidal wave to land ever since. Strangely, it has not. Why not? Fast-forward through those ten years, and I have learned countless lessons. Join me as I review the strengths and weakness of hardware platforms designed to get you to market, examine how Big Data may just hold the key to IoT success, and glimpse at an AI-powered future.</p>\n<p><strong><a href=\"http://oct2016.desertcodecamp.com/\">Desert Code Camp 2016</a></strong><br><br>\n<em>October 8, 2016</em></p>\n<p><strong>Session:</strong> <a href=\"http://oct2016.desertcodecamp.com/session/1313\">Explore Machine Learning with IBM Watson</a></p>\n<p>It has been suggested that in the future, developers will not program computers, as much as train them. This has largely to do with the rapid growth of machine learning. But what is machine learning? What can it do for developers today? And how does one leverage it in their application?</p>\n<p>In this session, join IBM Developer Advocate, Kevin Hoyt, and IBM Watson, to untangle the details. We will start with a little artificial intelligence history. From there we will move on to defining some of the more common machine learning features by implementing them in various applications (Node.js, iOS, JavaScript).</p>\n<p><strong><a href=\"http://www.libertyjs.com/\">LibertyJS 2016</a></strong><br><br>\n<em>September 30 - October 1, 2016</em></p>\n<p><strong>Workshop:</strong> <a href=\"http://www.libertyjs.com/speakers/\">Machine Learning for JavaScript Developers</a></p>\n<p>It has been suggested that in the future, developers will not so much program computers, as they will train them. Machine learning is having a broad impact across all industries, but what does it mean, and how does one use it in an application?</p>\n<p>Join IBM Developer Advocate, Kevin Hoyt, and IBM Watson, to learn a brief history of machine learning, common features of machine learning, and how to leverage those features in the browser, on the server, and on IoT devices, all with JavaScript.</p>\n<p>Coverage will include:</p>\n<ul>\n<li>Speech-To-Text</li>\n<li>Text-To-Speech</li>\n<li>Conversational Dialog</li>\n<li>Visual Recognition</li>\n<li>Translation</li>\n<li>Concept Extraction</li>\n<li>Tone Analysis</li>\n<li>Personality Insight</li>\n</ul>\n<p>Get ahead of the curve with what venture capitalist firm <a href=\"http://a16z.com/2016/06/10/ai-deep-learning-machines/\">Andreessen Horowitz</a> calls a fundamental technique that is expected to be in products.</p>\n<p><strong><a href=\"http://careerlink.com/hdc/\">Heartland Developer Conference 2016</a></strong><br><br>\n<em>September 7 - 9, 2016</em></p>\n<p><strong>Session:</strong> Node-RED for Faster Development</p>\n<p>If you have been programming long enough, you know that certain patterns emerge, and that work often gets wrapped in boilerplate code. An example would be CRUD (create, read, update, delete) operations on a database, that you might expose via an API. Sending an email or text message. What if you could develop, design, and orchestrate business logic using visual tooling?</p>\n<p>Node-RED is an open source visual tool for wiring together business logic, built on Node.js. Expose API endpoints via drag-and-drop. Consume APIs in the same way. Access databases, email server, Watson cognitive services, Twitter feeds, and more in a browser-based tool that allows you to visual control the flow of logic. IBM Developer Advocate, Kevin Hoyt, takes you on a tour of the tool that may just revolutionize how you work, with increased productivity, and faster time to market.</p>\n<p><strong>Workshop:</strong> <a href=\"http://careerlink.com/hdc/master-schedule\">Introduction to Deep Learning with IBM Watson</a></p>\n<p>What is the price of not knowing? Every day we generate 2.5 quintillion bytes of data, and 90% of the data in the world has been generated in the past two years alone. There are 7.3 billion people connected to the Internet around the world, creating 1.7 megabytes of data every minute. Unstructured data - dark data - accounts for 80% of all data generated today. Like dark matter, which cannot be seen, we know dark data exists by its impact (or lack thereof) on our industries.</p>\n<p>This is the dawn of the cognitive era - an inflection point that occurs once about every 40 years - that lends itself to machine learning. In this workshop, join IBM Developer Advocate, Kevin Hoyt, and go hands on with IBM Watson. Learn about APIs such as text-to-speech, and speech-to-text, translation, image recognition, natural language processing (NLP), concept and personality insights, and more. Leave with a new toolkit to help you leverage deep learning, and to solve the problems your industry faces.</p>\n<p><strong><a href=\"https://windycitythings.com/\">Windy City Things 2016</a></strong><br><br>\n<em>June 23 - 24, 2016</em></p>\n<p><strong>Session:</strong> <a href=\"https://windycitythings.com/schedule/\">IoT All The Things (An Audit)</a></p>\n<p>There are no shortage of IoT platforms to launch you next product, but which one is the right one for you? In this session join IBM Developer Advocate, Kevin Hoyt, on an audit of some of the more popular chipsets on the market. This will include overviews of Electric Imp, Photon Particle and Photon Electron, Tessel 2 from Technical Machines, and Intel Edison. Explore GPIO options, wireless pairing, development tooling, physical size and technical specifications (processor, battery, etc), set against a backdrop of real-time data delivery.</p>\n<p><strong><a href=\"http://www.webvisionsevent.com/portland/\">WebVisions Portland 2016</a></strong><br><br>\n<em>May 18 - 20, 2016</em></p>\n<p><strong>Workshop:</strong> <a href=\"http://www.webvisionsevent.com/portland/workshop/how-to-build-a-crowdsourced-pollution-monitoring-device/?loc=portland-2016\">On the Verge of Genius</a></p>\n<p>Cross a WebVisions workshop with Bill Nye the Science Guy (or Mr. Wizard depending on your generation), and you will get IBM's Kevin Hoyt, leading you through an interactive, hands-on, exploration of the increasingly connected world of cities, farms, and you.</p>\n<p>On this three-hour tour, the weather may just get rough, but smart cities with vast arrays of connected sensors will keep us on course and on time. Leaving the city behind, we will discover that data is the new fertilizer for the green acres of smart agriculture. The next stop on this fantastic voyage is inner space as we seek to leverage smart healthcare to unlock the secrets of heart disease and asthma.</p>\n<p>This workshop is packed with live demonstrations of a large number of scientific sensors in action. The PH of your drinking water. The air quality of the conference center. The galvanic skin response (sweating) of the presenter. And many more. Having established the possibilities, you will have the option to spend an hour with your very own Internet-connected hardware. Solving the world's problems is hard work, but together we can achieve genius.</p>\n<p><strong>Session:</strong> <a href=\"http://www.webvisionsevent.com/session/the-business-of-beacons/\">The Business of Beacons</a></p>\n<p>Built on BLE (Bluetooth Low Energy) beacons get a lot of press, and have seen increasing deployment around the globe, adding value to business and consumers. In this session, join IBM Developer Advocate, Kevin Hoyt, on a journey through beacon opportunities you may not have considered. Practical, real-world, references will be highlighted as we examine innovative use-cases for beacons in your business.</p>\n<p><strong><a href=\"http://www.heartlanddc.com\">Heartland Developer Conference 2015</a></strong><br><br>\n<em>September 9 - 11, 2015</em></p>\n<p><strong>Workshop:</strong> <a href=\"http://www.heartlanddc.com/a3\">Hands-On IoT</a></p>\n<p>This workshop will introduce you to IoT using the Particle (formerly Spark) Photon. The Particle Photon offers a suite of hardware and software tools to help you prototype, scale, and manage your IoT products. Based on Broadcom’s WICED architecture (used in Nest, Amazon Dash), combined with a powerful ARM Cortex M3, the Photon offer wi-fi for everything. During this workshop you will pair the Photon with a wireless network, and then learn to control the digital and analog features, both in firmware, and from your own web application.</p>\n<p><strong>Session:</strong> <a href=\"http://www.heartlanddc.com/session-201-2\">Unlocking Your Application with WebSocket</a></p>\n<p>WebSocket has been available to browsers (both mobile and desktop) for years, yet the potential remains largely untapped. In this session, get an introduction to using WebSocket in your applications. You will learn about the potential for WebSocket and real-time communication, and explore the usage of the publish/subscribe pattern over the commonplace request/response pattern. Packed with live, real-world demonstrations, IBM Developer Advocate, Kevin Hoyt, will show you how to unlock the potential latent in your applications. Other concepts that will be introduced include micro-services, container-based deployments (Docker), and the Internet of Things.</p>\n<p><strong>Keynote:</strong> <a href=\"http://www.heartlanddc.com/session-k3\">Welcome Makers</a></p>\n<p>In 1916, engineering student James Wilkins proposed spending $100 million to build a suspension bridge the world would eventually come to know as the Golden Gate Bridge.  Fast-forward 100 years, and we are rapidly throwing away our ability to innovate.  In this keynote address, join IBM Developer Adovcate, Kevin Hoyt, in taking a sneak peak at the next industrial revolution - the Internet of Things - and learn how it puts you, the developer, in the driver seat of opportunity.</p>\n<!--kg-card-end: markdown-->","comment_id":"33","plaintext":"TL;DR\n * DevNexus 2019 (session)\n * Heartland Developer Conference 2018 (workshop, session)\n * Index Conference 2018 (track manager)\n * Heartland Developer Conference 2017 (workshop, session)\n * FullStack London 2017 (session)\n * Jfokus 2017 (keynote)\n * Desert Code Camp 2016 (session)\n * LibertyJS 2016 (workshop)\n * Heartland Developer Conference 2016 (workshop, session)\n * WindyCityThings 2016 (session)\n * WebVisions Portland 2016 (workshop, session)\n * Heartland Developer Conference 2015 (workshop, session, keynote)\n\nBy Date (Newest to Oldest)\nDevNexus 2019 [https://devnexus.com]\n\nMarch 6 - 8, 2019\n\nSession: Computer Vision vs. Machine Learning\n[https://devnexus.com/presentations/3150]\n\nSet against the backdrop of the capabilities of the browser, this interactive\nsession compares and contrasts both approaches through two different\napplications. In the first, you will learn image processing and feature\ndetection techniques to track, and solve, a Rubik’s Cube. In the second, you\nwill learn to apply those techniques to build an object classifier for facial\nrecognition. Sit in the front row at your own risk!\n\nHeartland Developer Conference 2018 [http://careerlink.com/hdc]\n\nSeptember 5 - 7, 2018\n\nWorkshop: Computer Vision and Machine Learning in the Browser\n[http://sched.co/FEvn]\n\nThe browser, on both the desktop and mobile, are capable of much more than we\noften give them credit. In this workshop we will push the CPU to 100% as we\naudit several image processing libraries used for computer vision and machine\nlearning.\n\nLearn computer vision with real-time facial detection, optical character\nrecognition, color distance calculations, object tracking, and even reading\nbarcodes and augmented reality markers. From there, we will turn our focus to\nmachine learning where you will learn about developing models for neural\nnetworks.\n\nSession: Computer Vision vs. Machine Learning [http://sched.co/FEzB]\n\nComputer vision seeks to acquire, process, and analyze digital images based on\ndimensional data to produce numerical decisions. Machine learning applies\nconvolutional neural networks to classify, identify, and detect symbolic\ninformation from image data. The easiest way to decipher all of this and\nunderstand when to apply which technique to your application, is to attend this\nsession.\n\nSet against the backdrop of the capabilities of the browser, this interactive\nsession compares and contrasts both approaches through two different\napplications. In the first, you will learn image processing and feature\ndetection techniques to track, and solve, a Rubik’s Cube. In the second, you\nwill learn to apply those techniques to build an object classifier for facial\nrecognition. Sit in the front row at your own risk!\n\nIndex Conference 2018 [http://indexconf.com]\n\nFebruary 20 - 22, 2018\n\nTrack Manager: Geek Shop and the Future\n\nIn the fall of 2017, I was asked to manage a track centered around future\ntechnology for a new conference, Index. I had managed the web standards track at\nAdobe MAX years ago, and thoroughly enjoyed the collaboration. These days I tend\nto be more focused on emerging technology, so Index seemed like a great fit.\n\nAt first, as a passionate practitioner of desktop fabrication, and the maker\nmovement, I really wanted to focus on the \"geek shop\" side of things. Most of\nthe makers in my professional network were busy however, and the submitted\nabstracts pushed the track into the \"futures\" topic. Coverage included quantum\ncomputing, blockchain, augmented reality, and even \"wetware\" - the physical\nintegration of man and machine.\n\nHeartland Developer Conference 2017 [http://careerlink.com/hdc/]\n\nSeptember 6 - 8, 2017\n\nWorkshop: Understanding Blockchain [http://sched.co/B98S]\n\nThe cryptocurrency Bitcoin is well known even outside of technology spheres of\ninfluence. Underlying Bitcoin however is Blockchain - a distributed ledger with\nsome very special properties. These properties make Blockchain an ideal data\nstore for all variety of applications, not just financial records. This\nworkshop, half lecture and half interactive hands-on, is designed to help you\nunderstand blockchain, and how to use it in your business.\n\nLecture topics include:\n\n * Where does blockchain fit in your application infrastructure?\n * What are the differences between ledgers and databases?\n * How does chaining data work, and why does it matter?\n * What is decentralized consensus, and why does it matter?\n * What patterns identify an application as a fit for blockchain?\n * What are smart contracts, and how are they developed?\n\nHands-on activities include:\n\n * Model the business domain of a blockchain application\n * Write transaction functions using JavaScript\n * Test a blockchain application\n * Interact with blockchain via REST\n * Trade physical assets with other attendees via blockchain\n\nSession: Indoor Location with Beacons [http://sched.co/B9DA]\n\nHaving a highly capable and accurate GPS in everybody's pocket has fundamentally\nchanged the way we live, with broader impacts like self-driving cars, yet to\ncome. The moment you step inside the HDC conference center however, all that\ntechnology is virtually useless. In order to solve this problem, various\nvendors, from Apple to Google, and many others, have been implementing indoor\nlocation technologies, predominantly via beacons.\n\nIn this session learn how beacons function at the physical layer, how that gets\ninterpreted at the network layer, how to use the APIs at the presentation layer,\nand how to make sense of it all at the application layer. This will be\naccomplished as IBM Developer Advocate, Kevin Hoyt, configures and deploys a set\nof beacons in the conference room, live, during the session. We will also\nexplore how various industries are using indoor location from shopping malls to\nhotels, and airports to grocery stores.\n\nFullStack London 2017\n[https://skillsmatter.com/conferences/8264-fullstack-2017-the-conference-on-javascript-node-and-internet-of-things]\n\nJuly 12 - 14, 2017\n\nSession: Understanding Blockchain\n[https://skillsmatter.com/conferences/8264-fullstack-2017-the-conference-on-javascript-node-and-internet-of-things#program]\n\nThe cryptocurrency Bitcoin is well known even outside of technology spheres of\ninfluence. Underlying Bitcoin however is Blockchain - a distributed ledger with\nsome very special properties. These properties make Blockchain an ideal data\nstore for all variety of applications, not just financial records. In this\nsession, join IBM Developer Advocate, Kevin Hoyt, to understand blockchain\nterminology and unlock the potential for your business.\n\nCentered around a web-based to-do list application, built on the Polymer 2 RC,\ntopics covered will include spinning up your own Blockchain (Hyperledger Fabric\n1.0) node, writing and deploying a \"smart contract\" to perform CRUD operations,\nand integration using Web standards. With proficiency obtained, you will also\ntake a look at blockchain scaffolding tools. You will leave understanding where\nand how to use Blockchain technolgies, with the knowledge needed to get started.\n\nJfokus 2017 [https://www.jfokus.se/jfokus/]\n\nFebruary 6 - 8, 2017\n\nKeynote: Crossing the IoT Chasm\n[https://www.jfokus.se/jfokus/talks.jsp#CrossingtheIoTChasm]\n\nI gave my first IoT presentation over a decade ago. The response was\noverwhelming, and I've been waiting for the IoT tidal wave to land ever since.\nStrangely, it has not. Why not? Fast-forward through those ten years, and I have\nlearned countless lessons. Join me as I review the strengths and weakness of\nhardware platforms designed to get you to market, examine how Big Data may just\nhold the key to IoT success, and glimpse at an AI-powered future.\n\nDesert Code Camp 2016 [http://oct2016.desertcodecamp.com/]\n\nOctober 8, 2016\n\nSession: Explore Machine Learning with IBM Watson\n[http://oct2016.desertcodecamp.com/session/1313]\n\nIt has been suggested that in the future, developers will not program computers,\nas much as train them. This has largely to do with the rapid growth of machine\nlearning. But what is machine learning? What can it do for developers today? And\nhow does one leverage it in their application?\n\nIn this session, join IBM Developer Advocate, Kevin Hoyt, and IBM Watson, to\nuntangle the details. We will start with a little artificial intelligence\nhistory. From there we will move on to defining some of the more common machine\nlearning features by implementing them in various applications (Node.js, iOS,\nJavaScript).\n\nLibertyJS 2016 [http://www.libertyjs.com/]\n\nSeptember 30 - October 1, 2016\n\nWorkshop: Machine Learning for JavaScript Developers\n[http://www.libertyjs.com/speakers/]\n\nIt has been suggested that in the future, developers will not so much program\ncomputers, as they will train them. Machine learning is having a broad impact\nacross all industries, but what does it mean, and how does one use it in an\napplication?\n\nJoin IBM Developer Advocate, Kevin Hoyt, and IBM Watson, to learn a brief\nhistory of machine learning, common features of machine learning, and how to\nleverage those features in the browser, on the server, and on IoT devices, all\nwith JavaScript.\n\nCoverage will include:\n\n * Speech-To-Text\n * Text-To-Speech\n * Conversational Dialog\n * Visual Recognition\n * Translation\n * Concept Extraction\n * Tone Analysis\n * Personality Insight\n\nGet ahead of the curve with what venture capitalist firm Andreessen Horowitz\n[http://a16z.com/2016/06/10/ai-deep-learning-machines/] calls a fundamental\ntechnique that is expected to be in products.\n\nHeartland Developer Conference 2016 [http://careerlink.com/hdc/]\n\nSeptember 7 - 9, 2016\n\nSession: Node-RED for Faster Development\n\nIf you have been programming long enough, you know that certain patterns emerge,\nand that work often gets wrapped in boilerplate code. An example would be CRUD\n(create, read, update, delete) operations on a database, that you might expose\nvia an API. Sending an email or text message. What if you could develop, design,\nand orchestrate business logic using visual tooling?\n\nNode-RED is an open source visual tool for wiring together business logic, built\non Node.js. Expose API endpoints via drag-and-drop. Consume APIs in the same\nway. Access databases, email server, Watson cognitive services, Twitter feeds,\nand more in a browser-based tool that allows you to visual control the flow of\nlogic. IBM Developer Advocate, Kevin Hoyt, takes you on a tour of the tool that\nmay just revolutionize how you work, with increased productivity, and faster\ntime to market.\n\nWorkshop: Introduction to Deep Learning with IBM Watson\n[http://careerlink.com/hdc/master-schedule]\n\nWhat is the price of not knowing? Every day we generate 2.5 quintillion bytes of\ndata, and 90% of the data in the world has been generated in the past two years\nalone. There are 7.3 billion people connected to the Internet around the world,\ncreating 1.7 megabytes of data every minute. Unstructured data - dark data -\naccounts for 80% of all data generated today. Like dark matter, which cannot be\nseen, we know dark data exists by its impact (or lack thereof) on our\nindustries.\n\nThis is the dawn of the cognitive era - an inflection point that occurs once\nabout every 40 years - that lends itself to machine learning. In this workshop,\njoin IBM Developer Advocate, Kevin Hoyt, and go hands on with IBM Watson. Learn\nabout APIs such as text-to-speech, and speech-to-text, translation, image\nrecognition, natural language processing (NLP), concept and personality\ninsights, and more. Leave with a new toolkit to help you leverage deep learning,\nand to solve the problems your industry faces.\n\nWindy City Things 2016 [https://windycitythings.com/]\n\nJune 23 - 24, 2016\n\nSession: IoT All The Things (An Audit) [https://windycitythings.com/schedule/]\n\nThere are no shortage of IoT platforms to launch you next product, but which one\nis the right one for you? In this session join IBM Developer Advocate, Kevin\nHoyt, on an audit of some of the more popular chipsets on the market. This will\ninclude overviews of Electric Imp, Photon Particle and Photon Electron, Tessel 2\nfrom Technical Machines, and Intel Edison. Explore GPIO options, wireless\npairing, development tooling, physical size and technical specifications\n(processor, battery, etc), set against a backdrop of real-time data delivery.\n\nWebVisions Portland 2016 [http://www.webvisionsevent.com/portland/]\n\nMay 18 - 20, 2016\n\nWorkshop: On the Verge of Genius\n[http://www.webvisionsevent.com/portland/workshop/how-to-build-a-crowdsourced-pollution-monitoring-device/?loc=portland-2016]\n\nCross a WebVisions workshop with Bill Nye the Science Guy (or Mr. Wizard\ndepending on your generation), and you will get IBM's Kevin Hoyt, leading you\nthrough an interactive, hands-on, exploration of the increasingly connected\nworld of cities, farms, and you.\n\nOn this three-hour tour, the weather may just get rough, but smart cities with\nvast arrays of connected sensors will keep us on course and on time. Leaving the\ncity behind, we will discover that data is the new fertilizer for the green\nacres of smart agriculture. The next stop on this fantastic voyage is inner\nspace as we seek to leverage smart healthcare to unlock the secrets of heart\ndisease and asthma.\n\nThis workshop is packed with live demonstrations of a large number of scientific\nsensors in action. The PH of your drinking water. The air quality of the\nconference center. The galvanic skin response (sweating) of the presenter. And\nmany more. Having established the possibilities, you will have the option to\nspend an hour with your very own Internet-connected hardware. Solving the\nworld's problems is hard work, but together we can achieve genius.\n\nSession: The Business of Beacons\n[http://www.webvisionsevent.com/session/the-business-of-beacons/]\n\nBuilt on BLE (Bluetooth Low Energy) beacons get a lot of press, and have seen\nincreasing deployment around the globe, adding value to business and consumers.\nIn this session, join IBM Developer Advocate, Kevin Hoyt, on a journey through\nbeacon opportunities you may not have considered. Practical, real-world,\nreferences will be highlighted as we examine innovative use-cases for beacons in\nyour business.\n\nHeartland Developer Conference 2015 [http://www.heartlanddc.com]\n\nSeptember 9 - 11, 2015\n\nWorkshop: Hands-On IoT [http://www.heartlanddc.com/a3]\n\nThis workshop will introduce you to IoT using the Particle (formerly Spark)\nPhoton. The Particle Photon offers a suite of hardware and software tools to\nhelp you prototype, scale, and manage your IoT products. Based on Broadcom’s\nWICED architecture (used in Nest, Amazon Dash), combined with a powerful ARM\nCortex M3, the Photon offer wi-fi for everything. During this workshop you will\npair the Photon with a wireless network, and then learn to control the digital\nand analog features, both in firmware, and from your own web application.\n\nSession: Unlocking Your Application with WebSocket\n[http://www.heartlanddc.com/session-201-2]\n\nWebSocket has been available to browsers (both mobile and desktop) for years,\nyet the potential remains largely untapped. In this session, get an introduction\nto using WebSocket in your applications. You will learn about the potential for\nWebSocket and real-time communication, and explore the usage of the\npublish/subscribe pattern over the commonplace request/response pattern. Packed\nwith live, real-world demonstrations, IBM Developer Advocate, Kevin Hoyt, will\nshow you how to unlock the potential latent in your applications. Other concepts\nthat will be introduced include micro-services, container-based deployments\n(Docker), and the Internet of Things.\n\nKeynote: Welcome Makers [http://www.heartlanddc.com/session-k3]\n\nIn 1916, engineering student James Wilkins proposed spending $100 million to\nbuild a suspension bridge the world would eventually come to know as the Golden\nGate Bridge. Fast-forward 100 years, and we are rapidly throwing away our\nability to innovate. In this keynote address, join IBM Developer Adovcate, Kevin\nHoyt, in taking a sneak peak at the next industrial revolution - the Internet of\nThings - and learn how it puts you, the developer, in the driver seat of\nopportunity.","feature_image":"__GHOST_URL__/content/images/2019/02/audience.jpg","featured":0,"status":"published","locale":null,"visibility":"public","author_id":"1","created_at":"2015-06-22T20:48:36.000Z","updated_at":"2019-02-12T21:52:30.000Z","published_at":"2015-06-22T20:54:08.000Z","custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"type":"page","email_recipient_filter":"none"},{"id":"5adb8922351ffe0018a5772d","uuid":"21b46daf-4f25-40bf-a97e-80c0b7166645","title":"Bluemix Web Runtimes","slug":"bluemix-crawl-runtimes","mobiledoc":"{\"version\":\"0.3.1\",\"markups\":[],\"atoms\":[],\"cards\":[[\"markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\"Did you know that IBM has a PaaS (Platform as a Service) offering?  We do!  It is called [IBM Bluemix](http://www.ibm.com/bluemix), and it consists of some ~120 different components, each with their own array of features.  From Docker containers, to message brokers, from databases, to IoT (Internet of Things) integration - everything an enterprise needs is at your fingertips.  Over the next several weeks, I will be crawling these components and features.  Up first will be finding your way around.\\n\\n###Web Runtimes Overview\\n\\nWhen it comes to services, two different aspects are likely to come to mind.  The first is a service (perhaps RESTful) that you can use in your application.  IBM Bluemix offers no shortage of those types of services.  The second is ==running your own code in the cloud== to provide either services of your own, or a complete application.  The later of these two in Bluemix vernacular is called \\\"==runtimes==\\\".\\n\\n>At the time of this writing, IBM Bluemix supports Java EE applications, Node.js, Go, PHP, Python, ASP.NET, and Ruby.\\n\\nAll of these have a similar approach to deploying your code in the cloud.  I am going to cover the three I use most often - PHP, Node.js, and Java EE.  From there you will see a pattern emerge and be able to easily deploy an application on one of the other runtimes.  If there is another runtime you want to see covered, please [let me know](http://www.twitter.com/krhoyt).\\n\\n###Bluemix Dashboard\\n\\nThe first step in getting started with IBM Bluemix is to create your own account.  This is ==easy== enough, and ==costs you nothing== to get started.  Head on over to the Bluemix web site, and click on the button labeled \\\"Sign Up\\\".  Once you have completed the sign up process, you will be presented with the Bluemix Dashboard.\\n\\n![Bluemix Dashboard](http://images.kevinhoyt.com/bluemix-dashboard.png)\\n\\n### Creating Your Application\\n\\nThe IBM Bluemix Dashboard is regularly updated with new features.  The screenshot above may change over time.  What you are looking for is something along the lines of \\\"==Create App==\\\" or \\\"==Cloud Foundry Apps==\\\".  In the above image, it is the upper-lefthand box in the grid of four boxes presented at the top of the page.  Click \\\"==Create App==\\\" to start the process.\\n\\n![Create an application.](http://images.kevinhoyt.com/bluemix-dashboard-create.png)\\n\\nNext you will be prompted to create a \\\"Web\\\" or \\\"Mobile\\\" application.  The \\\"Web\\\" option refers to the language runtimes, while the \\\"Mobile\\\" option includes all that and more for creating iOS or Android applications.  Since we are interested using language runtimes, ==click on the \\\"Web\\\" option==.\\n\\n![Selecting a Bluemix runtime.](http://images.kevinhoyt.com/bluemix-dashboard-web.png)\\n\\nHere you will see all the runtimes you can choose from for your application.  Note that not only are various runtimes presented for you to choose from, you can also browse existing code samples (called \\\"Buildpacks\\\"), and even just jump straight to uploading you own code if you know what you are doing.  ==Select the a runtime option.==  For this tutorial, I selected the PHP runtime.\\n\\nWhen you click on a runtime option, you will get more details about what is provided.  This will include limits on the free version, and links for more details and documentation.  ==Click on the button labeled \\\"Continue\\\".==\\n\\n![Name your application.](http://images.kevinhoyt.com/bluemix-dashboard-name.png)\\n\\nNext up is giving your application a name.  In the interest of giving you something to work with, ==I will be covering a basic weather application== in all my runtime tutorials.  The weather application is the same client logic across all runtime examples.  Since I selected a PHP runtime, I will ==enter \\\"PHP Weather\\\" and click the \\\"Finish\\\" button.==\\n\\n![IBM Bluemix will start your application](http://images.kevinhoyt.com/bluemix-dashboard-start.png)\\n\\nFrom here, IBM Bluemix goes to work.  It will create the application, and start it.  A link to the running application will also be presented.  What is there initially, is some static files welcoming you to Bluemix.  What follows is a list of next steps to deploy your own application.\\n\\n###Command Line Interface\\n\\nThe next step is to download and install the \\\"CF Command Line Interface\\\".  The \\\"CF\\\" here stands for \\\"Cloud Foundry\\\".  Clicking on the download button will take you to the Cloud Foundry GitHub repository where you can ==download the installer for your platform==.  Supported platforms for the CLI (Command Line Interface) include Debian, Red Hat, Mac OS X, and Windows.\\n\\n> Once installed you can open a command line window and type \\\"cf -v\\\" to make sure the installation was successful.\\n\\nThe CLI gives us tooling for working with Bluemix.  If you prefer a Git workflow, that option is also available to you.  For the runtime tutorials, I will be covering the CLI approach to deployment.\\n\\n###Download Starter Code\\n\\nBack in the IBM Bluemix Dashboard, the next step is to \\\"==Download Starter Code==\\\".  Expanding the downloaded archive will give you everything you need to get started with your own Bluemix application.  The most important file for Bluemix here is the \\\"manifest.yml\\\" file.  \\n\\n```\\napplications:\\n- disk_quota: 1024M\\n  buildpack: php_buildpack\\n  host: php-weather\\n  name: PHP Weather\\n  path: .\\n  domain: mybluemix.net\\n  instances: 1\\n  memory: 128M\\n```\\n\\n==The manifest file tells IBM Bluemix how to deploy your application.==  Depending on the runtime you selected, you will find other files in a familiar project structure.  For example, with the selected PHP runtime, there is a \\\"composer.json\\\" dependency management file.  For Node.js there will be a \\\"package.json\\\" file.  For Java, a \\\"build.xml\\\" for use with Ant.\\n\\nJust so we can see something interesting, you may want to modify one of the visible HTML files by changing a few characters.  If you are just interested in the mechanics for now, we can move along to using the CF CLI to deploy our starter code to IBM Bluemix.\\n\\n###Deploy to IBM Bluemix\\n\\nIn a terminal window, ==change to the directory== containing the downloaded starter code.  \\n\\n```\\ncd ~/Desktop/PHP+Weather\\n```\\n\\nNext we will want to ==tell the CLI to connect== to IBM Bluemix.  Why do we have to tell the CLI, which we downloaded from Bluemix, to connect to Bluemix?  As it turns out, Cloud Foundry is open source, so it is entirely possible that you will use the same CLI tool for different environments.\\n\\n```\\ncf api https://api.ng.bluemix.net\\n```\\n\\nNow that the CLI knows where the Cloud Foundry instance is that we will be using, we will want to ==login to IBM Bluemix==.  \\n\\n```\\ncf login -u your_email@server.com -o your_email@server.com -s Development\\n```\\n\\nNotice that the last argument to the CLI is the word \\\"Development\\\".  This is the name I have given to the \\\"space\\\" created when I setup my IBM Bluemix account.  A \\\"space\\\" is a logical collection of your Bluemix components.  I also have a \\\"Production\\\" space as an example.  Be sure to ==change the value of the last argument== to the space you will be using on your account.\\n\\nThe final step is to push the code to IBM Bluemix.  How this happens will differ slightly based on the runtime you selected.  Ultimately however, Bluemix will stop the currently running instance, examine your code to assemble dependencies, and then actually deploy and start an instance with your new code.\\n\\n```\\ncf push \\\"PHP Weather\\\"\\n```\\n\\n==Not all of the CLI commands are necessary each and every time== you want to deploy new code to IBM Bluemix.  If you are actively developing, then you will likely just use \\\"cf push\\\" to upload your latest stable build.\\n\\nIt is also worth noting that the project structure in ==the starter code will give you a working application== that you can run locally for quicker development.  For example, when using PHP, after changing to the starter code directory, I ran \\\"==php -S localhost:8000==\\\" to provide a basic web server with PHP features to test against.  From there, I only used \\\"cf push\\\" when I felt a had a stable build.\\n\\n==Your application is now live on IBM Bluemix!==  The CLI will tell you the URL to find you application, as will the dashboard.  In the case of \\\"PHP Weather\\\" I was able to point my browser at \\\"http://php-weather.mybluemix.net\\\".\\n\\n###Next Steps\\n\\nNow that you are familiar with the basic of using the IBM Bluemix dashboard, and associated runtime tooling, the next step is to move on to deploying our own code.\\n\\nAs mentioned, I have built a simple weather application for the purposes of testing out the various runtime options on IBM Bluemix.  In my next post, I will take a look at the server-side code for powering the application using PHP, Node.js, and Java.\\n\\n![IBM Bluemix and PHP weather application.](http://images.kevinhoyt.com/bluemix-crawl-weather-php.png)\\n\\nIf you want to get a sneak peak at the code, you can download the source for all three projects on [my GitHub repository](https://github.com/krhoyt/IBM/tree/master/crawl/runtimes).  Until then, have fun exploring the vast catalog of features IBM Bluemix offers developers.\\n\"}]],\"sections\":[[10,0]],\"ghostVersion\":\"3.0\"}","html":"<!--kg-card-begin: markdown--><p>Did you know that IBM has a PaaS (Platform as a Service) offering?  We do!  It is called <a href=\"http://www.ibm.com/bluemix\">IBM Bluemix</a>, and it consists of some ~120 different components, each with their own array of features.  From Docker containers, to message brokers, from databases, to IoT (Internet of Things) integration - everything an enterprise needs is at your fingertips.  Over the next several weeks, I will be crawling these components and features.  Up first will be finding your way around.</p>\n<h3 id=\"webruntimesoverview\">Web Runtimes Overview</h3>\n<p>When it comes to services, two different aspects are likely to come to mind.  The first is a service (perhaps RESTful) that you can use in your application.  IBM Bluemix offers no shortage of those types of services.  The second is <mark>running your own code in the cloud</mark> to provide either services of your own, or a complete application.  The later of these two in Bluemix vernacular is called &quot;<mark>runtimes</mark>&quot;.</p>\n<blockquote>\n<p>At the time of this writing, IBM Bluemix supports Java EE applications, Node.js, Go, PHP, Python, ASP.NET, and Ruby.</p>\n</blockquote>\n<p>All of these have a similar approach to deploying your code in the cloud.  I am going to cover the three I use most often - PHP, Node.js, and Java EE.  From there you will see a pattern emerge and be able to easily deploy an application on one of the other runtimes.  If there is another runtime you want to see covered, please <a href=\"http://www.twitter.com/krhoyt\">let me know</a>.</p>\n<h3 id=\"bluemixdashboard\">Bluemix Dashboard</h3>\n<p>The first step in getting started with IBM Bluemix is to create your own account.  This is <mark>easy</mark> enough, and <mark>costs you nothing</mark> to get started.  Head on over to the Bluemix web site, and click on the button labeled &quot;Sign Up&quot;.  Once you have completed the sign up process, you will be presented with the Bluemix Dashboard.</p>\n<p><img src=\"http://images.kevinhoyt.com/bluemix-dashboard.png\" alt=\"Bluemix Dashboard\" loading=\"lazy\"></p>\n<h3 id=\"creatingyourapplication\">Creating Your Application</h3>\n<p>The IBM Bluemix Dashboard is regularly updated with new features.  The screenshot above may change over time.  What you are looking for is something along the lines of &quot;<mark>Create App</mark>&quot; or &quot;<mark>Cloud Foundry Apps</mark>&quot;.  In the above image, it is the upper-lefthand box in the grid of four boxes presented at the top of the page.  Click &quot;<mark>Create App</mark>&quot; to start the process.</p>\n<p><img src=\"http://images.kevinhoyt.com/bluemix-dashboard-create.png\" alt=\"Create an application.\" loading=\"lazy\"></p>\n<p>Next you will be prompted to create a &quot;Web&quot; or &quot;Mobile&quot; application.  The &quot;Web&quot; option refers to the language runtimes, while the &quot;Mobile&quot; option includes all that and more for creating iOS or Android applications.  Since we are interested using language runtimes, <mark>click on the &quot;Web&quot; option</mark>.</p>\n<p><img src=\"http://images.kevinhoyt.com/bluemix-dashboard-web.png\" alt=\"Selecting a Bluemix runtime.\" loading=\"lazy\"></p>\n<p>Here you will see all the runtimes you can choose from for your application.  Note that not only are various runtimes presented for you to choose from, you can also browse existing code samples (called &quot;Buildpacks&quot;), and even just jump straight to uploading you own code if you know what you are doing.  <mark>Select the a runtime option.</mark>  For this tutorial, I selected the PHP runtime.</p>\n<p>When you click on a runtime option, you will get more details about what is provided.  This will include limits on the free version, and links for more details and documentation.  <mark>Click on the button labeled &quot;Continue&quot;.</mark></p>\n<p><img src=\"http://images.kevinhoyt.com/bluemix-dashboard-name.png\" alt=\"Name your application.\" loading=\"lazy\"></p>\n<p>Next up is giving your application a name.  In the interest of giving you something to work with, <mark>I will be covering a basic weather application</mark> in all my runtime tutorials.  The weather application is the same client logic across all runtime examples.  Since I selected a PHP runtime, I will <mark>enter &quot;PHP Weather&quot; and click the &quot;Finish&quot; button.</mark></p>\n<p><img src=\"http://images.kevinhoyt.com/bluemix-dashboard-start.png\" alt=\"IBM Bluemix will start your application\" loading=\"lazy\"></p>\n<p>From here, IBM Bluemix goes to work.  It will create the application, and start it.  A link to the running application will also be presented.  What is there initially, is some static files welcoming you to Bluemix.  What follows is a list of next steps to deploy your own application.</p>\n<h3 id=\"commandlineinterface\">Command Line Interface</h3>\n<p>The next step is to download and install the &quot;CF Command Line Interface&quot;.  The &quot;CF&quot; here stands for &quot;Cloud Foundry&quot;.  Clicking on the download button will take you to the Cloud Foundry GitHub repository where you can <mark>download the installer for your platform</mark>.  Supported platforms for the CLI (Command Line Interface) include Debian, Red Hat, Mac OS X, and Windows.</p>\n<blockquote>\n<p>Once installed you can open a command line window and type &quot;cf -v&quot; to make sure the installation was successful.</p>\n</blockquote>\n<p>The CLI gives us tooling for working with Bluemix.  If you prefer a Git workflow, that option is also available to you.  For the runtime tutorials, I will be covering the CLI approach to deployment.</p>\n<h3 id=\"downloadstartercode\">Download Starter Code</h3>\n<p>Back in the IBM Bluemix Dashboard, the next step is to &quot;<mark>Download Starter Code</mark>&quot;.  Expanding the downloaded archive will give you everything you need to get started with your own Bluemix application.  The most important file for Bluemix here is the &quot;manifest.yml&quot; file.</p>\n<pre><code>applications:\n- disk_quota: 1024M\n  buildpack: php_buildpack\n  host: php-weather\n  name: PHP Weather\n  path: .\n  domain: mybluemix.net\n  instances: 1\n  memory: 128M\n</code></pre>\n<p><mark>The manifest file tells IBM Bluemix how to deploy your application.</mark>  Depending on the runtime you selected, you will find other files in a familiar project structure.  For example, with the selected PHP runtime, there is a &quot;composer.json&quot; dependency management file.  For Node.js there will be a &quot;package.json&quot; file.  For Java, a &quot;build.xml&quot; for use with Ant.</p>\n<p>Just so we can see something interesting, you may want to modify one of the visible HTML files by changing a few characters.  If you are just interested in the mechanics for now, we can move along to using the CF CLI to deploy our starter code to IBM Bluemix.</p>\n<h3 id=\"deploytoibmbluemix\">Deploy to IBM Bluemix</h3>\n<p>In a terminal window, <mark>change to the directory</mark> containing the downloaded starter code.</p>\n<pre><code>cd ~/Desktop/PHP+Weather\n</code></pre>\n<p>Next we will want to <mark>tell the CLI to connect</mark> to IBM Bluemix.  Why do we have to tell the CLI, which we downloaded from Bluemix, to connect to Bluemix?  As it turns out, Cloud Foundry is open source, so it is entirely possible that you will use the same CLI tool for different environments.</p>\n<pre><code>cf api https://api.ng.bluemix.net\n</code></pre>\n<p>Now that the CLI knows where the Cloud Foundry instance is that we will be using, we will want to <mark>login to IBM Bluemix</mark>.</p>\n<pre><code>cf login -u your_email@server.com -o your_email@server.com -s Development\n</code></pre>\n<p>Notice that the last argument to the CLI is the word &quot;Development&quot;.  This is the name I have given to the &quot;space&quot; created when I setup my IBM Bluemix account.  A &quot;space&quot; is a logical collection of your Bluemix components.  I also have a &quot;Production&quot; space as an example.  Be sure to <mark>change the value of the last argument</mark> to the space you will be using on your account.</p>\n<p>The final step is to push the code to IBM Bluemix.  How this happens will differ slightly based on the runtime you selected.  Ultimately however, Bluemix will stop the currently running instance, examine your code to assemble dependencies, and then actually deploy and start an instance with your new code.</p>\n<pre><code>cf push &quot;PHP Weather&quot;\n</code></pre>\n<p><mark>Not all of the CLI commands are necessary each and every time</mark> you want to deploy new code to IBM Bluemix.  If you are actively developing, then you will likely just use &quot;cf push&quot; to upload your latest stable build.</p>\n<p>It is also worth noting that the project structure in <mark>the starter code will give you a working application</mark> that you can run locally for quicker development.  For example, when using PHP, after changing to the starter code directory, I ran &quot;<mark>php -S localhost:8000</mark>&quot; to provide a basic web server with PHP features to test against.  From there, I only used &quot;cf push&quot; when I felt a had a stable build.</p>\n<p><mark>Your application is now live on IBM Bluemix!</mark>  The CLI will tell you the URL to find you application, as will the dashboard.  In the case of &quot;PHP Weather&quot; I was able to point my browser at &quot;<a href=\"http://php-weather.mybluemix.net\">http://php-weather.mybluemix.net</a>&quot;.</p>\n<h3 id=\"nextsteps\">Next Steps</h3>\n<p>Now that you are familiar with the basic of using the IBM Bluemix dashboard, and associated runtime tooling, the next step is to move on to deploying our own code.</p>\n<p>As mentioned, I have built a simple weather application for the purposes of testing out the various runtime options on IBM Bluemix.  In my next post, I will take a look at the server-side code for powering the application using PHP, Node.js, and Java.</p>\n<p><img src=\"http://images.kevinhoyt.com/bluemix-crawl-weather-php.png\" alt=\"IBM Bluemix and PHP weather application.\" loading=\"lazy\"></p>\n<p>If you want to get a sneak peak at the code, you can download the source for all three projects on <a href=\"https://github.com/krhoyt/IBM/tree/master/crawl/runtimes\">my GitHub repository</a>.  Until then, have fun exploring the vast catalog of features IBM Bluemix offers developers.</p>\n<!--kg-card-end: markdown-->","comment_id":"34","plaintext":"Did you know that IBM has a PaaS (Platform as a Service) offering? We do! It is\ncalled IBM Bluemix [http://www.ibm.com/bluemix], and it consists of some ~120\ndifferent components, each with their own array of features. From Docker\ncontainers, to message brokers, from databases, to IoT (Internet of Things)\nintegration - everything an enterprise needs is at your fingertips. Over the\nnext several weeks, I will be crawling these components and features. Up first\nwill be finding your way around.\n\nWeb Runtimes Overview\nWhen it comes to services, two different aspects are likely to come to mind. The\nfirst is a service (perhaps RESTful) that you can use in your application. IBM\nBluemix offers no shortage of those types of services. The second is running\nyour own code in the cloud to provide either services of your own, or a complete\napplication. The later of these two in Bluemix vernacular is called \"runtimes\".\n\n> At the time of this writing, IBM Bluemix supports Java EE applications, Node.js,\nGo, PHP, Python, ASP.NET, and Ruby.\n\n\nAll of these have a similar approach to deploying your code in the cloud. I am\ngoing to cover the three I use most often - PHP, Node.js, and Java EE. From\nthere you will see a pattern emerge and be able to easily deploy an application\non one of the other runtimes. If there is another runtime you want to see\ncovered, please let me know [http://www.twitter.com/krhoyt].\n\nBluemix Dashboard\nThe first step in getting started with IBM Bluemix is to create your own\naccount. This is easy enough, and costs you nothing to get started. Head on over\nto the Bluemix web site, and click on the button labeled \"Sign Up\". Once you\nhave completed the sign up process, you will be presented with the Bluemix\nDashboard.\n\n\n\nCreating Your Application\nThe IBM Bluemix Dashboard is regularly updated with new features. The screenshot\nabove may change over time. What you are looking for is something along the\nlines of \"Create App\" or \"Cloud Foundry Apps\". In the above image, it is the\nupper-lefthand box in the grid of four boxes presented at the top of the page.\nClick \"Create App\" to start the process.\n\n\n\nNext you will be prompted to create a \"Web\" or \"Mobile\" application. The \"Web\"\noption refers to the language runtimes, while the \"Mobile\" option includes all\nthat and more for creating iOS or Android applications. Since we are interested\nusing language runtimes, click on the \"Web\" option.\n\n\n\nHere you will see all the runtimes you can choose from for your application.\nNote that not only are various runtimes presented for you to choose from, you\ncan also browse existing code samples (called \"Buildpacks\"), and even just jump\nstraight to uploading you own code if you know what you are doing. Select the a\nruntime option. For this tutorial, I selected the PHP runtime.\n\nWhen you click on a runtime option, you will get more details about what is\nprovided. This will include limits on the free version, and links for more\ndetails and documentation. Click on the button labeled \"Continue\".\n\n\n\nNext up is giving your application a name. In the interest of giving you\nsomething to work with, I will be covering a basic weather application in all my\nruntime tutorials. The weather application is the same client logic across all\nruntime examples. Since I selected a PHP runtime, I will enter \"PHP Weather\" and\nclick the \"Finish\" button.\n\n\n\nFrom here, IBM Bluemix goes to work. It will create the application, and start\nit. A link to the running application will also be presented. What is there\ninitially, is some static files welcoming you to Bluemix. What follows is a list\nof next steps to deploy your own application.\n\nCommand Line Interface\nThe next step is to download and install the \"CF Command Line Interface\". The\n\"CF\" here stands for \"Cloud Foundry\". Clicking on the download button will take\nyou to the Cloud Foundry GitHub repository where you can download the installer\nfor your platform. Supported platforms for the CLI (Command Line Interface)\ninclude Debian, Red Hat, Mac OS X, and Windows.\n\n> Once installed you can open a command line window and type \"cf -v\" to make sure\nthe installation was successful.\n\n\nThe CLI gives us tooling for working with Bluemix. If you prefer a Git workflow,\nthat option is also available to you. For the runtime tutorials, I will be\ncovering the CLI approach to deployment.\n\nDownload Starter Code\nBack in the IBM Bluemix Dashboard, the next step is to \"Download Starter Code\".\nExpanding the downloaded archive will give you everything you need to get\nstarted with your own Bluemix application. The most important file for Bluemix\nhere is the \"manifest.yml\" file.\n\napplications:\n- disk_quota: 1024M\n  buildpack: php_buildpack\n  host: php-weather\n  name: PHP Weather\n  path: .\n  domain: mybluemix.net\n  instances: 1\n  memory: 128M\n\n\nThe manifest file tells IBM Bluemix how to deploy your application. Depending on\nthe runtime you selected, you will find other files in a familiar project\nstructure. For example, with the selected PHP runtime, there is a\n\"composer.json\" dependency management file. For Node.js there will be a\n\"package.json\" file. For Java, a \"build.xml\" for use with Ant.\n\nJust so we can see something interesting, you may want to modify one of the\nvisible HTML files by changing a few characters. If you are just interested in\nthe mechanics for now, we can move along to using the CF CLI to deploy our\nstarter code to IBM Bluemix.\n\nDeploy to IBM Bluemix\nIn a terminal window, change to the directory containing the downloaded starter\ncode.\n\ncd ~/Desktop/PHP+Weather\n\n\nNext we will want to tell the CLI to connect to IBM Bluemix. Why do we have to\ntell the CLI, which we downloaded from Bluemix, to connect to Bluemix? As it\nturns out, Cloud Foundry is open source, so it is entirely possible that you\nwill use the same CLI tool for different environments.\n\ncf api https://api.ng.bluemix.net\n\n\nNow that the CLI knows where the Cloud Foundry instance is that we will be\nusing, we will want to login to IBM Bluemix.\n\ncf login -u your_email@server.com -o your_email@server.com -s Development\n\n\nNotice that the last argument to the CLI is the word \"Development\". This is the\nname I have given to the \"space\" created when I setup my IBM Bluemix account. A\n\"space\" is a logical collection of your Bluemix components. I also have a\n\"Production\" space as an example. Be sure to change the value of the last\nargument to the space you will be using on your account.\n\nThe final step is to push the code to IBM Bluemix. How this happens will differ\nslightly based on the runtime you selected. Ultimately however, Bluemix will\nstop the currently running instance, examine your code to assemble dependencies,\nand then actually deploy and start an instance with your new code.\n\ncf push \"PHP Weather\"\n\n\nNot all of the CLI commands are necessary each and every time you want to deploy\nnew code to IBM Bluemix. If you are actively developing, then you will likely\njust use \"cf push\" to upload your latest stable build.\n\nIt is also worth noting that the project structure in the starter code will give\nyou a working application that you can run locally for quicker development. For\nexample, when using PHP, after changing to the starter code directory, I ran \"\nphp -S localhost:8000\" to provide a basic web server with PHP features to test\nagainst. From there, I only used \"cf push\" when I felt a had a stable build.\n\nYour application is now live on IBM Bluemix! The CLI will tell you the URL to\nfind you application, as will the dashboard. In the case of \"PHP Weather\" I was\nable to point my browser at \"http://php-weather.mybluemix.net\".\n\nNext Steps\nNow that you are familiar with the basic of using the IBM Bluemix dashboard, and\nassociated runtime tooling, the next step is to move on to deploying our own\ncode.\n\nAs mentioned, I have built a simple weather application for the purposes of\ntesting out the various runtime options on IBM Bluemix. In my next post, I will\ntake a look at the server-side code for powering the application using PHP,\nNode.js, and Java.\n\n\n\nIf you want to get a sneak peak at the code, you can download the source for all\nthree projects on my GitHub repository\n[https://github.com/krhoyt/IBM/tree/master/crawl/runtimes]. Until then, have fun\nexploring the vast catalog of features IBM Bluemix offers developers.","feature_image":"http://images.kevinhoyt.com/subway-platform-yellow-stripe.jpg","featured":0,"status":"published","locale":null,"visibility":"public","author_id":"1","created_at":"2015-07-22T21:43:36.000Z","updated_at":"2015-07-28T23:22:58.000Z","published_at":"2015-07-28T17:22:38.000Z","custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"type":"post","email_recipient_filter":"none"},{"id":"5adb8922351ffe0018a5772e","uuid":"43bd7ce5-3513-46fc-9aff-d96934091a9e","title":"Weather in Three Flavors","slug":"weather-in-three-flavors","mobiledoc":"{\"version\":\"0.3.1\",\"markups\":[],\"atoms\":[],\"cards\":[[\"markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\"In my [last post](http://blog.kevinhoyt.com/2015/07/28/bluemix-crawl-runtimes/), I gave an overview of IBM Bluemix, and walked through the process of deploying your web first application.  For this post, I will walk through a simple weather application I used to test out some of the Bluemix web runtimes.  The flavors for this application comes in PHP, Node.js, and Java.\\n\\n###Quick Recap\\n\\nJust as a quick reminder, ==IBM Bluemix is a PaaS== (Platform as a Service) offering from IBM.  It consists of ~120 different components at the time of this writing.  The ability to run your own server code is called \\\"Runtimes\\\" in Bluemix vernacular, and is just one of the available components.\\n\\nAs IBM Bluemix is based on the open source [Cloud Foundry](https://www.cloudfoundry.org) project, you use the Cloud Foundry tooling to deploy your applications - most notably a CLI (command line interface).  A \\\"manifest.yml\\\" is required to identify your application, and to supply the runtime options to Bluemix.\\n\\n###Make It Rain\\n\\nThis sample weather application consists of a few different pieces - none of them particularly complex.  We will start off by taking a look at the ==client-side== code, and the general behavior of the application.\\n\\n![IBM Bluemix weather application.](http://images.kevinhoyt.com/bluemix-crawl-weather-php.png)\\n\\nWhen the weather application is loaded in the browser, the first thing it does is use ==XHR== (XMLHttpRequest) to load the \\\"==weather.txt==\\\" file in the background.  This file tells the client where to get the weather data - the URI of the service.  I externalized this piece of information to make the client side more portable across different languages and configurations.\\n\\n```\\n// Geolocation\\nif( navigator.geolocation ) \\n{\\n  navigator.geolocation.getCurrentPosition( \\n    doLocationSuccess, \\n    doLocationError \\n  );\\n} else {\\n  doLocationError( 'Geolocation not supported.' );\\n}\\n\\n...\\n\\n// Lookup weather data\\nfunction doLocationSuccess( position )\\n{\\n    // Debug\\n    console.log(\\n        position.coords.latitude + \\n        ', ' +\\n        position.coords.longitude\\n    );\\n \\n    // Get weather\\n    weather( position );\\n}       \\n```\\n\\nWith the service endpoint URI obtained, the JavaScript will then use the browser [Geolocation API](http://caniuse.com/#feat=geolocation) to get the latitude and longitude of your computer/device.  Once obtained, the latitude and longitude are sent to the server for processing (the loaded URI), using XHR.\\n\\nThe called server resource will load a file called \\\"==forecast.io==\\\" which contains an API (application program interface) key for the ==Forecast== service.  [Forecast](http://forecast.io) is a weather service that I am using for this application.  The API is simple yet powerful, and there is a free tier for development and testing purposes.  Perfect for this sample application.\\n\\n> For security reasons, my Forecast API key is not included in the GitHub repository.  You will need to sign up for [your own Forecast account](https://developer.forecast.io), get your own key, and put it in a file named \\\"forecast.io\\\" in the same directory as the rest of the application files.\\n\\nWith latitude, longitude, and API key in hand, the code then makes a server-side HTTP request against the Forecast API.  Current weather details are parsed from the JSON (JavaScript Object Notation) response.  The proper name of the location (city, state) is not included in the response, so a subsequent call is made against a ==Google API for reverse geocoding== (address lookup).\\n\\nThe results of all the pertinent data is sent back to the browser as a ==JSON-encoded== string.  The browser gets that information, populates the various UI elements, and the weather is presented to the user.  This happens once per minute so long as the web page is front and center in the browser.\\n\\n###PHP\\n\\nMy biggest concern for using PHP on IBM Bluemix was ==file IO==.  Not that file IO is a particularly tough task for PHP, but that I have had my share of unsupported file IO features on cloud servers in the past.  I have also seen cloud service providers lock down file IO as a security risk, then provide their own, less than ideal, library wrapper for you to use.\\n\\n```\\n// Get API key\\n$forecast_key = trim( file_get_contents( $KEY_FILE ) );\\n```\\n\\nMuch to my surprise, IBM Bluemix actually lets you pick the [version of PHP](https://www.ng.bluemix.net/docs/starters/php/index.html#phpversions) that you want to run.  Version 5.5.23 is the default, but you can also specify versions in the 5.4.x range and 5.6.x range.  Once my development version (minor) was matched with the Bluemix version, I encountered ==no problems, or limits==, at all.\\n\\n> All things being equal, PHP was probably the easiest of the three to deploy and use on IBM Bluemix.  Drop some files in a directory, add a \\\"manifest.yml\\\", push to the cloud, and done.\\n\\n###Node.js\\n\\nWhen it comes to Node.js, [Express](http://expressjs.com) is the de facto standard web framework.  When you download the starter code from IBM Bluemix, you will find an Express template already waiting for you.  In fact, Node.js and Express are widely used in other areas of Bluemix as well.\\n\\n```\\nvar cfenv = require( 'cfenv' );\\nvar bluemix = cfenv.getAppEnv();\\n\\n// Start server\\napp.listen( bluemix.port, bluemix.bind, function() {\\n  console.log( 'Server starting on: ' + bluemix.url );\\n} );\\n```\\n\\nWhen running Node.js in the cloud however, you may not know anything about the deployment directory, port, or other environment variables.  This means you need to abstract them from your code.  IBM Bluemix helps you out here with the \\\"==cfenv==\\\" library.  This handy library lets your application know how Bluemix has deployed it.  ==There is even access to custom environment variables you can set in the Bluemix dashboard.==\\n\\n> You can also use a \\\"package.json\\\" file to have Bluemix load dependencies for you during deployment - and there is no limitation on which libraries you can use.\\n\\n###Java\\n\\nIf we are comparing PHP, Node.js, and Java, then Java is a whole different animal.  But if you are a Java developer, then you probably already know that.  When you download the starter code for Java from IBM Bluemix, you get the boilerplate for a ==full Java EE== deployment.  Indeed, when you run a Java application on Bluemix, it runs no the [WebSphere Application Server Liberty Profile](https://developer.ibm.com/wasdev/websphere-liberty/) (full Java EE 7).\\n\\nIf you know your way around a Java EE application, then you should feel right at home in the starter code.  There is an [Apache Ant](http://ant.apache.org) build file already included.  Import this project into Eclipse for Java EE, and you will be off to the races in no time.\\n\\n```\\n@Path( \\\"/weather\\\" )\\npublic class WeatherService \\n{\\n  // Get request for weather data\\n  @GET\\t\\n  public String doWeather( \\n    @QueryParam( \\\"latitude\\\" ) String latitude, \\n    @QueryParam( \\\"longitude\\\" ) String longitude ) {\\n\\n    // Service code here\\n\\n  }\\n}\\n```\\n\\n> When I settled into this project, I completely expected to build a servlet for my web service handling.  What tickled me pink however, was the support for ==JAX-RS on IBM Bluemix==!  \\n\\nIf you have not worked with JAX-RS for your REST-based web services before, you really owe it to yourself to check it out.  You do not even have to crack open the configuration files.  You make a class with some annotations, compile, and you have a web service.  Brilliant!\\n\\n###Next Steps\\n\\nWhat I find most interesting about these three different overviews is the ==subtle difference in approach==.  Not that one is better or worse than the other, but that they all clearly come at the market in their own ways.\\n\\nOut of the box PHP development excels at being drop-dead ==simple==.  Since you have immediate control over the web framework, Node.js is amazingly ==nimble==, but requires a bit more consideration up front.  Java EE really makes you think, but pays off in ==robustness== (JMS, EJB, etc.).\\n\\nWhat is great from the IBM Bluemix perspective is how ==similar it is to deploy== these web runtimes.  Even better yet is that there are ==several other options== beyond these three.  I would encourage you to take a couple hours to try one out for yourself.  If you have questions, you can find me on [Twitter](http://www.twitter.com/krhoyt).  If you want code to compare against, all my work is on [GitHub](https://github.com/krhoyt/IBM/tree/master/crawl/runtimes).\\n\"}]],\"sections\":[[10,0]],\"ghostVersion\":\"3.0\"}","html":"<!--kg-card-begin: markdown--><p>In my <a href=\"http://blog.kevinhoyt.com/2015/07/28/bluemix-crawl-runtimes/\">last post</a>, I gave an overview of IBM Bluemix, and walked through the process of deploying your web first application.  For this post, I will walk through a simple weather application I used to test out some of the Bluemix web runtimes.  The flavors for this application comes in PHP, Node.js, and Java.</p>\n<h3 id=\"quickrecap\">Quick Recap</h3>\n<p>Just as a quick reminder, <mark>IBM Bluemix is a PaaS</mark> (Platform as a Service) offering from IBM.  It consists of ~120 different components at the time of this writing.  The ability to run your own server code is called &quot;Runtimes&quot; in Bluemix vernacular, and is just one of the available components.</p>\n<p>As IBM Bluemix is based on the open source <a href=\"https://www.cloudfoundry.org\">Cloud Foundry</a> project, you use the Cloud Foundry tooling to deploy your applications - most notably a CLI (command line interface).  A &quot;manifest.yml&quot; is required to identify your application, and to supply the runtime options to Bluemix.</p>\n<h3 id=\"makeitrain\">Make It Rain</h3>\n<p>This sample weather application consists of a few different pieces - none of them particularly complex.  We will start off by taking a look at the <mark>client-side</mark> code, and the general behavior of the application.</p>\n<p><img src=\"http://images.kevinhoyt.com/bluemix-crawl-weather-php.png\" alt=\"IBM Bluemix weather application.\" loading=\"lazy\"></p>\n<p>When the weather application is loaded in the browser, the first thing it does is use <mark>XHR</mark> (XMLHttpRequest) to load the &quot;<mark>weather.txt</mark>&quot; file in the background.  This file tells the client where to get the weather data - the URI of the service.  I externalized this piece of information to make the client side more portable across different languages and configurations.</p>\n<pre><code>// Geolocation\nif( navigator.geolocation ) \n{\n  navigator.geolocation.getCurrentPosition( \n    doLocationSuccess, \n    doLocationError \n  );\n} else {\n  doLocationError( 'Geolocation not supported.' );\n}\n\n...\n\n// Lookup weather data\nfunction doLocationSuccess( position )\n{\n    // Debug\n    console.log(\n        position.coords.latitude + \n        ', ' +\n        position.coords.longitude\n    );\n \n    // Get weather\n    weather( position );\n}       \n</code></pre>\n<p>With the service endpoint URI obtained, the JavaScript will then use the browser <a href=\"http://caniuse.com/#feat=geolocation\">Geolocation API</a> to get the latitude and longitude of your computer/device.  Once obtained, the latitude and longitude are sent to the server for processing (the loaded URI), using XHR.</p>\n<p>The called server resource will load a file called &quot;<mark>forecast.io</mark>&quot; which contains an API (application program interface) key for the <mark>Forecast</mark> service.  <a href=\"http://forecast.io\">Forecast</a> is a weather service that I am using for this application.  The API is simple yet powerful, and there is a free tier for development and testing purposes.  Perfect for this sample application.</p>\n<blockquote>\n<p>For security reasons, my Forecast API key is not included in the GitHub repository.  You will need to sign up for <a href=\"https://developer.forecast.io\">your own Forecast account</a>, get your own key, and put it in a file named &quot;forecast.io&quot; in the same directory as the rest of the application files.</p>\n</blockquote>\n<p>With latitude, longitude, and API key in hand, the code then makes a server-side HTTP request against the Forecast API.  Current weather details are parsed from the JSON (JavaScript Object Notation) response.  The proper name of the location (city, state) is not included in the response, so a subsequent call is made against a <mark>Google API for reverse geocoding</mark> (address lookup).</p>\n<p>The results of all the pertinent data is sent back to the browser as a <mark>JSON-encoded</mark> string.  The browser gets that information, populates the various UI elements, and the weather is presented to the user.  This happens once per minute so long as the web page is front and center in the browser.</p>\n<h3 id=\"php\">PHP</h3>\n<p>My biggest concern for using PHP on IBM Bluemix was <mark>file IO</mark>.  Not that file IO is a particularly tough task for PHP, but that I have had my share of unsupported file IO features on cloud servers in the past.  I have also seen cloud service providers lock down file IO as a security risk, then provide their own, less than ideal, library wrapper for you to use.</p>\n<pre><code>// Get API key\n$forecast_key = trim( file_get_contents( $KEY_FILE ) );\n</code></pre>\n<p>Much to my surprise, IBM Bluemix actually lets you pick the <a href=\"https://www.ng.bluemix.net/docs/starters/php/index.html#phpversions\">version of PHP</a> that you want to run.  Version 5.5.23 is the default, but you can also specify versions in the 5.4.x range and 5.6.x range.  Once my development version (minor) was matched with the Bluemix version, I encountered <mark>no problems, or limits</mark>, at all.</p>\n<blockquote>\n<p>All things being equal, PHP was probably the easiest of the three to deploy and use on IBM Bluemix.  Drop some files in a directory, add a &quot;manifest.yml&quot;, push to the cloud, and done.</p>\n</blockquote>\n<h3 id=\"nodejs\">Node.js</h3>\n<p>When it comes to Node.js, <a href=\"http://expressjs.com\">Express</a> is the de facto standard web framework.  When you download the starter code from IBM Bluemix, you will find an Express template already waiting for you.  In fact, Node.js and Express are widely used in other areas of Bluemix as well.</p>\n<pre><code>var cfenv = require( 'cfenv' );\nvar bluemix = cfenv.getAppEnv();\n\n// Start server\napp.listen( bluemix.port, bluemix.bind, function() {\n  console.log( 'Server starting on: ' + bluemix.url );\n} );\n</code></pre>\n<p>When running Node.js in the cloud however, you may not know anything about the deployment directory, port, or other environment variables.  This means you need to abstract them from your code.  IBM Bluemix helps you out here with the &quot;<mark>cfenv</mark>&quot; library.  This handy library lets your application know how Bluemix has deployed it.  <mark>There is even access to custom environment variables you can set in the Bluemix dashboard.</mark></p>\n<blockquote>\n<p>You can also use a &quot;package.json&quot; file to have Bluemix load dependencies for you during deployment - and there is no limitation on which libraries you can use.</p>\n</blockquote>\n<h3 id=\"java\">Java</h3>\n<p>If we are comparing PHP, Node.js, and Java, then Java is a whole different animal.  But if you are a Java developer, then you probably already know that.  When you download the starter code for Java from IBM Bluemix, you get the boilerplate for a <mark>full Java EE</mark> deployment.  Indeed, when you run a Java application on Bluemix, it runs no the <a href=\"https://developer.ibm.com/wasdev/websphere-liberty/\">WebSphere Application Server Liberty Profile</a> (full Java EE 7).</p>\n<p>If you know your way around a Java EE application, then you should feel right at home in the starter code.  There is an <a href=\"http://ant.apache.org\">Apache Ant</a> build file already included.  Import this project into Eclipse for Java EE, and you will be off to the races in no time.</p>\n<pre><code>@Path( &quot;/weather&quot; )\npublic class WeatherService \n{\n  // Get request for weather data\n  @GET\t\n  public String doWeather( \n    @QueryParam( &quot;latitude&quot; ) String latitude, \n    @QueryParam( &quot;longitude&quot; ) String longitude ) {\n\n    // Service code here\n\n  }\n}\n</code></pre>\n<blockquote>\n<p>When I settled into this project, I completely expected to build a servlet for my web service handling.  What tickled me pink however, was the support for <mark>JAX-RS on IBM Bluemix</mark>!</p>\n</blockquote>\n<p>If you have not worked with JAX-RS for your REST-based web services before, you really owe it to yourself to check it out.  You do not even have to crack open the configuration files.  You make a class with some annotations, compile, and you have a web service.  Brilliant!</p>\n<h3 id=\"nextsteps\">Next Steps</h3>\n<p>What I find most interesting about these three different overviews is the <mark>subtle difference in approach</mark>.  Not that one is better or worse than the other, but that they all clearly come at the market in their own ways.</p>\n<p>Out of the box PHP development excels at being drop-dead <mark>simple</mark>.  Since you have immediate control over the web framework, Node.js is amazingly <mark>nimble</mark>, but requires a bit more consideration up front.  Java EE really makes you think, but pays off in <mark>robustness</mark> (JMS, EJB, etc.).</p>\n<p>What is great from the IBM Bluemix perspective is how <mark>similar it is to deploy</mark> these web runtimes.  Even better yet is that there are <mark>several other options</mark> beyond these three.  I would encourage you to take a couple hours to try one out for yourself.  If you have questions, you can find me on <a href=\"http://www.twitter.com/krhoyt\">Twitter</a>.  If you want code to compare against, all my work is on <a href=\"https://github.com/krhoyt/IBM/tree/master/crawl/runtimes\">GitHub</a>.</p>\n<!--kg-card-end: markdown-->","comment_id":"36","plaintext":"In my last post [http://blog.kevinhoyt.com/2015/07/28/bluemix-crawl-runtimes/],\nI gave an overview of IBM Bluemix, and walked through the process of deploying\nyour web first application. For this post, I will walk through a simple weather\napplication I used to test out some of the Bluemix web runtimes. The flavors for\nthis application comes in PHP, Node.js, and Java.\n\nQuick Recap\nJust as a quick reminder, IBM Bluemix is a PaaS (Platform as a Service) offering\nfrom IBM. It consists of ~120 different components at the time of this writing.\nThe ability to run your own server code is called \"Runtimes\" in Bluemix\nvernacular, and is just one of the available components.\n\nAs IBM Bluemix is based on the open source Cloud Foundry\n[https://www.cloudfoundry.org] project, you use the Cloud Foundry tooling to\ndeploy your applications - most notably a CLI (command line interface). A\n\"manifest.yml\" is required to identify your application, and to supply the\nruntime options to Bluemix.\n\nMake It Rain\nThis sample weather application consists of a few different pieces - none of\nthem particularly complex. We will start off by taking a look at the client-side \ncode, and the general behavior of the application.\n\n\n\nWhen the weather application is loaded in the browser, the first thing it does\nis use XHR (XMLHttpRequest) to load the \"weather.txt\" file in the background.\nThis file tells the client where to get the weather data - the URI of the\nservice. I externalized this piece of information to make the client side more\nportable across different languages and configurations.\n\n// Geolocation\nif( navigator.geolocation ) \n{\n  navigator.geolocation.getCurrentPosition( \n    doLocationSuccess, \n    doLocationError \n  );\n} else {\n  doLocationError( 'Geolocation not supported.' );\n}\n\n...\n\n// Lookup weather data\nfunction doLocationSuccess( position )\n{\n    // Debug\n    console.log(\n        position.coords.latitude + \n        ', ' +\n        position.coords.longitude\n    );\n \n    // Get weather\n    weather( position );\n}       \n\n\nWith the service endpoint URI obtained, the JavaScript will then use the browser \nGeolocation API [http://caniuse.com/#feat=geolocation] to get the latitude and\nlongitude of your computer/device. Once obtained, the latitude and longitude are\nsent to the server for processing (the loaded URI), using XHR.\n\nThe called server resource will load a file called \"forecast.io\" which contains\nan API (application program interface) key for the Forecast service. Forecast\n[http://forecast.io] is a weather service that I am using for this application.\nThe API is simple yet powerful, and there is a free tier for development and\ntesting purposes. Perfect for this sample application.\n\n> For security reasons, my Forecast API key is not included in the GitHub\nrepository. You will need to sign up for your own Forecast account\n[https://developer.forecast.io], get your own key, and put it in a file named\n\"forecast.io\" in the same directory as the rest of the application files.\n\n\nWith latitude, longitude, and API key in hand, the code then makes a server-side\nHTTP request against the Forecast API. Current weather details are parsed from\nthe JSON (JavaScript Object Notation) response. The proper name of the location\n(city, state) is not included in the response, so a subsequent call is made\nagainst a Google API for reverse geocoding (address lookup).\n\nThe results of all the pertinent data is sent back to the browser as a \nJSON-encoded string. The browser gets that information, populates the various UI\nelements, and the weather is presented to the user. This happens once per minute\nso long as the web page is front and center in the browser.\n\nPHP\nMy biggest concern for using PHP on IBM Bluemix was file IO. Not that file IO is\na particularly tough task for PHP, but that I have had my share of unsupported\nfile IO features on cloud servers in the past. I have also seen cloud service\nproviders lock down file IO as a security risk, then provide their own, less\nthan ideal, library wrapper for you to use.\n\n// Get API key\n$forecast_key = trim( file_get_contents( $KEY_FILE ) );\n\n\nMuch to my surprise, IBM Bluemix actually lets you pick the version of PHP\n[https://www.ng.bluemix.net/docs/starters/php/index.html#phpversions] that you\nwant to run. Version 5.5.23 is the default, but you can also specify versions in\nthe 5.4.x range and 5.6.x range. Once my development version (minor) was matched\nwith the Bluemix version, I encountered no problems, or limits, at all.\n\n> All things being equal, PHP was probably the easiest of the three to deploy and\nuse on IBM Bluemix. Drop some files in a directory, add a \"manifest.yml\", push\nto the cloud, and done.\n\n\nNode.js\nWhen it comes to Node.js, Express [http://expressjs.com] is the de facto\nstandard web framework. When you download the starter code from IBM Bluemix, you\nwill find an Express template already waiting for you. In fact, Node.js and\nExpress are widely used in other areas of Bluemix as well.\n\nvar cfenv = require( 'cfenv' );\nvar bluemix = cfenv.getAppEnv();\n\n// Start server\napp.listen( bluemix.port, bluemix.bind, function() {\n  console.log( 'Server starting on: ' + bluemix.url );\n} );\n\n\nWhen running Node.js in the cloud however, you may not know anything about the\ndeployment directory, port, or other environment variables. This means you need\nto abstract them from your code. IBM Bluemix helps you out here with the \"cfenv\"\nlibrary. This handy library lets your application know how Bluemix has deployed\nit. There is even access to custom environment variables you can set in the\nBluemix dashboard.\n\n> You can also use a \"package.json\" file to have Bluemix load dependencies for you\nduring deployment - and there is no limitation on which libraries you can use.\n\n\nJava\nIf we are comparing PHP, Node.js, and Java, then Java is a whole different\nanimal. But if you are a Java developer, then you probably already know that.\nWhen you download the starter code for Java from IBM Bluemix, you get the\nboilerplate for a full Java EE deployment. Indeed, when you run a Java\napplication on Bluemix, it runs no the WebSphere Application Server Liberty\nProfile [https://developer.ibm.com/wasdev/websphere-liberty/] (full Java EE 7).\n\nIf you know your way around a Java EE application, then you should feel right at\nhome in the starter code. There is an Apache Ant [http://ant.apache.org] build\nfile already included. Import this project into Eclipse for Java EE, and you\nwill be off to the races in no time.\n\n@Path( \"/weather\" )\npublic class WeatherService \n{\n  // Get request for weather data\n  @GET\t\n  public String doWeather( \n    @QueryParam( \"latitude\" ) String latitude, \n    @QueryParam( \"longitude\" ) String longitude ) {\n\n    // Service code here\n\n  }\n}\n\n\n> When I settled into this project, I completely expected to build a servlet for\nmy web service handling. What tickled me pink however, was the support for \nJAX-RS on IBM Bluemix!\n\n\nIf you have not worked with JAX-RS for your REST-based web services before, you\nreally owe it to yourself to check it out. You do not even have to crack open\nthe configuration files. You make a class with some annotations, compile, and\nyou have a web service. Brilliant!\n\nNext Steps\nWhat I find most interesting about these three different overviews is the subtle\ndifference in approach. Not that one is better or worse than the other, but that\nthey all clearly come at the market in their own ways.\n\nOut of the box PHP development excels at being drop-dead simple. Since you have\nimmediate control over the web framework, Node.js is amazingly nimble, but\nrequires a bit more consideration up front. Java EE really makes you think, but\npays off in robustness (JMS, EJB, etc.).\n\nWhat is great from the IBM Bluemix perspective is how similar it is to deploy \nthese web runtimes. Even better yet is that there are several other options \nbeyond these three. I would encourage you to take a couple hours to try one out\nfor yourself. If you have questions, you can find me on Twitter\n[http://www.twitter.com/krhoyt]. If you want code to compare against, all my\nwork is on GitHub [https://github.com/krhoyt/IBM/tree/master/crawl/runtimes].","feature_image":"http://images.kevinhoyt.com/bluemix-web-weather.jpg","featured":0,"status":"published","locale":null,"visibility":"public","author_id":"1","created_at":"2015-07-24T14:10:26.000Z","updated_at":"2015-07-30T15:25:46.000Z","published_at":"2015-07-29T14:04:00.000Z","custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"type":"post","email_recipient_filter":"none"},{"id":"5adb8922351ffe0018a5772f","uuid":"fe7c6d81-08b6-4815-beb7-3ec8dd7d6281","title":"Bluemix Mobile Runtimes","slug":"bluemix-mobile-runtimes","mobiledoc":"{\"version\":\"0.3.1\",\"markups\":[],\"atoms\":[],\"cards\":[[\"markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\"In a [previous post](http://blog.kevinhoyt.com/2015/07/28/bluemix-crawl-runtimes/), I talked about IBM Bluemix web runtimes features.  Along the way, I showed how to create your first application on the web runtimes.  We took a fork in the road however, and never returned to close the loop.  In this post I will introduce the concept of Bluemix mobile runtimes.\\n\\n###Closing the Loop\\n\\nAfter logging into [IBM Bluemix](http://ibm.com/bluemix), the fork in the road we took was in choosing between creating a \\\"==Web==\\\" application or a \\\"==Mobile==\\\" application.  Last time we chose to create a \\\"Web\\\" application.  ==This time we will focus on the \\\"Mobile\\\" application side.==\\n\\n![Picking up where we left off.](http://images.kevinhoyt.com/bluemix-dashboard-create.png)\\n\\nAfter selecting \\\"Mobile\\\" you will be presented with the option to choose from \\\"==Mobile==\\\" or \\\"==iOS==\\\".  I agree that it is a strange labeling considering iOS is a mobile platform.  For now however, the term \\\"Mobile\\\" refers to applications created for ==iOS version prior to iOS8, Android, and hybrid applications with Apache Cordova==.\\n\\n![Mobile or iOS](http://images.kevinhoyt.com/bluemix-mobile-runtimes.png)\\n\\n> Regardless of which mobile operating system you choose, the IBM Bluemix features are nearly identical.  If you are entirely new to these features, you may find this walkthrough beneficial as an overview.\\n\\nIf you are looking for more in-depth usage of iOS with IBM Bluemix, I suggest checking out my colleague, ==Andy Trice==, [blog](http://www.tricedesigns.com).  For this tutorial, I am going to be focused on Android.  For hybrid applications check out the [blog](http://www.raymondcamden.com) of my colleague, ==Ray Camden==.  As for this tutorial, we will be focused on the ==Android== workflow, so ==click the \\\"Mobile\\\" option==.\\n\\n![Get more information and continue to application creation.](http://images.kevinhoyt.com/bluemix-mobile-runtimes-platform.png)\\n\\n###Pieces Parts\\n\\nOnce you click on the \\\"Mobile\\\" option, you will be presented with a panel containing some important information and links.  More than just informational, I strongly suggest you take a deeper look at all the details.  Several core features of IBM Bluemix mobile runtimes are presented.  They are as follows:\\n\\n**SDK for Node.js**\\n\\nIBM Bluemix mobile runtimes allow you to do pretty much everything right from the mobile client.  There are times however when you will want additional functionality.  As an example, you may want to ==integrate with existing infrastructure==.  To do this you can add Node.js server infrastructure to your Bluemix application.\\n\\n**Push**\\n\\nPush notifications for ==both iOS and Android== are available as part of your IBM Bluemix mobile runtime.  Integrating with these services is beyond the scope of this tutorial.  What this boils down to however is a convenient dashboard by which you can control your push services integration with the providers (Apple, Google), as well as have a consistent API to use across both platforms.\\n\\n**Mobile Quality Assurance**\\n\\nIs your application getting good reviews?  Are you sure?  Do you manually crawl the application stores to find out?  The quality assurance feature of IBM Bluemix mobile runtimes gives you access to ==sentiment analysis==.  Why do your customers feel that way?  Find out with deep access to ==crash reports, logs, and more==.\\n\\n**Mobile Application Security**\\n\\nIn this day and age, deploying your application without security in mind is just asking for trouble.  The application security feature allows you to build a framework with which you can ==control access== from the device to your cloud services.  \\n\\n**Mobile Data**\\n\\nPerhaps my favorite feature of IBM Bluemix mobile runtimes, is the close integration with ==Cloudant (NoSQL)==.  This feature allows you to integrate directly from a mobile client (Node.js SDK as well), to [Cloudant](https://cloudant.com).  Connectivity flows through Bluemix, and hooks into the quality assurance, and application security features.  You also get a handy dashboard for viewing your data, and access metrics.\\n\\n> If you put all these parts and pieces together, your get the foundation of what IBM broadly refers to as ==MobileFirst==.  \\n\\nAnother feature of [MobileFirst](http://www.ibm.com/cloud-computing/bluemix/solutions/mobilefirst/) is called ==Presence Insights==.  You will hear me talking more about this feature in future posts.  In short however [Presence Insights](https://console.ng.bluemix.net/catalog/presence-insights/) helps you to incorporate location data such as ==BTLE beacons== to build a more personalized customer experience.\\n\\nPlease do make sure you come back and check out all those capabilities.  In the meantime however, simply click on the button labeled \\\"Continue\\\" to ... um ... continue!\\n\\n###Registration Credentials\\n\\nKeeping the theme or a weather application, we will call this application \\\"Android Weather\\\" and then click the \\\"==Finish==\\\" button.\\n\\n![Name your application and click the finish button.](http://images.kevinhoyt.com/bluemix-mobile-runtimes-name.png)\\n\\nAt this point, you will be presented with links for various ==boilerplate code==.  This includes the Node.js SDK, as well a examples for the various mobile development approaches.  There is even a complete \\\"to-do\\\" list application for you to learn from.\\n\\n![Registration credentials.](http://images.kevinhoyt.com/bluemix-mobile-runtimes-credentials.png)\\n\\nBefore you skim over the rest of the details on the page however, and just click the \\\"Continue\\\" button, note that there are three distinct pieces of information at the bottom of the page - ==route==, ==application key==, and ==application secret==.  You can access these pieces of information from the IBM Bluemix dashboard for your application at any time, but it might be worth copying them down now, and placing them in a secure location.\\n\\nThese credentials are what the mobile cloud services (MobileFirst) uses to ==identify== and manage how the APIs interaction with your services.  It may be hard to see at this point, but there is also a lot of work done for you in ==managing versions== of your deployed application.  These credentials also play a key role in that aspect of your mobile cloud.\\n\\n###Service Catalog\\n\\nWhen you are ready, click the \\\"==Continue==\\\" button to finish creating your application.  This will take you to the dashboard page for your newly created application.  You will see your registration credentials, be able to stop/start your application, and more from this screen.  Clicking the \\\"==Mobile Options==\\\" link will toggle the display of the credentials.\\n\\n![IBM Bluemix mobile application with services.](http://images.kevinhoyt.com/bluemix-mobile-runtimes-landing.png)\\n\\nAlso worth noting on the dashboard page for your mobile application is the \\\"==Services==\\\" menu on the left side of the screen.  These are the services I mentioned - and each had their own dashboard screen for your mobile application.  \\n\\n>Services in IBM Bluemix are functionally ==complete offerings, with their own set of features==, not just a set of APIs.  There are ~120 services at the time of this writing.\\n\\nYou can also see the services for mobile applications listed below the \\\"Add a Service or API\\\" and \\\"Bind a Service or API\\\" buttons.  You can manage the services by clicking on them here as well.  Want to have ==IBM Watson== work on your application?  You can add that as a service.  ==Message broker==?  ==Workflow==?  Add the service!\\n\\n*Really want to ==blow your mind== with the robust capabilities of IBM Bluemix?  Head to the \\\"==Catalog==\\\" button in the top menu bar, to see a more complete listing of Bluemix services you can use.*\\n\\n###Next Steps\\n\\nAt this point, you hopefully have a better understanding of the IBM Bluemix mobile runtimes.  You may see it written as \\\"==Mobile Cloud Services==\\\" as well as \\\"==MobileFirst==\\\".  This all depends on the services and deployment scenario.  Bottom line is that you get a robust suite of offerings that can be run in the cloud or on-premis.\\n\\n![Weather application on Android.](http://images.kevinhoyt.com/bluemix-mobile-android-weather.png)\\n\\nIn my next post, I will detail bringing the ==weather application== from the web runtimes over to Android using the mobile runtimes features.  I will talk about using the ==Node.js SDK==, as well as the ==Android== mobile runtimes features (namely ==geolocation== and ==Cloud Code== integration).  Building a mobile application has never been so robust.\"}]],\"sections\":[[10,0]],\"ghostVersion\":\"3.0\"}","html":"<!--kg-card-begin: markdown--><p>In a <a href=\"http://blog.kevinhoyt.com/2015/07/28/bluemix-crawl-runtimes/\">previous post</a>, I talked about IBM Bluemix web runtimes features.  Along the way, I showed how to create your first application on the web runtimes.  We took a fork in the road however, and never returned to close the loop.  In this post I will introduce the concept of Bluemix mobile runtimes.</p>\n<h3 id=\"closingtheloop\">Closing the Loop</h3>\n<p>After logging into <a href=\"http://ibm.com/bluemix\">IBM Bluemix</a>, the fork in the road we took was in choosing between creating a &quot;<mark>Web</mark>&quot; application or a &quot;<mark>Mobile</mark>&quot; application.  Last time we chose to create a &quot;Web&quot; application.  <mark>This time we will focus on the &quot;Mobile&quot; application side.</mark></p>\n<p><img src=\"http://images.kevinhoyt.com/bluemix-dashboard-create.png\" alt=\"Picking up where we left off.\" loading=\"lazy\"></p>\n<p>After selecting &quot;Mobile&quot; you will be presented with the option to choose from &quot;<mark>Mobile</mark>&quot; or &quot;<mark>iOS</mark>&quot;.  I agree that it is a strange labeling considering iOS is a mobile platform.  For now however, the term &quot;Mobile&quot; refers to applications created for <mark>iOS version prior to iOS8, Android, and hybrid applications with Apache Cordova</mark>.</p>\n<p><img src=\"http://images.kevinhoyt.com/bluemix-mobile-runtimes.png\" alt=\"Mobile or iOS\" loading=\"lazy\"></p>\n<blockquote>\n<p>Regardless of which mobile operating system you choose, the IBM Bluemix features are nearly identical.  If you are entirely new to these features, you may find this walkthrough beneficial as an overview.</p>\n</blockquote>\n<p>If you are looking for more in-depth usage of iOS with IBM Bluemix, I suggest checking out my colleague, <mark>Andy Trice</mark>, <a href=\"http://www.tricedesigns.com\">blog</a>.  For this tutorial, I am going to be focused on Android.  For hybrid applications check out the <a href=\"http://www.raymondcamden.com\">blog</a> of my colleague, <mark>Ray Camden</mark>.  As for this tutorial, we will be focused on the <mark>Android</mark> workflow, so <mark>click the &quot;Mobile&quot; option</mark>.</p>\n<p><img src=\"http://images.kevinhoyt.com/bluemix-mobile-runtimes-platform.png\" alt=\"Get more information and continue to application creation.\" loading=\"lazy\"></p>\n<h3 id=\"piecesparts\">Pieces Parts</h3>\n<p>Once you click on the &quot;Mobile&quot; option, you will be presented with a panel containing some important information and links.  More than just informational, I strongly suggest you take a deeper look at all the details.  Several core features of IBM Bluemix mobile runtimes are presented.  They are as follows:</p>\n<p><strong>SDK for Node.js</strong></p>\n<p>IBM Bluemix mobile runtimes allow you to do pretty much everything right from the mobile client.  There are times however when you will want additional functionality.  As an example, you may want to <mark>integrate with existing infrastructure</mark>.  To do this you can add Node.js server infrastructure to your Bluemix application.</p>\n<p><strong>Push</strong></p>\n<p>Push notifications for <mark>both iOS and Android</mark> are available as part of your IBM Bluemix mobile runtime.  Integrating with these services is beyond the scope of this tutorial.  What this boils down to however is a convenient dashboard by which you can control your push services integration with the providers (Apple, Google), as well as have a consistent API to use across both platforms.</p>\n<p><strong>Mobile Quality Assurance</strong></p>\n<p>Is your application getting good reviews?  Are you sure?  Do you manually crawl the application stores to find out?  The quality assurance feature of IBM Bluemix mobile runtimes gives you access to <mark>sentiment analysis</mark>.  Why do your customers feel that way?  Find out with deep access to <mark>crash reports, logs, and more</mark>.</p>\n<p><strong>Mobile Application Security</strong></p>\n<p>In this day and age, deploying your application without security in mind is just asking for trouble.  The application security feature allows you to build a framework with which you can <mark>control access</mark> from the device to your cloud services.</p>\n<p><strong>Mobile Data</strong></p>\n<p>Perhaps my favorite feature of IBM Bluemix mobile runtimes, is the close integration with <mark>Cloudant (NoSQL)</mark>.  This feature allows you to integrate directly from a mobile client (Node.js SDK as well), to <a href=\"https://cloudant.com\">Cloudant</a>.  Connectivity flows through Bluemix, and hooks into the quality assurance, and application security features.  You also get a handy dashboard for viewing your data, and access metrics.</p>\n<blockquote>\n<p>If you put all these parts and pieces together, your get the foundation of what IBM broadly refers to as <mark>MobileFirst</mark>.</p>\n</blockquote>\n<p>Another feature of <a href=\"http://www.ibm.com/cloud-computing/bluemix/solutions/mobilefirst/\">MobileFirst</a> is called <mark>Presence Insights</mark>.  You will hear me talking more about this feature in future posts.  In short however <a href=\"https://console.ng.bluemix.net/catalog/presence-insights/\">Presence Insights</a> helps you to incorporate location data such as <mark>BTLE beacons</mark> to build a more personalized customer experience.</p>\n<p>Please do make sure you come back and check out all those capabilities.  In the meantime however, simply click on the button labeled &quot;Continue&quot; to ... um ... continue!</p>\n<h3 id=\"registrationcredentials\">Registration Credentials</h3>\n<p>Keeping the theme or a weather application, we will call this application &quot;Android Weather&quot; and then click the &quot;<mark>Finish</mark>&quot; button.</p>\n<p><img src=\"http://images.kevinhoyt.com/bluemix-mobile-runtimes-name.png\" alt=\"Name your application and click the finish button.\" loading=\"lazy\"></p>\n<p>At this point, you will be presented with links for various <mark>boilerplate code</mark>.  This includes the Node.js SDK, as well a examples for the various mobile development approaches.  There is even a complete &quot;to-do&quot; list application for you to learn from.</p>\n<p><img src=\"http://images.kevinhoyt.com/bluemix-mobile-runtimes-credentials.png\" alt=\"Registration credentials.\" loading=\"lazy\"></p>\n<p>Before you skim over the rest of the details on the page however, and just click the &quot;Continue&quot; button, note that there are three distinct pieces of information at the bottom of the page - <mark>route</mark>, <mark>application key</mark>, and <mark>application secret</mark>.  You can access these pieces of information from the IBM Bluemix dashboard for your application at any time, but it might be worth copying them down now, and placing them in a secure location.</p>\n<p>These credentials are what the mobile cloud services (MobileFirst) uses to <mark>identify</mark> and manage how the APIs interaction with your services.  It may be hard to see at this point, but there is also a lot of work done for you in <mark>managing versions</mark> of your deployed application.  These credentials also play a key role in that aspect of your mobile cloud.</p>\n<h3 id=\"servicecatalog\">Service Catalog</h3>\n<p>When you are ready, click the &quot;<mark>Continue</mark>&quot; button to finish creating your application.  This will take you to the dashboard page for your newly created application.  You will see your registration credentials, be able to stop/start your application, and more from this screen.  Clicking the &quot;<mark>Mobile Options</mark>&quot; link will toggle the display of the credentials.</p>\n<p><img src=\"http://images.kevinhoyt.com/bluemix-mobile-runtimes-landing.png\" alt=\"IBM Bluemix mobile application with services.\" loading=\"lazy\"></p>\n<p>Also worth noting on the dashboard page for your mobile application is the &quot;<mark>Services</mark>&quot; menu on the left side of the screen.  These are the services I mentioned - and each had their own dashboard screen for your mobile application.</p>\n<blockquote>\n<p>Services in IBM Bluemix are functionally <mark>complete offerings, with their own set of features</mark>, not just a set of APIs.  There are ~120 services at the time of this writing.</p>\n</blockquote>\n<p>You can also see the services for mobile applications listed below the &quot;Add a Service or API&quot; and &quot;Bind a Service or API&quot; buttons.  You can manage the services by clicking on them here as well.  Want to have <mark>IBM Watson</mark> work on your application?  You can add that as a service.  <mark>Message broker</mark>?  <mark>Workflow</mark>?  Add the service!</p>\n<p><em>Really want to <mark>blow your mind</mark> with the robust capabilities of IBM Bluemix?  Head to the &quot;<mark>Catalog</mark>&quot; button in the top menu bar, to see a more complete listing of Bluemix services you can use.</em></p>\n<h3 id=\"nextsteps\">Next Steps</h3>\n<p>At this point, you hopefully have a better understanding of the IBM Bluemix mobile runtimes.  You may see it written as &quot;<mark>Mobile Cloud Services</mark>&quot; as well as &quot;<mark>MobileFirst</mark>&quot;.  This all depends on the services and deployment scenario.  Bottom line is that you get a robust suite of offerings that can be run in the cloud or on-premis.</p>\n<p><img src=\"http://images.kevinhoyt.com/bluemix-mobile-android-weather.png\" alt=\"Weather application on Android.\" loading=\"lazy\"></p>\n<p>In my next post, I will detail bringing the <mark>weather application</mark> from the web runtimes over to Android using the mobile runtimes features.  I will talk about using the <mark>Node.js SDK</mark>, as well as the <mark>Android</mark> mobile runtimes features (namely <mark>geolocation</mark> and <mark>Cloud Code</mark> integration).  Building a mobile application has never been so robust.</p>\n<!--kg-card-end: markdown-->","comment_id":"37","plaintext":"In a previous post\n[http://blog.kevinhoyt.com/2015/07/28/bluemix-crawl-runtimes/], I talked about\nIBM Bluemix web runtimes features. Along the way, I showed how to create your\nfirst application on the web runtimes. We took a fork in the road however, and\nnever returned to close the loop. In this post I will introduce the concept of\nBluemix mobile runtimes.\n\nClosing the Loop\nAfter logging into IBM Bluemix [http://ibm.com/bluemix], the fork in the road we\ntook was in choosing between creating a \"Web\" application or a \"Mobile\"\napplication. Last time we chose to create a \"Web\" application. This time we will\nfocus on the \"Mobile\" application side.\n\n\n\nAfter selecting \"Mobile\" you will be presented with the option to choose from \"\nMobile\" or \"iOS\". I agree that it is a strange labeling considering iOS is a\nmobile platform. For now however, the term \"Mobile\" refers to applications\ncreated for iOS version prior to iOS8, Android, and hybrid applications with\nApache Cordova.\n\n\n\n> Regardless of which mobile operating system you choose, the IBM Bluemix features\nare nearly identical. If you are entirely new to these features, you may find\nthis walkthrough beneficial as an overview.\n\n\nIf you are looking for more in-depth usage of iOS with IBM Bluemix, I suggest\nchecking out my colleague, Andy Trice, blog [http://www.tricedesigns.com]. For\nthis tutorial, I am going to be focused on Android. For hybrid applications\ncheck out the blog [http://www.raymondcamden.com] of my colleague, Ray Camden.\nAs for this tutorial, we will be focused on the Android workflow, so click the\n\"Mobile\" option.\n\n\n\nPieces Parts\nOnce you click on the \"Mobile\" option, you will be presented with a panel\ncontaining some important information and links. More than just informational, I\nstrongly suggest you take a deeper look at all the details. Several core\nfeatures of IBM Bluemix mobile runtimes are presented. They are as follows:\n\nSDK for Node.js\n\nIBM Bluemix mobile runtimes allow you to do pretty much everything right from\nthe mobile client. There are times however when you will want additional\nfunctionality. As an example, you may want to integrate with existing\ninfrastructure. To do this you can add Node.js server infrastructure to your\nBluemix application.\n\nPush\n\nPush notifications for both iOS and Android are available as part of your IBM\nBluemix mobile runtime. Integrating with these services is beyond the scope of\nthis tutorial. What this boils down to however is a convenient dashboard by\nwhich you can control your push services integration with the providers (Apple,\nGoogle), as well as have a consistent API to use across both platforms.\n\nMobile Quality Assurance\n\nIs your application getting good reviews? Are you sure? Do you manually crawl\nthe application stores to find out? The quality assurance feature of IBM Bluemix\nmobile runtimes gives you access to sentiment analysis. Why do your customers\nfeel that way? Find out with deep access to crash reports, logs, and more.\n\nMobile Application Security\n\nIn this day and age, deploying your application without security in mind is just\nasking for trouble. The application security feature allows you to build a\nframework with which you can control access from the device to your cloud\nservices.\n\nMobile Data\n\nPerhaps my favorite feature of IBM Bluemix mobile runtimes, is the close\nintegration with Cloudant (NoSQL). This feature allows you to integrate directly\nfrom a mobile client (Node.js SDK as well), to Cloudant [https://cloudant.com].\nConnectivity flows through Bluemix, and hooks into the quality assurance, and\napplication security features. You also get a handy dashboard for viewing your\ndata, and access metrics.\n\n> If you put all these parts and pieces together, your get the foundation of what\nIBM broadly refers to as MobileFirst.\n\n\nAnother feature of MobileFirst\n[http://www.ibm.com/cloud-computing/bluemix/solutions/mobilefirst/] is called \nPresence Insights. You will hear me talking more about this feature in future\nposts. In short however Presence Insights\n[https://console.ng.bluemix.net/catalog/presence-insights/] helps you to\nincorporate location data such as BTLE beacons to build a more personalized\ncustomer experience.\n\nPlease do make sure you come back and check out all those capabilities. In the\nmeantime however, simply click on the button labeled \"Continue\" to ... um ...\ncontinue!\n\nRegistration Credentials\nKeeping the theme or a weather application, we will call this application\n\"Android Weather\" and then click the \"Finish\" button.\n\n\n\nAt this point, you will be presented with links for various boilerplate code.\nThis includes the Node.js SDK, as well a examples for the various mobile\ndevelopment approaches. There is even a complete \"to-do\" list application for\nyou to learn from.\n\n\n\nBefore you skim over the rest of the details on the page however, and just click\nthe \"Continue\" button, note that there are three distinct pieces of information\nat the bottom of the page - route, application key, and application secret. You\ncan access these pieces of information from the IBM Bluemix dashboard for your\napplication at any time, but it might be worth copying them down now, and\nplacing them in a secure location.\n\nThese credentials are what the mobile cloud services (MobileFirst) uses to \nidentify and manage how the APIs interaction with your services. It may be hard\nto see at this point, but there is also a lot of work done for you in managing\nversions of your deployed application. These credentials also play a key role in\nthat aspect of your mobile cloud.\n\nService Catalog\nWhen you are ready, click the \"Continue\" button to finish creating your\napplication. This will take you to the dashboard page for your newly created\napplication. You will see your registration credentials, be able to stop/start\nyour application, and more from this screen. Clicking the \"Mobile Options\" link\nwill toggle the display of the credentials.\n\n\n\nAlso worth noting on the dashboard page for your mobile application is the \"\nServices\" menu on the left side of the screen. These are the services I\nmentioned - and each had their own dashboard screen for your mobile application.\n\n> Services in IBM Bluemix are functionally complete offerings, with their own set\nof features, not just a set of APIs. There are ~120 services at the time of this\nwriting.\n\n\nYou can also see the services for mobile applications listed below the \"Add a\nService or API\" and \"Bind a Service or API\" buttons. You can manage the services\nby clicking on them here as well. Want to have IBM Watson work on your\napplication? You can add that as a service. Message broker? Workflow? Add the\nservice!\n\nReally want to blow your mind with the robust capabilities of IBM Bluemix? Head\nto the \"Catalog\" button in the top menu bar, to see a more complete listing of\nBluemix services you can use.\n\nNext Steps\nAt this point, you hopefully have a better understanding of the IBM Bluemix\nmobile runtimes. You may see it written as \"Mobile Cloud Services\" as well as \"\nMobileFirst\". This all depends on the services and deployment scenario. Bottom\nline is that you get a robust suite of offerings that can be run in the cloud or\non-premis.\n\n\n\nIn my next post, I will detail bringing the weather application from the web\nruntimes over to Android using the mobile runtimes features. I will talk about\nusing the Node.js SDK, as well as the Android mobile runtimes features (namely \ngeolocation and Cloud Code integration). Building a mobile application has never\nbeen so robust.","feature_image":"http://images.kevinhoyt.com/smartphone-and-coffee.jpg","featured":0,"status":"published","locale":null,"visibility":"public","author_id":"1","created_at":"2015-08-03T02:05:50.000Z","updated_at":"2015-08-04T15:38:26.000Z","published_at":"2015-08-04T15:38:26.000Z","custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"type":"post","email_recipient_filter":"none"},{"id":"5adb8922351ffe0018a57730","uuid":"10a23784-b364-4813-9658-43eac8107eeb","title":"Mobile Cloud Weather on Android","slug":"weather-on-android","mobiledoc":"{\"version\":\"0.3.1\",\"markups\":[],\"atoms\":[],\"cards\":[[\"markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\"With an overview of IBM Bluemix mobile runtimes [established](http://blog.kevinhoyt.com/2015/08/04/bluemix-mobile-runtimes/), the next question is how to use those runtimes - Android and iOS.  In this post we will take a closer look at building and using an application that uses third-party weather data.  We will start at the server, then move our way onto the client with a native Android application.\\n\\n###Server\\n\\nWhen it comes to mobile runtimes, the server code is written using Node.js.  You are free to use whatever packages you may already be using, but there is also a package for IBM Bluemix.  You can install the Node.js SDK and add it to your \\\"package.json\\\" file using \\\"==npm install ibmbluemix --save==\\\".\\n\\n```\\n// Package\\nvar ibmbluemix = require( 'ibmbluemix' );\\n\\n// Bluemix\\nibmbluemix.initialize( {\\n  applicationId: '_YOUR_APP_KEY_',\\n  applicationRoute: '_YOUR_APP_ROUTE_',\\n  applicationSecret: '_YOUR_APP_SECRET_'\\n} );\\n\\n// Environment\\nvar ibmconfig = ibmbluemix.getConfig();\\n\\n// Logging\\nvar ibmlogger = ibmbluemix.getLogger();\\n```\\n\\nThe first thing we will need to do is to ==initialize the IBM Bluemix communication==.  This uses the key, route, and secret which can be obtained from the Bluemix dashboard under \\\"Mobile Options\\\".\\n\\nFrom there, we cannot make assumptions about the ==environment that IBM Bluemix will use to deploy our application==.  An example of this might be the port, certainly the directory structure, and even other services in use by your application such as IBM Cloudant.\\n\\nYou may also want to do some ==logging== along the way.  This can be as basic or as robust as you would like.  In this example, I am just logging out to the ==console==.  You can also use ==Winston== or ==other logging utilities==.  Again however, keep in mind that the path you place your logs for local development, may not be the same path on IBM Bluemix.\\n\\n> As with my web runtime examples, I will be using the Forecast service for weather data.  If you need more information on using that service, please see my [earlier post](http://blog.kevinhoyt.com/2015/07/29/weather-in-three-flavors/).\\n\\n```\\napp.get( ibmconfig.getContextRoot() + '/weather', function( req, res ) {\\n  latitude = req.query.latitude;\\n  longitude = req.query.longitude;\\n\\n  // Call to Forecast\\n  // Call to Google reverse gelocation lookup\\n\\n  res.json( weather );\\n} );\\n\\napp.listen( ibmconfig.getPort() );\\nibmlogger.info( 'Server started on port: ' + ibmconfig.getPort() );\\n```\\n\\nThe Android client will access the location of the device, and provide that to the \\\"weather\\\" service.  More on that in a moment.  Note that I am using the \\\"==ibmconfig.getContextRoot()==\\\" function to ==map Express to the IBM Bluemix== environment for the service.  I am also using \\\"==ibmconfig.getPort()==\\\" to map the port to my Express instance.\\n\\nAs it is, we can use this as a ==REST service==.  In the browser, we might access this service from a URL relative to the page that was loaded.  On a native mobile application however, we have no root context for a loaded page.  Let us take a look at the Android side of things.\\n\\n###Android\\n\\nThe first task on Android is to know where the device is located.  This will allow us to get accurate conditions.  This is accomplished using a feature of the ==Mobile Cloud SDK== called, aptly enough called \\\"==Location==\\\".  This location feature is roughly the same across multiple platforms, speeding up the development process.\\n\\n```\\n// Get location\\nIBMLocation.getService().acquireGeoPosition(\\n  IBMGeoAcquisitionPolicy.getLiveTrackingProfile() ).continueWith( \\n  new Continuation<IBMPosition, Void>() {\\n    // Async off UI thread\\n    @Override\\n    public Void then( Task<IBMPosition> task ) throws Exception {\\n      Exception   faulted;\\n      IBMPosition position;\\n\\n      // Problem\\n      if( task.isFaulted() ) {\\n        faulted = task.getError();\\n        Log.e( \\\"LOCATION\\\", faulted.getMessage() );\\n      } else {\\n        // Get position from results\\n        position = task.getResult();\\n\\n        // Debug\\n       Log.i( \\n         \\\"LOCATION\\\", \\n         position.getLocation().getLatitude() + \\\", \\\" +       \\n         position.getLocation().getLongitude() \\n       );\\n\\n       // Call Cloud Code\\n      forecast(                              \\n        position.getLocation().getLatitude(),\\n        position.getLocation().getLongitude()\\n      );\\n    }\\n\\n    return null;\\n  }\\n} );\\n```\\n\\nOnce we have located the device geographically, we can then make a call to get the weather conditions.\\n\\nTo help us locate and interoperate with our weather service, we will use a part of the Mobile Cloud SDK called ==Cloud Code==.  Cloud Code knows how to access our application by the initialization parameters we provide.  It then exposes methods for REST access to those endpoints.\\n\\n> There is a Mobile Cloud SDK for multiple platforms, and working with it on each platform is nearly identical.  Check out the [documentation](http://mbaas-gettingstarted.ng.bluemix.net/index.html) for your platform, including the [API](https://mobile.ng.bluemix.net/mbaas-api/docs/Android/index.html).\\n\\n```\\nprotected void forecast( \\n  double latitude, \\n  double longitude ) {\\n  String  lat;\\n  String  lng;\\n  String  url;\\n\\n  // String representation of coordinates\\n  lat = Double.toString( latitude );\\n  lng = Double.toString( longitude );\\n\\n  // Assemble Cloud Code URL\\n  url = \\n    WEATHER_ROOT + \\n    \\\"?\\\" + \\n    LATITUDE + \\n    \\\"=\\\" + \\n    lat + \\n    \\\"&\\\" + \\n    LONGITUDE + \\n    \\\"=\\\" + \\n    lng;\\n\\n  // Call Cloud Code\\n  // Separate thread\\n  ibmcloud.get( url ).continueWith( \\n    new Continuation<IBMHttpResponse, Void>() {\\n\\n      @Override\\n      public Void then( Task<IBMHttpResponse> task ) throws Exception {\\n        Exception       faulted;\\n        IBMHttpResponse response;\\n        String          content;\\n\\n        // Problem\\n        if( task.isFaulted() ) {\\n          faulted = task.getError();\\n          Log.e( \\\"CLOUDCODE\\\", faulted.getMessage() );\\n        } else {\\n          // Get results\\n          response = task.getResult();\\n\\n          // OK\\n          if( response.getHttpResponseCode() == 200 )\\n          {\\n            // Get String content from response\\n            content = stringFromResponse( \\n              response.getInputStream() \\n            );\\n\\n            // Debug\\n            Log.i( \\\"CLOUDCODE\\\", content );\\n\\n            // Parse resulting JSON\\n            parseAndPopulate( content );\\n          }\\n        }\\n\\n        return null;\\n      }\\n    } );\\n  }\\n```\\n\\nNote that the Cloud Code object is calling this method via \\\"==GET==\\\" or \\\"==get( url )==\\\" in the code.  The \\\"url\\\" here is assembled the line before.  That URL uses a constant called \\\"==WEATHER_ROOT==\\\" whose value is \\\"/weather\\\".  Notice that there is ==no domain as part of the URL==.  Cloud Code know how to talk to IBM Bluemix, and to your weather service without you having to worry about the details.\\n\\nAll that is left from here is to parse the JSON response created by our Node.js weather service, and populate the user interface accordingly.\\n\\n![Android weather application.](http://images.kevinhoyt.com/bluemix-mobile-android-weather.png)\\n\\n###One More Thing\\n\\nTo show how ==Cloud Code is nearly identical across platforms==, I took one more step and created a web version of the user interface as well.  The location classes are strangely absent on in the ==JavaScript SDK==, so I use the browser geolocation API.\\n\\n```\\n// Geolocation\\n// Timeout clears stack\\n// Avoids Safari implementation bug\\nsetTimeout( function() {\\n  if( navigator.geolocation ) {\\n    navigator.geolocation.getCurrentPosition( \\n      doLocationSuccess, \\n      doLocationError \\n    );\\n  } else {\\n    doLocationError( 'Geolocation not supported.' );\\n  }            \\t\\t\\n}, 1000 );\\n```\\n\\nOnce I have location information, I store if globally, so I can continue to poll the server for weather conditions.  ==Polling using Cloud Code looks almost identical to the Android version==.  You will even see the \\\"WEATHER_ROOT\\\" constant, which once again, simply points to \\\"/weather\\\" without having to think about the underlying details.\\n\\n```\\n// Mobile Cloud Services\\nIBMBluemix.initialize( {\\n  applicationId: APPLICATION_ID,\\n  applicationRoute: APPLICATION_ROUTE,\\n  applicationSecret: APPLICATION_SECRET\\n} );\\n\\t\\n// Cloud Code\\ncloud = IBMCloudCode.initializeService();\\n\\n...\\n\\n// URL for service\\nurl = \\n  WEATHER_ROOT + \\n  \\\"?\\\" + \\n  LATITUDE + \\n  \\\"=\\\" + \\n  latitude + \\n  \\\"&\\\" + \\n  LONGITUDE + \\n  \\\"=\\\" + \\n  longitude;\\n\\t\\n// Call service\\ncloud.get( url ).then( \\n  doWeatherLoad, \\n  doWeatherError \\n);\\n```\\n\\n![Weather application in the browser.](http://images.kevinhoyt.com/bluemix-iot-weather.jpg)\\n\\n###Next Steps\\n\\nGetting access to weather conditions from a service is fine and dandy.  Forecast is a joy to work with.  In this day and age of ==IoT== however, it seems like we could up our game by ==using sensors== in and around our physical location - your home for example.  Up next we will take a look at getting IoT data into IBM Bluemix.  This is where things start getting interesting.\\n\\nThe complete code for this example, and much more, is in my [GitHub repository](http://github.com/krhoyt).  If you have any questions, feel free to leave a comment below, or send me a message on [Twitter](http://twitter.com/krhoyt).\\n\"}]],\"sections\":[[10,0]],\"ghostVersion\":\"3.0\"}","html":"<!--kg-card-begin: markdown--><p>With an overview of IBM Bluemix mobile runtimes <a href=\"http://blog.kevinhoyt.com/2015/08/04/bluemix-mobile-runtimes/\">established</a>, the next question is how to use those runtimes - Android and iOS.  In this post we will take a closer look at building and using an application that uses third-party weather data.  We will start at the server, then move our way onto the client with a native Android application.</p>\n<h3 id=\"server\">Server</h3>\n<p>When it comes to mobile runtimes, the server code is written using Node.js.  You are free to use whatever packages you may already be using, but there is also a package for IBM Bluemix.  You can install the Node.js SDK and add it to your &quot;package.json&quot; file using &quot;<mark>npm install ibmbluemix --save</mark>&quot;.</p>\n<pre><code>// Package\nvar ibmbluemix = require( 'ibmbluemix' );\n\n// Bluemix\nibmbluemix.initialize( {\n  applicationId: '_YOUR_APP_KEY_',\n  applicationRoute: '_YOUR_APP_ROUTE_',\n  applicationSecret: '_YOUR_APP_SECRET_'\n} );\n\n// Environment\nvar ibmconfig = ibmbluemix.getConfig();\n\n// Logging\nvar ibmlogger = ibmbluemix.getLogger();\n</code></pre>\n<p>The first thing we will need to do is to <mark>initialize the IBM Bluemix communication</mark>.  This uses the key, route, and secret which can be obtained from the Bluemix dashboard under &quot;Mobile Options&quot;.</p>\n<p>From there, we cannot make assumptions about the <mark>environment that IBM Bluemix will use to deploy our application</mark>.  An example of this might be the port, certainly the directory structure, and even other services in use by your application such as IBM Cloudant.</p>\n<p>You may also want to do some <mark>logging</mark> along the way.  This can be as basic or as robust as you would like.  In this example, I am just logging out to the <mark>console</mark>.  You can also use <mark>Winston</mark> or <mark>other logging utilities</mark>.  Again however, keep in mind that the path you place your logs for local development, may not be the same path on IBM Bluemix.</p>\n<blockquote>\n<p>As with my web runtime examples, I will be using the Forecast service for weather data.  If you need more information on using that service, please see my <a href=\"http://blog.kevinhoyt.com/2015/07/29/weather-in-three-flavors/\">earlier post</a>.</p>\n</blockquote>\n<pre><code>app.get( ibmconfig.getContextRoot() + '/weather', function( req, res ) {\n  latitude = req.query.latitude;\n  longitude = req.query.longitude;\n\n  // Call to Forecast\n  // Call to Google reverse gelocation lookup\n\n  res.json( weather );\n} );\n\napp.listen( ibmconfig.getPort() );\nibmlogger.info( 'Server started on port: ' + ibmconfig.getPort() );\n</code></pre>\n<p>The Android client will access the location of the device, and provide that to the &quot;weather&quot; service.  More on that in a moment.  Note that I am using the &quot;<mark>ibmconfig.getContextRoot()</mark>&quot; function to <mark>map Express to the IBM Bluemix</mark> environment for the service.  I am also using &quot;<mark>ibmconfig.getPort()</mark>&quot; to map the port to my Express instance.</p>\n<p>As it is, we can use this as a <mark>REST service</mark>.  In the browser, we might access this service from a URL relative to the page that was loaded.  On a native mobile application however, we have no root context for a loaded page.  Let us take a look at the Android side of things.</p>\n<h3 id=\"android\">Android</h3>\n<p>The first task on Android is to know where the device is located.  This will allow us to get accurate conditions.  This is accomplished using a feature of the <mark>Mobile Cloud SDK</mark> called, aptly enough called &quot;<mark>Location</mark>&quot;.  This location feature is roughly the same across multiple platforms, speeding up the development process.</p>\n<pre><code>// Get location\nIBMLocation.getService().acquireGeoPosition(\n  IBMGeoAcquisitionPolicy.getLiveTrackingProfile() ).continueWith( \n  new Continuation&lt;IBMPosition, Void&gt;() {\n    // Async off UI thread\n    @Override\n    public Void then( Task&lt;IBMPosition&gt; task ) throws Exception {\n      Exception   faulted;\n      IBMPosition position;\n\n      // Problem\n      if( task.isFaulted() ) {\n        faulted = task.getError();\n        Log.e( &quot;LOCATION&quot;, faulted.getMessage() );\n      } else {\n        // Get position from results\n        position = task.getResult();\n\n        // Debug\n       Log.i( \n         &quot;LOCATION&quot;, \n         position.getLocation().getLatitude() + &quot;, &quot; +       \n         position.getLocation().getLongitude() \n       );\n\n       // Call Cloud Code\n      forecast(                              \n        position.getLocation().getLatitude(),\n        position.getLocation().getLongitude()\n      );\n    }\n\n    return null;\n  }\n} );\n</code></pre>\n<p>Once we have located the device geographically, we can then make a call to get the weather conditions.</p>\n<p>To help us locate and interoperate with our weather service, we will use a part of the Mobile Cloud SDK called <mark>Cloud Code</mark>.  Cloud Code knows how to access our application by the initialization parameters we provide.  It then exposes methods for REST access to those endpoints.</p>\n<blockquote>\n<p>There is a Mobile Cloud SDK for multiple platforms, and working with it on each platform is nearly identical.  Check out the <a href=\"http://mbaas-gettingstarted.ng.bluemix.net/index.html\">documentation</a> for your platform, including the <a href=\"https://mobile.ng.bluemix.net/mbaas-api/docs/Android/index.html\">API</a>.</p>\n</blockquote>\n<pre><code>protected void forecast( \n  double latitude, \n  double longitude ) {\n  String  lat;\n  String  lng;\n  String  url;\n\n  // String representation of coordinates\n  lat = Double.toString( latitude );\n  lng = Double.toString( longitude );\n\n  // Assemble Cloud Code URL\n  url = \n    WEATHER_ROOT + \n    &quot;?&quot; + \n    LATITUDE + \n    &quot;=&quot; + \n    lat + \n    &quot;&amp;&quot; + \n    LONGITUDE + \n    &quot;=&quot; + \n    lng;\n\n  // Call Cloud Code\n  // Separate thread\n  ibmcloud.get( url ).continueWith( \n    new Continuation&lt;IBMHttpResponse, Void&gt;() {\n\n      @Override\n      public Void then( Task&lt;IBMHttpResponse&gt; task ) throws Exception {\n        Exception       faulted;\n        IBMHttpResponse response;\n        String          content;\n\n        // Problem\n        if( task.isFaulted() ) {\n          faulted = task.getError();\n          Log.e( &quot;CLOUDCODE&quot;, faulted.getMessage() );\n        } else {\n          // Get results\n          response = task.getResult();\n\n          // OK\n          if( response.getHttpResponseCode() == 200 )\n          {\n            // Get String content from response\n            content = stringFromResponse( \n              response.getInputStream() \n            );\n\n            // Debug\n            Log.i( &quot;CLOUDCODE&quot;, content );\n\n            // Parse resulting JSON\n            parseAndPopulate( content );\n          }\n        }\n\n        return null;\n      }\n    } );\n  }\n</code></pre>\n<p>Note that the Cloud Code object is calling this method via &quot;<mark>GET</mark>&quot; or &quot;<mark>get( url )</mark>&quot; in the code.  The &quot;url&quot; here is assembled the line before.  That URL uses a constant called &quot;<mark>WEATHER_ROOT</mark>&quot; whose value is &quot;/weather&quot;.  Notice that there is <mark>no domain as part of the URL</mark>.  Cloud Code know how to talk to IBM Bluemix, and to your weather service without you having to worry about the details.</p>\n<p>All that is left from here is to parse the JSON response created by our Node.js weather service, and populate the user interface accordingly.</p>\n<p><img src=\"http://images.kevinhoyt.com/bluemix-mobile-android-weather.png\" alt=\"Android weather application.\" loading=\"lazy\"></p>\n<h3 id=\"onemorething\">One More Thing</h3>\n<p>To show how <mark>Cloud Code is nearly identical across platforms</mark>, I took one more step and created a web version of the user interface as well.  The location classes are strangely absent on in the <mark>JavaScript SDK</mark>, so I use the browser geolocation API.</p>\n<pre><code>// Geolocation\n// Timeout clears stack\n// Avoids Safari implementation bug\nsetTimeout( function() {\n  if( navigator.geolocation ) {\n    navigator.geolocation.getCurrentPosition( \n      doLocationSuccess, \n      doLocationError \n    );\n  } else {\n    doLocationError( 'Geolocation not supported.' );\n  }            \t\t\n}, 1000 );\n</code></pre>\n<p>Once I have location information, I store if globally, so I can continue to poll the server for weather conditions.  <mark>Polling using Cloud Code looks almost identical to the Android version</mark>.  You will even see the &quot;WEATHER_ROOT&quot; constant, which once again, simply points to &quot;/weather&quot; without having to think about the underlying details.</p>\n<pre><code>// Mobile Cloud Services\nIBMBluemix.initialize( {\n  applicationId: APPLICATION_ID,\n  applicationRoute: APPLICATION_ROUTE,\n  applicationSecret: APPLICATION_SECRET\n} );\n\t\n// Cloud Code\ncloud = IBMCloudCode.initializeService();\n\n...\n\n// URL for service\nurl = \n  WEATHER_ROOT + \n  &quot;?&quot; + \n  LATITUDE + \n  &quot;=&quot; + \n  latitude + \n  &quot;&amp;&quot; + \n  LONGITUDE + \n  &quot;=&quot; + \n  longitude;\n\t\n// Call service\ncloud.get( url ).then( \n  doWeatherLoad, \n  doWeatherError \n);\n</code></pre>\n<p><img src=\"http://images.kevinhoyt.com/bluemix-iot-weather.jpg\" alt=\"Weather application in the browser.\" loading=\"lazy\"></p>\n<h3 id=\"nextsteps\">Next Steps</h3>\n<p>Getting access to weather conditions from a service is fine and dandy.  Forecast is a joy to work with.  In this day and age of <mark>IoT</mark> however, it seems like we could up our game by <mark>using sensors</mark> in and around our physical location - your home for example.  Up next we will take a look at getting IoT data into IBM Bluemix.  This is where things start getting interesting.</p>\n<p>The complete code for this example, and much more, is in my <a href=\"http://github.com/krhoyt\">GitHub repository</a>.  If you have any questions, feel free to leave a comment below, or send me a message on <a href=\"http://twitter.com/krhoyt\">Twitter</a>.</p>\n<!--kg-card-end: markdown-->","comment_id":"38","plaintext":"With an overview of IBM Bluemix mobile runtimes established\n[http://blog.kevinhoyt.com/2015/08/04/bluemix-mobile-runtimes/], the next\nquestion is how to use those runtimes - Android and iOS. In this post we will\ntake a closer look at building and using an application that uses third-party\nweather data. We will start at the server, then move our way onto the client\nwith a native Android application.\n\nServer\nWhen it comes to mobile runtimes, the server code is written using Node.js. You\nare free to use whatever packages you may already be using, but there is also a\npackage for IBM Bluemix. You can install the Node.js SDK and add it to your\n\"package.json\" file using \"npm install ibmbluemix --save\".\n\n// Package\nvar ibmbluemix = require( 'ibmbluemix' );\n\n// Bluemix\nibmbluemix.initialize( {\n  applicationId: '_YOUR_APP_KEY_',\n  applicationRoute: '_YOUR_APP_ROUTE_',\n  applicationSecret: '_YOUR_APP_SECRET_'\n} );\n\n// Environment\nvar ibmconfig = ibmbluemix.getConfig();\n\n// Logging\nvar ibmlogger = ibmbluemix.getLogger();\n\n\nThe first thing we will need to do is to initialize the IBM Bluemix\ncommunication. This uses the key, route, and secret which can be obtained from\nthe Bluemix dashboard under \"Mobile Options\".\n\nFrom there, we cannot make assumptions about the environment that IBM Bluemix\nwill use to deploy our application. An example of this might be the port,\ncertainly the directory structure, and even other services in use by your\napplication such as IBM Cloudant.\n\nYou may also want to do some logging along the way. This can be as basic or as\nrobust as you would like. In this example, I am just logging out to the console.\nYou can also use Winston or other logging utilities. Again however, keep in mind\nthat the path you place your logs for local development, may not be the same\npath on IBM Bluemix.\n\n> As with my web runtime examples, I will be using the Forecast service for\nweather data. If you need more information on using that service, please see my \nearlier post [http://blog.kevinhoyt.com/2015/07/29/weather-in-three-flavors/].\n\n\napp.get( ibmconfig.getContextRoot() + '/weather', function( req, res ) {\n  latitude = req.query.latitude;\n  longitude = req.query.longitude;\n\n  // Call to Forecast\n  // Call to Google reverse gelocation lookup\n\n  res.json( weather );\n} );\n\napp.listen( ibmconfig.getPort() );\nibmlogger.info( 'Server started on port: ' + ibmconfig.getPort() );\n\n\nThe Android client will access the location of the device, and provide that to\nthe \"weather\" service. More on that in a moment. Note that I am using the \"\nibmconfig.getContextRoot()\" function to map Express to the IBM Bluemix \nenvironment for the service. I am also using \"ibmconfig.getPort()\" to map the\nport to my Express instance.\n\nAs it is, we can use this as a REST service. In the browser, we might access\nthis service from a URL relative to the page that was loaded. On a native mobile\napplication however, we have no root context for a loaded page. Let us take a\nlook at the Android side of things.\n\nAndroid\nThe first task on Android is to know where the device is located. This will\nallow us to get accurate conditions. This is accomplished using a feature of the \nMobile Cloud SDK called, aptly enough called \"Location\". This location feature\nis roughly the same across multiple platforms, speeding up the development\nprocess.\n\n// Get location\nIBMLocation.getService().acquireGeoPosition(\n  IBMGeoAcquisitionPolicy.getLiveTrackingProfile() ).continueWith( \n  new Continuation<IBMPosition, Void>() {\n    // Async off UI thread\n    @Override\n    public Void then( Task<IBMPosition> task ) throws Exception {\n      Exception   faulted;\n      IBMPosition position;\n\n      // Problem\n      if( task.isFaulted() ) {\n        faulted = task.getError();\n        Log.e( \"LOCATION\", faulted.getMessage() );\n      } else {\n        // Get position from results\n        position = task.getResult();\n\n        // Debug\n       Log.i( \n         \"LOCATION\", \n         position.getLocation().getLatitude() + \", \" +       \n         position.getLocation().getLongitude() \n       );\n\n       // Call Cloud Code\n      forecast(                              \n        position.getLocation().getLatitude(),\n        position.getLocation().getLongitude()\n      );\n    }\n\n    return null;\n  }\n} );\n\n\nOnce we have located the device geographically, we can then make a call to get\nthe weather conditions.\n\nTo help us locate and interoperate with our weather service, we will use a part\nof the Mobile Cloud SDK called Cloud Code. Cloud Code knows how to access our\napplication by the initialization parameters we provide. It then exposes methods\nfor REST access to those endpoints.\n\n> There is a Mobile Cloud SDK for multiple platforms, and working with it on each\nplatform is nearly identical. Check out the documentation\n[http://mbaas-gettingstarted.ng.bluemix.net/index.html] for your platform,\nincluding the API\n[https://mobile.ng.bluemix.net/mbaas-api/docs/Android/index.html].\n\n\nprotected void forecast( \n  double latitude, \n  double longitude ) {\n  String  lat;\n  String  lng;\n  String  url;\n\n  // String representation of coordinates\n  lat = Double.toString( latitude );\n  lng = Double.toString( longitude );\n\n  // Assemble Cloud Code URL\n  url = \n    WEATHER_ROOT + \n    \"?\" + \n    LATITUDE + \n    \"=\" + \n    lat + \n    \"&\" + \n    LONGITUDE + \n    \"=\" + \n    lng;\n\n  // Call Cloud Code\n  // Separate thread\n  ibmcloud.get( url ).continueWith( \n    new Continuation<IBMHttpResponse, Void>() {\n\n      @Override\n      public Void then( Task<IBMHttpResponse> task ) throws Exception {\n        Exception       faulted;\n        IBMHttpResponse response;\n        String          content;\n\n        // Problem\n        if( task.isFaulted() ) {\n          faulted = task.getError();\n          Log.e( \"CLOUDCODE\", faulted.getMessage() );\n        } else {\n          // Get results\n          response = task.getResult();\n\n          // OK\n          if( response.getHttpResponseCode() == 200 )\n          {\n            // Get String content from response\n            content = stringFromResponse( \n              response.getInputStream() \n            );\n\n            // Debug\n            Log.i( \"CLOUDCODE\", content );\n\n            // Parse resulting JSON\n            parseAndPopulate( content );\n          }\n        }\n\n        return null;\n      }\n    } );\n  }\n\n\nNote that the Cloud Code object is calling this method via \"GET\" or \"get( url )\"\nin the code. The \"url\" here is assembled the line before. That URL uses a\nconstant called \"WEATHER_ROOT\" whose value is \"/weather\". Notice that there is \nno domain as part of the URL. Cloud Code know how to talk to IBM Bluemix, and to\nyour weather service without you having to worry about the details.\n\nAll that is left from here is to parse the JSON response created by our Node.js\nweather service, and populate the user interface accordingly.\n\n\n\nOne More Thing\nTo show how Cloud Code is nearly identical across platforms, I took one more\nstep and created a web version of the user interface as well. The location\nclasses are strangely absent on in the JavaScript SDK, so I use the browser\ngeolocation API.\n\n// Geolocation\n// Timeout clears stack\n// Avoids Safari implementation bug\nsetTimeout( function() {\n  if( navigator.geolocation ) {\n    navigator.geolocation.getCurrentPosition( \n      doLocationSuccess, \n      doLocationError \n    );\n  } else {\n    doLocationError( 'Geolocation not supported.' );\n  }            \t\t\n}, 1000 );\n\n\nOnce I have location information, I store if globally, so I can continue to poll\nthe server for weather conditions. Polling using Cloud Code looks almost\nidentical to the Android version. You will even see the \"WEATHER_ROOT\" constant,\nwhich once again, simply points to \"/weather\" without having to think about the\nunderlying details.\n\n// Mobile Cloud Services\nIBMBluemix.initialize( {\n  applicationId: APPLICATION_ID,\n  applicationRoute: APPLICATION_ROUTE,\n  applicationSecret: APPLICATION_SECRET\n} );\n\t\n// Cloud Code\ncloud = IBMCloudCode.initializeService();\n\n...\n\n// URL for service\nurl = \n  WEATHER_ROOT + \n  \"?\" + \n  LATITUDE + \n  \"=\" + \n  latitude + \n  \"&\" + \n  LONGITUDE + \n  \"=\" + \n  longitude;\n\t\n// Call service\ncloud.get( url ).then( \n  doWeatherLoad, \n  doWeatherError \n);\n\n\n\n\nNext Steps\nGetting access to weather conditions from a service is fine and dandy. Forecast\nis a joy to work with. In this day and age of IoT however, it seems like we\ncould up our game by using sensors in and around our physical location - your\nhome for example. Up next we will take a look at getting IoT data into IBM\nBluemix. This is where things start getting interesting.\n\nThe complete code for this example, and much more, is in my GitHub repository\n[http://github.com/krhoyt]. If you have any questions, feel free to leave a\ncomment below, or send me a message on Twitter [http://twitter.com/krhoyt].","feature_image":"http://images.kevinhoyt.com/bluemix-web-weather.jpg","featured":0,"status":"published","locale":null,"visibility":"public","author_id":"1","created_at":"2015-08-06T19:54:45.000Z","updated_at":"2015-08-13T19:51:45.000Z","published_at":"2015-08-13T19:51:45.000Z","custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"type":"post","email_recipient_filter":"none"},{"id":"5adb8922351ffe0018a57731","uuid":"7d205001-9921-46ad-af8e-6bdd55b84b62","title":"Cloudant Continuous Feed","slug":"iot-with-cloudant-follow","mobiledoc":"{\"version\":\"0.3.1\",\"markups\":[],\"atoms\":[],\"cards\":[[\"markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\"Over the [past](http://blog.kevinhoyt.com/2015/08/13/weather-on-android/) [several](http://blog.kevinhoyt.com/2015/08/13/weather-on-android/) [examples](http://blog.kevinhoyt.com/2015/08/18/iot-weather-on-android/), new documents in [IBM Cloudant](http://cloudant.com) have been discovered by continuously polling the database.  This is not only inefficient at scale, but creates a tight coupling between our clients and the server.  With a little tweaking to our existing code, Cloudant can tell us when changes have taken place, which can then be pushed to the interested clients.\\n\\n###Server\\n\\nBefore we go jumping into new features, let us set the ==baseline== of what our code looks like.  I will be using the ==IBM Cloudant Node.js== package to connect to a Cloudant database.  Interaction with the database (CRUD) is done using ==Express routing==.\\n\\n```\\n// Database\\nvar ibmdb = null;\\n\\n// Connect\\nCloudant( {\\n  account: configuration.cloudant.username,\\n  password: configuration.cloudant.password\\n}, function( error, cloudant ) {\\n  if( error ) {\\n    console.log( 'Could not connect to Cloudant.' );\\n    console.log( 'Message: ' + error.message );\\n  }\\t\\n\\t\\n  // Debug\\n  console.log( 'Connected to database.' );\\n\\n  // Use database\\n  // Assumes the database exists\\n  console.log( 'Using database.' );\\n  ibmdb = cloudant.db.use( CLOUDANT_DATABASE );\\n}\\n```\\n\\nI make a lot of ==assumptions== here about the existence of the database, whether we have access to write to the database, etc.  You will want to adjust this boilerplate to match your specific requirements.\\n\\n###Feed Me\\n\\nNow that we are connected to IBM Cloudant, we want to tell the database to notify our client (in this case Node.js) when changes have occurred.  One of the parameters you need to consider then is \\\"==changes since when?==\\\"  You may also want to know ==what the changes were==, not only that there was a change.\\n\\n```\\nstream = ibmdb.follow( {\\n  include_docs: true, \\n  since: 'now'\\n} );\\n```\\n\\nYup!  ==That is all it takes!==  Add this code snippet to the bottom of the previous one, and your Node.js application will get updates about changes to the database ... Kind of.\\n\\n> Note that I use the \\\"==include_docs==\\\" parameter to get the content of the changes to the database.  This may impact IBM Cloudant ==performance==, so consider if you really need it, or if you can just get away with the document ID and revision.\\n\\nThe variable \\\"stream\\\" here is declared on the ==module scope== in order to keep it around.  This is because we need to ==add an event listener== to it if we actually want to see the changes that have been made to the database.\\n\\n```\\n// Database change\\nstream.on( 'change', function( change ) {\\n  console.log( 'Change: ' + change.doc );\\n} );\\n```\\n\\nNow not only does our Node.js client get notifications, but we can actually see them in the log (==smile==).  What about getting those changes to the browser?\\n\\n###Publish-Subscribe\\n\\nEarlier I mentioned that access to IBM Cloudant was done through Express routing.  This leads us to a pattern of ==request-response==.  If you have never done any other type of pattern, it may seem like the only one available.  The result is that everything becomes a ==REST== endpoint.\\n\\nA fantastic alternative worth learning more about is ==publish-subscribe==.  The publish-subscribe pattern has been around for ages in IT, but with the addition of WebSocket to the browser, is really only finding its way there more recently.  ==Using publish-subscribe gives you a very loosely coupled architecture.==\\n\\nThere are many ways to get started with publish-subscribe, but perhaps my favorite is a service called, [PubNub](http://pubnub.com).  ==PubNub== is also an ==IBM partner==, and you can find them in the IBM Marketplace, and soon the IBM Bluemix catalog of services.  Simply put, PubNub provides libraries for many different platforms that allow you to tap into their real-time network.\\n\\n```\\n// Declared on the module scope\\nvar pubnub = require( 'pubnub' )( configuration.pubnub );\\n\\n...\\n\\n// Added just after the log code above\\npubnub.publish( { \\n  channel: PUBNUB_CHANNEL,\\n  message: change.doc,\\n  callback : function( results ) {\\n    console.log( results[1] + ': ' + results[2] );  \\n  },\\n  error: function( error ) {\\n    console.log( 'Failed to publish message.' );\\n    console.log( error );  \\n  }\\n} );\\n```\\n\\nWith this addition, your Node.js process is now not only getting notified of changes to IBM Cloudant, but also ==pushing notifications== to any interested parties - on any interested platform - PubNub is not just WebSocket, and can be used in many other ways, on many other clients.\\n\\n###Browser\\n\\nSpeaking of clients, let us take a jump over to the browser.  This is where we really want database changes to show up - at least for the weather application we have been working through over the past several posts.\\n\\nIf you recall, we have a ==Particle Photon== sending data to our ==Node.js== infrastructure on ==IBM Bluemix==.  We then have a browser application ==polling== the Node.js infrastructure for the latest updates.  What we really want to happen is that new weather readings arriving from the Photon are pushed to the browser.\\n\\n```\\n<!-- PubNub library -->\\n<script src=\\\"https://cdn.pubnub.com/pubnub-3.7.13.min.js\\\" type=\\\"text/javascript\\\"></script>\\n\\n...\\n\\n// Initialize\\nvar pubnub = PUBNUB( configuration.pubnub );\\n\\t\\n// Subscribe\\t\\npubnub.subscribe( {\\n  channel: PUBNUB_CHANNEL,\\n  message: update\\t\\n} );\\n\\n...\\n\\n// A new update has arrived\\n// Show the related document\\nfunction update( data )\\n{\\n  console.log( data );\\n}\\n```\\n\\nThat is all there is to it.  We ==include== the PubNub library, ==initialize== it, and then ==subscribe to messages==.  Messages are delivered on a specific channel - the same channel we defined in our Node.js/IBM Cloudant configuration.  In this case, the log at the browser will show the same document that we log from Node.js.\\n\\n###Next Steps\\n\\nWhat is really going to ==bake your noodle== is that the Photon sensor, could also communicate with PubNub directly.  This shifts our development paradigm considerably.  The sensor publishes data to PubNub, then the Node.js server, as well as the browser, are both subscribed (listening) for those messages.  ==None of them know anything about the other, yet the same results are possible - and more scalable.==\\n\\n>It is entirely possible under this architecture, that you might not even need the IBM Cloudant continuous feed feature.  \\n\\nWant an Android device to take part in the conversation?  Just have it subscribe as well.  Maybe you want to report weather data from the sensors on the Android phone?  Just have it publish the data.  The device does not need to worry about IBM Cloudant at all - or the browser, or Node.js, or anything else.\\n\\nLoosely decoupled architectures is something I will be exploring in greater detail in a future series of posts.  If you have any questions in the meantime, please feel free to post a comment below.  You can also get the complete code for my IoT-enabled weather application on [GitHub](https://github.com/krhoyt/IBM/tree/master/iotweather).\"}]],\"sections\":[[10,0]],\"ghostVersion\":\"3.0\"}","html":"<!--kg-card-begin: markdown--><p>Over the <a href=\"http://blog.kevinhoyt.com/2015/08/13/weather-on-android/\">past</a> <a href=\"http://blog.kevinhoyt.com/2015/08/13/weather-on-android/\">several</a> <a href=\"http://blog.kevinhoyt.com/2015/08/18/iot-weather-on-android/\">examples</a>, new documents in <a href=\"http://cloudant.com\">IBM Cloudant</a> have been discovered by continuously polling the database.  This is not only inefficient at scale, but creates a tight coupling between our clients and the server.  With a little tweaking to our existing code, Cloudant can tell us when changes have taken place, which can then be pushed to the interested clients.</p>\n<h3 id=\"server\">Server</h3>\n<p>Before we go jumping into new features, let us set the <mark>baseline</mark> of what our code looks like.  I will be using the <mark>IBM Cloudant Node.js</mark> package to connect to a Cloudant database.  Interaction with the database (CRUD) is done using <mark>Express routing</mark>.</p>\n<pre><code>// Database\nvar ibmdb = null;\n\n// Connect\nCloudant( {\n  account: configuration.cloudant.username,\n  password: configuration.cloudant.password\n}, function( error, cloudant ) {\n  if( error ) {\n    console.log( 'Could not connect to Cloudant.' );\n    console.log( 'Message: ' + error.message );\n  }\t\n\t\n  // Debug\n  console.log( 'Connected to database.' );\n\n  // Use database\n  // Assumes the database exists\n  console.log( 'Using database.' );\n  ibmdb = cloudant.db.use( CLOUDANT_DATABASE );\n}\n</code></pre>\n<p>I make a lot of <mark>assumptions</mark> here about the existence of the database, whether we have access to write to the database, etc.  You will want to adjust this boilerplate to match your specific requirements.</p>\n<h3 id=\"feedme\">Feed Me</h3>\n<p>Now that we are connected to IBM Cloudant, we want to tell the database to notify our client (in this case Node.js) when changes have occurred.  One of the parameters you need to consider then is &quot;<mark>changes since when?</mark>&quot;  You may also want to know <mark>what the changes were</mark>, not only that there was a change.</p>\n<pre><code>stream = ibmdb.follow( {\n  include_docs: true, \n  since: 'now'\n} );\n</code></pre>\n<p>Yup!  <mark>That is all it takes!</mark>  Add this code snippet to the bottom of the previous one, and your Node.js application will get updates about changes to the database ... Kind of.</p>\n<blockquote>\n<p>Note that I use the &quot;<mark>include_docs</mark>&quot; parameter to get the content of the changes to the database.  This may impact IBM Cloudant <mark>performance</mark>, so consider if you really need it, or if you can just get away with the document ID and revision.</p>\n</blockquote>\n<p>The variable &quot;stream&quot; here is declared on the <mark>module scope</mark> in order to keep it around.  This is because we need to <mark>add an event listener</mark> to it if we actually want to see the changes that have been made to the database.</p>\n<pre><code>// Database change\nstream.on( 'change', function( change ) {\n  console.log( 'Change: ' + change.doc );\n} );\n</code></pre>\n<p>Now not only does our Node.js client get notifications, but we can actually see them in the log (<mark>smile</mark>).  What about getting those changes to the browser?</p>\n<h3 id=\"publishsubscribe\">Publish-Subscribe</h3>\n<p>Earlier I mentioned that access to IBM Cloudant was done through Express routing.  This leads us to a pattern of <mark>request-response</mark>.  If you have never done any other type of pattern, it may seem like the only one available.  The result is that everything becomes a <mark>REST</mark> endpoint.</p>\n<p>A fantastic alternative worth learning more about is <mark>publish-subscribe</mark>.  The publish-subscribe pattern has been around for ages in IT, but with the addition of WebSocket to the browser, is really only finding its way there more recently.  <mark>Using publish-subscribe gives you a very loosely coupled architecture.</mark></p>\n<p>There are many ways to get started with publish-subscribe, but perhaps my favorite is a service called, <a href=\"http://pubnub.com\">PubNub</a>.  <mark>PubNub</mark> is also an <mark>IBM partner</mark>, and you can find them in the IBM Marketplace, and soon the IBM Bluemix catalog of services.  Simply put, PubNub provides libraries for many different platforms that allow you to tap into their real-time network.</p>\n<pre><code>// Declared on the module scope\nvar pubnub = require( 'pubnub' )( configuration.pubnub );\n\n...\n\n// Added just after the log code above\npubnub.publish( { \n  channel: PUBNUB_CHANNEL,\n  message: change.doc,\n  callback : function( results ) {\n    console.log( results[1] + ': ' + results[2] );  \n  },\n  error: function( error ) {\n    console.log( 'Failed to publish message.' );\n    console.log( error );  \n  }\n} );\n</code></pre>\n<p>With this addition, your Node.js process is now not only getting notified of changes to IBM Cloudant, but also <mark>pushing notifications</mark> to any interested parties - on any interested platform - PubNub is not just WebSocket, and can be used in many other ways, on many other clients.</p>\n<h3 id=\"browser\">Browser</h3>\n<p>Speaking of clients, let us take a jump over to the browser.  This is where we really want database changes to show up - at least for the weather application we have been working through over the past several posts.</p>\n<p>If you recall, we have a <mark>Particle Photon</mark> sending data to our <mark>Node.js</mark> infrastructure on <mark>IBM Bluemix</mark>.  We then have a browser application <mark>polling</mark> the Node.js infrastructure for the latest updates.  What we really want to happen is that new weather readings arriving from the Photon are pushed to the browser.</p>\n<pre><code>&lt;!-- PubNub library --&gt;\n&lt;script src=&quot;https://cdn.pubnub.com/pubnub-3.7.13.min.js&quot; type=&quot;text/javascript&quot;&gt;&lt;/script&gt;\n\n...\n\n// Initialize\nvar pubnub = PUBNUB( configuration.pubnub );\n\t\n// Subscribe\t\npubnub.subscribe( {\n  channel: PUBNUB_CHANNEL,\n  message: update\t\n} );\n\n...\n\n// A new update has arrived\n// Show the related document\nfunction update( data )\n{\n  console.log( data );\n}\n</code></pre>\n<p>That is all there is to it.  We <mark>include</mark> the PubNub library, <mark>initialize</mark> it, and then <mark>subscribe to messages</mark>.  Messages are delivered on a specific channel - the same channel we defined in our Node.js/IBM Cloudant configuration.  In this case, the log at the browser will show the same document that we log from Node.js.</p>\n<h3 id=\"nextsteps\">Next Steps</h3>\n<p>What is really going to <mark>bake your noodle</mark> is that the Photon sensor, could also communicate with PubNub directly.  This shifts our development paradigm considerably.  The sensor publishes data to PubNub, then the Node.js server, as well as the browser, are both subscribed (listening) for those messages.  <mark>None of them know anything about the other, yet the same results are possible - and more scalable.</mark></p>\n<blockquote>\n<p>It is entirely possible under this architecture, that you might not even need the IBM Cloudant continuous feed feature.</p>\n</blockquote>\n<p>Want an Android device to take part in the conversation?  Just have it subscribe as well.  Maybe you want to report weather data from the sensors on the Android phone?  Just have it publish the data.  The device does not need to worry about IBM Cloudant at all - or the browser, or Node.js, or anything else.</p>\n<p>Loosely decoupled architectures is something I will be exploring in greater detail in a future series of posts.  If you have any questions in the meantime, please feel free to post a comment below.  You can also get the complete code for my IoT-enabled weather application on <a href=\"https://github.com/krhoyt/IBM/tree/master/iotweather\">GitHub</a>.</p>\n<!--kg-card-end: markdown-->","comment_id":"39","plaintext":"Over the past [http://blog.kevinhoyt.com/2015/08/13/weather-on-android/] several\n[http://blog.kevinhoyt.com/2015/08/13/weather-on-android/] examples\n[http://blog.kevinhoyt.com/2015/08/18/iot-weather-on-android/], new documents in \nIBM Cloudant [http://cloudant.com] have been discovered by continuously polling\nthe database. This is not only inefficient at scale, but creates a tight\ncoupling between our clients and the server. With a little tweaking to our\nexisting code, Cloudant can tell us when changes have taken place, which can\nthen be pushed to the interested clients.\n\nServer\nBefore we go jumping into new features, let us set the baseline of what our code\nlooks like. I will be using the IBM Cloudant Node.js package to connect to a\nCloudant database. Interaction with the database (CRUD) is done using Express\nrouting.\n\n// Database\nvar ibmdb = null;\n\n// Connect\nCloudant( {\n  account: configuration.cloudant.username,\n  password: configuration.cloudant.password\n}, function( error, cloudant ) {\n  if( error ) {\n    console.log( 'Could not connect to Cloudant.' );\n    console.log( 'Message: ' + error.message );\n  }\t\n\t\n  // Debug\n  console.log( 'Connected to database.' );\n\n  // Use database\n  // Assumes the database exists\n  console.log( 'Using database.' );\n  ibmdb = cloudant.db.use( CLOUDANT_DATABASE );\n}\n\n\nI make a lot of assumptions here about the existence of the database, whether we\nhave access to write to the database, etc. You will want to adjust this\nboilerplate to match your specific requirements.\n\nFeed Me\nNow that we are connected to IBM Cloudant, we want to tell the database to\nnotify our client (in this case Node.js) when changes have occurred. One of the\nparameters you need to consider then is \"changes since when?\" You may also want\nto know what the changes were, not only that there was a change.\n\nstream = ibmdb.follow( {\n  include_docs: true, \n  since: 'now'\n} );\n\n\nYup! That is all it takes! Add this code snippet to the bottom of the previous\none, and your Node.js application will get updates about changes to the database\n... Kind of.\n\n> Note that I use the \"include_docs\" parameter to get the content of the changes\nto the database. This may impact IBM Cloudant performance, so consider if you\nreally need it, or if you can just get away with the document ID and revision.\n\n\nThe variable \"stream\" here is declared on the module scope in order to keep it\naround. This is because we need to add an event listener to it if we actually\nwant to see the changes that have been made to the database.\n\n// Database change\nstream.on( 'change', function( change ) {\n  console.log( 'Change: ' + change.doc );\n} );\n\n\nNow not only does our Node.js client get notifications, but we can actually see\nthem in the log (smile). What about getting those changes to the browser?\n\nPublish-Subscribe\nEarlier I mentioned that access to IBM Cloudant was done through Express\nrouting. This leads us to a pattern of request-response. If you have never done\nany other type of pattern, it may seem like the only one available. The result\nis that everything becomes a REST endpoint.\n\nA fantastic alternative worth learning more about is publish-subscribe. The\npublish-subscribe pattern has been around for ages in IT, but with the addition\nof WebSocket to the browser, is really only finding its way there more recently. \nUsing publish-subscribe gives you a very loosely coupled architecture.\n\nThere are many ways to get started with publish-subscribe, but perhaps my\nfavorite is a service called, PubNub [http://pubnub.com]. PubNub is also an IBM\npartner, and you can find them in the IBM Marketplace, and soon the IBM Bluemix\ncatalog of services. Simply put, PubNub provides libraries for many different\nplatforms that allow you to tap into their real-time network.\n\n// Declared on the module scope\nvar pubnub = require( 'pubnub' )( configuration.pubnub );\n\n...\n\n// Added just after the log code above\npubnub.publish( { \n  channel: PUBNUB_CHANNEL,\n  message: change.doc,\n  callback : function( results ) {\n    console.log( results[1] + ': ' + results[2] );  \n  },\n  error: function( error ) {\n    console.log( 'Failed to publish message.' );\n    console.log( error );  \n  }\n} );\n\n\nWith this addition, your Node.js process is now not only getting notified of\nchanges to IBM Cloudant, but also pushing notifications to any interested\nparties - on any interested platform - PubNub is not just WebSocket, and can be\nused in many other ways, on many other clients.\n\nBrowser\nSpeaking of clients, let us take a jump over to the browser. This is where we\nreally want database changes to show up - at least for the weather application\nwe have been working through over the past several posts.\n\nIf you recall, we have a Particle Photon sending data to our Node.js \ninfrastructure on IBM Bluemix. We then have a browser application polling the\nNode.js infrastructure for the latest updates. What we really want to happen is\nthat new weather readings arriving from the Photon are pushed to the browser.\n\n<!-- PubNub library -->\n<script src=\"https://cdn.pubnub.com/pubnub-3.7.13.min.js\" type=\"text/javascript\"></script>\n\n...\n\n// Initialize\nvar pubnub = PUBNUB( configuration.pubnub );\n\t\n// Subscribe\t\npubnub.subscribe( {\n  channel: PUBNUB_CHANNEL,\n  message: update\t\n} );\n\n...\n\n// A new update has arrived\n// Show the related document\nfunction update( data )\n{\n  console.log( data );\n}\n\n\nThat is all there is to it. We include the PubNub library, initialize it, and\nthen subscribe to messages. Messages are delivered on a specific channel - the\nsame channel we defined in our Node.js/IBM Cloudant configuration. In this case,\nthe log at the browser will show the same document that we log from Node.js.\n\nNext Steps\nWhat is really going to bake your noodle is that the Photon sensor, could also\ncommunicate with PubNub directly. This shifts our development paradigm\nconsiderably. The sensor publishes data to PubNub, then the Node.js server, as\nwell as the browser, are both subscribed (listening) for those messages. None of\nthem know anything about the other, yet the same results are possible - and more\nscalable.\n\n> It is entirely possible under this architecture, that you might not even need\nthe IBM Cloudant continuous feed feature.\n\n\nWant an Android device to take part in the conversation? Just have it subscribe\nas well. Maybe you want to report weather data from the sensors on the Android\nphone? Just have it publish the data. The device does not need to worry about\nIBM Cloudant at all - or the browser, or Node.js, or anything else.\n\nLoosely decoupled architectures is something I will be exploring in greater\ndetail in a future series of posts. If you have any questions in the meantime,\nplease feel free to post a comment below. You can also get the complete code for\nmy IoT-enabled weather application on GitHub\n[https://github.com/krhoyt/IBM/tree/master/iotweather].","feature_image":"http://images.kevinhoyt.com/the.cloud.jpg","featured":0,"status":"published","locale":null,"visibility":"public","author_id":"1","created_at":"2015-08-13T16:18:47.000Z","updated_at":"2015-08-20T18:58:26.000Z","published_at":"2015-08-20T18:54:27.000Z","custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"type":"post","email_recipient_filter":"none"},{"id":"5adb8922351ffe0018a57732","uuid":"f7d6df44-4ac3-44f6-83ce-eff9b0494b16","title":"IoT Weather on Cloudant","slug":"iot-weather-on-android","mobiledoc":"{\"version\":\"0.3.1\",\"markups\":[],\"atoms\":[],\"cards\":[[\"markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\"Getting weather data from a [service](http://blog.kevinhoyt.com/2015/07/29/weather-in-three-flavors/) is all fine and dandy, but in this day and age of IoT (Internet of Things), it almost seems more likely that you will be getting data from sensor in and around your home.  In this post I will take you through connecting a Particle Photon to IBM Bluemix, including storing data in IBM Cloudant.\\n\\n###Particle Photon\\n\\nI got hooked on IoT circa 2007 when I built an RFID-controlled beer keg using [Phidgets](http://www.phidgets.com).  ==Phidgets== are a great place to start with IoT if you have a project in mind, but know nothing about electronics.  The downside is that they are expensive.  They are also designed to be controlled by your computer, which is cool, but not so much IoT.\\n\\nFrom there I moved into the now obligatory world of [Arduino](http://arduino.cc).  If you feel like you can tackle at least some electronics, or have the thirst to learn, the ==Arduino==, and associated community, will take you very far.  Getting wireless Internet access on an Ardiuno however comes with its own hurdles - not the least of which is size and battery consumption.\\n\\nThese days, when I start an IoT project, my go-to board is the [Particle Photon](http://particle.io).  These little gems are tiny, include ==wireless on-board==, and are ==Arduino compatible==.  You can also program them wireless using the ==Particle Build tool - a web-based IDE==.  You also can not beat the price.  The ==Photon== will run you ==$19 USD==.  Everything is ==open source==, and there is a very active, and growing, community.\\n\\n###Server\\n\\nBefore we get into programming our Photon, let us take a look at the server infrastructure.  Like most things ==IBM Bluemix==, this will start with a splash of ==Node.js==.  I will be using ==Express==, and creating a ==route for an HTTP POST== to create a new reading from the Photon sensor.\\n\\n```\\n// Create a new reading\\nrouter.post( '/reading', function( req, res ) {\\n  req.data.insert( req.body, function( error, body ) {\\n    if( error ) {\\n      req.logger.info( 'Problem creating document.' );\\n      req.logger.info( 'Message: ' + error.message );\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n    }\\n\\t\\t\\n    res.json( body );\\t\\t\\t\\t\\t\\n  } );\\t\\n} );\\n```\\n\\n###Cloudant\\n\\nIn the above code snippet, there are a few extra objects on the Express \\\"req\\\" (request) object.  The \\\"==logger==\\\" object is a logger that integrates with IBM Bluemix logging.  You can use [Winston](https://github.com/winstonjs/winston) or other logging tools for the output as well.  \\n\\nThe other object is named \\\"==data==\\\".  The \\\"data\\\" object is a reference to a [Cloudant](https://cloudant.com) ==NoSQL== database.  This uses the Node.js Cloudant [package](https://github.com/cloudant/nodejs-cloudant).  There are other libraries for other platforms as well.  Both the logging and data access are configured in the \\\"==app.js==\\\" setup for the application.\\n\\n```\\n// Bluemix\\nibmbluemix.initialize( configuration.bluemix );\\n\\n// Environment\\nvar ibmconfig = ibmbluemix.getConfig();\\n\\n// Logging\\nvar ibmlogger = ibmbluemix.getLogger();\\n\\n// Database\\nvar ibmdb = null;\\n\\n// Connect\\nCloudant( {\\n  account: configuration.cloudant.username,\\n  password: configuration.cloudant.password\\n}, function( error, cloudant ) {\\n  if( error ) {\\n    ibmlogger.info( 'Could not connect to Cloudant.' );\\n    ibmlogger.info( 'Message: ' + error.message );\\n  }\\t\\n\\n  // Use database\\n  ibmlogger.info( 'Using database.' );\\n  ibmdb = cloudant.db.use( CLOUDANT_DATABASE );\\n}\\n\\n// Add functionality to request\\napp.use( function( req, res, next ) {\\n  req.data = ibmdb;\\n  req.logger = ibmlogger;\\n  next();\\n} );\\n```\\n\\nThe \\\"==insert==\\\" method call places a JSON document into the Cloudant database, and returns whatever document results from the insert operation.  This is usually a document containing the ==ID== of the document, and the ==revision== (rev) number.  I do not use revisions here, but they are a helpful feature of the ==online/offline== [replication](https://docs.cloudant.com/replication.html) (==sync==) of Cloudant.\\n\\n###Microcontroller\\n\\nBack on the Particle Photon, we need to add some logic to send a JSON document with the data over to our Node.js infrastructure.  Getting started with the Photon is beyond the scope of this post.  The [documentation](https://docs.particle.io/guide/getting-started/start/photon/) for the Photon is extremely robust, and a post to the [forum](http://community.particle.io) is generally answered within 24-hours.\\n\\n![HIH-6130 Temperature and Humidity](http://images.kevinhoyt.com/sparkfun-hih-6130.jpg)\\n\\n*Photo courtesy of SparkFun.**\\n\\nWe will be using an [HIH-6130](https://www.sparkfun.com/products/11295) ==temperature and humidity== sensor for this project.  Although the ==HIH-6130== is a bit on the pricey side, if find the reliability to be unparalleled.  There is a Particle library for working with the HIH-6130 (like most other sensors), so getting started is just a matter of dropping the library into your Build project.\\n\\n```\\n// HIH library\\n#include \\\"HIH61XX/HIH61XX.h\\\"\\n\\n// Sensor\\nHIH61XX   hih( 0x27, D3 );\\n\\n// Setup\\nvoid setup()\\n{\\n  long reset;\\n\\n  // Wait for clock to update\\n  do {\\n    reset = Time.now();\\n    delay( 10 );\\n  } while ( reset < 1000000 && millis() < 20000 );\\n\\n  // I2C\\n  Wire.begin();\\n}\\n```\\n\\nThe Particle can use the local wireless to determine the current time, which will be valuable as a ==timestamp== in Cloudant that we can ==index, and sort==.\\n\\nWhile the Particle would be capable of dumping data into Cloudant quite rapidly (~1 update/second), I will be using an interval of once per minute.\\n\\n```\\n// Loop\\nvoid loop()\\n{\\n  // Device identification\\n  device = System.deviceID();\\n\\n  // Start HIH\\n  hih.start();\\n  \\n  // Update HIH\\n  hih.update();\\n\\n  // Sensor values\\n  celcius = hih.temperature();\\n  fahrenheit = ( celcius * 1.8 ) + 32;\\n  humidity = hih.humidity() * 100;\\n\\n  // Connect to server\\n  // Send sensor data\\n  if( client.connect( BLUEMIX_URL, BLUEMIX_PORT ) )\\n  {\\n     request();\\n     wait();\\n     response();\\n  }\\n\\n  // Delay\\n  delay( UPDATE_RATE );\\n}\\n```\\n\\nThe magic here happens with the \\\"==client.connect()==\\\" call, and the subsequent \\\"==request()==\\\" call.  This effectively translates into \\\"if we can make a connection to IBM Bluemix, then send the HIH-6130 data as JSON.\\\"  Temperature and humidity are stored as global variables for ease of access from the \\\"request()\\\" function.\\n\\n```\\n// Request\\nvoid request()\\n{\\n  char    content[255];\\n  char    photon[50];\\n\\n  char    c[6];\\n  char    f[6];\\n  char    h[6];  \\n\\n  // Get values as character arrays\\n  device.toCharArray( photon, 50 );\\n  String( celcius, 2 ).toCharArray( c, 6 );\\n  String( fahrenheit, 2 ).toCharArray( f, 6 );\\n  String( humidity, 2 ).toCharArray( h, 6 );\\n\\n  // Format JSON request body\\n  sprintf(\\n    content,\\n    \\\"{ \\\\\\\"sensor\\\\\\\": \\\\\\\"%s\\\\\\\", \\\\\\\"celcius\\\\\\\": %s, \\\\\\\"fahrenheit\\\\\\\": %s, \\\\\\\"humidity\\\\\\\": %s, \\\\\\\"timestamp\\\\\\\": %lu, \\\\\\\"version\\\\\\\": \\\\\\\"%s\\\\\\\" }\\\",\\n    photon,\\n    c,\\n    f,\\n    h,\\n    Time.now(),\\n    SENSOR_VERSION\\n  );\\n\\n  // Make request to server\\n  // Send sensor data as JSON\\n  client.print( \\\"POST \\\" );\\n  client.print( BLUEMIX_PATH );\\n  client.println( \\\" HTTP/1.1\\\" );\\n  client.println( \\\"User-Agent: Particle Photon\\\" );\\n  client.print( \\\"Host: \\\" );\\n  client.println( BLUEMIX_URL );\\n  client.println( \\\"Content-Type: application/json\\\" );\\n  client.print( \\\"Content-Length: \\\" );\\n  client.println( strlen( content ) );\\n  client.println( \\\"Cache-Control: no-cache\\\" );\\n  client.println();\\n  client.print( content );\\n}\\n```\\n\\nHere I use the \\\"==sprintf()==\\\" function to make the ==JSON== document that we will be sending.  With that complete, the Particle then sends the ==raw HTTP headers==, and JSON document, across the connection to IBM Bluemix.  The values arrive at Bluemix, enter our Node.js POST handler, and then directly inserts the JSON document into the Cloudant database.\\n\\n###Web\\n\\nWhere would our project be without some way to visualize the data?  To get the data back out of the database, we will need to add an HTTP GET handler to our Express routing.\\n\\n```\\n// Get all readings\\n// Get latest reading\\n// Get readings within a page\\nrouter.get( '/reading', function( req, res ) {\\n  var params = null;\\n\\n  // Query parameters\\t\\t\\n  params = {\\n    selector: {\\n      timestamp: {'$gt': 0}\\n    },\\n    sort: [\\n      {timestamp: 'desc'}\\n    ]\\n  };\\n\\t\\n  // Support getting only latest reading\\n  // Also supports paging\\n  if( req.query.limit != undefined ) {\\n    params.limit = parseInt( req.query.limit );\\t\\n  }\\n\\t\\n  // More support for paging\\n  // Arguments supplied via query string\\n  if( req.query.skip != undefined ) {\\n    params.skip = parseInt( req.query.skip );\\t\\n  }\\t\\t\\n\\t\\n  // Find using selector\\n  // Returns documents\\n  // Not the list function\\n  req.data.find( params, function( error, body ) {\\n    if( error ) {\\n      req.logger.info( 'Problem reading documents.' );\\n      req.logger.info( 'Message: ' + error.message );\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n    }\\n\\t\\t\\n    res.json( body );\\n  } );\\n} );\\n```\\n\\nThis implementation of a ==GET== request will return all the readings by default.  This is useful if you want to chart all the data.  If you just want a chunk of the data, I have added handling for \\\"==limit==\\\" and \\\"==skip==\\\" query string parameters.  Using these, you can get a ==specific page== of data, or in this case, the ==single most recent document==.\\n\\n> The Cloudant database has an index setup on the \\\"timestamp\\\" attribute.  This in turn allows us to sort the results using the \\\"param\\\" object.\\n\\nOn the client (the browser) we can then use \\\"==setInterval()==\\\" to periodically call the GET handler, and then display the most result document.  You might chose to use XHR directly, or the more robust and convenient [Cloud Code](http://blog.kevinhoyt.com/2015/08/13/weather-on-android/) library.\\n\\n![The temperature and humidity in my office.](http://images.kevinhoyt.com/bluemix-iot-weather.jpg)\\n\\n###Next Steps\\n\\nWhile polling from the client is certainly one way to approach getting the latest data, it is certainly far from the most efficient.  ==At scale==, random requests to check for data that may not have yet updated, will eventually start to ==tax your infrastructure==.  You may also poll at a point in the update that your data could be stale.\\n\\nThe better approach to this problem would be to ==push== new weather data to the client as it arrives in the database.  Cloudant supports this type of operation using a functionality called a \\\"==continuous feed==\\\".  In my next post, I will exploring moving from a request/response weather checking algorithm, to a publish-subscribe approach.\\n\\nUntil then, if you want to get a jump on all the code, take a look at this project in my [GitHub](http://github.com/krhoyt) repository.  If you have questions, please feel free to leave a comment below, or ask me on [Twitter](http://twitter.com/krhoyt).\"}]],\"sections\":[[10,0]],\"ghostVersion\":\"3.0\"}","html":"<!--kg-card-begin: markdown--><p>Getting weather data from a <a href=\"http://blog.kevinhoyt.com/2015/07/29/weather-in-three-flavors/\">service</a> is all fine and dandy, but in this day and age of IoT (Internet of Things), it almost seems more likely that you will be getting data from sensor in and around your home.  In this post I will take you through connecting a Particle Photon to IBM Bluemix, including storing data in IBM Cloudant.</p>\n<h3 id=\"particlephoton\">Particle Photon</h3>\n<p>I got hooked on IoT circa 2007 when I built an RFID-controlled beer keg using <a href=\"http://www.phidgets.com\">Phidgets</a>.  <mark>Phidgets</mark> are a great place to start with IoT if you have a project in mind, but know nothing about electronics.  The downside is that they are expensive.  They are also designed to be controlled by your computer, which is cool, but not so much IoT.</p>\n<p>From there I moved into the now obligatory world of <a href=\"http://arduino.cc\">Arduino</a>.  If you feel like you can tackle at least some electronics, or have the thirst to learn, the <mark>Arduino</mark>, and associated community, will take you very far.  Getting wireless Internet access on an Ardiuno however comes with its own hurdles - not the least of which is size and battery consumption.</p>\n<p>These days, when I start an IoT project, my go-to board is the <a href=\"http://particle.io\">Particle Photon</a>.  These little gems are tiny, include <mark>wireless on-board</mark>, and are <mark>Arduino compatible</mark>.  You can also program them wireless using the <mark>Particle Build tool - a web-based IDE</mark>.  You also can not beat the price.  The <mark>Photon</mark> will run you <mark>$19 USD</mark>.  Everything is <mark>open source</mark>, and there is a very active, and growing, community.</p>\n<h3 id=\"server\">Server</h3>\n<p>Before we get into programming our Photon, let us take a look at the server infrastructure.  Like most things <mark>IBM Bluemix</mark>, this will start with a splash of <mark>Node.js</mark>.  I will be using <mark>Express</mark>, and creating a <mark>route for an HTTP POST</mark> to create a new reading from the Photon sensor.</p>\n<pre><code>// Create a new reading\nrouter.post( '/reading', function( req, res ) {\n  req.data.insert( req.body, function( error, body ) {\n    if( error ) {\n      req.logger.info( 'Problem creating document.' );\n      req.logger.info( 'Message: ' + error.message );\t\t\t\t\t\t\t\t\t\t\t\t\n    }\n\t\t\n    res.json( body );\t\t\t\t\t\n  } );\t\n} );\n</code></pre>\n<h3 id=\"cloudant\">Cloudant</h3>\n<p>In the above code snippet, there are a few extra objects on the Express &quot;req&quot; (request) object.  The &quot;<mark>logger</mark>&quot; object is a logger that integrates with IBM Bluemix logging.  You can use <a href=\"https://github.com/winstonjs/winston\">Winston</a> or other logging tools for the output as well.</p>\n<p>The other object is named &quot;<mark>data</mark>&quot;.  The &quot;data&quot; object is a reference to a <a href=\"https://cloudant.com\">Cloudant</a> <mark>NoSQL</mark> database.  This uses the Node.js Cloudant <a href=\"https://github.com/cloudant/nodejs-cloudant\">package</a>.  There are other libraries for other platforms as well.  Both the logging and data access are configured in the &quot;<mark>app.js</mark>&quot; setup for the application.</p>\n<pre><code>// Bluemix\nibmbluemix.initialize( configuration.bluemix );\n\n// Environment\nvar ibmconfig = ibmbluemix.getConfig();\n\n// Logging\nvar ibmlogger = ibmbluemix.getLogger();\n\n// Database\nvar ibmdb = null;\n\n// Connect\nCloudant( {\n  account: configuration.cloudant.username,\n  password: configuration.cloudant.password\n}, function( error, cloudant ) {\n  if( error ) {\n    ibmlogger.info( 'Could not connect to Cloudant.' );\n    ibmlogger.info( 'Message: ' + error.message );\n  }\t\n\n  // Use database\n  ibmlogger.info( 'Using database.' );\n  ibmdb = cloudant.db.use( CLOUDANT_DATABASE );\n}\n\n// Add functionality to request\napp.use( function( req, res, next ) {\n  req.data = ibmdb;\n  req.logger = ibmlogger;\n  next();\n} );\n</code></pre>\n<p>The &quot;<mark>insert</mark>&quot; method call places a JSON document into the Cloudant database, and returns whatever document results from the insert operation.  This is usually a document containing the <mark>ID</mark> of the document, and the <mark>revision</mark> (rev) number.  I do not use revisions here, but they are a helpful feature of the <mark>online/offline</mark> <a href=\"https://docs.cloudant.com/replication.html\">replication</a> (<mark>sync</mark>) of Cloudant.</p>\n<h3 id=\"microcontroller\">Microcontroller</h3>\n<p>Back on the Particle Photon, we need to add some logic to send a JSON document with the data over to our Node.js infrastructure.  Getting started with the Photon is beyond the scope of this post.  The <a href=\"https://docs.particle.io/guide/getting-started/start/photon/\">documentation</a> for the Photon is extremely robust, and a post to the <a href=\"http://community.particle.io\">forum</a> is generally answered within 24-hours.</p>\n<p><img src=\"http://images.kevinhoyt.com/sparkfun-hih-6130.jpg\" alt=\"HIH-6130 Temperature and Humidity\" loading=\"lazy\"></p>\n<p><em>Photo courtesy of SparkFun.</em>*</p>\n<p>We will be using an <a href=\"https://www.sparkfun.com/products/11295\">HIH-6130</a> <mark>temperature and humidity</mark> sensor for this project.  Although the <mark>HIH-6130</mark> is a bit on the pricey side, if find the reliability to be unparalleled.  There is a Particle library for working with the HIH-6130 (like most other sensors), so getting started is just a matter of dropping the library into your Build project.</p>\n<pre><code>// HIH library\n#include &quot;HIH61XX/HIH61XX.h&quot;\n\n// Sensor\nHIH61XX   hih( 0x27, D3 );\n\n// Setup\nvoid setup()\n{\n  long reset;\n\n  // Wait for clock to update\n  do {\n    reset = Time.now();\n    delay( 10 );\n  } while ( reset &lt; 1000000 &amp;&amp; millis() &lt; 20000 );\n\n  // I2C\n  Wire.begin();\n}\n</code></pre>\n<p>The Particle can use the local wireless to determine the current time, which will be valuable as a <mark>timestamp</mark> in Cloudant that we can <mark>index, and sort</mark>.</p>\n<p>While the Particle would be capable of dumping data into Cloudant quite rapidly (~1 update/second), I will be using an interval of once per minute.</p>\n<pre><code>// Loop\nvoid loop()\n{\n  // Device identification\n  device = System.deviceID();\n\n  // Start HIH\n  hih.start();\n  \n  // Update HIH\n  hih.update();\n\n  // Sensor values\n  celcius = hih.temperature();\n  fahrenheit = ( celcius * 1.8 ) + 32;\n  humidity = hih.humidity() * 100;\n\n  // Connect to server\n  // Send sensor data\n  if( client.connect( BLUEMIX_URL, BLUEMIX_PORT ) )\n  {\n     request();\n     wait();\n     response();\n  }\n\n  // Delay\n  delay( UPDATE_RATE );\n}\n</code></pre>\n<p>The magic here happens with the &quot;<mark>client.connect()</mark>&quot; call, and the subsequent &quot;<mark>request()</mark>&quot; call.  This effectively translates into &quot;if we can make a connection to IBM Bluemix, then send the HIH-6130 data as JSON.&quot;  Temperature and humidity are stored as global variables for ease of access from the &quot;request()&quot; function.</p>\n<pre><code>// Request\nvoid request()\n{\n  char    content[255];\n  char    photon[50];\n\n  char    c[6];\n  char    f[6];\n  char    h[6];  \n\n  // Get values as character arrays\n  device.toCharArray( photon, 50 );\n  String( celcius, 2 ).toCharArray( c, 6 );\n  String( fahrenheit, 2 ).toCharArray( f, 6 );\n  String( humidity, 2 ).toCharArray( h, 6 );\n\n  // Format JSON request body\n  sprintf(\n    content,\n    &quot;{ \\&quot;sensor\\&quot;: \\&quot;%s\\&quot;, \\&quot;celcius\\&quot;: %s, \\&quot;fahrenheit\\&quot;: %s, \\&quot;humidity\\&quot;: %s, \\&quot;timestamp\\&quot;: %lu, \\&quot;version\\&quot;: \\&quot;%s\\&quot; }&quot;,\n    photon,\n    c,\n    f,\n    h,\n    Time.now(),\n    SENSOR_VERSION\n  );\n\n  // Make request to server\n  // Send sensor data as JSON\n  client.print( &quot;POST &quot; );\n  client.print( BLUEMIX_PATH );\n  client.println( &quot; HTTP/1.1&quot; );\n  client.println( &quot;User-Agent: Particle Photon&quot; );\n  client.print( &quot;Host: &quot; );\n  client.println( BLUEMIX_URL );\n  client.println( &quot;Content-Type: application/json&quot; );\n  client.print( &quot;Content-Length: &quot; );\n  client.println( strlen( content ) );\n  client.println( &quot;Cache-Control: no-cache&quot; );\n  client.println();\n  client.print( content );\n}\n</code></pre>\n<p>Here I use the &quot;<mark>sprintf()</mark>&quot; function to make the <mark>JSON</mark> document that we will be sending.  With that complete, the Particle then sends the <mark>raw HTTP headers</mark>, and JSON document, across the connection to IBM Bluemix.  The values arrive at Bluemix, enter our Node.js POST handler, and then directly inserts the JSON document into the Cloudant database.</p>\n<h3 id=\"web\">Web</h3>\n<p>Where would our project be without some way to visualize the data?  To get the data back out of the database, we will need to add an HTTP GET handler to our Express routing.</p>\n<pre><code>// Get all readings\n// Get latest reading\n// Get readings within a page\nrouter.get( '/reading', function( req, res ) {\n  var params = null;\n\n  // Query parameters\t\t\n  params = {\n    selector: {\n      timestamp: {'$gt': 0}\n    },\n    sort: [\n      {timestamp: 'desc'}\n    ]\n  };\n\t\n  // Support getting only latest reading\n  // Also supports paging\n  if( req.query.limit != undefined ) {\n    params.limit = parseInt( req.query.limit );\t\n  }\n\t\n  // More support for paging\n  // Arguments supplied via query string\n  if( req.query.skip != undefined ) {\n    params.skip = parseInt( req.query.skip );\t\n  }\t\t\n\t\n  // Find using selector\n  // Returns documents\n  // Not the list function\n  req.data.find( params, function( error, body ) {\n    if( error ) {\n      req.logger.info( 'Problem reading documents.' );\n      req.logger.info( 'Message: ' + error.message );\t\t\t\t\t\t\t\t\t\t\t\t\n    }\n\t\t\n    res.json( body );\n  } );\n} );\n</code></pre>\n<p>This implementation of a <mark>GET</mark> request will return all the readings by default.  This is useful if you want to chart all the data.  If you just want a chunk of the data, I have added handling for &quot;<mark>limit</mark>&quot; and &quot;<mark>skip</mark>&quot; query string parameters.  Using these, you can get a <mark>specific page</mark> of data, or in this case, the <mark>single most recent document</mark>.</p>\n<blockquote>\n<p>The Cloudant database has an index setup on the &quot;timestamp&quot; attribute.  This in turn allows us to sort the results using the &quot;param&quot; object.</p>\n</blockquote>\n<p>On the client (the browser) we can then use &quot;<mark>setInterval()</mark>&quot; to periodically call the GET handler, and then display the most result document.  You might chose to use XHR directly, or the more robust and convenient <a href=\"http://blog.kevinhoyt.com/2015/08/13/weather-on-android/\">Cloud Code</a> library.</p>\n<p><img src=\"http://images.kevinhoyt.com/bluemix-iot-weather.jpg\" alt=\"The temperature and humidity in my office.\" loading=\"lazy\"></p>\n<h3 id=\"nextsteps\">Next Steps</h3>\n<p>While polling from the client is certainly one way to approach getting the latest data, it is certainly far from the most efficient.  <mark>At scale</mark>, random requests to check for data that may not have yet updated, will eventually start to <mark>tax your infrastructure</mark>.  You may also poll at a point in the update that your data could be stale.</p>\n<p>The better approach to this problem would be to <mark>push</mark> new weather data to the client as it arrives in the database.  Cloudant supports this type of operation using a functionality called a &quot;<mark>continuous feed</mark>&quot;.  In my next post, I will exploring moving from a request/response weather checking algorithm, to a publish-subscribe approach.</p>\n<p>Until then, if you want to get a jump on all the code, take a look at this project in my <a href=\"http://github.com/krhoyt\">GitHub</a> repository.  If you have questions, please feel free to leave a comment below, or ask me on <a href=\"http://twitter.com/krhoyt\">Twitter</a>.</p>\n<!--kg-card-end: markdown-->","comment_id":"40","plaintext":"Getting weather data from a service\n[http://blog.kevinhoyt.com/2015/07/29/weather-in-three-flavors/] is all fine and\ndandy, but in this day and age of IoT (Internet of Things), it almost seems more\nlikely that you will be getting data from sensor in and around your home. In\nthis post I will take you through connecting a Particle Photon to IBM Bluemix,\nincluding storing data in IBM Cloudant.\n\nParticle Photon\nI got hooked on IoT circa 2007 when I built an RFID-controlled beer keg using \nPhidgets [http://www.phidgets.com]. Phidgets are a great place to start with IoT\nif you have a project in mind, but know nothing about electronics. The downside\nis that they are expensive. They are also designed to be controlled by your\ncomputer, which is cool, but not so much IoT.\n\nFrom there I moved into the now obligatory world of Arduino [http://arduino.cc].\nIf you feel like you can tackle at least some electronics, or have the thirst to\nlearn, the Arduino, and associated community, will take you very far. Getting\nwireless Internet access on an Ardiuno however comes with its own hurdles - not\nthe least of which is size and battery consumption.\n\nThese days, when I start an IoT project, my go-to board is the Particle Photon\n[http://particle.io]. These little gems are tiny, include wireless on-board, and\nare Arduino compatible. You can also program them wireless using the Particle\nBuild tool - a web-based IDE. You also can not beat the price. The Photon will\nrun you $19 USD. Everything is open source, and there is a very active, and\ngrowing, community.\n\nServer\nBefore we get into programming our Photon, let us take a look at the server\ninfrastructure. Like most things IBM Bluemix, this will start with a splash of \nNode.js. I will be using Express, and creating a route for an HTTP POST to\ncreate a new reading from the Photon sensor.\n\n// Create a new reading\nrouter.post( '/reading', function( req, res ) {\n  req.data.insert( req.body, function( error, body ) {\n    if( error ) {\n      req.logger.info( 'Problem creating document.' );\n      req.logger.info( 'Message: ' + error.message );\t\t\t\t\t\t\t\t\t\t\t\t\n    }\n\t\t\n    res.json( body );\t\t\t\t\t\n  } );\t\n} );\n\n\nCloudant\nIn the above code snippet, there are a few extra objects on the Express \"req\"\n(request) object. The \"logger\" object is a logger that integrates with IBM\nBluemix logging. You can use Winston [https://github.com/winstonjs/winston] or\nother logging tools for the output as well.\n\nThe other object is named \"data\". The \"data\" object is a reference to a Cloudant\n[https://cloudant.com] NoSQL database. This uses the Node.js Cloudant package\n[https://github.com/cloudant/nodejs-cloudant]. There are other libraries for\nother platforms as well. Both the logging and data access are configured in the\n\"app.js\" setup for the application.\n\n// Bluemix\nibmbluemix.initialize( configuration.bluemix );\n\n// Environment\nvar ibmconfig = ibmbluemix.getConfig();\n\n// Logging\nvar ibmlogger = ibmbluemix.getLogger();\n\n// Database\nvar ibmdb = null;\n\n// Connect\nCloudant( {\n  account: configuration.cloudant.username,\n  password: configuration.cloudant.password\n}, function( error, cloudant ) {\n  if( error ) {\n    ibmlogger.info( 'Could not connect to Cloudant.' );\n    ibmlogger.info( 'Message: ' + error.message );\n  }\t\n\n  // Use database\n  ibmlogger.info( 'Using database.' );\n  ibmdb = cloudant.db.use( CLOUDANT_DATABASE );\n}\n\n// Add functionality to request\napp.use( function( req, res, next ) {\n  req.data = ibmdb;\n  req.logger = ibmlogger;\n  next();\n} );\n\n\nThe \"insert\" method call places a JSON document into the Cloudant database, and\nreturns whatever document results from the insert operation. This is usually a\ndocument containing the ID of the document, and the revision (rev) number. I do\nnot use revisions here, but they are a helpful feature of the online/offline \nreplication [https://docs.cloudant.com/replication.html] (sync) of Cloudant.\n\nMicrocontroller\nBack on the Particle Photon, we need to add some logic to send a JSON document\nwith the data over to our Node.js infrastructure. Getting started with the\nPhoton is beyond the scope of this post. The documentation\n[https://docs.particle.io/guide/getting-started/start/photon/] for the Photon is\nextremely robust, and a post to the forum [http://community.particle.io] is\ngenerally answered within 24-hours.\n\n\n\nPhoto courtesy of SparkFun.*\n\nWe will be using an HIH-6130 [https://www.sparkfun.com/products/11295] \ntemperature and humidity sensor for this project. Although the HIH-6130 is a bit\non the pricey side, if find the reliability to be unparalleled. There is a\nParticle library for working with the HIH-6130 (like most other sensors), so\ngetting started is just a matter of dropping the library into your Build\nproject.\n\n// HIH library\n#include \"HIH61XX/HIH61XX.h\"\n\n// Sensor\nHIH61XX   hih( 0x27, D3 );\n\n// Setup\nvoid setup()\n{\n  long reset;\n\n  // Wait for clock to update\n  do {\n    reset = Time.now();\n    delay( 10 );\n  } while ( reset < 1000000 && millis() < 20000 );\n\n  // I2C\n  Wire.begin();\n}\n\n\nThe Particle can use the local wireless to determine the current time, which\nwill be valuable as a timestamp in Cloudant that we can index, and sort.\n\nWhile the Particle would be capable of dumping data into Cloudant quite rapidly\n(~1 update/second), I will be using an interval of once per minute.\n\n// Loop\nvoid loop()\n{\n  // Device identification\n  device = System.deviceID();\n\n  // Start HIH\n  hih.start();\n  \n  // Update HIH\n  hih.update();\n\n  // Sensor values\n  celcius = hih.temperature();\n  fahrenheit = ( celcius * 1.8 ) + 32;\n  humidity = hih.humidity() * 100;\n\n  // Connect to server\n  // Send sensor data\n  if( client.connect( BLUEMIX_URL, BLUEMIX_PORT ) )\n  {\n     request();\n     wait();\n     response();\n  }\n\n  // Delay\n  delay( UPDATE_RATE );\n}\n\n\nThe magic here happens with the \"client.connect()\" call, and the subsequent \"\nrequest()\" call. This effectively translates into \"if we can make a connection\nto IBM Bluemix, then send the HIH-6130 data as JSON.\" Temperature and humidity\nare stored as global variables for ease of access from the \"request()\" function.\n\n// Request\nvoid request()\n{\n  char    content[255];\n  char    photon[50];\n\n  char    c[6];\n  char    f[6];\n  char    h[6];  \n\n  // Get values as character arrays\n  device.toCharArray( photon, 50 );\n  String( celcius, 2 ).toCharArray( c, 6 );\n  String( fahrenheit, 2 ).toCharArray( f, 6 );\n  String( humidity, 2 ).toCharArray( h, 6 );\n\n  // Format JSON request body\n  sprintf(\n    content,\n    \"{ \\\"sensor\\\": \\\"%s\\\", \\\"celcius\\\": %s, \\\"fahrenheit\\\": %s, \\\"humidity\\\": %s, \\\"timestamp\\\": %lu, \\\"version\\\": \\\"%s\\\" }\",\n    photon,\n    c,\n    f,\n    h,\n    Time.now(),\n    SENSOR_VERSION\n  );\n\n  // Make request to server\n  // Send sensor data as JSON\n  client.print( \"POST \" );\n  client.print( BLUEMIX_PATH );\n  client.println( \" HTTP/1.1\" );\n  client.println( \"User-Agent: Particle Photon\" );\n  client.print( \"Host: \" );\n  client.println( BLUEMIX_URL );\n  client.println( \"Content-Type: application/json\" );\n  client.print( \"Content-Length: \" );\n  client.println( strlen( content ) );\n  client.println( \"Cache-Control: no-cache\" );\n  client.println();\n  client.print( content );\n}\n\n\nHere I use the \"sprintf()\" function to make the JSON document that we will be\nsending. With that complete, the Particle then sends the raw HTTP headers, and\nJSON document, across the connection to IBM Bluemix. The values arrive at\nBluemix, enter our Node.js POST handler, and then directly inserts the JSON\ndocument into the Cloudant database.\n\nWeb\nWhere would our project be without some way to visualize the data? To get the\ndata back out of the database, we will need to add an HTTP GET handler to our\nExpress routing.\n\n// Get all readings\n// Get latest reading\n// Get readings within a page\nrouter.get( '/reading', function( req, res ) {\n  var params = null;\n\n  // Query parameters\t\t\n  params = {\n    selector: {\n      timestamp: {'$gt': 0}\n    },\n    sort: [\n      {timestamp: 'desc'}\n    ]\n  };\n\t\n  // Support getting only latest reading\n  // Also supports paging\n  if( req.query.limit != undefined ) {\n    params.limit = parseInt( req.query.limit );\t\n  }\n\t\n  // More support for paging\n  // Arguments supplied via query string\n  if( req.query.skip != undefined ) {\n    params.skip = parseInt( req.query.skip );\t\n  }\t\t\n\t\n  // Find using selector\n  // Returns documents\n  // Not the list function\n  req.data.find( params, function( error, body ) {\n    if( error ) {\n      req.logger.info( 'Problem reading documents.' );\n      req.logger.info( 'Message: ' + error.message );\t\t\t\t\t\t\t\t\t\t\t\t\n    }\n\t\t\n    res.json( body );\n  } );\n} );\n\n\nThis implementation of a GET request will return all the readings by default.\nThis is useful if you want to chart all the data. If you just want a chunk of\nthe data, I have added handling for \"limit\" and \"skip\" query string parameters.\nUsing these, you can get a specific page of data, or in this case, the single\nmost recent document.\n\n> The Cloudant database has an index setup on the \"timestamp\" attribute. This in\nturn allows us to sort the results using the \"param\" object.\n\n\nOn the client (the browser) we can then use \"setInterval()\" to periodically call\nthe GET handler, and then display the most result document. You might chose to\nuse XHR directly, or the more robust and convenient Cloud Code\n[http://blog.kevinhoyt.com/2015/08/13/weather-on-android/] library.\n\n\n\nNext Steps\nWhile polling from the client is certainly one way to approach getting the\nlatest data, it is certainly far from the most efficient. At scale, random\nrequests to check for data that may not have yet updated, will eventually start\nto tax your infrastructure. You may also poll at a point in the update that your\ndata could be stale.\n\nThe better approach to this problem would be to push new weather data to the\nclient as it arrives in the database. Cloudant supports this type of operation\nusing a functionality called a \"continuous feed\". In my next post, I will\nexploring moving from a request/response weather checking algorithm, to a\npublish-subscribe approach.\n\nUntil then, if you want to get a jump on all the code, take a look at this\nproject in my GitHub [http://github.com/krhoyt] repository. If you have\nquestions, please feel free to leave a comment below, or ask me on Twitter\n[http://twitter.com/krhoyt].","feature_image":"http://images.kevinhoyt.com/bluemix-web-weather.jpg","featured":0,"status":"published","locale":null,"visibility":"public","author_id":"1","created_at":"2015-08-13T16:19:03.000Z","updated_at":"2015-08-18T15:30:05.000Z","published_at":"2015-08-18T15:30:05.000Z","custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"type":"post","email_recipient_filter":"none"},{"id":"5adb8922351ffe0018a57733","uuid":"9489934f-4517-4d6a-a8e8-1b339780a633","title":"IBM Cube: Hardware","slug":"diversions-ibm-cube","mobiledoc":"{\"version\":\"0.3.1\",\"markups\":[],\"atoms\":[],\"cards\":[[\"markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\"IBM has a distinctly \\\"executive\\\" feel to it.  Long gone are the mandatory ties and corporate theme song (yes, seriously), but that culture is still very much alive and well.  While I work remote most of the time, I wanted something with that executive feel on my desk.  The ==IBM Cube== project was born.\\n\\n![IBM Chicago Office](http://images.kevinhoyt.com/ibm-logo-chicago.jpg)\\n\\nAs I looked around the web for IBM logos, I ran across this marker for the ==Chicago, IL office==.  It seemed like the perfect fit, so I pulled the initial dimensions (obviously at a much smaller scale) from that design.  The white that lights up at night could be cut out to reveal the ==RGB LED== lights, and of course I would put a ==wireless== micro-controller inside to control the lighting remotely.\\n\\n###Fabrication\\n\\nDesktop fabrication has come a long way in recent years.  Where before, a laser cutter would cost you $16,000 at an entry level, today you can upload a ==vector file== to a service like [Ponoko](https://www.ponoko.com/), and have the results a few days later.  Cannot afford a 3D printer?  Upload to [Shapeways](http://www.shapeways.com/), and pick from numerous materials (including metal).\\n\\nComing from Adobe, I am pretty fluent in ==Illustrator==, especially as it pertains to fabrication, so that was my tool of choice.  I chose a high quality ==walnut veneer== product at ==3.5mm== thick for that executive look.  I also baked in a little \\\"P\\\" for my daughter, Paige, as a material test.\\n\\n![Laser Cutter Plan](http://images.kevinhoyt.com/cube-laser-cutter.png)\\n\\nI have learned over the years of working with a laser cutter to ==etch guides== on the faces that do not show, which is why the IBM logo is backwards in the design.  You can see the outline of the [WS2812 LEDs](https://www.sparkfun.com/products/13282), as well as the [Photon](https://www.particle.io/prototype) wireless controller from Particle.  \\n\\nTrying something new this time, I etched the entire area where the the [USB Micro-B](https://www.sparkfun.com/products/12035) breakout board would go.  The hope here was to shave a little thickness off the wood to provide more wiggle room for the breakout board.  The idea did not work.\\n\\n![Laser Cutter Results](http://images.kevinhoyt.com/cube-laser-result.jpg)\\n\\nI was worried that the cutouts for the IBM logo would be too tight for the ==laser kerf== (material lost in the cut of the laser itself), but the results came out great.  In the picture above, Ponoko uses a sheet of ==masking tape== to keep the hot flame from scorching the surrounding material.  The sheet of material measures in at 12\\\" x 24\\\", with the IBM Cube being 100mm cubed.\\n\\n![Without masking tape, somewhat assembled.](http://images.kevinhoyt.com/cube-no-tape.jpg)\\n\\nAfter pulling off the masking tape comes the part that makes me the most nervous with every project - test assembly.  ==You are just never quite sure if you missed a cut or measurement in translating from the 3D concept from your head to the 2D vector design.==  This time around, at first glance, it looks like I got everything right.\\n\\n###Assembly\\n\\nAfter researching SparkFun breakout board thickness, and reviewing the data sheet for the USB Micro-B adapter used in the breakout board, I calculated that the total height of the assembled breakout board would be about 4.5mm.  Since I am working with a 3.5mm wood sheet product, that leaves me with a ==1mm problem==.\\n\\nAs mentioned earlier, my hope was that a heavy etching of the breakout board area, on the actual wood, would give me that extra 1mm.  It did not, and I had to use a ==rotary tool== to trim things up.  For a future iteration, or future designs that use the USB Micro-B breakout board, I think I will design a ==3D printed enclosure== for it to fit cleanly in the project.\\n\\n![Particle Photon and WS2812B RGB LEDs.](http://images.kevinhoyt.com/cube-photon-led.jpg)\\n\\nAfter gluing together the base of the project using ==Elmer's Glue-All==, running wires from the USB Micro-B up the center support, and glueing the support together, it was time to get to the meat of the project - the micro-controller and WS2812B RGB LEDs.\\n\\nIf you have ever worked with RGB LEDs before, you know that the wiring, and individual addressing can become a nightmare very quickly.  The WS2812 series of RGB LED have a micro-controller baked into them.  The LED controller needs ==in/out for 5V, GND, and a signal wire==.  That signal wire gets hooked to your project micro-controller.  The result is addressable RGB LEDs that can be easily strung together in a series.\\n\\n> The downside of WS2812 RGB LEDs is that they cost more than an RGB LED of the common cathode variety, and that they draw a lot more current - up to 60mA per LED.\\n\\nFor projects like this, I really enjoy using the [wire wrapping](http://www.digikey.com/product-detail/en/WSU-30M/K105-ND/5986) technique.  This allows you to assemble the project and test it out without soldering anything.  If you make a mistake, you can simply unwrap that terminal.  Once everything is in place and verified to be working, I go back and solder over the wires to make sure they stay where I want them.\\n\\n![IBM Cube assembled, testing, and waiting to dry.](http://images.kevinhoyt.com/cube-powered-tested.jpg)\\n\\nWhen I originally assembled the cube/top part of the project, without glue, but with the hardware working, I noticed that the light was not ==diffused== enough.  A common approach I have used for this before is a thin sheet of [vellum](http://www.hobbylobby.com/Scrapbook-Paper-Crafts/Paper-Cardstock/Vellum/12%22-x-12%22---10-Sheets-Clear-Vellum-Paper-Pack/p/111222).  The diffused light illuminates the IBM logo crisply and clearly.\\n\\nI also used ==rubber bands== to hold the cube together while the glue dried.  I have used clamps before, but they tend to be a bit heavy handed in how they hold a project together.  Rubber bands provide stability for the glue to dry, without applying too much pressure.  You can also position them more easily than clamps.\\n\\n###Next Steps\\n\\n<iframe width=\\\"560\\\" height=\\\"315\\\" src=\\\"https://www.youtube.com/embed/hw76HBeK5c8\\\" frameborder=\\\"0\\\" allowfullscreen></iframe>\\n\\nWhen I designed the IBM Cube project, I wanted it to be more than just a box with IBM carved into it.  The center support lifts the top part of the cube 3.5mm off the base of the cube.  The result is almost a ==hovering effect==, which is exactly what I was trying to achieve.\\n\\nThe two main ==changes== I would make for future iterations, would be to (a) ==3D print== an enclosure for the ==USB Micro-B breakout== and (b) use an already assembled ==strip of WS2812== RGB LEDs.  I would also look into ==doubling== the width and depth of the ==internal support== to allow for more/thicker wires (from 3.5mm to 7.0mm would be plenty).\\n\\nWith the hardware in place, the next step is to design and build a user interface.  Since the Photon can be easily controlled over REST, or even MQTT, there is no shortage of options.  We will take a look at the code in a future post.\\n\"}]],\"sections\":[[10,0]],\"ghostVersion\":\"3.0\"}","html":"<!--kg-card-begin: markdown--><p>IBM has a distinctly &quot;executive&quot; feel to it.  Long gone are the mandatory ties and corporate theme song (yes, seriously), but that culture is still very much alive and well.  While I work remote most of the time, I wanted something with that executive feel on my desk.  The <mark>IBM Cube</mark> project was born.</p>\n<p><img src=\"http://images.kevinhoyt.com/ibm-logo-chicago.jpg\" alt=\"IBM Chicago Office\" loading=\"lazy\"></p>\n<p>As I looked around the web for IBM logos, I ran across this marker for the <mark>Chicago, IL office</mark>.  It seemed like the perfect fit, so I pulled the initial dimensions (obviously at a much smaller scale) from that design.  The white that lights up at night could be cut out to reveal the <mark>RGB LED</mark> lights, and of course I would put a <mark>wireless</mark> micro-controller inside to control the lighting remotely.</p>\n<h3 id=\"fabrication\">Fabrication</h3>\n<p>Desktop fabrication has come a long way in recent years.  Where before, a laser cutter would cost you $16,000 at an entry level, today you can upload a <mark>vector file</mark> to a service like <a href=\"https://www.ponoko.com/\">Ponoko</a>, and have the results a few days later.  Cannot afford a 3D printer?  Upload to <a href=\"http://www.shapeways.com/\">Shapeways</a>, and pick from numerous materials (including metal).</p>\n<p>Coming from Adobe, I am pretty fluent in <mark>Illustrator</mark>, especially as it pertains to fabrication, so that was my tool of choice.  I chose a high quality <mark>walnut veneer</mark> product at <mark>3.5mm</mark> thick for that executive look.  I also baked in a little &quot;P&quot; for my daughter, Paige, as a material test.</p>\n<p><img src=\"http://images.kevinhoyt.com/cube-laser-cutter.png\" alt=\"Laser Cutter Plan\" loading=\"lazy\"></p>\n<p>I have learned over the years of working with a laser cutter to <mark>etch guides</mark> on the faces that do not show, which is why the IBM logo is backwards in the design.  You can see the outline of the <a href=\"https://www.sparkfun.com/products/13282\">WS2812 LEDs</a>, as well as the <a href=\"https://www.particle.io/prototype\">Photon</a> wireless controller from Particle.</p>\n<p>Trying something new this time, I etched the entire area where the the <a href=\"https://www.sparkfun.com/products/12035\">USB Micro-B</a> breakout board would go.  The hope here was to shave a little thickness off the wood to provide more wiggle room for the breakout board.  The idea did not work.</p>\n<p><img src=\"http://images.kevinhoyt.com/cube-laser-result.jpg\" alt=\"Laser Cutter Results\" loading=\"lazy\"></p>\n<p>I was worried that the cutouts for the IBM logo would be too tight for the <mark>laser kerf</mark> (material lost in the cut of the laser itself), but the results came out great.  In the picture above, Ponoko uses a sheet of <mark>masking tape</mark> to keep the hot flame from scorching the surrounding material.  The sheet of material measures in at 12&quot; x 24&quot;, with the IBM Cube being 100mm cubed.</p>\n<p><img src=\"http://images.kevinhoyt.com/cube-no-tape.jpg\" alt=\"Without masking tape, somewhat assembled.\" loading=\"lazy\"></p>\n<p>After pulling off the masking tape comes the part that makes me the most nervous with every project - test assembly.  <mark>You are just never quite sure if you missed a cut or measurement in translating from the 3D concept from your head to the 2D vector design.</mark>  This time around, at first glance, it looks like I got everything right.</p>\n<h3 id=\"assembly\">Assembly</h3>\n<p>After researching SparkFun breakout board thickness, and reviewing the data sheet for the USB Micro-B adapter used in the breakout board, I calculated that the total height of the assembled breakout board would be about 4.5mm.  Since I am working with a 3.5mm wood sheet product, that leaves me with a <mark>1mm problem</mark>.</p>\n<p>As mentioned earlier, my hope was that a heavy etching of the breakout board area, on the actual wood, would give me that extra 1mm.  It did not, and I had to use a <mark>rotary tool</mark> to trim things up.  For a future iteration, or future designs that use the USB Micro-B breakout board, I think I will design a <mark>3D printed enclosure</mark> for it to fit cleanly in the project.</p>\n<p><img src=\"http://images.kevinhoyt.com/cube-photon-led.jpg\" alt=\"Particle Photon and WS2812B RGB LEDs.\" loading=\"lazy\"></p>\n<p>After gluing together the base of the project using <mark>Elmer's Glue-All</mark>, running wires from the USB Micro-B up the center support, and glueing the support together, it was time to get to the meat of the project - the micro-controller and WS2812B RGB LEDs.</p>\n<p>If you have ever worked with RGB LEDs before, you know that the wiring, and individual addressing can become a nightmare very quickly.  The WS2812 series of RGB LED have a micro-controller baked into them.  The LED controller needs <mark>in/out for 5V, GND, and a signal wire</mark>.  That signal wire gets hooked to your project micro-controller.  The result is addressable RGB LEDs that can be easily strung together in a series.</p>\n<blockquote>\n<p>The downside of WS2812 RGB LEDs is that they cost more than an RGB LED of the common cathode variety, and that they draw a lot more current - up to 60mA per LED.</p>\n</blockquote>\n<p>For projects like this, I really enjoy using the <a href=\"http://www.digikey.com/product-detail/en/WSU-30M/K105-ND/5986\">wire wrapping</a> technique.  This allows you to assemble the project and test it out without soldering anything.  If you make a mistake, you can simply unwrap that terminal.  Once everything is in place and verified to be working, I go back and solder over the wires to make sure they stay where I want them.</p>\n<p><img src=\"http://images.kevinhoyt.com/cube-powered-tested.jpg\" alt=\"IBM Cube assembled, testing, and waiting to dry.\" loading=\"lazy\"></p>\n<p>When I originally assembled the cube/top part of the project, without glue, but with the hardware working, I noticed that the light was not <mark>diffused</mark> enough.  A common approach I have used for this before is a thin sheet of <a href=\"http://www.hobbylobby.com/Scrapbook-Paper-Crafts/Paper-Cardstock/Vellum/12%22-x-12%22---10-Sheets-Clear-Vellum-Paper-Pack/p/111222\">vellum</a>.  The diffused light illuminates the IBM logo crisply and clearly.</p>\n<p>I also used <mark>rubber bands</mark> to hold the cube together while the glue dried.  I have used clamps before, but they tend to be a bit heavy handed in how they hold a project together.  Rubber bands provide stability for the glue to dry, without applying too much pressure.  You can also position them more easily than clamps.</p>\n<h3 id=\"nextsteps\">Next Steps</h3>\n<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/hw76HBeK5c8\" frameborder=\"0\" allowfullscreen></iframe>\n<p>When I designed the IBM Cube project, I wanted it to be more than just a box with IBM carved into it.  The center support lifts the top part of the cube 3.5mm off the base of the cube.  The result is almost a <mark>hovering effect</mark>, which is exactly what I was trying to achieve.</p>\n<p>The two main <mark>changes</mark> I would make for future iterations, would be to (a) <mark>3D print</mark> an enclosure for the <mark>USB Micro-B breakout</mark> and (b) use an already assembled <mark>strip of WS2812</mark> RGB LEDs.  I would also look into <mark>doubling</mark> the width and depth of the <mark>internal support</mark> to allow for more/thicker wires (from 3.5mm to 7.0mm would be plenty).</p>\n<p>With the hardware in place, the next step is to design and build a user interface.  Since the Photon can be easily controlled over REST, or even MQTT, there is no shortage of options.  We will take a look at the code in a future post.</p>\n<!--kg-card-end: markdown-->","comment_id":"41","plaintext":"IBM has a distinctly \"executive\" feel to it. Long gone are the mandatory ties\nand corporate theme song (yes, seriously), but that culture is still very much\nalive and well. While I work remote most of the time, I wanted something with\nthat executive feel on my desk. The IBM Cube project was born.\n\n\n\nAs I looked around the web for IBM logos, I ran across this marker for the \nChicago, IL office. It seemed like the perfect fit, so I pulled the initial\ndimensions (obviously at a much smaller scale) from that design. The white that\nlights up at night could be cut out to reveal the RGB LED lights, and of course\nI would put a wireless micro-controller inside to control the lighting remotely.\n\nFabrication\nDesktop fabrication has come a long way in recent years. Where before, a laser\ncutter would cost you $16,000 at an entry level, today you can upload a vector\nfile to a service like Ponoko [https://www.ponoko.com/], and have the results a\nfew days later. Cannot afford a 3D printer? Upload to Shapeways\n[http://www.shapeways.com/], and pick from numerous materials (including metal).\n\nComing from Adobe, I am pretty fluent in Illustrator, especially as it pertains\nto fabrication, so that was my tool of choice. I chose a high quality walnut\nveneer product at 3.5mm thick for that executive look. I also baked in a little\n\"P\" for my daughter, Paige, as a material test.\n\n\n\nI have learned over the years of working with a laser cutter to etch guides on\nthe faces that do not show, which is why the IBM logo is backwards in the\ndesign. You can see the outline of the WS2812 LEDs\n[https://www.sparkfun.com/products/13282], as well as the Photon\n[https://www.particle.io/prototype] wireless controller from Particle.\n\nTrying something new this time, I etched the entire area where the the USB\nMicro-B [https://www.sparkfun.com/products/12035] breakout board would go. The\nhope here was to shave a little thickness off the wood to provide more wiggle\nroom for the breakout board. The idea did not work.\n\n\n\nI was worried that the cutouts for the IBM logo would be too tight for the laser\nkerf (material lost in the cut of the laser itself), but the results came out\ngreat. In the picture above, Ponoko uses a sheet of masking tape to keep the hot\nflame from scorching the surrounding material. The sheet of material measures in\nat 12\" x 24\", with the IBM Cube being 100mm cubed.\n\n\n\nAfter pulling off the masking tape comes the part that makes me the most nervous\nwith every project - test assembly. You are just never quite sure if you missed\na cut or measurement in translating from the 3D concept from your head to the 2D\nvector design. This time around, at first glance, it looks like I got everything\nright.\n\nAssembly\nAfter researching SparkFun breakout board thickness, and reviewing the data\nsheet for the USB Micro-B adapter used in the breakout board, I calculated that\nthe total height of the assembled breakout board would be about 4.5mm. Since I\nam working with a 3.5mm wood sheet product, that leaves me with a 1mm problem.\n\nAs mentioned earlier, my hope was that a heavy etching of the breakout board\narea, on the actual wood, would give me that extra 1mm. It did not, and I had to\nuse a rotary tool to trim things up. For a future iteration, or future designs\nthat use the USB Micro-B breakout board, I think I will design a 3D printed\nenclosure for it to fit cleanly in the project.\n\n\n\nAfter gluing together the base of the project using Elmer's Glue-All, running\nwires from the USB Micro-B up the center support, and glueing the support\ntogether, it was time to get to the meat of the project - the micro-controller\nand WS2812B RGB LEDs.\n\nIf you have ever worked with RGB LEDs before, you know that the wiring, and\nindividual addressing can become a nightmare very quickly. The WS2812 series of\nRGB LED have a micro-controller baked into them. The LED controller needs in/out\nfor 5V, GND, and a signal wire. That signal wire gets hooked to your project\nmicro-controller. The result is addressable RGB LEDs that can be easily strung\ntogether in a series.\n\n> The downside of WS2812 RGB LEDs is that they cost more than an RGB LED of the\ncommon cathode variety, and that they draw a lot more current - up to 60mA per\nLED.\n\n\nFor projects like this, I really enjoy using the wire wrapping\n[http://www.digikey.com/product-detail/en/WSU-30M/K105-ND/5986] technique. This\nallows you to assemble the project and test it out without soldering anything.\nIf you make a mistake, you can simply unwrap that terminal. Once everything is\nin place and verified to be working, I go back and solder over the wires to make\nsure they stay where I want them.\n\n\n\nWhen I originally assembled the cube/top part of the project, without glue, but\nwith the hardware working, I noticed that the light was not diffused enough. A\ncommon approach I have used for this before is a thin sheet of vellum\n[http://www.hobbylobby.com/Scrapbook-Paper-Crafts/Paper-Cardstock/Vellum/12%22-x-12%22---10-Sheets-Clear-Vellum-Paper-Pack/p/111222]\n. The diffused light illuminates the IBM logo crisply and clearly.\n\nI also used rubber bands to hold the cube together while the glue dried. I have\nused clamps before, but they tend to be a bit heavy handed in how they hold a\nproject together. Rubber bands provide stability for the glue to dry, without\napplying too much pressure. You can also position them more easily than clamps.\n\nNext Steps\nWhen I designed the IBM Cube project, I wanted it to be more than just a box\nwith IBM carved into it. The center support lifts the top part of the cube 3.5mm\noff the base of the cube. The result is almost a hovering effect, which is\nexactly what I was trying to achieve.\n\nThe two main changes I would make for future iterations, would be to (a) 3D\nprint an enclosure for the USB Micro-B breakout and (b) use an already assembled \nstrip of WS2812 RGB LEDs. I would also look into doubling the width and depth of\nthe internal support to allow for more/thicker wires (from 3.5mm to 7.0mm would\nbe plenty).\n\nWith the hardware in place, the next step is to design and build a user\ninterface. Since the Photon can be easily controlled over REST, or even MQTT,\nthere is no shortage of options. We will take a look at the code in a future\npost.","feature_image":"__GHOST_URL__/content/images/2019/07/cube-completed-day.jpg","featured":0,"status":"published","locale":null,"visibility":"public","author_id":"1","created_at":"2015-08-13T16:27:39.000Z","updated_at":"2019-07-03T20:54:00.000Z","published_at":"2016-02-15T18:09:53.000Z","custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"type":"post","email_recipient_filter":"none"},{"id":"5adb8922351ffe0018a57734","uuid":"6d7cbf4c-9ad7-459f-80b0-53a7aafb9965","title":"IoT Weather on Android","slug":"iot-weather-on-android-2","mobiledoc":"{\"version\":\"0.3.1\",\"markups\":[],\"atoms\":[],\"cards\":[[\"markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\"In a [previous post](http://blog.kevinhoyt.com/2015/08/20/iot-with-cloudant-follow/) I talked about moving from request-response to using the publish-subscribe pattern.  This pattern allows our application parts to listen for changes in a decoupled fashion.  This works great with the continuous feed feature of [IBM Cloudant](http://cloudant.com).  In this iteration I will bring the data to Android, and cover a couple great utilities.\\n\\n###Volley\\n\\nOn the surface, there is not much new or exciting in this Android iteration of the weather data we have been working with over the last several posts.  The application starts and loads data via a ==REST== exchange with the same server infrastructure we have been using.  \\n\\nIn making that request for the initial data however, I am using a library from Google called \\\"==Volley==\\\".  [Volley](https://android.googlesource.com/platform/frameworks/volley/) is a high performance library for managing network communications.  It even caches the data on the client for the all too common \\\"==destroy==\\\" event that comes with device rotation.\\n\\n```\\nJsonObjectRequest   request;\\n\\nrequest = new JsonObjectRequest(\\n  Request.Method.GET,\\n  getString(R.string.cloud_code),\\n  null,\\n  new Response.Listener<JSONObject>() {\\n    @Override\\n    public void onResponse(JSONObject response) {\\n      // Process server response\\n    }\\n  },\\n  new Response.ErrorListener() {\\n    @Override\\n    public void onErrorResponse(VolleyError error) {\\n      Log.d(\\\"VOLLEY\\\", error.getMessage());\\n    }\\n  }\\n);\\n\\nVolley.newRequestQueue(this).add(request);\\n```\\n\\nNotice that last line.  In this case, I am using a ==static method== to initiate the request.  You can also create an instance variable, and then ==queue== up multiple requests.  You can then in turn cancel requests based on responses you might get, etc.  The callbacks also seem to land on the UI thread, so it is easy to update the user interface.\\n\\nI have commented out the details of \\\"==onResponse==\\\" for brevity.  What happens in there is that the JSON data is ==marshaled into a domain object== called \\\"Weather\\\".  A populated instance of the \\\"Weather\\\" class is then sent to an \\\"update()\\\" method to ==update the user interface==.\\n\\nYou might be wondering why I do not just update the user interface right from the ==response handler== - especially seeing as it has easy access to the UI-thread?  I extracted that code into a separate method because it will get ==reused== as new data arrives.\\n\\n###PubNub\\n\\nIf you remember, IBM Cloudant has a feature called \\\"==continuous feed==\\\" which allows us to be notified when changes happen at the database.  We catch these changes in our Node.js infrastructure, and then use ==PubNub== to push a message to interested clients.\\n\\nIn this case, our Android device is also an interested client.  We use the ==PubNub Android library== in our application to handle new messages - messages that represent that new weather data is available.\\n\\n```\\npubnub = new Pubnub(\\n  getString(R.string.pubnub_publish),\\n  getString(R.string.pubnub_subscribe)\\n);\\n\\ntry {\\n  pubnub.subscribe(\\n  getString(R.string.pubnub_channel),\\n  new Callback() {\\n    @Override\\n    public void successCallback(String channel, Object data) {\\n      // Process incoming message\\n    }\\n  }\\n);\\n```\\n\\nWhat is interesting here is that the Android client knows nothing about the Node.js server or IBM Cloudant database.  Just like the browser-based client in our previous example, the parts and pieces are ==decoupled== thanks to the use of the ==publish-subscribe== pattern.\\n\\n###Weak Reference\\n\\nThe PubNub communication however happens on a ==separate thread== from the UI-thread.  That means that it cannot populate the user interface directly.  The handler has to get the data over to the UI-thread.  This is traditionally done using the Android \\\"==Handler==\\\" [class](http://developer.android.com/reference/android/os/Handler.html).\\n\\n```\\n// Get key data\\nbundle = new Bundle();\\nbundle.putDouble(KEY_CELCIUS, document.getDouble(KEY_CELCIUS));\\nbundle.putDouble(KEY_FAHRENHEIT, document.getDouble(KEY_FAHRENHEIT));\\nbundle.putDouble(KEY_HUMIDITY, document.getDouble(KEY_HUMIDITY));\\nbundle.putLong(KEY_TIMESTAMP, document.getLong(KEY_TIMESTAMP));\\n\\n// Assemble message\\n// Cross-thead communication\\nmessage = new Message();\\nmessage.setData(bundle);\\n\\n// Send message\\nhandler.sendMessage(message);\\n```\\n\\nMuch like publish-subscribe on client and server, ==handlers in Android allow decoupled thread to communicate== with one another, without knowing about one another.  In the PubNub success callback, I place the pertinent data on an instance of the \\\"==Bundle==\\\" [class](http://developer.android.com/reference/android/os/Bundle.html).  That bundle is put into an instance of the \\\"==Message==\\\" [class](http://developer.android.com/reference/android/os/Message.html).  The message is then sent via the handler to the UI-thread.\\n\\nThe problem with handlers is that they are [known](http://www.androiddesignpatterns.com/2013/01/inner-class-handler-memory-leak.html) to ==leak memory== if not use properly - and using them properly requires a whole bunch of sophisticated overhead code.  The overhead must be written correctly as well in order to prevent leaking memory.  This is a lot of work for a simple message that wants to update the user interface.\\n\\n> Android Studio specifically will warn you extensively that you have potentially entered a memory leak zone when you start using the Android \\\"Handler\\\" class.\\n\\nTo make this magic much easier, I use a library from [Badoo](https://techblog.badoo.com/blog/2014/08/28/android-handler-memory-leaks) called \\\"==WeakReference==\\\".  This class encapsulates all the handler nuances for us.  We still use it as a typical handler, and it provides the same interface.\\n\\n```\\nhandler = new WeakHandler(new Handler.Callback() {\\n  @Override\\n  public boolean handleMessage(Message message) {\\n    Bundle  bundle;\\n    Weather weather;\\n\\n    // Get pertinent data\\n    bundle = message.getData();\\n\\n    // Marshall to domain\\n    weather = new Weather();\\n    weather.celcius = bundle.getDouble(KEY_CELCIUS);\\n\\n    ... More marshaling ...\\n\\n    // Update user interface\\n    update(weather);\\n\\n    return false;\\n  }\\n} );\\n```\\n\\nSo in this case, the message ==data arrives== into the Android client via PubNub subscribe handler, the pertinent data is ==marshaled into a bundle==, and then ==sent as a message to the handler==.  The handler ==marshals the data== from a bundle to an instance of the \\\"Weather\\\" class.  The handler is also on the UI-thread, so it has permission to call the \\\"==update()==\\\" method with the pertinent data.\\n\\nLong story short then, the \\\"==update()==\\\" method is extracted for reuse, and that reuse happens from a ==REST response== on the ==UI-thread==, but also a ==message push== from PubNub on a ==separate thread==.  Since the REST handler gets a JSONObject, and the subscribe handler (on the UI-thread side), gets a Bundle instance, data must be ==marshaled to a common domain class==.\\n\\n###Next Steps\\n\\nIncluding the ~30 or so lines used for import statements, this entire Android application comes in at ==under 250 lines== of code thanks to the use of some very powerful ==helper libraries==.  ==Volley== makes ==REST== communication a snap, and ==WeakReference== helps keep your ==threads== talking without leaking memory.\\n\\nYou now have several examples of using IoT data from a ==Particle Photon== on ==IBM Bluemix== to include ==Node.js==, ==IBM Cloudant==, a ==browser== client, and an ==Android== client.  There is just one problem - all of these are ==one-way communication==.  From here what we really want is to be able to communicate back to the sensor as well; perhaps to take some physical action at the installation.\"}]],\"sections\":[[10,0]],\"ghostVersion\":\"3.0\"}","html":"<!--kg-card-begin: markdown--><p>In a <a href=\"http://blog.kevinhoyt.com/2015/08/20/iot-with-cloudant-follow/\">previous post</a> I talked about moving from request-response to using the publish-subscribe pattern.  This pattern allows our application parts to listen for changes in a decoupled fashion.  This works great with the continuous feed feature of <a href=\"http://cloudant.com\">IBM Cloudant</a>.  In this iteration I will bring the data to Android, and cover a couple great utilities.</p>\n<h3 id=\"volley\">Volley</h3>\n<p>On the surface, there is not much new or exciting in this Android iteration of the weather data we have been working with over the last several posts.  The application starts and loads data via a <mark>REST</mark> exchange with the same server infrastructure we have been using.</p>\n<p>In making that request for the initial data however, I am using a library from Google called &quot;<mark>Volley</mark>&quot;.  <a href=\"https://android.googlesource.com/platform/frameworks/volley/\">Volley</a> is a high performance library for managing network communications.  It even caches the data on the client for the all too common &quot;<mark>destroy</mark>&quot; event that comes with device rotation.</p>\n<pre><code>JsonObjectRequest   request;\n\nrequest = new JsonObjectRequest(\n  Request.Method.GET,\n  getString(R.string.cloud_code),\n  null,\n  new Response.Listener&lt;JSONObject&gt;() {\n    @Override\n    public void onResponse(JSONObject response) {\n      // Process server response\n    }\n  },\n  new Response.ErrorListener() {\n    @Override\n    public void onErrorResponse(VolleyError error) {\n      Log.d(&quot;VOLLEY&quot;, error.getMessage());\n    }\n  }\n);\n\nVolley.newRequestQueue(this).add(request);\n</code></pre>\n<p>Notice that last line.  In this case, I am using a <mark>static method</mark> to initiate the request.  You can also create an instance variable, and then <mark>queue</mark> up multiple requests.  You can then in turn cancel requests based on responses you might get, etc.  The callbacks also seem to land on the UI thread, so it is easy to update the user interface.</p>\n<p>I have commented out the details of &quot;<mark>onResponse</mark>&quot; for brevity.  What happens in there is that the JSON data is <mark>marshaled into a domain object</mark> called &quot;Weather&quot;.  A populated instance of the &quot;Weather&quot; class is then sent to an &quot;update()&quot; method to <mark>update the user interface</mark>.</p>\n<p>You might be wondering why I do not just update the user interface right from the <mark>response handler</mark> - especially seeing as it has easy access to the UI-thread?  I extracted that code into a separate method because it will get <mark>reused</mark> as new data arrives.</p>\n<h3 id=\"pubnub\">PubNub</h3>\n<p>If you remember, IBM Cloudant has a feature called &quot;<mark>continuous feed</mark>&quot; which allows us to be notified when changes happen at the database.  We catch these changes in our Node.js infrastructure, and then use <mark>PubNub</mark> to push a message to interested clients.</p>\n<p>In this case, our Android device is also an interested client.  We use the <mark>PubNub Android library</mark> in our application to handle new messages - messages that represent that new weather data is available.</p>\n<pre><code>pubnub = new Pubnub(\n  getString(R.string.pubnub_publish),\n  getString(R.string.pubnub_subscribe)\n);\n\ntry {\n  pubnub.subscribe(\n  getString(R.string.pubnub_channel),\n  new Callback() {\n    @Override\n    public void successCallback(String channel, Object data) {\n      // Process incoming message\n    }\n  }\n);\n</code></pre>\n<p>What is interesting here is that the Android client knows nothing about the Node.js server or IBM Cloudant database.  Just like the browser-based client in our previous example, the parts and pieces are <mark>decoupled</mark> thanks to the use of the <mark>publish-subscribe</mark> pattern.</p>\n<h3 id=\"weakreference\">Weak Reference</h3>\n<p>The PubNub communication however happens on a <mark>separate thread</mark> from the UI-thread.  That means that it cannot populate the user interface directly.  The handler has to get the data over to the UI-thread.  This is traditionally done using the Android &quot;<mark>Handler</mark>&quot; <a href=\"http://developer.android.com/reference/android/os/Handler.html\">class</a>.</p>\n<pre><code>// Get key data\nbundle = new Bundle();\nbundle.putDouble(KEY_CELCIUS, document.getDouble(KEY_CELCIUS));\nbundle.putDouble(KEY_FAHRENHEIT, document.getDouble(KEY_FAHRENHEIT));\nbundle.putDouble(KEY_HUMIDITY, document.getDouble(KEY_HUMIDITY));\nbundle.putLong(KEY_TIMESTAMP, document.getLong(KEY_TIMESTAMP));\n\n// Assemble message\n// Cross-thead communication\nmessage = new Message();\nmessage.setData(bundle);\n\n// Send message\nhandler.sendMessage(message);\n</code></pre>\n<p>Much like publish-subscribe on client and server, <mark>handlers in Android allow decoupled thread to communicate</mark> with one another, without knowing about one another.  In the PubNub success callback, I place the pertinent data on an instance of the &quot;<mark>Bundle</mark>&quot; <a href=\"http://developer.android.com/reference/android/os/Bundle.html\">class</a>.  That bundle is put into an instance of the &quot;<mark>Message</mark>&quot; <a href=\"http://developer.android.com/reference/android/os/Message.html\">class</a>.  The message is then sent via the handler to the UI-thread.</p>\n<p>The problem with handlers is that they are <a href=\"http://www.androiddesignpatterns.com/2013/01/inner-class-handler-memory-leak.html\">known</a> to <mark>leak memory</mark> if not use properly - and using them properly requires a whole bunch of sophisticated overhead code.  The overhead must be written correctly as well in order to prevent leaking memory.  This is a lot of work for a simple message that wants to update the user interface.</p>\n<blockquote>\n<p>Android Studio specifically will warn you extensively that you have potentially entered a memory leak zone when you start using the Android &quot;Handler&quot; class.</p>\n</blockquote>\n<p>To make this magic much easier, I use a library from <a href=\"https://techblog.badoo.com/blog/2014/08/28/android-handler-memory-leaks\">Badoo</a> called &quot;<mark>WeakReference</mark>&quot;.  This class encapsulates all the handler nuances for us.  We still use it as a typical handler, and it provides the same interface.</p>\n<pre><code>handler = new WeakHandler(new Handler.Callback() {\n  @Override\n  public boolean handleMessage(Message message) {\n    Bundle  bundle;\n    Weather weather;\n\n    // Get pertinent data\n    bundle = message.getData();\n\n    // Marshall to domain\n    weather = new Weather();\n    weather.celcius = bundle.getDouble(KEY_CELCIUS);\n\n    ... More marshaling ...\n\n    // Update user interface\n    update(weather);\n\n    return false;\n  }\n} );\n</code></pre>\n<p>So in this case, the message <mark>data arrives</mark> into the Android client via PubNub subscribe handler, the pertinent data is <mark>marshaled into a bundle</mark>, and then <mark>sent as a message to the handler</mark>.  The handler <mark>marshals the data</mark> from a bundle to an instance of the &quot;Weather&quot; class.  The handler is also on the UI-thread, so it has permission to call the &quot;<mark>update()</mark>&quot; method with the pertinent data.</p>\n<p>Long story short then, the &quot;<mark>update()</mark>&quot; method is extracted for reuse, and that reuse happens from a <mark>REST response</mark> on the <mark>UI-thread</mark>, but also a <mark>message push</mark> from PubNub on a <mark>separate thread</mark>.  Since the REST handler gets a JSONObject, and the subscribe handler (on the UI-thread side), gets a Bundle instance, data must be <mark>marshaled to a common domain class</mark>.</p>\n<h3 id=\"nextsteps\">Next Steps</h3>\n<p>Including the ~30 or so lines used for import statements, this entire Android application comes in at <mark>under 250 lines</mark> of code thanks to the use of some very powerful <mark>helper libraries</mark>.  <mark>Volley</mark> makes <mark>REST</mark> communication a snap, and <mark>WeakReference</mark> helps keep your <mark>threads</mark> talking without leaking memory.</p>\n<p>You now have several examples of using IoT data from a <mark>Particle Photon</mark> on <mark>IBM Bluemix</mark> to include <mark>Node.js</mark>, <mark>IBM Cloudant</mark>, a <mark>browser</mark> client, and an <mark>Android</mark> client.  There is just one problem - all of these are <mark>one-way communication</mark>.  From here what we really want is to be able to communicate back to the sensor as well; perhaps to take some physical action at the installation.</p>\n<!--kg-card-end: markdown-->","comment_id":"42","plaintext":"In a previous post\n[http://blog.kevinhoyt.com/2015/08/20/iot-with-cloudant-follow/] I talked about\nmoving from request-response to using the publish-subscribe pattern. This\npattern allows our application parts to listen for changes in a decoupled\nfashion. This works great with the continuous feed feature of IBM Cloudant\n[http://cloudant.com]. In this iteration I will bring the data to Android, and\ncover a couple great utilities.\n\nVolley\nOn the surface, there is not much new or exciting in this Android iteration of\nthe weather data we have been working with over the last several posts. The\napplication starts and loads data via a REST exchange with the same server\ninfrastructure we have been using.\n\nIn making that request for the initial data however, I am using a library from\nGoogle called \"Volley\". Volley\n[https://android.googlesource.com/platform/frameworks/volley/] is a high\nperformance library for managing network communications. It even caches the data\non the client for the all too common \"destroy\" event that comes with device\nrotation.\n\nJsonObjectRequest   request;\n\nrequest = new JsonObjectRequest(\n  Request.Method.GET,\n  getString(R.string.cloud_code),\n  null,\n  new Response.Listener<JSONObject>() {\n    @Override\n    public void onResponse(JSONObject response) {\n      // Process server response\n    }\n  },\n  new Response.ErrorListener() {\n    @Override\n    public void onErrorResponse(VolleyError error) {\n      Log.d(\"VOLLEY\", error.getMessage());\n    }\n  }\n);\n\nVolley.newRequestQueue(this).add(request);\n\n\nNotice that last line. In this case, I am using a static method to initiate the\nrequest. You can also create an instance variable, and then queue up multiple\nrequests. You can then in turn cancel requests based on responses you might get,\netc. The callbacks also seem to land on the UI thread, so it is easy to update\nthe user interface.\n\nI have commented out the details of \"onResponse\" for brevity. What happens in\nthere is that the JSON data is marshaled into a domain object called \"Weather\".\nA populated instance of the \"Weather\" class is then sent to an \"update()\" method\nto update the user interface.\n\nYou might be wondering why I do not just update the user interface right from\nthe response handler - especially seeing as it has easy access to the UI-thread?\nI extracted that code into a separate method because it will get reused as new\ndata arrives.\n\nPubNub\nIf you remember, IBM Cloudant has a feature called \"continuous feed\" which\nallows us to be notified when changes happen at the database. We catch these\nchanges in our Node.js infrastructure, and then use PubNub to push a message to\ninterested clients.\n\nIn this case, our Android device is also an interested client. We use the PubNub\nAndroid library in our application to handle new messages - messages that\nrepresent that new weather data is available.\n\npubnub = new Pubnub(\n  getString(R.string.pubnub_publish),\n  getString(R.string.pubnub_subscribe)\n);\n\ntry {\n  pubnub.subscribe(\n  getString(R.string.pubnub_channel),\n  new Callback() {\n    @Override\n    public void successCallback(String channel, Object data) {\n      // Process incoming message\n    }\n  }\n);\n\n\nWhat is interesting here is that the Android client knows nothing about the\nNode.js server or IBM Cloudant database. Just like the browser-based client in\nour previous example, the parts and pieces are decoupled thanks to the use of\nthe publish-subscribe pattern.\n\nWeak Reference\nThe PubNub communication however happens on a separate thread from the\nUI-thread. That means that it cannot populate the user interface directly. The\nhandler has to get the data over to the UI-thread. This is traditionally done\nusing the Android \"Handler\" class\n[http://developer.android.com/reference/android/os/Handler.html].\n\n// Get key data\nbundle = new Bundle();\nbundle.putDouble(KEY_CELCIUS, document.getDouble(KEY_CELCIUS));\nbundle.putDouble(KEY_FAHRENHEIT, document.getDouble(KEY_FAHRENHEIT));\nbundle.putDouble(KEY_HUMIDITY, document.getDouble(KEY_HUMIDITY));\nbundle.putLong(KEY_TIMESTAMP, document.getLong(KEY_TIMESTAMP));\n\n// Assemble message\n// Cross-thead communication\nmessage = new Message();\nmessage.setData(bundle);\n\n// Send message\nhandler.sendMessage(message);\n\n\nMuch like publish-subscribe on client and server, handlers in Android allow\ndecoupled thread to communicate with one another, without knowing about one\nanother. In the PubNub success callback, I place the pertinent data on an\ninstance of the \"Bundle\" class\n[http://developer.android.com/reference/android/os/Bundle.html]. That bundle is\nput into an instance of the \"Message\" class\n[http://developer.android.com/reference/android/os/Message.html]. The message is\nthen sent via the handler to the UI-thread.\n\nThe problem with handlers is that they are known\n[http://www.androiddesignpatterns.com/2013/01/inner-class-handler-memory-leak.html] \nto leak memory if not use properly - and using them properly requires a whole\nbunch of sophisticated overhead code. The overhead must be written correctly as\nwell in order to prevent leaking memory. This is a lot of work for a simple\nmessage that wants to update the user interface.\n\n> Android Studio specifically will warn you extensively that you have potentially\nentered a memory leak zone when you start using the Android \"Handler\" class.\n\n\nTo make this magic much easier, I use a library from Badoo\n[https://techblog.badoo.com/blog/2014/08/28/android-handler-memory-leaks] called\n\"WeakReference\". This class encapsulates all the handler nuances for us. We\nstill use it as a typical handler, and it provides the same interface.\n\nhandler = new WeakHandler(new Handler.Callback() {\n  @Override\n  public boolean handleMessage(Message message) {\n    Bundle  bundle;\n    Weather weather;\n\n    // Get pertinent data\n    bundle = message.getData();\n\n    // Marshall to domain\n    weather = new Weather();\n    weather.celcius = bundle.getDouble(KEY_CELCIUS);\n\n    ... More marshaling ...\n\n    // Update user interface\n    update(weather);\n\n    return false;\n  }\n} );\n\n\nSo in this case, the message data arrives into the Android client via PubNub\nsubscribe handler, the pertinent data is marshaled into a bundle, and then sent\nas a message to the handler. The handler marshals the data from a bundle to an\ninstance of the \"Weather\" class. The handler is also on the UI-thread, so it has\npermission to call the \"update()\" method with the pertinent data.\n\nLong story short then, the \"update()\" method is extracted for reuse, and that\nreuse happens from a REST response on the UI-thread, but also a message push \nfrom PubNub on a separate thread. Since the REST handler gets a JSONObject, and\nthe subscribe handler (on the UI-thread side), gets a Bundle instance, data must\nbe marshaled to a common domain class.\n\nNext Steps\nIncluding the ~30 or so lines used for import statements, this entire Android\napplication comes in at under 250 lines of code thanks to the use of some very\npowerful helper libraries. Volley makes REST communication a snap, and \nWeakReference helps keep your threads talking without leaking memory.\n\nYou now have several examples of using IoT data from a Particle Photon on IBM\nBluemix to include Node.js, IBM Cloudant, a browser client, and an Android \nclient. There is just one problem - all of these are one-way communication. From\nhere what we really want is to be able to communicate back to the sensor as\nwell; perhaps to take some physical action at the installation.","feature_image":"http://images.kevinhoyt.com/bluemix-web-weather.jpg","featured":0,"status":"published","locale":null,"visibility":"public","author_id":"1","created_at":"2015-08-18T15:34:10.000Z","updated_at":"2015-08-27T16:43:18.000Z","published_at":"2015-08-27T16:43:18.000Z","custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"type":"post","email_recipient_filter":"none"},{"id":"5adb8922351ffe0018a57735","uuid":"9ad90c44-2025-478a-91c8-d7b16a68b462","title":"IBM MobileFirst 7.1 with Android","slug":"ibm-mobilefirst-7-1-with-android","mobiledoc":"{\"version\":\"0.3.1\",\"markups\":[],\"atoms\":[],\"cards\":[[\"markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\"Branching off Ray Camden's [excellent post](http://www.raymondcamden.com/2015/08/17/getting-started-with-mobile-development-and-ibm-mobilefirst-7-1) on getting started with [IBM MobileFirst 7.1](http://www.raymondcamden.com/2015/08/17/getting-started-with-mobile-development-and-ibm-mobilefirst-7-1), this post will focus on the continuing workflow from an Android perspective.  It is assumed that you have followed Ray's tutorial on installing the IBM MobileFirst 7.1 CLI (if you have an earlier version, you should upgrade), and have [Android Studio](http://developer.android.com/tools/studio/index.html) at the ready.\\n\\n###The Projects\\n\\nWhen working with ==IBM MobileFirst==, there is almost always two projects - ==the client and the server==.  The client in this case is an ==Android== application.  The server is functionally a ==Java EE 7== application, and with MobileFirst Foundation, it is deployed on [IBM WebSphere Liberty](https://developer.ibm.com/wasdev/websphere-liberty/).  Starting on the server side then, let us create a project.\\n\\n```\\nmfp create HelloProject\\n```\\n\\nAt the ==command prompt==, you will want to move to directory that you want to work with for this walkthrough.  The above command will ==create a directory== called \\\"HelloProject\\\", which will hold a baseline MobileFirst directory setup.  \\n\\n> I generally run the above command from a directory named for the application I am building.  That directory then has a folder for the client and the server parts.\\n\\nNow change into that newly created MobileFirst project directory.  Here we will create an \\\"==adapter==\\\" or effectively expose a ==REST API== for application data and resources.  You can actually create adapters written in ==JavaScript== (not Node.js), but as developers working with Android, you might find it more robust to simply use ==Java== throughout.\\n\\n```\\nmfp add adapter HelloAdapter —type java —package com.ibm.us.krhoyt\\nmfp start\\nmfp push\\n```\\n\\nThese ==three commands== will create a ==Java adapter== for you in the \\\"adapter\\\" folder, ==start== the IBM MobileFirst server if it is not already started, and then ==deploy== (push) the project, including the newly created adapter, onto the server.  We will come back to that adapter code in a moment, but if you want to ==test it out==, it is populated with some boilerplate code.\\n\\n```\\nmfp adapter call HelloAdapter/users/Kevin\\n```\\n\\n###Android Studio\\n\\nStart up ==Android Studio== and create a new project with a ==blank activity==.  In order to tell Android Studio about the IBM MobileFirst ==dependencies==, we will need to make a few changes.  With your project created, go to **Project -> Gradle -> build.gradle (Module: app)** and add the following snippets.\\n\\n```\\n// Below \\\"apply plugin: ‘com.android.application’\\\"\\nrepositories {\\n  jcenter()\\n}\\n\\n// Inside “android\\\"\\npackagingOptions {\\n  pickFirst 'META-INF/ASL2.0'\\n  pickFirst 'META-INF/LICENSE'\\n  pickFirst 'META-INF/NOTICE'\\n}\\n\\n// Inside “dependencies\\\"\\ncompile group: 'com.ibm.mobile.foundation',\\n  name: 'ibmmobilefirstplatformfoundation',\\n  version: '7.1.0.0',\\n  ext: 'aar',\\n  transitive: true\\n```\\n\\nWith these changes, Gradle will want to ==rebuild the project==.  Let Gradle complete that task before moving on to the next step.  When Gradle is done, head to your \\\"==ApplicationManifest.xml==\\\" file and make the following changes.\\n\\n```\\n// After the opening manifest tag\\n<uses-permission android:name=\\\"android.permission.INTERNET\\\"/>\\n<uses-permission android:name=\\\"android.permission.ACCESS_WIFI_STATE\\\"/>\\n<uses-permission android:name=\\\"android.permission.REAL_GET_TASKS\\\" />\\n\\n// After the closing activity tag\\n<activity android:name=\\\"com.worklight.wlclient.ui.UIActivity\\\" />\\n```\\n\\nThe ==permissions== give your Android application the ability to call IBM MobileFirst services.  The additional activity is a ==background service== used by some of the MobileFirst features.  Head back to the ==command line==, make sure you are in the Android Studio project directory, and run the following command to ==tell MobileFirst about your Android application==.\\n\\n```\\nmfp push\\n```\\n\\n>Notice how the \\\"mfp\\\" command performs different actions depending on what directory structure you are in.  If you are in a server, it deploys your code.  In an Android project, it tells the server about your application.\\n\\nNow you are ready to leverage the IBM MobileFirst server from your Android application.  Add the following code to your main activity source file, inside the \\\"onCreate\\\" method.\\n\\n```\\nfinal WLClient client = WLClient.createInstance(this);\\n\\nclient.connect(new WLResponseListener() {\\n  @Override\\n  public void onSuccess(WLResponse response) {\\n    URI                path;                    \\n    WLResourceRequest  request;\\n\\n    try {\\n      path = new URI(\\\"/adapters/HelloAdapter/users/Kevin\\\");\\n      request = new WLResourceRequest(path, WLResourceRequest.GET);\\n      request.send(new WLResponseListener() {\\n        @Override\\n        public void onSuccess(WLResponse response) {\\n          Log.d(“MOBILEFIRST”, “Success: “ + response.getResponseText());\\n        }\\n\\n        @Override\\n        public void onFailure(WLFailResponse response) {\\n          Log.d(“MOBILEFIRST”, “Fail: “ + response.getErrorMsg());\\n        }\\n      });\\n    } catch(URISyntaxException urise) {\\n      Log.d(“MOBILEFIRST”, “URI: “ + urise.getMessage());\\n    }\\n  }\\n});\\n```\\n\\nThe first thing that happens here is a ==client== used to call IBM MobileFirst services.  The URI is the \\\"==adapters==\\\" functionality of MobileFirst, using the \\\"==HelloAdapter==\\\" we created in the previous step.  We are calling the \\\"==users==\\\" REST endpoint using an HTTP GET, and sending the parameter \\\"==Kevin==\\\".  Once executed, we log out the response.\\n\\n###Extra Credit\\n\\nWhat exactly is going on in that Java adapter?  This is one of my favorite parts.  ==Java adapters are JAX-RS==, which effectively means they are plain old Java objects, with some annotations to tell the server how to expose them.  JAX-RS is beyond the scope of this post, but here is a glimpse of a \\\"hello world\\\" adapter.\\n\\n```\\npackage com.ibm.us.krhoyt;\\n\\nimport javax.ws.rs.GET;\\nimport javax.ws.rs.Path;\\nimport javax.ws.rs.PathParam;\\n\\n@Path(\\\"/users\\\")\\npublic class HelloAdapterResource \\n{\\n  @GET\\n  @Path(\\\"/{name}\\\")\\n  public String hello(@PathParam(\\\"name\\\") String name) {\\t\\t\\n    return \\\"Hello \\\" + name.trim() + \\\"!\\\";\\n  }\\n}\\n```\\n\\nIt does not get much easier than that folks!  The \\\"==Path==\\\" annotations tell the server where to expose this REST service.  The \\\"==GET==\\\" annotation says to route to this method for an HTTP GET.  The \\\"==PathParam==\\\" annotation maps the URL parameter to a method instance for us to use.  JAX-RS goes much deep than this, and I would encourage you to explore the awesomeness if you have not already.  \\n\\n###Next Steps\\n\\nLike most \\\"hello world\\\" examples, this really does not do MobileFirst justice.  For example, if you try ==accessing the REST endpoint directly== in the browser (or via [Postman](https://www.getpostman.com), [Paw](https://luckymarmot.com/paw), or other client), you will get an message that ==access is denied==.  At this point, only the Android application is permitted to access the endpoint.\\n\\n>Put another way, you get application security right out of the box.\\n\\nAnother cool feature is ==reporting== for our application usage.  You can actually see ==how many devices== are accessing your APIs, and with a little work, monitor what APIs they use most ==frequently==, their ==performance==, and so on.  That all takes place in the ==IBM MobileFirst console==.  You can access it by running the following command from within a MobileFirst project folder (server or client).\\n\\n```\\nmfp console\\n```\\n\\nIf you are accessing an ==XML== resource from your adapter, MobileFirst will even let you easily specify an ==XSL transform== to run on the data before returning it to the client.  Even further, you can ==access a database directly== from your Android clients, and still keep the security and reporting features.\\n\\nThe list goes on and on.  If you are building mobile applications (and who is not these days), then IBM MobileFirst gives you a vast suite of tools.  \\n\\nIf you want to check out ==my sample source code== for a \\\"hello world\\\" (==slightly improved== from this boilerplate), you can find it in my [GitHub](http://github.com/krhoyt) repository.  Have questions?  Leave a comment below, or send me a message on [Twitter](http://twitter.com/krhoyt).\\n\\n*Header [photo](https://www.flickr.com/photos/-heinecke-/4406341989) is \\\"IBM Logo\\\" by Flickr user \\\"heinecke\\\"*\"}]],\"sections\":[[10,0]],\"ghostVersion\":\"3.0\"}","html":"<!--kg-card-begin: markdown--><p>Branching off Ray Camden's <a href=\"http://www.raymondcamden.com/2015/08/17/getting-started-with-mobile-development-and-ibm-mobilefirst-7-1\">excellent post</a> on getting started with <a href=\"http://www.raymondcamden.com/2015/08/17/getting-started-with-mobile-development-and-ibm-mobilefirst-7-1\">IBM MobileFirst 7.1</a>, this post will focus on the continuing workflow from an Android perspective.  It is assumed that you have followed Ray's tutorial on installing the IBM MobileFirst 7.1 CLI (if you have an earlier version, you should upgrade), and have <a href=\"http://developer.android.com/tools/studio/index.html\">Android Studio</a> at the ready.</p>\n<h3 id=\"theprojects\">The Projects</h3>\n<p>When working with <mark>IBM MobileFirst</mark>, there is almost always two projects - <mark>the client and the server</mark>.  The client in this case is an <mark>Android</mark> application.  The server is functionally a <mark>Java EE 7</mark> application, and with MobileFirst Foundation, it is deployed on <a href=\"https://developer.ibm.com/wasdev/websphere-liberty/\">IBM WebSphere Liberty</a>.  Starting on the server side then, let us create a project.</p>\n<pre><code>mfp create HelloProject\n</code></pre>\n<p>At the <mark>command prompt</mark>, you will want to move to directory that you want to work with for this walkthrough.  The above command will <mark>create a directory</mark> called &quot;HelloProject&quot;, which will hold a baseline MobileFirst directory setup.</p>\n<blockquote>\n<p>I generally run the above command from a directory named for the application I am building.  That directory then has a folder for the client and the server parts.</p>\n</blockquote>\n<p>Now change into that newly created MobileFirst project directory.  Here we will create an &quot;<mark>adapter</mark>&quot; or effectively expose a <mark>REST API</mark> for application data and resources.  You can actually create adapters written in <mark>JavaScript</mark> (not Node.js), but as developers working with Android, you might find it more robust to simply use <mark>Java</mark> throughout.</p>\n<pre><code>mfp add adapter HelloAdapter —type java —package com.ibm.us.krhoyt\nmfp start\nmfp push\n</code></pre>\n<p>These <mark>three commands</mark> will create a <mark>Java adapter</mark> for you in the &quot;adapter&quot; folder, <mark>start</mark> the IBM MobileFirst server if it is not already started, and then <mark>deploy</mark> (push) the project, including the newly created adapter, onto the server.  We will come back to that adapter code in a moment, but if you want to <mark>test it out</mark>, it is populated with some boilerplate code.</p>\n<pre><code>mfp adapter call HelloAdapter/users/Kevin\n</code></pre>\n<h3 id=\"androidstudio\">Android Studio</h3>\n<p>Start up <mark>Android Studio</mark> and create a new project with a <mark>blank activity</mark>.  In order to tell Android Studio about the IBM MobileFirst <mark>dependencies</mark>, we will need to make a few changes.  With your project created, go to <strong>Project -&gt; Gradle -&gt; build.gradle (Module: app)</strong> and add the following snippets.</p>\n<pre><code>// Below &quot;apply plugin: ‘com.android.application’&quot;\nrepositories {\n  jcenter()\n}\n\n// Inside “android&quot;\npackagingOptions {\n  pickFirst 'META-INF/ASL2.0'\n  pickFirst 'META-INF/LICENSE'\n  pickFirst 'META-INF/NOTICE'\n}\n\n// Inside “dependencies&quot;\ncompile group: 'com.ibm.mobile.foundation',\n  name: 'ibmmobilefirstplatformfoundation',\n  version: '7.1.0.0',\n  ext: 'aar',\n  transitive: true\n</code></pre>\n<p>With these changes, Gradle will want to <mark>rebuild the project</mark>.  Let Gradle complete that task before moving on to the next step.  When Gradle is done, head to your &quot;<mark>ApplicationManifest.xml</mark>&quot; file and make the following changes.</p>\n<pre><code>// After the opening manifest tag\n&lt;uses-permission android:name=&quot;android.permission.INTERNET&quot;/&gt;\n&lt;uses-permission android:name=&quot;android.permission.ACCESS_WIFI_STATE&quot;/&gt;\n&lt;uses-permission android:name=&quot;android.permission.REAL_GET_TASKS&quot; /&gt;\n\n// After the closing activity tag\n&lt;activity android:name=&quot;com.worklight.wlclient.ui.UIActivity&quot; /&gt;\n</code></pre>\n<p>The <mark>permissions</mark> give your Android application the ability to call IBM MobileFirst services.  The additional activity is a <mark>background service</mark> used by some of the MobileFirst features.  Head back to the <mark>command line</mark>, make sure you are in the Android Studio project directory, and run the following command to <mark>tell MobileFirst about your Android application</mark>.</p>\n<pre><code>mfp push\n</code></pre>\n<blockquote>\n<p>Notice how the &quot;mfp&quot; command performs different actions depending on what directory structure you are in.  If you are in a server, it deploys your code.  In an Android project, it tells the server about your application.</p>\n</blockquote>\n<p>Now you are ready to leverage the IBM MobileFirst server from your Android application.  Add the following code to your main activity source file, inside the &quot;onCreate&quot; method.</p>\n<pre><code>final WLClient client = WLClient.createInstance(this);\n\nclient.connect(new WLResponseListener() {\n  @Override\n  public void onSuccess(WLResponse response) {\n    URI                path;                    \n    WLResourceRequest  request;\n\n    try {\n      path = new URI(&quot;/adapters/HelloAdapter/users/Kevin&quot;);\n      request = new WLResourceRequest(path, WLResourceRequest.GET);\n      request.send(new WLResponseListener() {\n        @Override\n        public void onSuccess(WLResponse response) {\n          Log.d(“MOBILEFIRST”, “Success: “ + response.getResponseText());\n        }\n\n        @Override\n        public void onFailure(WLFailResponse response) {\n          Log.d(“MOBILEFIRST”, “Fail: “ + response.getErrorMsg());\n        }\n      });\n    } catch(URISyntaxException urise) {\n      Log.d(“MOBILEFIRST”, “URI: “ + urise.getMessage());\n    }\n  }\n});\n</code></pre>\n<p>The first thing that happens here is a <mark>client</mark> used to call IBM MobileFirst services.  The URI is the &quot;<mark>adapters</mark>&quot; functionality of MobileFirst, using the &quot;<mark>HelloAdapter</mark>&quot; we created in the previous step.  We are calling the &quot;<mark>users</mark>&quot; REST endpoint using an HTTP GET, and sending the parameter &quot;<mark>Kevin</mark>&quot;.  Once executed, we log out the response.</p>\n<h3 id=\"extracredit\">Extra Credit</h3>\n<p>What exactly is going on in that Java adapter?  This is one of my favorite parts.  <mark>Java adapters are JAX-RS</mark>, which effectively means they are plain old Java objects, with some annotations to tell the server how to expose them.  JAX-RS is beyond the scope of this post, but here is a glimpse of a &quot;hello world&quot; adapter.</p>\n<pre><code>package com.ibm.us.krhoyt;\n\nimport javax.ws.rs.GET;\nimport javax.ws.rs.Path;\nimport javax.ws.rs.PathParam;\n\n@Path(&quot;/users&quot;)\npublic class HelloAdapterResource \n{\n  @GET\n  @Path(&quot;/{name}&quot;)\n  public String hello(@PathParam(&quot;name&quot;) String name) {\t\t\n    return &quot;Hello &quot; + name.trim() + &quot;!&quot;;\n  }\n}\n</code></pre>\n<p>It does not get much easier than that folks!  The &quot;<mark>Path</mark>&quot; annotations tell the server where to expose this REST service.  The &quot;<mark>GET</mark>&quot; annotation says to route to this method for an HTTP GET.  The &quot;<mark>PathParam</mark>&quot; annotation maps the URL parameter to a method instance for us to use.  JAX-RS goes much deep than this, and I would encourage you to explore the awesomeness if you have not already.</p>\n<h3 id=\"nextsteps\">Next Steps</h3>\n<p>Like most &quot;hello world&quot; examples, this really does not do MobileFirst justice.  For example, if you try <mark>accessing the REST endpoint directly</mark> in the browser (or via <a href=\"https://www.getpostman.com\">Postman</a>, <a href=\"https://luckymarmot.com/paw\">Paw</a>, or other client), you will get an message that <mark>access is denied</mark>.  At this point, only the Android application is permitted to access the endpoint.</p>\n<blockquote>\n<p>Put another way, you get application security right out of the box.</p>\n</blockquote>\n<p>Another cool feature is <mark>reporting</mark> for our application usage.  You can actually see <mark>how many devices</mark> are accessing your APIs, and with a little work, monitor what APIs they use most <mark>frequently</mark>, their <mark>performance</mark>, and so on.  That all takes place in the <mark>IBM MobileFirst console</mark>.  You can access it by running the following command from within a MobileFirst project folder (server or client).</p>\n<pre><code>mfp console\n</code></pre>\n<p>If you are accessing an <mark>XML</mark> resource from your adapter, MobileFirst will even let you easily specify an <mark>XSL transform</mark> to run on the data before returning it to the client.  Even further, you can <mark>access a database directly</mark> from your Android clients, and still keep the security and reporting features.</p>\n<p>The list goes on and on.  If you are building mobile applications (and who is not these days), then IBM MobileFirst gives you a vast suite of tools.</p>\n<p>If you want to check out <mark>my sample source code</mark> for a &quot;hello world&quot; (<mark>slightly improved</mark> from this boilerplate), you can find it in my <a href=\"http://github.com/krhoyt\">GitHub</a> repository.  Have questions?  Leave a comment below, or send me a message on <a href=\"http://twitter.com/krhoyt\">Twitter</a>.</p>\n<p><em>Header <a href=\"https://www.flickr.com/photos/-heinecke-/4406341989\">photo</a> is &quot;IBM Logo&quot; by Flickr user &quot;heinecke&quot;</em></p>\n<!--kg-card-end: markdown-->","comment_id":"43","plaintext":"Branching off Ray Camden's excellent post\n[http://www.raymondcamden.com/2015/08/17/getting-started-with-mobile-development-and-ibm-mobilefirst-7-1] \non getting started with IBM MobileFirst 7.1\n[http://www.raymondcamden.com/2015/08/17/getting-started-with-mobile-development-and-ibm-mobilefirst-7-1]\n, this post will focus on the continuing workflow from an Android perspective.\nIt is assumed that you have followed Ray's tutorial on installing the IBM\nMobileFirst 7.1 CLI (if you have an earlier version, you should upgrade), and\nhave Android Studio [http://developer.android.com/tools/studio/index.html] at\nthe ready.\n\nThe Projects\nWhen working with IBM MobileFirst, there is almost always two projects - the\nclient and the server. The client in this case is an Android application. The\nserver is functionally a Java EE 7 application, and with MobileFirst Foundation,\nit is deployed on IBM WebSphere Liberty\n[https://developer.ibm.com/wasdev/websphere-liberty/]. Starting on the server\nside then, let us create a project.\n\nmfp create HelloProject\n\n\nAt the command prompt, you will want to move to directory that you want to work\nwith for this walkthrough. The above command will create a directory called\n\"HelloProject\", which will hold a baseline MobileFirst directory setup.\n\n> I generally run the above command from a directory named for the application I\nam building. That directory then has a folder for the client and the server\nparts.\n\n\nNow change into that newly created MobileFirst project directory. Here we will\ncreate an \"adapter\" or effectively expose a REST API for application data and\nresources. You can actually create adapters written in JavaScript (not Node.js),\nbut as developers working with Android, you might find it more robust to simply\nuse Java throughout.\n\nmfp add adapter HelloAdapter —type java —package com.ibm.us.krhoyt\nmfp start\nmfp push\n\n\nThese three commands will create a Java adapter for you in the \"adapter\" folder, \nstart the IBM MobileFirst server if it is not already started, and then deploy \n(push) the project, including the newly created adapter, onto the server. We\nwill come back to that adapter code in a moment, but if you want to test it out,\nit is populated with some boilerplate code.\n\nmfp adapter call HelloAdapter/users/Kevin\n\n\nAndroid Studio\nStart up Android Studio and create a new project with a blank activity. In order\nto tell Android Studio about the IBM MobileFirst dependencies, we will need to\nmake a few changes. With your project created, go to Project -> Gradle ->\nbuild.gradle (Module: app) and add the following snippets.\n\n// Below \"apply plugin: ‘com.android.application’\"\nrepositories {\n  jcenter()\n}\n\n// Inside “android\"\npackagingOptions {\n  pickFirst 'META-INF/ASL2.0'\n  pickFirst 'META-INF/LICENSE'\n  pickFirst 'META-INF/NOTICE'\n}\n\n// Inside “dependencies\"\ncompile group: 'com.ibm.mobile.foundation',\n  name: 'ibmmobilefirstplatformfoundation',\n  version: '7.1.0.0',\n  ext: 'aar',\n  transitive: true\n\n\nWith these changes, Gradle will want to rebuild the project. Let Gradle complete\nthat task before moving on to the next step. When Gradle is done, head to your \"\nApplicationManifest.xml\" file and make the following changes.\n\n// After the opening manifest tag\n<uses-permission android:name=\"android.permission.INTERNET\"/>\n<uses-permission android:name=\"android.permission.ACCESS_WIFI_STATE\"/>\n<uses-permission android:name=\"android.permission.REAL_GET_TASKS\" />\n\n// After the closing activity tag\n<activity android:name=\"com.worklight.wlclient.ui.UIActivity\" />\n\n\nThe permissions give your Android application the ability to call IBM\nMobileFirst services. The additional activity is a background service used by\nsome of the MobileFirst features. Head back to the command line, make sure you\nare in the Android Studio project directory, and run the following command to \ntell MobileFirst about your Android application.\n\nmfp push\n\n\n> Notice how the \"mfp\" command performs different actions depending on what\ndirectory structure you are in. If you are in a server, it deploys your code. In\nan Android project, it tells the server about your application.\n\n\nNow you are ready to leverage the IBM MobileFirst server from your Android\napplication. Add the following code to your main activity source file, inside\nthe \"onCreate\" method.\n\nfinal WLClient client = WLClient.createInstance(this);\n\nclient.connect(new WLResponseListener() {\n  @Override\n  public void onSuccess(WLResponse response) {\n    URI                path;                    \n    WLResourceRequest  request;\n\n    try {\n      path = new URI(\"/adapters/HelloAdapter/users/Kevin\");\n      request = new WLResourceRequest(path, WLResourceRequest.GET);\n      request.send(new WLResponseListener() {\n        @Override\n        public void onSuccess(WLResponse response) {\n          Log.d(“MOBILEFIRST”, “Success: “ + response.getResponseText());\n        }\n\n        @Override\n        public void onFailure(WLFailResponse response) {\n          Log.d(“MOBILEFIRST”, “Fail: “ + response.getErrorMsg());\n        }\n      });\n    } catch(URISyntaxException urise) {\n      Log.d(“MOBILEFIRST”, “URI: “ + urise.getMessage());\n    }\n  }\n});\n\n\nThe first thing that happens here is a client used to call IBM MobileFirst\nservices. The URI is the \"adapters\" functionality of MobileFirst, using the \"\nHelloAdapter\" we created in the previous step. We are calling the \"users\" REST\nendpoint using an HTTP GET, and sending the parameter \"Kevin\". Once executed, we\nlog out the response.\n\nExtra Credit\nWhat exactly is going on in that Java adapter? This is one of my favorite parts. \nJava adapters are JAX-RS, which effectively means they are plain old Java\nobjects, with some annotations to tell the server how to expose them. JAX-RS is\nbeyond the scope of this post, but here is a glimpse of a \"hello world\" adapter.\n\npackage com.ibm.us.krhoyt;\n\nimport javax.ws.rs.GET;\nimport javax.ws.rs.Path;\nimport javax.ws.rs.PathParam;\n\n@Path(\"/users\")\npublic class HelloAdapterResource \n{\n  @GET\n  @Path(\"/{name}\")\n  public String hello(@PathParam(\"name\") String name) {\t\t\n    return \"Hello \" + name.trim() + \"!\";\n  }\n}\n\n\nIt does not get much easier than that folks! The \"Path\" annotations tell the\nserver where to expose this REST service. The \"GET\" annotation says to route to\nthis method for an HTTP GET. The \"PathParam\" annotation maps the URL parameter\nto a method instance for us to use. JAX-RS goes much deep than this, and I would\nencourage you to explore the awesomeness if you have not already.\n\nNext Steps\nLike most \"hello world\" examples, this really does not do MobileFirst justice.\nFor example, if you try accessing the REST endpoint directly in the browser (or\nvia Postman [https://www.getpostman.com], Paw [https://luckymarmot.com/paw], or\nother client), you will get an message that access is denied. At this point,\nonly the Android application is permitted to access the endpoint.\n\n> Put another way, you get application security right out of the box.\n\n\nAnother cool feature is reporting for our application usage. You can actually\nsee how many devices are accessing your APIs, and with a little work, monitor\nwhat APIs they use most frequently, their performance, and so on. That all takes\nplace in the IBM MobileFirst console. You can access it by running the following\ncommand from within a MobileFirst project folder (server or client).\n\nmfp console\n\n\nIf you are accessing an XML resource from your adapter, MobileFirst will even\nlet you easily specify an XSL transform to run on the data before returning it\nto the client. Even further, you can access a database directly from your\nAndroid clients, and still keep the security and reporting features.\n\nThe list goes on and on. If you are building mobile applications (and who is not\nthese days), then IBM MobileFirst gives you a vast suite of tools.\n\nIf you want to check out my sample source code for a \"hello world\" (slightly\nimproved from this boilerplate), you can find it in my GitHub\n[http://github.com/krhoyt] repository. Have questions? Leave a comment below, or\nsend me a message on Twitter [http://twitter.com/krhoyt].\n\nHeader photo [https://www.flickr.com/photos/-heinecke-/4406341989] is \"IBM Logo\"\nby Flickr user \"heinecke\"","feature_image":"http://images.kevinhoyt.com/ibm-logo-on-wall.jpg","featured":0,"status":"published","locale":null,"visibility":"public","author_id":"1","created_at":"2015-08-20T19:15:22.000Z","updated_at":"2015-08-25T18:36:49.000Z","published_at":"2015-08-25T18:36:49.000Z","custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"type":"post","email_recipient_filter":"none"},{"id":"5adb8922351ffe0018a57736","uuid":"c96bd705-3692-4581-8d39-abcbe9ec94a3","title":"WebSocket on IBM Bluemix","slug":"websockets-on-ibm-bluemix","mobiledoc":"{\"version\":\"0.3.1\",\"markups\":[],\"atoms\":[],\"cards\":[[\"markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\"As IBM Bluemix can host a variety of language runtimes (PHP, Node.js, Python, Ruby, ASP.NET, to name a few), and most languages have a WebSocket server implementation, you can then in turn run a WebSocket server on IBM Bluemix.  In this post I will take a look at building and deploying a chat application on Node.js using WebSocket.\\n\\n###IBM Bluemix\\n\\nIf you do not ==already have an account== on [IBM Bluemix](http://bluemix.net), then you will want to create one, and familiarize yourself with deploying applications.  The code used by the default application instance (inclusive of any flavor) is provided for download when you start said instance.  The process of getting started with IBM Bluemix is beyond the scope of this post.  Please look back at some of my earlier posts to [learn](http://blog.kevinhoyt.com/2015/08/04/bluemix-mobile-runtimes) [more](http://blog.kevinhoyt.com/2015/07/28/bluemix-crawl-runtimes/).\\n\\n###Server\\n\\nFor this implementation, I will be using [Node.js](http://nodejs.org).  Because of the vast breadth of packages available, ==Node.js== tends to be my server implementation of choice these days.  When it comes to ==WebSocket==, there are several Node.js implementations.  I will get into what differentiates them in a little bit, but for this server I will be using \\\"[ws](https://github.com/websockets/ws)\\\".\\n\\n```\\nvar cfenv = require( 'cfenv' );\\nvar express = require( 'express' );\\nvar http = require( 'http' );\\nvar ws = require( 'ws' );\\n\\n// Environment\\nvar environment = cfenv.getAppEnv();\\n\\n// Web\\nvar app = express();\\n\\n// Static\\napp.use( '/', express.static( 'public' ) );\\n\\n// Sockets\\nvar server = http.createServer();\\nvar sockets = new ws.Server( {\\n  server: server\\n} );\\n\\n// Listeners\\nsockets.on( 'connection', function( client ) {\\n  // Debug\\n  console.log( 'Connection.' );\\n\\n  // Echo messages to all clients\\n  client.on( 'message', function( message ) {\\n    for( var c = 0; c < sockets.clients.length; c++ ) {\\n      sockets.clients[c].send( message );   \\n    }\\n  } );\\n} );\\n\\n// Start\\nserver.on( 'request', app );\\nserver.listen( environment.port, function() {\\n  console.log( environment.url );\\n} );\\n```\\n\\n==That is the entirety of a chat server using WebSocket.==\\n\\n**Environment**\\n\\nAfter including the necessary packages, we use the \\\"==cfenv==\\\" ([Cloud Foundry](https://www.cloudfoundry.org) environment) functionality to get information about the environment in which our application is running.  This is key for ==assigning ports==.  \\n\\n==WebSocket== connections initially start life as ==normal HTTP requests== on port 80 or 443.  Along the way however the connection is \\\"==upgraded==\\\" at which point it becomes an open socket for you to send any data type your application needs (even binary data).\\n\\n> Aside from the HTTP handshake and upgrade, a WebSocket is a TCP socket.  If you had a [VNC server](http://www.tightvnc.com) that supported that handshake, you could run a remote connection client directly in the browser.\\n\\n**Web**\\n\\nWhile it is probably a bit overkill for this application, I am using [Express](http://expressjs.com) for the ==web server==.  Keep in mind that a WebSocket is (generally) initiated from a web page in a browser.  This means you need to deliver the web page to the browser in the first place, and that means you need a web server in addition to the WebSocket server.\\n\\nIn a slightly more well-rounded application, I actually use Express routing to expose a ==REST API== to control the ==features== of the chat server.  It also uses [Mongoose](http://mongoosejs.com) with [IBM Compose](http://compose.io) (==MongoDB==) to record the ==chat history==.  There is even ==custom avatar== support, ==geolocation==, and ==inline images== in the chat.  This is where using Express really shines.\\n\\n**Sockets**\\n\\nAs mentioned, WebSockets start their lives as normal HTTP requests.  This is why we use the \\\"http\\\" package to ==create a server==.  We then tell the WebSocket server, to latch onto the HTTP server instance.  I am not sure what happens under the covers here, but I would guess that the \\\"ws\\\" package is configuring the HTTP server to get access to the ==raw incoming request==, so it can look for a WebSocket request, and handle the ==upgrade==, connections, etc.\\n\\n**Listeners**\\n\\nWhen a connection comes in, we can take a number of roads.  As a chat server, we want to get any incoming messages, and route them back to any of the connected clients.  The WebSocket server keeps track of the ==connected clients==, which we can ==iterate== over and ==echo== the incoming message.\\n\\nYou might notice that there is ==no differentiation between clients== - including the client sending the original message.  As a matter of preference, for most of my applications, I do not ==take action== in the user interface of the sending ==client==, until the message has been ==received==.  This has saved me countless times while ==debugging== real-time applications.\\n\\n```\\n// Not back to sender\\nif( sockets.clients[c] != this ) {\\n  sockets.clients[c].send( message );                                    \\n} \\n```\\n\\nThe above snippet will ==not send== the incoming message back to the ==original sender== of that message.  This is where the aforementioned Express routing can come in handy.  You could setup a REST API to set a flag on the server that says whether or not it should echo the message back to the sender.  [IBM Redis](https://console.ng.bluemix.net/catalog/redis-by-compose/) makes a great solution for these types of ==flags==.\\n\\n**Start**\\n\\nLast but not least, we need to ==start our server==.  Because we want to use Express for the routing, we need to tell the server about our Express settings.  Then we need to start the server, but we need to be considerate of the IBM Bluemix ==Cloud Foundry environment==.\\n\\nYou can ==run this locally==, and a port will be assigned to your application.  You can also detect if the application is running locally, and then assign your ==own port==.  I am not picky about port assignments when developing locally, so I just let it assign whatever it wants, and then tell me at the console.\\n\\n###Browser\\n\\nThe [WebSocket API](http://www.w3.org/TR/websockets/) in the browser is well documented.  I would encourage you to look around the WebSocket documentation to learn more.  In the interest of completeness however, here is the basics of a chat application for our server.\\n\\n```\\nvar input = null;\\nvar socket = null;\\n\\n// Input\\ninput = document.querySelector( 'input[type=\\\\'text\\\\']' ); \\ninput.addEventListener( 'keypress', doInputKey );    \\n\\n// WebSocket\\nsocket = new WebSocket( 'ws://' + window.location.host );\\nsocket.addEventListener( 'message', doSocketMessage );\\n\\n// Keyboard\\nfunction doInputKey( event ) {\\n  var message = null;\\n\\n  if( event.keyCode == 13 && this.value.trim().length > 0 ) {\\n    message = {\\n      content: this.value.trim(),\\n    };\\n    \\n    // Send message\\n    socket.send( JSON.stringify( message ) );  \\n  }     \\n}\\n\\n// Message\\nfunction doSocketMessage( message ) {\\n  var data = null;\\n  var element = null;\\n\\n  // Parse\\n  data = JSON.parse( message.data );\\n\\n  // Build and append\\n  element = document.createElement( 'p' );\\n  element.innerHTML = data.content;\\n  document.body.appendChild( element );\\n}\\n```\\n\\nGiven an ==HTML== document, with an ==INPUT== element inside it, this is functionally all you need for a chat application on the client side.  As I mentioned earlier, you could certainly get far more ambitious by adding custom avatars, inline images, etc.\\n\\n**Input**\\n\\nIn order to have a chat client, we need a place for the user to ==enter some content==.  That is the INPUT element, though you could certainly use an editable DIV element as well.  In fact, if you were building a game, the \\\"input\\\" might be an [HTML5 Canvas](https://html.spec.whatwg.org/multipage/scripting.html#the-canvas-element) with a ==thumb control and buttons==.\\n\\n**WebSocket**\\n\\nThere are ==polyfills for WebSocket==, but it has been around now for some time, and is [broadly available](http://caniuse.com/#feat=websockets) across ==desktops and devices==.  Notice that rather than \\\"http://\\\" we tell the socket to connect to \\\"==ws://==\\\" - this is part of how the handshake works.  Just as there is \\\"https://\\\" there is also \\\"==wss://==\\\" in the WebSocket world.\\n\\nBecause I am often developing locally, pushing to IBM Bluemix, testing, and then making changes locally again before pushing a finished build, I use \\\"==window.location.host==\\\" to abstract the server for me.  However, WebSocket connections can be made ==cross-domain==.  This is an implementation detail of the server and handshake.\\n\\n**Keyboard**\\n\\nRather than provide a button to send a message, most chat applications these days (from [Google Hangouts](https://www.google.com/search?client=safari&rls=en&q=google+hangouts&ie=UTF-8&oe=UTF-8) to [iOS Messages](http://www.apple.com/ios/whats-new/messages)) use the ==return key== to trigger sending a message.  This is easy enough to do, but I also check to make sure there is at least some content to send (where this would otherwise be an dis/enabled button).\\n\\nThe data I am sending across the WebSocket is a ==JSON object==.  The format of that JSON object - that is to say the properties it has - are ==completely up to you==.  I would suggest that many smaller message generally work better than fewer longer ones, but the problem you are trying to solve may differ.\\n\\nYou could for example, store an update to the entire application model in a database, and then send a WebSocket message to tell the other applications to pull the latest model.  ==The number of interesting techniques you can employ as a developer expands considerably when you leverage WebSocket.==\\n\\n**Message**\\n\\nWhen a message arrives from a WebSocket server, the data itself is actually on a \\\"==data==\\\" property.  There are other properties on the ==event== itself, that are injected along the way.  In this case, the \\\"==message==\\\" object from the previous step is now found at \\\"==message.data==\\\".  We parse that string and populate the DOM.\\n\\n###Next Steps\\n\\nWebSocket brings with it a ==vast array of new options==.  It is not however without problems.  ==Problems== that can be solved, to be sure, but problems that take real consideration up front to make sure that you have a system that will last as long as you want it to.  Some of those considerations might include:\\n\\n- Given that the \\\"message\\\" object in our application can take on any form we desire, this could quickly lead to a brittle ==dependency== between systems.  \\n- We also have no ==guarantee== that the message we sent actually made it to the server - or the other clients.  \\n- We have no ==presence awareness== here either - clients can come and go, and your application would never know the difference.  If you want to show who is in a chat room, this presents a real challenge.  \\n- What if the ==server goes down== while delivering a message?  What about all those abandoned ==disconnected clients== that do not know the server went down?\\n\\nAs it turns out, using WebSocket introduces a pattern that has long since been around in enterprise system design known as,  ==publish-subscribe== (message brokers).  All of these problems can be solved by leveraging a more robust server and message protocol (format of the data).  There are even cloud offerings for this pattern, such as the IBM Partner, [PubNub](http://pubnub.com).\\n\\n*Image courtesy of [Wikipedia](https://en.wikipedia.org/wiki/Tap_and_die).*\"}]],\"sections\":[[10,0]],\"ghostVersion\":\"3.0\"}","html":"<!--kg-card-begin: markdown--><p>As IBM Bluemix can host a variety of language runtimes (PHP, Node.js, Python, Ruby, ASP.NET, to name a few), and most languages have a WebSocket server implementation, you can then in turn run a WebSocket server on IBM Bluemix.  In this post I will take a look at building and deploying a chat application on Node.js using WebSocket.</p>\n<h3 id=\"ibmbluemix\">IBM Bluemix</h3>\n<p>If you do not <mark>already have an account</mark> on <a href=\"http://bluemix.net\">IBM Bluemix</a>, then you will want to create one, and familiarize yourself with deploying applications.  The code used by the default application instance (inclusive of any flavor) is provided for download when you start said instance.  The process of getting started with IBM Bluemix is beyond the scope of this post.  Please look back at some of my earlier posts to <a href=\"http://blog.kevinhoyt.com/2015/08/04/bluemix-mobile-runtimes\">learn</a> <a href=\"http://blog.kevinhoyt.com/2015/07/28/bluemix-crawl-runtimes/\">more</a>.</p>\n<h3 id=\"server\">Server</h3>\n<p>For this implementation, I will be using <a href=\"http://nodejs.org\">Node.js</a>.  Because of the vast breadth of packages available, <mark>Node.js</mark> tends to be my server implementation of choice these days.  When it comes to <mark>WebSocket</mark>, there are several Node.js implementations.  I will get into what differentiates them in a little bit, but for this server I will be using &quot;<a href=\"https://github.com/websockets/ws\">ws</a>&quot;.</p>\n<pre><code>var cfenv = require( 'cfenv' );\nvar express = require( 'express' );\nvar http = require( 'http' );\nvar ws = require( 'ws' );\n\n// Environment\nvar environment = cfenv.getAppEnv();\n\n// Web\nvar app = express();\n\n// Static\napp.use( '/', express.static( 'public' ) );\n\n// Sockets\nvar server = http.createServer();\nvar sockets = new ws.Server( {\n  server: server\n} );\n\n// Listeners\nsockets.on( 'connection', function( client ) {\n  // Debug\n  console.log( 'Connection.' );\n\n  // Echo messages to all clients\n  client.on( 'message', function( message ) {\n    for( var c = 0; c &lt; sockets.clients.length; c++ ) {\n      sockets.clients[c].send( message );   \n    }\n  } );\n} );\n\n// Start\nserver.on( 'request', app );\nserver.listen( environment.port, function() {\n  console.log( environment.url );\n} );\n</code></pre>\n<p><mark>That is the entirety of a chat server using WebSocket.</mark></p>\n<p><strong>Environment</strong></p>\n<p>After including the necessary packages, we use the &quot;<mark>cfenv</mark>&quot; (<a href=\"https://www.cloudfoundry.org\">Cloud Foundry</a> environment) functionality to get information about the environment in which our application is running.  This is key for <mark>assigning ports</mark>.</p>\n<p><mark>WebSocket</mark> connections initially start life as <mark>normal HTTP requests</mark> on port 80 or 443.  Along the way however the connection is &quot;<mark>upgraded</mark>&quot; at which point it becomes an open socket for you to send any data type your application needs (even binary data).</p>\n<blockquote>\n<p>Aside from the HTTP handshake and upgrade, a WebSocket is a TCP socket.  If you had a <a href=\"http://www.tightvnc.com\">VNC server</a> that supported that handshake, you could run a remote connection client directly in the browser.</p>\n</blockquote>\n<p><strong>Web</strong></p>\n<p>While it is probably a bit overkill for this application, I am using <a href=\"http://expressjs.com\">Express</a> for the <mark>web server</mark>.  Keep in mind that a WebSocket is (generally) initiated from a web page in a browser.  This means you need to deliver the web page to the browser in the first place, and that means you need a web server in addition to the WebSocket server.</p>\n<p>In a slightly more well-rounded application, I actually use Express routing to expose a <mark>REST API</mark> to control the <mark>features</mark> of the chat server.  It also uses <a href=\"http://mongoosejs.com\">Mongoose</a> with <a href=\"http://compose.io\">IBM Compose</a> (<mark>MongoDB</mark>) to record the <mark>chat history</mark>.  There is even <mark>custom avatar</mark> support, <mark>geolocation</mark>, and <mark>inline images</mark> in the chat.  This is where using Express really shines.</p>\n<p><strong>Sockets</strong></p>\n<p>As mentioned, WebSockets start their lives as normal HTTP requests.  This is why we use the &quot;http&quot; package to <mark>create a server</mark>.  We then tell the WebSocket server, to latch onto the HTTP server instance.  I am not sure what happens under the covers here, but I would guess that the &quot;ws&quot; package is configuring the HTTP server to get access to the <mark>raw incoming request</mark>, so it can look for a WebSocket request, and handle the <mark>upgrade</mark>, connections, etc.</p>\n<p><strong>Listeners</strong></p>\n<p>When a connection comes in, we can take a number of roads.  As a chat server, we want to get any incoming messages, and route them back to any of the connected clients.  The WebSocket server keeps track of the <mark>connected clients</mark>, which we can <mark>iterate</mark> over and <mark>echo</mark> the incoming message.</p>\n<p>You might notice that there is <mark>no differentiation between clients</mark> - including the client sending the original message.  As a matter of preference, for most of my applications, I do not <mark>take action</mark> in the user interface of the sending <mark>client</mark>, until the message has been <mark>received</mark>.  This has saved me countless times while <mark>debugging</mark> real-time applications.</p>\n<pre><code>// Not back to sender\nif( sockets.clients[c] != this ) {\n  sockets.clients[c].send( message );                                    \n} \n</code></pre>\n<p>The above snippet will <mark>not send</mark> the incoming message back to the <mark>original sender</mark> of that message.  This is where the aforementioned Express routing can come in handy.  You could setup a REST API to set a flag on the server that says whether or not it should echo the message back to the sender.  <a href=\"https://console.ng.bluemix.net/catalog/redis-by-compose/\">IBM Redis</a> makes a great solution for these types of <mark>flags</mark>.</p>\n<p><strong>Start</strong></p>\n<p>Last but not least, we need to <mark>start our server</mark>.  Because we want to use Express for the routing, we need to tell the server about our Express settings.  Then we need to start the server, but we need to be considerate of the IBM Bluemix <mark>Cloud Foundry environment</mark>.</p>\n<p>You can <mark>run this locally</mark>, and a port will be assigned to your application.  You can also detect if the application is running locally, and then assign your <mark>own port</mark>.  I am not picky about port assignments when developing locally, so I just let it assign whatever it wants, and then tell me at the console.</p>\n<h3 id=\"browser\">Browser</h3>\n<p>The <a href=\"http://www.w3.org/TR/websockets/\">WebSocket API</a> in the browser is well documented.  I would encourage you to look around the WebSocket documentation to learn more.  In the interest of completeness however, here is the basics of a chat application for our server.</p>\n<pre><code>var input = null;\nvar socket = null;\n\n// Input\ninput = document.querySelector( 'input[type=\\'text\\']' ); \ninput.addEventListener( 'keypress', doInputKey );    \n\n// WebSocket\nsocket = new WebSocket( 'ws://' + window.location.host );\nsocket.addEventListener( 'message', doSocketMessage );\n\n// Keyboard\nfunction doInputKey( event ) {\n  var message = null;\n\n  if( event.keyCode == 13 &amp;&amp; this.value.trim().length &gt; 0 ) {\n    message = {\n      content: this.value.trim(),\n    };\n    \n    // Send message\n    socket.send( JSON.stringify( message ) );  \n  }     \n}\n\n// Message\nfunction doSocketMessage( message ) {\n  var data = null;\n  var element = null;\n\n  // Parse\n  data = JSON.parse( message.data );\n\n  // Build and append\n  element = document.createElement( 'p' );\n  element.innerHTML = data.content;\n  document.body.appendChild( element );\n}\n</code></pre>\n<p>Given an <mark>HTML</mark> document, with an <mark>INPUT</mark> element inside it, this is functionally all you need for a chat application on the client side.  As I mentioned earlier, you could certainly get far more ambitious by adding custom avatars, inline images, etc.</p>\n<p><strong>Input</strong></p>\n<p>In order to have a chat client, we need a place for the user to <mark>enter some content</mark>.  That is the INPUT element, though you could certainly use an editable DIV element as well.  In fact, if you were building a game, the &quot;input&quot; might be an <a href=\"https://html.spec.whatwg.org/multipage/scripting.html#the-canvas-element\">HTML5 Canvas</a> with a <mark>thumb control and buttons</mark>.</p>\n<p><strong>WebSocket</strong></p>\n<p>There are <mark>polyfills for WebSocket</mark>, but it has been around now for some time, and is <a href=\"http://caniuse.com/#feat=websockets\">broadly available</a> across <mark>desktops and devices</mark>.  Notice that rather than &quot;http://&quot; we tell the socket to connect to &quot;<mark>ws://</mark>&quot; - this is part of how the handshake works.  Just as there is &quot;https://&quot; there is also &quot;<mark>wss://</mark>&quot; in the WebSocket world.</p>\n<p>Because I am often developing locally, pushing to IBM Bluemix, testing, and then making changes locally again before pushing a finished build, I use &quot;<mark>window.location.host</mark>&quot; to abstract the server for me.  However, WebSocket connections can be made <mark>cross-domain</mark>.  This is an implementation detail of the server and handshake.</p>\n<p><strong>Keyboard</strong></p>\n<p>Rather than provide a button to send a message, most chat applications these days (from <a href=\"https://www.google.com/search?client=safari&amp;rls=en&amp;q=google+hangouts&amp;ie=UTF-8&amp;oe=UTF-8\">Google Hangouts</a> to <a href=\"http://www.apple.com/ios/whats-new/messages\">iOS Messages</a>) use the <mark>return key</mark> to trigger sending a message.  This is easy enough to do, but I also check to make sure there is at least some content to send (where this would otherwise be an dis/enabled button).</p>\n<p>The data I am sending across the WebSocket is a <mark>JSON object</mark>.  The format of that JSON object - that is to say the properties it has - are <mark>completely up to you</mark>.  I would suggest that many smaller message generally work better than fewer longer ones, but the problem you are trying to solve may differ.</p>\n<p>You could for example, store an update to the entire application model in a database, and then send a WebSocket message to tell the other applications to pull the latest model.  <mark>The number of interesting techniques you can employ as a developer expands considerably when you leverage WebSocket.</mark></p>\n<p><strong>Message</strong></p>\n<p>When a message arrives from a WebSocket server, the data itself is actually on a &quot;<mark>data</mark>&quot; property.  There are other properties on the <mark>event</mark> itself, that are injected along the way.  In this case, the &quot;<mark>message</mark>&quot; object from the previous step is now found at &quot;<mark>message.data</mark>&quot;.  We parse that string and populate the DOM.</p>\n<h3 id=\"nextsteps\">Next Steps</h3>\n<p>WebSocket brings with it a <mark>vast array of new options</mark>.  It is not however without problems.  <mark>Problems</mark> that can be solved, to be sure, but problems that take real consideration up front to make sure that you have a system that will last as long as you want it to.  Some of those considerations might include:</p>\n<ul>\n<li>Given that the &quot;message&quot; object in our application can take on any form we desire, this could quickly lead to a brittle <mark>dependency</mark> between systems.</li>\n<li>We also have no <mark>guarantee</mark> that the message we sent actually made it to the server - or the other clients.</li>\n<li>We have no <mark>presence awareness</mark> here either - clients can come and go, and your application would never know the difference.  If you want to show who is in a chat room, this presents a real challenge.</li>\n<li>What if the <mark>server goes down</mark> while delivering a message?  What about all those abandoned <mark>disconnected clients</mark> that do not know the server went down?</li>\n</ul>\n<p>As it turns out, using WebSocket introduces a pattern that has long since been around in enterprise system design known as,  <mark>publish-subscribe</mark> (message brokers).  All of these problems can be solved by leveraging a more robust server and message protocol (format of the data).  There are even cloud offerings for this pattern, such as the IBM Partner, <a href=\"http://pubnub.com\">PubNub</a>.</p>\n<p><em>Image courtesy of <a href=\"https://en.wikipedia.org/wiki/Tap_and_die\">Wikipedia</a>.</em></p>\n<!--kg-card-end: markdown-->","comment_id":"44","plaintext":"As IBM Bluemix can host a variety of language runtimes (PHP, Node.js, Python,\nRuby, ASP.NET, to name a few), and most languages have a WebSocket server\nimplementation, you can then in turn run a WebSocket server on IBM Bluemix. In\nthis post I will take a look at building and deploying a chat application on\nNode.js using WebSocket.\n\nIBM Bluemix\nIf you do not already have an account on IBM Bluemix [http://bluemix.net], then\nyou will want to create one, and familiarize yourself with deploying\napplications. The code used by the default application instance (inclusive of\nany flavor) is provided for download when you start said instance. The process\nof getting started with IBM Bluemix is beyond the scope of this post. Please\nlook back at some of my earlier posts to learn\n[http://blog.kevinhoyt.com/2015/08/04/bluemix-mobile-runtimes] more\n[http://blog.kevinhoyt.com/2015/07/28/bluemix-crawl-runtimes/].\n\nServer\nFor this implementation, I will be using Node.js [http://nodejs.org]. Because of\nthe vast breadth of packages available, Node.js tends to be my server\nimplementation of choice these days. When it comes to WebSocket, there are\nseveral Node.js implementations. I will get into what differentiates them in a\nlittle bit, but for this server I will be using \"ws\n[https://github.com/websockets/ws]\".\n\nvar cfenv = require( 'cfenv' );\nvar express = require( 'express' );\nvar http = require( 'http' );\nvar ws = require( 'ws' );\n\n// Environment\nvar environment = cfenv.getAppEnv();\n\n// Web\nvar app = express();\n\n// Static\napp.use( '/', express.static( 'public' ) );\n\n// Sockets\nvar server = http.createServer();\nvar sockets = new ws.Server( {\n  server: server\n} );\n\n// Listeners\nsockets.on( 'connection', function( client ) {\n  // Debug\n  console.log( 'Connection.' );\n\n  // Echo messages to all clients\n  client.on( 'message', function( message ) {\n    for( var c = 0; c < sockets.clients.length; c++ ) {\n      sockets.clients[c].send( message );   \n    }\n  } );\n} );\n\n// Start\nserver.on( 'request', app );\nserver.listen( environment.port, function() {\n  console.log( environment.url );\n} );\n\n\nThat is the entirety of a chat server using WebSocket.\n\nEnvironment\n\nAfter including the necessary packages, we use the \"cfenv\" (Cloud Foundry\n[https://www.cloudfoundry.org] environment) functionality to get information\nabout the environment in which our application is running. This is key for \nassigning ports.\n\nWebSocket connections initially start life as normal HTTP requests on port 80 or\n443. Along the way however the connection is \"upgraded\" at which point it\nbecomes an open socket for you to send any data type your application needs\n(even binary data).\n\n> Aside from the HTTP handshake and upgrade, a WebSocket is a TCP socket. If you\nhad a VNC server [http://www.tightvnc.com] that supported that handshake, you\ncould run a remote connection client directly in the browser.\n\n\nWeb\n\nWhile it is probably a bit overkill for this application, I am using Express\n[http://expressjs.com] for the web server. Keep in mind that a WebSocket is\n(generally) initiated from a web page in a browser. This means you need to\ndeliver the web page to the browser in the first place, and that means you need\na web server in addition to the WebSocket server.\n\nIn a slightly more well-rounded application, I actually use Express routing to\nexpose a REST API to control the features of the chat server. It also uses \nMongoose [http://mongoosejs.com] with IBM Compose [http://compose.io] (MongoDB)\nto record the chat history. There is even custom avatar support, geolocation,\nand inline images in the chat. This is where using Express really shines.\n\nSockets\n\nAs mentioned, WebSockets start their lives as normal HTTP requests. This is why\nwe use the \"http\" package to create a server. We then tell the WebSocket server,\nto latch onto the HTTP server instance. I am not sure what happens under the\ncovers here, but I would guess that the \"ws\" package is configuring the HTTP\nserver to get access to the raw incoming request, so it can look for a WebSocket\nrequest, and handle the upgrade, connections, etc.\n\nListeners\n\nWhen a connection comes in, we can take a number of roads. As a chat server, we\nwant to get any incoming messages, and route them back to any of the connected\nclients. The WebSocket server keeps track of the connected clients, which we can \niterate over and echo the incoming message.\n\nYou might notice that there is no differentiation between clients - including\nthe client sending the original message. As a matter of preference, for most of\nmy applications, I do not take action in the user interface of the sending \nclient, until the message has been received. This has saved me countless times\nwhile debugging real-time applications.\n\n// Not back to sender\nif( sockets.clients[c] != this ) {\n  sockets.clients[c].send( message );                                    \n} \n\n\nThe above snippet will not send the incoming message back to the original sender \nof that message. This is where the aforementioned Express routing can come in\nhandy. You could setup a REST API to set a flag on the server that says whether\nor not it should echo the message back to the sender. IBM Redis\n[https://console.ng.bluemix.net/catalog/redis-by-compose/] makes a great\nsolution for these types of flags.\n\nStart\n\nLast but not least, we need to start our server. Because we want to use Express\nfor the routing, we need to tell the server about our Express settings. Then we\nneed to start the server, but we need to be considerate of the IBM Bluemix Cloud\nFoundry environment.\n\nYou can run this locally, and a port will be assigned to your application. You\ncan also detect if the application is running locally, and then assign your own\nport. I am not picky about port assignments when developing locally, so I just\nlet it assign whatever it wants, and then tell me at the console.\n\nBrowser\nThe WebSocket API [http://www.w3.org/TR/websockets/] in the browser is well\ndocumented. I would encourage you to look around the WebSocket documentation to\nlearn more. In the interest of completeness however, here is the basics of a\nchat application for our server.\n\nvar input = null;\nvar socket = null;\n\n// Input\ninput = document.querySelector( 'input[type=\\'text\\']' ); \ninput.addEventListener( 'keypress', doInputKey );    \n\n// WebSocket\nsocket = new WebSocket( 'ws://' + window.location.host );\nsocket.addEventListener( 'message', doSocketMessage );\n\n// Keyboard\nfunction doInputKey( event ) {\n  var message = null;\n\n  if( event.keyCode == 13 && this.value.trim().length > 0 ) {\n    message = {\n      content: this.value.trim(),\n    };\n    \n    // Send message\n    socket.send( JSON.stringify( message ) );  \n  }     \n}\n\n// Message\nfunction doSocketMessage( message ) {\n  var data = null;\n  var element = null;\n\n  // Parse\n  data = JSON.parse( message.data );\n\n  // Build and append\n  element = document.createElement( 'p' );\n  element.innerHTML = data.content;\n  document.body.appendChild( element );\n}\n\n\nGiven an HTML document, with an INPUT element inside it, this is functionally\nall you need for a chat application on the client side. As I mentioned earlier,\nyou could certainly get far more ambitious by adding custom avatars, inline\nimages, etc.\n\nInput\n\nIn order to have a chat client, we need a place for the user to enter some\ncontent. That is the INPUT element, though you could certainly use an editable\nDIV element as well. In fact, if you were building a game, the \"input\" might be\nan HTML5 Canvas\n[https://html.spec.whatwg.org/multipage/scripting.html#the-canvas-element] with\na thumb control and buttons.\n\nWebSocket\n\nThere are polyfills for WebSocket, but it has been around now for some time, and\nis broadly available [http://caniuse.com/#feat=websockets] across desktops and\ndevices. Notice that rather than \"http://\" we tell the socket to connect to \"\nws://\" - this is part of how the handshake works. Just as there is \"https://\"\nthere is also \"wss://\" in the WebSocket world.\n\nBecause I am often developing locally, pushing to IBM Bluemix, testing, and then\nmaking changes locally again before pushing a finished build, I use \"\nwindow.location.host\" to abstract the server for me. However, WebSocket\nconnections can be made cross-domain. This is an implementation detail of the\nserver and handshake.\n\nKeyboard\n\nRather than provide a button to send a message, most chat applications these\ndays (from Google Hangouts\n[https://www.google.com/search?client=safari&rls=en&q=google+hangouts&ie=UTF-8&oe=UTF-8] \nto iOS Messages [http://www.apple.com/ios/whats-new/messages]) use the return\nkey to trigger sending a message. This is easy enough to do, but I also check to\nmake sure there is at least some content to send (where this would otherwise be\nan dis/enabled button).\n\nThe data I am sending across the WebSocket is a JSON object. The format of that\nJSON object - that is to say the properties it has - are completely up to you. I\nwould suggest that many smaller message generally work better than fewer longer\nones, but the problem you are trying to solve may differ.\n\nYou could for example, store an update to the entire application model in a\ndatabase, and then send a WebSocket message to tell the other applications to\npull the latest model. The number of interesting techniques you can employ as a\ndeveloper expands considerably when you leverage WebSocket.\n\nMessage\n\nWhen a message arrives from a WebSocket server, the data itself is actually on a\n\"data\" property. There are other properties on the event itself, that are\ninjected along the way. In this case, the \"message\" object from the previous\nstep is now found at \"message.data\". We parse that string and populate the DOM.\n\nNext Steps\nWebSocket brings with it a vast array of new options. It is not however without\nproblems. Problems that can be solved, to be sure, but problems that take real\nconsideration up front to make sure that you have a system that will last as\nlong as you want it to. Some of those considerations might include:\n\n * Given that the \"message\" object in our application can take on any form we\n   desire, this could quickly lead to a brittle dependency between systems.\n * We also have no guarantee that the message we sent actually made it to the\n   server - or the other clients.\n * We have no presence awareness here either - clients can come and go, and your\n   application would never know the difference. If you want to show who is in a\n   chat room, this presents a real challenge.\n * What if the server goes down while delivering a message? What about all those\n   abandoned disconnected clients that do not know the server went down?\n\nAs it turns out, using WebSocket introduces a pattern that has long since been\naround in enterprise system design known as, publish-subscribe (message\nbrokers). All of these problems can be solved by leveraging a more robust server\nand message protocol (format of the data). There are even cloud offerings for\nthis pattern, such as the IBM Partner, PubNub [http://pubnub.com].\n\nImage courtesy of Wikipedia [https://en.wikipedia.org/wiki/Tap_and_die].","feature_image":"http://images.kevinhoyt.com/threading.dies.jpg","featured":0,"status":"published","locale":null,"visibility":"public","author_id":"1","created_at":"2015-08-27T14:29:43.000Z","updated_at":"2015-09-02T00:25:21.000Z","published_at":"2015-09-02T00:25:21.000Z","custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"type":"post","email_recipient_filter":"none"},{"id":"5adb8922351ffe0018a57737","uuid":"cb0bd681-edbe-4a78-b609-85557ffd2baf","title":"Decoupling IoT Rendering","slug":"decoupling-iot-rendering","mobiledoc":"{\"version\":\"0.3.1\",\"markups\":[],\"atoms\":[],\"cards\":[[\"markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\"I recently received a comment from a visitor using one of my Arduino-to-WebSocket examples.  They had a requirement for a specific charting library.  The problem is that the library could not handle the data rates coming from the Arduino.  In this post I break down why that is, and give you a couple possible solutions.\\n\\n###Arduino\\n\\nJust so we are all on the same page, the Arduino code that this visitor was using looks something like the following snippet.\\n\\n```\\nconst int PHOTOCELL = 0;\\n\\nvoid setup() \\n{\\n  Serial.begin( 9600 );\\n  pinMode( PHOTOCELL, INPUT );\\n}\\n\\nvoid loop() \\n{\\n  int light = 0;\\n\\n  light = analogRead( PHOTOCELL );  \\n  Serial.println( light );\\n  delay( 10 );\\n }\\n```\\n\\nThe key thing to notice here is that the data is being sent at a rate of ever 10 milliseconds.\\n\\nYou might also notice that there is no WebSocket in this code.  While you can put a wireless shield on an Arduino, and connect it directly to a WebSocket server, in this case, the Arduino is standalone, and only providing serial output of the data.\\n\\n###WebSocket\\n\\nOn the other side of the serial port is a Node.js-based WebSocket solution.  The person who sent me this question was using a WebSocket server, with a splash of HTTP for serving of web assets such as the page that would render the data.  You could also just use a WebSocket server from Node.js that connected to a WebSocket server elsewhere (perhaps even running on Node.js).\\n\\n> When I am connected to devices from Node.js over the serial port, I use the excellent [Node.js Serial Port](https://github.com/voodootikigod/node-serialport) package.  \\n\\nIn this case, the serial port code looks for the newline character injected by the Arduino Serial.println() function.  It then splits that off as a line, and sends the line across the serial port.  The WebSocket implementation gets that line, does some formatting into JSON, and then sends it as a message to all the connected clients.\\n\\nThe code for the WebSocket implementation is not particularly important for this discussion.  If you are interested, you can check it out in my [GitHub repository](https://github.com/krhoyt/Arduino/blob/master/photocell/photocell.js).  The code is a bit older, and I would do things slightly different today, but this is the basis of the code that this person was using.\\n\\n###Charting\\n\\nThe requirement that this individual had was to use the [JustGage](http://justgage.com) library.  This library presents a simple gauge chart with title and value.  It is rendered using SVG, which is definitely my preference for clean charts across platforms.\\n\\n![JustGage library screenshot.](http://images.kevinhoyt.com/justgage.jpg)\\n\\nIn looking at the source, the library uses Rafael for SVG rendering.  It also exposes a number of configuration properties.  One of those properties is the time used for animation.  Even with this set to zero, the library will stutter with a sub-second data rate.\\n\\n```\\nvar gauge = new JustGage( {\\n  id: 'gauge',\\n  value: 67,\\n  min: 0,\\n  max: 100,\\n  title: 'Visitors'\\n} );\\n```\\n\\n###Rendering Problems\\n\\nIn this configuration, the Arduino does what it needs to without any problem.  Serial port access also has no problem getting the data at a sub-second rate.  The WebSocket in turn, also has nearly no lag in getting the data to the client.  The client WebSocket gets the data with minimal lag beyond that.\\n\\n*Put another way, the data delivery is happening without any latency problems.*\\n\\nThe problem surfaces when the library goes to render the changes.  To be clear, this is not specific to this charting library.  I have seen numerous chart libraries crumble under the weight of real-time data rendering.  In fact, most are not even designed to handle refreshing the chart without a page refresh.\\n\\nThe chart is configured by default to animate updates over one second.  Since the data hits it ten times that rate, the animation goes nuts.  It does still render, but it does so with what I would call \\\"stuttering artifacts\\\".\\n\\nThe developer kindly made the refresh animation rate available as a configuration parameter.  I turned this down to zero to tell the library not to render the changes at all.  The result was better rendering, but still with noticeable stuttering.\\n\\nNow what?!\\n\\n###Rendering Solutions\\n\\nI want to be clear here that this particular library is not at fault.  This is common, in my experience, across charting libraries.  When data arrives at a rate that is faster than rendering, this problem will surface.\\n\\nI have used two different solutions over the years to get around this problem.\\n\\n**Decouple Rendering**\\n\\nThe first approach to get around this problem is to decouple the arrival of data, with the rendering of the chart.  In the case of WebSocket then, you would get a message, and then put the data in a variable for later reference.  Then separate from the WebSocket messaging, you set up an interval (with either setInterval or requestAnaimationFrame).\\n\\nWith a setInterval approach, you might set the interval to be the same amount of time that an animated rendering is set to take.  The default for the aforementioned chart was one second.  So an interval rate of one second would give the chart time to update, and then immediately update it again.  The result is clean animation.\\n\\n```\\n// Global\\n// Update in separate workflow\\nvar value = 0;\\n\\n// Set interval\\nsetInterval( function() {\\n  gauge.refresh( value, 100 );\\n}, 1000 );\\n```\\n\\nThe problem with the approach of decoupling is that your data is no longer real-time.  It is certainly more real-time than making a background HTTP request (XHR), but the latency has dramatically increased to be sure.  If your requirements allow for a refresh rate of a second or more, than this is a great way to still get low-latency updates, that do not bring the rendering pipeline to its knees.\\n\\n**Do Not Use a Library**\\n\\nThe other approach I have used is not to leverage a charting library at all.  Managing the vast number of options that come with being a library lends itself to rendering overhead.  In the case of JustGage, there is even a dependency on Rafael for further abstraction.\\n\\n*The further away you get from the rendering pipeline, the slower rendering will be.*\\n\\nIn this case, the gauge could be rendered with just a handful of SVG elements.  Transforms could be applied to one or two of them at most.  In fact, a similar effect could be achieve in CSS alone if you had the option of leveraging the latest browser implementations.\\n\\nWhile there is no doubt a lot more work in rolling your own, the result is often much faster rendering.  I have been able to achieve charting with my own SVG that is more than capable of handling sub-second updates - even at a rate of 10x such as in this case.\\n\\n###Next Steps\\n\\nDecoupling rendering is one solution to managing the rendering pipeline.  However, it is possible to decouple the data delivery pipeline as well.  You could remove the delay entirely from the Arduino side, and cache the values in a variable on the other side of the serial port.  Then the WebSocket, set on an interval of its own, could incrementally send out the data.\\n\\n> Further, you may have an array of devices sending data.  If you have this requirement, then multiplexing the data is even one more step of optimization.\\n\\nWhere you do this, or if you decide to do both, is all about your requirements.  Most of the modern web is not designed with volumes of real-time data coming from an Internet of Things.  It still lives very much in a legacy world of request-response.  As the number of devices connected to the Internet continues to grow exponentially, I have no doubt that changes will come, but for now, hopefully these solutions will give your users the best possible experience.\\n\\n*Circuit board photo courtesy [Wikipedia](https://en.wikipedia.org/wiki/Printed_circuit_board).*\"}]],\"sections\":[[10,0]],\"ghostVersion\":\"3.0\"}","html":"<!--kg-card-begin: markdown--><p>I recently received a comment from a visitor using one of my Arduino-to-WebSocket examples.  They had a requirement for a specific charting library.  The problem is that the library could not handle the data rates coming from the Arduino.  In this post I break down why that is, and give you a couple possible solutions.</p>\n<h3 id=\"arduino\">Arduino</h3>\n<p>Just so we are all on the same page, the Arduino code that this visitor was using looks something like the following snippet.</p>\n<pre><code>const int PHOTOCELL = 0;\n\nvoid setup() \n{\n  Serial.begin( 9600 );\n  pinMode( PHOTOCELL, INPUT );\n}\n\nvoid loop() \n{\n  int light = 0;\n\n  light = analogRead( PHOTOCELL );  \n  Serial.println( light );\n  delay( 10 );\n }\n</code></pre>\n<p>The key thing to notice here is that the data is being sent at a rate of ever 10 milliseconds.</p>\n<p>You might also notice that there is no WebSocket in this code.  While you can put a wireless shield on an Arduino, and connect it directly to a WebSocket server, in this case, the Arduino is standalone, and only providing serial output of the data.</p>\n<h3 id=\"websocket\">WebSocket</h3>\n<p>On the other side of the serial port is a Node.js-based WebSocket solution.  The person who sent me this question was using a WebSocket server, with a splash of HTTP for serving of web assets such as the page that would render the data.  You could also just use a WebSocket server from Node.js that connected to a WebSocket server elsewhere (perhaps even running on Node.js).</p>\n<blockquote>\n<p>When I am connected to devices from Node.js over the serial port, I use the excellent <a href=\"https://github.com/voodootikigod/node-serialport\">Node.js Serial Port</a> package.</p>\n</blockquote>\n<p>In this case, the serial port code looks for the newline character injected by the Arduino Serial.println() function.  It then splits that off as a line, and sends the line across the serial port.  The WebSocket implementation gets that line, does some formatting into JSON, and then sends it as a message to all the connected clients.</p>\n<p>The code for the WebSocket implementation is not particularly important for this discussion.  If you are interested, you can check it out in my <a href=\"https://github.com/krhoyt/Arduino/blob/master/photocell/photocell.js\">GitHub repository</a>.  The code is a bit older, and I would do things slightly different today, but this is the basis of the code that this person was using.</p>\n<h3 id=\"charting\">Charting</h3>\n<p>The requirement that this individual had was to use the <a href=\"http://justgage.com\">JustGage</a> library.  This library presents a simple gauge chart with title and value.  It is rendered using SVG, which is definitely my preference for clean charts across platforms.</p>\n<p><img src=\"http://images.kevinhoyt.com/justgage.jpg\" alt=\"JustGage library screenshot.\" loading=\"lazy\"></p>\n<p>In looking at the source, the library uses Rafael for SVG rendering.  It also exposes a number of configuration properties.  One of those properties is the time used for animation.  Even with this set to zero, the library will stutter with a sub-second data rate.</p>\n<pre><code>var gauge = new JustGage( {\n  id: 'gauge',\n  value: 67,\n  min: 0,\n  max: 100,\n  title: 'Visitors'\n} );\n</code></pre>\n<h3 id=\"renderingproblems\">Rendering Problems</h3>\n<p>In this configuration, the Arduino does what it needs to without any problem.  Serial port access also has no problem getting the data at a sub-second rate.  The WebSocket in turn, also has nearly no lag in getting the data to the client.  The client WebSocket gets the data with minimal lag beyond that.</p>\n<p><em>Put another way, the data delivery is happening without any latency problems.</em></p>\n<p>The problem surfaces when the library goes to render the changes.  To be clear, this is not specific to this charting library.  I have seen numerous chart libraries crumble under the weight of real-time data rendering.  In fact, most are not even designed to handle refreshing the chart without a page refresh.</p>\n<p>The chart is configured by default to animate updates over one second.  Since the data hits it ten times that rate, the animation goes nuts.  It does still render, but it does so with what I would call &quot;stuttering artifacts&quot;.</p>\n<p>The developer kindly made the refresh animation rate available as a configuration parameter.  I turned this down to zero to tell the library not to render the changes at all.  The result was better rendering, but still with noticeable stuttering.</p>\n<p>Now what?!</p>\n<h3 id=\"renderingsolutions\">Rendering Solutions</h3>\n<p>I want to be clear here that this particular library is not at fault.  This is common, in my experience, across charting libraries.  When data arrives at a rate that is faster than rendering, this problem will surface.</p>\n<p>I have used two different solutions over the years to get around this problem.</p>\n<p><strong>Decouple Rendering</strong></p>\n<p>The first approach to get around this problem is to decouple the arrival of data, with the rendering of the chart.  In the case of WebSocket then, you would get a message, and then put the data in a variable for later reference.  Then separate from the WebSocket messaging, you set up an interval (with either setInterval or requestAnaimationFrame).</p>\n<p>With a setInterval approach, you might set the interval to be the same amount of time that an animated rendering is set to take.  The default for the aforementioned chart was one second.  So an interval rate of one second would give the chart time to update, and then immediately update it again.  The result is clean animation.</p>\n<pre><code>// Global\n// Update in separate workflow\nvar value = 0;\n\n// Set interval\nsetInterval( function() {\n  gauge.refresh( value, 100 );\n}, 1000 );\n</code></pre>\n<p>The problem with the approach of decoupling is that your data is no longer real-time.  It is certainly more real-time than making a background HTTP request (XHR), but the latency has dramatically increased to be sure.  If your requirements allow for a refresh rate of a second or more, than this is a great way to still get low-latency updates, that do not bring the rendering pipeline to its knees.</p>\n<p><strong>Do Not Use a Library</strong></p>\n<p>The other approach I have used is not to leverage a charting library at all.  Managing the vast number of options that come with being a library lends itself to rendering overhead.  In the case of JustGage, there is even a dependency on Rafael for further abstraction.</p>\n<p><em>The further away you get from the rendering pipeline, the slower rendering will be.</em></p>\n<p>In this case, the gauge could be rendered with just a handful of SVG elements.  Transforms could be applied to one or two of them at most.  In fact, a similar effect could be achieve in CSS alone if you had the option of leveraging the latest browser implementations.</p>\n<p>While there is no doubt a lot more work in rolling your own, the result is often much faster rendering.  I have been able to achieve charting with my own SVG that is more than capable of handling sub-second updates - even at a rate of 10x such as in this case.</p>\n<h3 id=\"nextsteps\">Next Steps</h3>\n<p>Decoupling rendering is one solution to managing the rendering pipeline.  However, it is possible to decouple the data delivery pipeline as well.  You could remove the delay entirely from the Arduino side, and cache the values in a variable on the other side of the serial port.  Then the WebSocket, set on an interval of its own, could incrementally send out the data.</p>\n<blockquote>\n<p>Further, you may have an array of devices sending data.  If you have this requirement, then multiplexing the data is even one more step of optimization.</p>\n</blockquote>\n<p>Where you do this, or if you decide to do both, is all about your requirements.  Most of the modern web is not designed with volumes of real-time data coming from an Internet of Things.  It still lives very much in a legacy world of request-response.  As the number of devices connected to the Internet continues to grow exponentially, I have no doubt that changes will come, but for now, hopefully these solutions will give your users the best possible experience.</p>\n<p><em>Circuit board photo courtesy <a href=\"https://en.wikipedia.org/wiki/Printed_circuit_board\">Wikipedia</a>.</em></p>\n<!--kg-card-end: markdown-->","comment_id":"46","plaintext":"I recently received a comment from a visitor using one of my\nArduino-to-WebSocket examples. They had a requirement for a specific charting\nlibrary. The problem is that the library could not handle the data rates coming\nfrom the Arduino. In this post I break down why that is, and give you a couple\npossible solutions.\n\nArduino\nJust so we are all on the same page, the Arduino code that this visitor was\nusing looks something like the following snippet.\n\nconst int PHOTOCELL = 0;\n\nvoid setup() \n{\n  Serial.begin( 9600 );\n  pinMode( PHOTOCELL, INPUT );\n}\n\nvoid loop() \n{\n  int light = 0;\n\n  light = analogRead( PHOTOCELL );  \n  Serial.println( light );\n  delay( 10 );\n }\n\n\nThe key thing to notice here is that the data is being sent at a rate of ever 10\nmilliseconds.\n\nYou might also notice that there is no WebSocket in this code. While you can put\na wireless shield on an Arduino, and connect it directly to a WebSocket server,\nin this case, the Arduino is standalone, and only providing serial output of the\ndata.\n\nWebSocket\nOn the other side of the serial port is a Node.js-based WebSocket solution. The\nperson who sent me this question was using a WebSocket server, with a splash of\nHTTP for serving of web assets such as the page that would render the data. You\ncould also just use a WebSocket server from Node.js that connected to a\nWebSocket server elsewhere (perhaps even running on Node.js).\n\n> When I am connected to devices from Node.js over the serial port, I use the\nexcellent Node.js Serial Port [https://github.com/voodootikigod/node-serialport] \npackage.\n\n\nIn this case, the serial port code looks for the newline character injected by\nthe Arduino Serial.println() function. It then splits that off as a line, and\nsends the line across the serial port. The WebSocket implementation gets that\nline, does some formatting into JSON, and then sends it as a message to all the\nconnected clients.\n\nThe code for the WebSocket implementation is not particularly important for this\ndiscussion. If you are interested, you can check it out in my GitHub repository\n[https://github.com/krhoyt/Arduino/blob/master/photocell/photocell.js]. The code\nis a bit older, and I would do things slightly different today, but this is the\nbasis of the code that this person was using.\n\nCharting\nThe requirement that this individual had was to use the JustGage\n[http://justgage.com] library. This library presents a simple gauge chart with\ntitle and value. It is rendered using SVG, which is definitely my preference for\nclean charts across platforms.\n\n\n\nIn looking at the source, the library uses Rafael for SVG rendering. It also\nexposes a number of configuration properties. One of those properties is the\ntime used for animation. Even with this set to zero, the library will stutter\nwith a sub-second data rate.\n\nvar gauge = new JustGage( {\n  id: 'gauge',\n  value: 67,\n  min: 0,\n  max: 100,\n  title: 'Visitors'\n} );\n\n\nRendering Problems\nIn this configuration, the Arduino does what it needs to without any problem.\nSerial port access also has no problem getting the data at a sub-second rate.\nThe WebSocket in turn, also has nearly no lag in getting the data to the client.\nThe client WebSocket gets the data with minimal lag beyond that.\n\nPut another way, the data delivery is happening without any latency problems.\n\nThe problem surfaces when the library goes to render the changes. To be clear,\nthis is not specific to this charting library. I have seen numerous chart\nlibraries crumble under the weight of real-time data rendering. In fact, most\nare not even designed to handle refreshing the chart without a page refresh.\n\nThe chart is configured by default to animate updates over one second. Since the\ndata hits it ten times that rate, the animation goes nuts. It does still render,\nbut it does so with what I would call \"stuttering artifacts\".\n\nThe developer kindly made the refresh animation rate available as a\nconfiguration parameter. I turned this down to zero to tell the library not to\nrender the changes at all. The result was better rendering, but still with\nnoticeable stuttering.\n\nNow what?!\n\nRendering Solutions\nI want to be clear here that this particular library is not at fault. This is\ncommon, in my experience, across charting libraries. When data arrives at a rate\nthat is faster than rendering, this problem will surface.\n\nI have used two different solutions over the years to get around this problem.\n\nDecouple Rendering\n\nThe first approach to get around this problem is to decouple the arrival of\ndata, with the rendering of the chart. In the case of WebSocket then, you would\nget a message, and then put the data in a variable for later reference. Then\nseparate from the WebSocket messaging, you set up an interval (with either\nsetInterval or requestAnaimationFrame).\n\nWith a setInterval approach, you might set the interval to be the same amount of\ntime that an animated rendering is set to take. The default for the\naforementioned chart was one second. So an interval rate of one second would\ngive the chart time to update, and then immediately update it again. The result\nis clean animation.\n\n// Global\n// Update in separate workflow\nvar value = 0;\n\n// Set interval\nsetInterval( function() {\n  gauge.refresh( value, 100 );\n}, 1000 );\n\n\nThe problem with the approach of decoupling is that your data is no longer\nreal-time. It is certainly more real-time than making a background HTTP request\n(XHR), but the latency has dramatically increased to be sure. If your\nrequirements allow for a refresh rate of a second or more, than this is a great\nway to still get low-latency updates, that do not bring the rendering pipeline\nto its knees.\n\nDo Not Use a Library\n\nThe other approach I have used is not to leverage a charting library at all.\nManaging the vast number of options that come with being a library lends itself\nto rendering overhead. In the case of JustGage, there is even a dependency on\nRafael for further abstraction.\n\nThe further away you get from the rendering pipeline, the slower rendering will\nbe.\n\nIn this case, the gauge could be rendered with just a handful of SVG elements.\nTransforms could be applied to one or two of them at most. In fact, a similar\neffect could be achieve in CSS alone if you had the option of leveraging the\nlatest browser implementations.\n\nWhile there is no doubt a lot more work in rolling your own, the result is often\nmuch faster rendering. I have been able to achieve charting with my own SVG that\nis more than capable of handling sub-second updates - even at a rate of 10x such\nas in this case.\n\nNext Steps\nDecoupling rendering is one solution to managing the rendering pipeline.\nHowever, it is possible to decouple the data delivery pipeline as well. You\ncould remove the delay entirely from the Arduino side, and cache the values in a\nvariable on the other side of the serial port. Then the WebSocket, set on an\ninterval of its own, could incrementally send out the data.\n\n> Further, you may have an array of devices sending data. If you have this\nrequirement, then multiplexing the data is even one more step of optimization.\n\n\nWhere you do this, or if you decide to do both, is all about your requirements.\nMost of the modern web is not designed with volumes of real-time data coming\nfrom an Internet of Things. It still lives very much in a legacy world of\nrequest-response. As the number of devices connected to the Internet continues\nto grow exponentially, I have no doubt that changes will come, but for now,\nhopefully these solutions will give your users the best possible experience.\n\nCircuit board photo courtesy Wikipedia\n[https://en.wikipedia.org/wiki/Printed_circuit_board].","feature_image":"http://images.kevinhoyt.com/circuit.board.jpg","featured":0,"status":"published","locale":null,"visibility":"public","author_id":"1","created_at":"2015-09-04T19:54:28.000Z","updated_at":"2015-09-09T00:33:38.000Z","published_at":"2015-09-09T00:33:38.000Z","custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"type":"post","email_recipient_filter":"none"},{"id":"5adb8922351ffe0018a57738","uuid":"6c942eed-81eb-4a96-bd1f-44b22a1199a6","title":"WebSocket on Android","slug":"websocket-on-android","mobiledoc":"{\"version\":\"0.3.1\",\"markups\":[],\"atoms\":[],\"cards\":[[\"markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\"In a previous post, I talked about hosting a WebSocket server on IBM Bluemix.  With the help of a little Node.js on the server, and a browser in hand, we were able to create a functional chat program.  Since WebSocket is an open standard however, we can talk to more than just the browser (despite the name).  In this post, I will show you how to create a native Android application that uses the same WebSocket server to participate in a chat with the browser.\\n\\n###Handshake\\n\\nWebSocket is an interesting topic not only because of the ==real-time== capabilities is can bring to your application, but also because of how it functions.  When opening a WebSocket from the browser, ==the initial contact with the server is an HTTP request==.  Assuming the web server knows what to do with that request, it is then \\\"upgraded\\\" to a WebSocket.  This is called the \\\"handshake\\\".\\n\\nAt that point, the WebSocket connection ==stays open== (as opposed to a traditional HTTP request that is closed after the response).  WebSocket can handle not only ==textual content==, but also ==binary content==.  One could effectively build a telnet client using WebSocket, if the target server knew WebSocket.\\n\\nAn interesting aspect of this is that since ==WebSocket is a standard==, there is nothing prohibiting a server from implementing that handshake.  In fact, ==many server applications== do (message brokers for example), and more regularly add support.  But this does not have to happen just at the server, it can also happen ==from other clients== that implement WebSocket.\\n\\n###Android\\n\\nClearly, Android is capable of speaking raw sockets, HTTP, and much more.  It is then certainly capable of speaking WebSocket.  Indeed, a company called [Tavendo](http://tavendo.com), which focuses on real-time data, has an open source framework, called [Autobahn](http://autobahn.ws), which implements ==WebSocket on Android== (and many others).  It is tested, reliable, and fast.\\n\\nAs much as [Gradle](http://gradle.org) has changed my Android development workflow (specifically, the integration with Android Studio), I am an old school Java guy at heart, and still very much enjoy a ==good old fashioned JAR== to deploy into my project - and that is exactly what Autobahn provides.  You will first need to ==download that library==, and add it to your Android project.\\n\\n```\\nprivate final WebSocketConnection   socket = new WebSocketConnection();\\n\\n...\\n\\ntry {\\n  socket.connect(BLUEMIX, new WebSocketHandler() {\\n    @Override\\n    public void onOpen() {\\n      // Debug\\n      Log.d(\\\"WEBSOCKETS\\\", \\\"Connected to server.\\\");\\n    }\\n\\n    @Override\\n    public void onTextMessage(String payload) {\\n      // Debug\\n      Log.d(\\\"WEBSOCKETS\\\", payload);\\n    }\\n\\n    @Override\\n    public void onClose(int code, String reason) {\\n      // Debug\\n      Log.d(\\\"WEBSOCKETS\\\", \\\"Connection lost.\\\");\\n    }\\n  });\\n} catch(WebSocketException wse) {\\n  Log.d(\\\"WEBSOCKETS\\\", wse.getMessage());\\n}\\n```\\n\\nThe declaration of the WebSocket client is declared \\\"final\\\" to ==avoid memory leaks==.  After that, you can call the \\\"connect()\\\" method and pass it a \\\"WebSocketHandler\\\" instance to get going.  You can override those events you are interested in hearing about.\\n\\nSince the WebSocket chat client (and server) I demonstrated earlier is ==JSON-based==, we are going to be interested in ==textual content==.  Since we already have that infrastructure in place, let us start there in inserting the rest of our Android chat client.\\n\\n```\\n@Override\\npublic void onTextMessage(String payload) {\\n  Bundle      bundle;\\n  Message     message;\\n\\n  // Debug\\n  Log.d(\\\"WEBSOCKETS\\\", payload);\\n\\n  bundle = new Bundle();\\n  bundle.putString(KEY_PAYLOAD, payload);\\n\\n  message = new Message();\\n  message.setData(bundle);\\n\\n  handler.sendMessage(message);\\n}\\n\\n...\\n\\ntry {\\n  // JSON object\\n  data = new JSONObject(bundle.getString(KEY_PAYLOAD));\\n\\n  item = data.getJSONObject(KEY_DATA);\\n\\n  chat = new ChatMessage();\\n  chat.client = item.getString(ChatMessage.KEY_CLIENT);\\n  chat.red = item.getInt(ChatMessage.KEY_RED);\\n  chat.green = item.getInt(ChatMessage.KEY_GREEN);\\n  chat.blue = item.getInt(ChatMessage.KEY_BLUE);\\n  chat.message = item.getString(ChatMessage.KEY_MESSAGE);\\n\\n  items.add(chat);\\n} catch(JSONException jsone) {\\n  jsone.printStackTrace();\\n}\\n\\n// Update render\\nadapter.notifyDataSetChanged();\\n\\n// Scroll to bottom\\n// Most recent message\\nlstHistory.smoothScrollToPosition(adapter.getCount() - 1);\\n```\\n\\nBecause the WebSocket client communication is ==not happening on the UI thread== (a good thing), we need to send a message to interested \\\"Handler\\\" implementations.  This is how Android communicates across threads, and it is very reminiscent of the ==publish-subscribe== pattern found in message brokers.  To do this we will simply take the unparsed JSON string, put it into a \\\"Bundle\\\" instance, place that in a \\\"Message\\\" instance, and send it along.\\n\\n> In an [earlier post](http://blog.kevinhoyt.com/2015/08/27/iot-weather-on-android-2/) I mentioned how to use the WeakHandler library from Badoo to avoid memory leaks in your Android applications.\\n\\nWhen the data arrives at the \\\"Handler\\\" instance, it is then ==parsed from JSON== into a \\\"JSONObject\\\" and then ==marshaled into a Java object== meant to hold chat details (POJO).  Once the chat object has been populated, it is placed into an \\\"ArrayList\\\" instance, which is in turn the source for a \\\"ListView\\\".  To make sure the new chat shows up on the screen, we notify the adapter of the changes.  Finally, we scroll the to bottom of the \\\"ListView\\\" to show the recent addition.\\n\\n```\\ntxtContent.setOnKeyListener(new View.OnKeyListener() {\\n  @Override\\n  public boolean onKey(View v, int keyCode, KeyEvent event) {\\n    ChatMessage chat;\\n\\n    // Send\\n    if((event.getAction() == KeyEvent.ACTION_DOWN) && (keyCode == KeyEvent.KEYCODE_ENTER)) {\\n      // Message present\\n      if(txtContent.getText().toString().trim().length() > 0) {\\n        // Build message\\n        chat = new ChatMessage();\\n        chat.action = ChatMessage.ACTION_CREATE;\\n        chat.client = client;\\n        chat.red = red;\\n        chat.green = green;\\n        chat.blue = blue;\\n        chat.message = txtContent.getText().toString();\\n        chat.css = \\\"rgb( \\\" + red + \\\", \\\" + green + \\\", \\\" + blue + \\\" )\\\";\\n\\n        // Publish\\n        socket.sendTextMessage(chat.toJSON());\\n\\n        // Clear field\\n        txtContent.setText(null);\\n      }\\n    }\\n\\n    return false;\\n  }\\n});\\n```\\n\\nTo ==send a message== we will capture the \\\"return\\\" key on the Android keyboard.  I put all the pertinent data into an object reflecting the chat message - the same object I used to marshal incoming data.  I added to this object a \\\"ChatMessage.toJSON()\\\" method that ==serializes the data into a JSON string==.  \\n\\n> It should be mentioned that there are many other great ways to marshal data between Java objects and JSON strings.  I use a manual approach to keep dependencies in my demos to a minimum.\\n\\nThe actual sending of the message happens with a call to \\\"==socket.sendTextMessage()==\\\".  From there the message field is cleared.  At this point, the Android client we just built, and the Web client from my previous posts will receive and render the message.  The user interface may look similar, but one is Web and the other native Android.\\n\\n![Android and Web chat.](http://images.kevinhoyt.com/android.websocket.chat.jpg)\\n\\n###Next Steps\\n\\n==Modernization== always calls the enterprise developer.  ==Open standards==, regardless of the stack in which they evolve, are ==immensely helpful== in that process.  If your stack needs ==real-time data== communication, then you can certainly look to the ==WebSocket== standard to help.  Do not let the term \\\"web\\\" confuse you about the possibilities and/or options.\\n\\nYou may have noticed that I use a ==key== in the chat object called \\\"==action==\\\".  In the code above there is no obvious use.  The action is used in the broader example (which you can find on my [GitHub](https://github.com/krhoyt/IBM/tree/master/cloud/websocket) repository), to ==differentiate what it is the client wants==.  For example, the server, running on [IBM Bluemix](http://bluemix.net), stores chat history in [IBM Compose](http://compose.io).  A command issued over the WebSocket can invoke getting the chat history, but needs some key to differentiate it between creating a new chat message.\\n\\n*Image courtesy of [Wikipedia](https://en.wikipedia.org/wiki/Tap_and_die).*\"}]],\"sections\":[[10,0]],\"ghostVersion\":\"3.0\"}","html":"<!--kg-card-begin: markdown--><p>In a previous post, I talked about hosting a WebSocket server on IBM Bluemix.  With the help of a little Node.js on the server, and a browser in hand, we were able to create a functional chat program.  Since WebSocket is an open standard however, we can talk to more than just the browser (despite the name).  In this post, I will show you how to create a native Android application that uses the same WebSocket server to participate in a chat with the browser.</p>\n<h3 id=\"handshake\">Handshake</h3>\n<p>WebSocket is an interesting topic not only because of the <mark>real-time</mark> capabilities is can bring to your application, but also because of how it functions.  When opening a WebSocket from the browser, <mark>the initial contact with the server is an HTTP request</mark>.  Assuming the web server knows what to do with that request, it is then &quot;upgraded&quot; to a WebSocket.  This is called the &quot;handshake&quot;.</p>\n<p>At that point, the WebSocket connection <mark>stays open</mark> (as opposed to a traditional HTTP request that is closed after the response).  WebSocket can handle not only <mark>textual content</mark>, but also <mark>binary content</mark>.  One could effectively build a telnet client using WebSocket, if the target server knew WebSocket.</p>\n<p>An interesting aspect of this is that since <mark>WebSocket is a standard</mark>, there is nothing prohibiting a server from implementing that handshake.  In fact, <mark>many server applications</mark> do (message brokers for example), and more regularly add support.  But this does not have to happen just at the server, it can also happen <mark>from other clients</mark> that implement WebSocket.</p>\n<h3 id=\"android\">Android</h3>\n<p>Clearly, Android is capable of speaking raw sockets, HTTP, and much more.  It is then certainly capable of speaking WebSocket.  Indeed, a company called <a href=\"http://tavendo.com\">Tavendo</a>, which focuses on real-time data, has an open source framework, called <a href=\"http://autobahn.ws\">Autobahn</a>, which implements <mark>WebSocket on Android</mark> (and many others).  It is tested, reliable, and fast.</p>\n<p>As much as <a href=\"http://gradle.org\">Gradle</a> has changed my Android development workflow (specifically, the integration with Android Studio), I am an old school Java guy at heart, and still very much enjoy a <mark>good old fashioned JAR</mark> to deploy into my project - and that is exactly what Autobahn provides.  You will first need to <mark>download that library</mark>, and add it to your Android project.</p>\n<pre><code>private final WebSocketConnection   socket = new WebSocketConnection();\n\n...\n\ntry {\n  socket.connect(BLUEMIX, new WebSocketHandler() {\n    @Override\n    public void onOpen() {\n      // Debug\n      Log.d(&quot;WEBSOCKETS&quot;, &quot;Connected to server.&quot;);\n    }\n\n    @Override\n    public void onTextMessage(String payload) {\n      // Debug\n      Log.d(&quot;WEBSOCKETS&quot;, payload);\n    }\n\n    @Override\n    public void onClose(int code, String reason) {\n      // Debug\n      Log.d(&quot;WEBSOCKETS&quot;, &quot;Connection lost.&quot;);\n    }\n  });\n} catch(WebSocketException wse) {\n  Log.d(&quot;WEBSOCKETS&quot;, wse.getMessage());\n}\n</code></pre>\n<p>The declaration of the WebSocket client is declared &quot;final&quot; to <mark>avoid memory leaks</mark>.  After that, you can call the &quot;connect()&quot; method and pass it a &quot;WebSocketHandler&quot; instance to get going.  You can override those events you are interested in hearing about.</p>\n<p>Since the WebSocket chat client (and server) I demonstrated earlier is <mark>JSON-based</mark>, we are going to be interested in <mark>textual content</mark>.  Since we already have that infrastructure in place, let us start there in inserting the rest of our Android chat client.</p>\n<pre><code>@Override\npublic void onTextMessage(String payload) {\n  Bundle      bundle;\n  Message     message;\n\n  // Debug\n  Log.d(&quot;WEBSOCKETS&quot;, payload);\n\n  bundle = new Bundle();\n  bundle.putString(KEY_PAYLOAD, payload);\n\n  message = new Message();\n  message.setData(bundle);\n\n  handler.sendMessage(message);\n}\n\n...\n\ntry {\n  // JSON object\n  data = new JSONObject(bundle.getString(KEY_PAYLOAD));\n\n  item = data.getJSONObject(KEY_DATA);\n\n  chat = new ChatMessage();\n  chat.client = item.getString(ChatMessage.KEY_CLIENT);\n  chat.red = item.getInt(ChatMessage.KEY_RED);\n  chat.green = item.getInt(ChatMessage.KEY_GREEN);\n  chat.blue = item.getInt(ChatMessage.KEY_BLUE);\n  chat.message = item.getString(ChatMessage.KEY_MESSAGE);\n\n  items.add(chat);\n} catch(JSONException jsone) {\n  jsone.printStackTrace();\n}\n\n// Update render\nadapter.notifyDataSetChanged();\n\n// Scroll to bottom\n// Most recent message\nlstHistory.smoothScrollToPosition(adapter.getCount() - 1);\n</code></pre>\n<p>Because the WebSocket client communication is <mark>not happening on the UI thread</mark> (a good thing), we need to send a message to interested &quot;Handler&quot; implementations.  This is how Android communicates across threads, and it is very reminiscent of the <mark>publish-subscribe</mark> pattern found in message brokers.  To do this we will simply take the unparsed JSON string, put it into a &quot;Bundle&quot; instance, place that in a &quot;Message&quot; instance, and send it along.</p>\n<blockquote>\n<p>In an <a href=\"http://blog.kevinhoyt.com/2015/08/27/iot-weather-on-android-2/\">earlier post</a> I mentioned how to use the WeakHandler library from Badoo to avoid memory leaks in your Android applications.</p>\n</blockquote>\n<p>When the data arrives at the &quot;Handler&quot; instance, it is then <mark>parsed from JSON</mark> into a &quot;JSONObject&quot; and then <mark>marshaled into a Java object</mark> meant to hold chat details (POJO).  Once the chat object has been populated, it is placed into an &quot;ArrayList&quot; instance, which is in turn the source for a &quot;ListView&quot;.  To make sure the new chat shows up on the screen, we notify the adapter of the changes.  Finally, we scroll the to bottom of the &quot;ListView&quot; to show the recent addition.</p>\n<pre><code>txtContent.setOnKeyListener(new View.OnKeyListener() {\n  @Override\n  public boolean onKey(View v, int keyCode, KeyEvent event) {\n    ChatMessage chat;\n\n    // Send\n    if((event.getAction() == KeyEvent.ACTION_DOWN) &amp;&amp; (keyCode == KeyEvent.KEYCODE_ENTER)) {\n      // Message present\n      if(txtContent.getText().toString().trim().length() &gt; 0) {\n        // Build message\n        chat = new ChatMessage();\n        chat.action = ChatMessage.ACTION_CREATE;\n        chat.client = client;\n        chat.red = red;\n        chat.green = green;\n        chat.blue = blue;\n        chat.message = txtContent.getText().toString();\n        chat.css = &quot;rgb( &quot; + red + &quot;, &quot; + green + &quot;, &quot; + blue + &quot; )&quot;;\n\n        // Publish\n        socket.sendTextMessage(chat.toJSON());\n\n        // Clear field\n        txtContent.setText(null);\n      }\n    }\n\n    return false;\n  }\n});\n</code></pre>\n<p>To <mark>send a message</mark> we will capture the &quot;return&quot; key on the Android keyboard.  I put all the pertinent data into an object reflecting the chat message - the same object I used to marshal incoming data.  I added to this object a &quot;ChatMessage.toJSON()&quot; method that <mark>serializes the data into a JSON string</mark>.</p>\n<blockquote>\n<p>It should be mentioned that there are many other great ways to marshal data between Java objects and JSON strings.  I use a manual approach to keep dependencies in my demos to a minimum.</p>\n</blockquote>\n<p>The actual sending of the message happens with a call to &quot;<mark>socket.sendTextMessage()</mark>&quot;.  From there the message field is cleared.  At this point, the Android client we just built, and the Web client from my previous posts will receive and render the message.  The user interface may look similar, but one is Web and the other native Android.</p>\n<p><img src=\"http://images.kevinhoyt.com/android.websocket.chat.jpg\" alt=\"Android and Web chat.\" loading=\"lazy\"></p>\n<h3 id=\"nextsteps\">Next Steps</h3>\n<p><mark>Modernization</mark> always calls the enterprise developer.  <mark>Open standards</mark>, regardless of the stack in which they evolve, are <mark>immensely helpful</mark> in that process.  If your stack needs <mark>real-time data</mark> communication, then you can certainly look to the <mark>WebSocket</mark> standard to help.  Do not let the term &quot;web&quot; confuse you about the possibilities and/or options.</p>\n<p>You may have noticed that I use a <mark>key</mark> in the chat object called &quot;<mark>action</mark>&quot;.  In the code above there is no obvious use.  The action is used in the broader example (which you can find on my <a href=\"https://github.com/krhoyt/IBM/tree/master/cloud/websocket\">GitHub</a> repository), to <mark>differentiate what it is the client wants</mark>.  For example, the server, running on <a href=\"http://bluemix.net\">IBM Bluemix</a>, stores chat history in <a href=\"http://compose.io\">IBM Compose</a>.  A command issued over the WebSocket can invoke getting the chat history, but needs some key to differentiate it between creating a new chat message.</p>\n<p><em>Image courtesy of <a href=\"https://en.wikipedia.org/wiki/Tap_and_die\">Wikipedia</a>.</em></p>\n<!--kg-card-end: markdown-->","comment_id":"47","plaintext":"In a previous post, I talked about hosting a WebSocket server on IBM Bluemix.\nWith the help of a little Node.js on the server, and a browser in hand, we were\nable to create a functional chat program. Since WebSocket is an open standard\nhowever, we can talk to more than just the browser (despite the name). In this\npost, I will show you how to create a native Android application that uses the\nsame WebSocket server to participate in a chat with the browser.\n\nHandshake\nWebSocket is an interesting topic not only because of the real-time capabilities\nis can bring to your application, but also because of how it functions. When\nopening a WebSocket from the browser, the initial contact with the server is an\nHTTP request. Assuming the web server knows what to do with that request, it is\nthen \"upgraded\" to a WebSocket. This is called the \"handshake\".\n\nAt that point, the WebSocket connection stays open (as opposed to a traditional\nHTTP request that is closed after the response). WebSocket can handle not only \ntextual content, but also binary content. One could effectively build a telnet\nclient using WebSocket, if the target server knew WebSocket.\n\nAn interesting aspect of this is that since WebSocket is a standard, there is\nnothing prohibiting a server from implementing that handshake. In fact, many\nserver applications do (message brokers for example), and more regularly add\nsupport. But this does not have to happen just at the server, it can also happen \nfrom other clients that implement WebSocket.\n\nAndroid\nClearly, Android is capable of speaking raw sockets, HTTP, and much more. It is\nthen certainly capable of speaking WebSocket. Indeed, a company called Tavendo\n[http://tavendo.com], which focuses on real-time data, has an open source\nframework, called Autobahn [http://autobahn.ws], which implements WebSocket on\nAndroid (and many others). It is tested, reliable, and fast.\n\nAs much as Gradle [http://gradle.org] has changed my Android development\nworkflow (specifically, the integration with Android Studio), I am an old school\nJava guy at heart, and still very much enjoy a good old fashioned JAR to deploy\ninto my project - and that is exactly what Autobahn provides. You will first\nneed to download that library, and add it to your Android project.\n\nprivate final WebSocketConnection   socket = new WebSocketConnection();\n\n...\n\ntry {\n  socket.connect(BLUEMIX, new WebSocketHandler() {\n    @Override\n    public void onOpen() {\n      // Debug\n      Log.d(\"WEBSOCKETS\", \"Connected to server.\");\n    }\n\n    @Override\n    public void onTextMessage(String payload) {\n      // Debug\n      Log.d(\"WEBSOCKETS\", payload);\n    }\n\n    @Override\n    public void onClose(int code, String reason) {\n      // Debug\n      Log.d(\"WEBSOCKETS\", \"Connection lost.\");\n    }\n  });\n} catch(WebSocketException wse) {\n  Log.d(\"WEBSOCKETS\", wse.getMessage());\n}\n\n\nThe declaration of the WebSocket client is declared \"final\" to avoid memory\nleaks. After that, you can call the \"connect()\" method and pass it a\n\"WebSocketHandler\" instance to get going. You can override those events you are\ninterested in hearing about.\n\nSince the WebSocket chat client (and server) I demonstrated earlier is \nJSON-based, we are going to be interested in textual content. Since we already\nhave that infrastructure in place, let us start there in inserting the rest of\nour Android chat client.\n\n@Override\npublic void onTextMessage(String payload) {\n  Bundle      bundle;\n  Message     message;\n\n  // Debug\n  Log.d(\"WEBSOCKETS\", payload);\n\n  bundle = new Bundle();\n  bundle.putString(KEY_PAYLOAD, payload);\n\n  message = new Message();\n  message.setData(bundle);\n\n  handler.sendMessage(message);\n}\n\n...\n\ntry {\n  // JSON object\n  data = new JSONObject(bundle.getString(KEY_PAYLOAD));\n\n  item = data.getJSONObject(KEY_DATA);\n\n  chat = new ChatMessage();\n  chat.client = item.getString(ChatMessage.KEY_CLIENT);\n  chat.red = item.getInt(ChatMessage.KEY_RED);\n  chat.green = item.getInt(ChatMessage.KEY_GREEN);\n  chat.blue = item.getInt(ChatMessage.KEY_BLUE);\n  chat.message = item.getString(ChatMessage.KEY_MESSAGE);\n\n  items.add(chat);\n} catch(JSONException jsone) {\n  jsone.printStackTrace();\n}\n\n// Update render\nadapter.notifyDataSetChanged();\n\n// Scroll to bottom\n// Most recent message\nlstHistory.smoothScrollToPosition(adapter.getCount() - 1);\n\n\nBecause the WebSocket client communication is not happening on the UI thread (a\ngood thing), we need to send a message to interested \"Handler\" implementations.\nThis is how Android communicates across threads, and it is very reminiscent of\nthe publish-subscribe pattern found in message brokers. To do this we will\nsimply take the unparsed JSON string, put it into a \"Bundle\" instance, place\nthat in a \"Message\" instance, and send it along.\n\n> In an earlier post\n[http://blog.kevinhoyt.com/2015/08/27/iot-weather-on-android-2/] I mentioned how\nto use the WeakHandler library from Badoo to avoid memory leaks in your Android\napplications.\n\n\nWhen the data arrives at the \"Handler\" instance, it is then parsed from JSON \ninto a \"JSONObject\" and then marshaled into a Java object meant to hold chat\ndetails (POJO). Once the chat object has been populated, it is placed into an\n\"ArrayList\" instance, which is in turn the source for a \"ListView\". To make sure\nthe new chat shows up on the screen, we notify the adapter of the changes.\nFinally, we scroll the to bottom of the \"ListView\" to show the recent addition.\n\ntxtContent.setOnKeyListener(new View.OnKeyListener() {\n  @Override\n  public boolean onKey(View v, int keyCode, KeyEvent event) {\n    ChatMessage chat;\n\n    // Send\n    if((event.getAction() == KeyEvent.ACTION_DOWN) && (keyCode == KeyEvent.KEYCODE_ENTER)) {\n      // Message present\n      if(txtContent.getText().toString().trim().length() > 0) {\n        // Build message\n        chat = new ChatMessage();\n        chat.action = ChatMessage.ACTION_CREATE;\n        chat.client = client;\n        chat.red = red;\n        chat.green = green;\n        chat.blue = blue;\n        chat.message = txtContent.getText().toString();\n        chat.css = \"rgb( \" + red + \", \" + green + \", \" + blue + \" )\";\n\n        // Publish\n        socket.sendTextMessage(chat.toJSON());\n\n        // Clear field\n        txtContent.setText(null);\n      }\n    }\n\n    return false;\n  }\n});\n\n\nTo send a message we will capture the \"return\" key on the Android keyboard. I\nput all the pertinent data into an object reflecting the chat message - the same\nobject I used to marshal incoming data. I added to this object a\n\"ChatMessage.toJSON()\" method that serializes the data into a JSON string.\n\n> It should be mentioned that there are many other great ways to marshal data\nbetween Java objects and JSON strings. I use a manual approach to keep\ndependencies in my demos to a minimum.\n\n\nThe actual sending of the message happens with a call to \"\nsocket.sendTextMessage()\". From there the message field is cleared. At this\npoint, the Android client we just built, and the Web client from my previous\nposts will receive and render the message. The user interface may look similar,\nbut one is Web and the other native Android.\n\n\n\nNext Steps\nModernization always calls the enterprise developer. Open standards, regardless\nof the stack in which they evolve, are immensely helpful in that process. If\nyour stack needs real-time data communication, then you can certainly look to\nthe WebSocket standard to help. Do not let the term \"web\" confuse you about the\npossibilities and/or options.\n\nYou may have noticed that I use a key in the chat object called \"action\". In the\ncode above there is no obvious use. The action is used in the broader example\n(which you can find on my GitHub\n[https://github.com/krhoyt/IBM/tree/master/cloud/websocket] repository), to \ndifferentiate what it is the client wants. For example, the server, running on \nIBM Bluemix [http://bluemix.net], stores chat history in IBM Compose\n[http://compose.io]. A command issued over the WebSocket can invoke getting the\nchat history, but needs some key to differentiate it between creating a new chat\nmessage.\n\nImage courtesy of Wikipedia [https://en.wikipedia.org/wiki/Tap_and_die].","feature_image":"http://images.kevinhoyt.com/threading.dies.jpg","featured":0,"status":"published","locale":null,"visibility":"public","author_id":"1","created_at":"2015-09-04T21:24:11.000Z","updated_at":"2017-09-27T17:58:20.000Z","published_at":"2015-09-16T15:02:04.000Z","custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"type":"post","email_recipient_filter":"none"},{"id":"5adb8922351ffe0018a57739","uuid":"40934fd1-6fb5-4fb5-bd89-00929e9a2ebd","title":"Node.js for Java EE Developers","slug":"node-js-for-java-ee-developers","mobiledoc":"{\"version\":\"0.3.1\",\"markups\":[],\"atoms\":[],\"cards\":[[\"markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\"http://www.oracle.com/technetwork/java/javaee/tech/index.html\\n\\nAs a Developer Advocate for IBM, I spend a lot of time at conferences.  In this role, you do not even have to attend the sessions to see where the technology landscape is shifting (to be clear, I thoroughly enjoy attending sessions as well).  Of late, sessions around ==Node.js== are bursting at the seams.\\n\\nOne can speculate on why this is, but it led me to thinking of the Java EE development I have done in the past.  Jumping stacks can be ==intimidating==, especially when your stack is as ==thorough== as Java EE, or as ==community-driven== as Node.js.  With that in mind, I put together the following thoughts on where Java EE developers might look for Node.js ==equivalents==.\\n\\n*Before I even start, let me be clear that this list is ==not all-inclusive==.  I am calling out the most prominent parts that I recall from my days of Java EE.  If you have additions or corrections, please leave a comment below, and I will update the post accordingly.*\\n\\n###Web Application Technologies\\n\\nGiven that Node.js is JavaScript, and JavaScript originated in the browser, it should be no surprise that it does pretty much anything Web related particularly well.\\n\\n**WebSocket**\\n\\nJust like JavaScript, ==WebSocket is born of the Web==, and there are many WebSocket implementations available for Node.js.  Probably the most popular is [Socket.IO](http://socket.io), which layers robust functionality on top of plain WebSocket.  If WebSocket is all you want however, then I personally like \\\"[ws](https://einaros.github.io/ws/)\\\".\\n\\n**JSON Processing**\\n\\nThe \\\"JS\\\" part of \\\"JSON\\\" stands for JavaScript, so it should be no surprise that JSON is natively supported in Node.js.  You will use ==JSON.parse()== and ==JSON.stringify()== to achieve this task.  The result is a JavaScript \\\"Object\\\" instance.  Properties are publicly exposed on that object, so there is ==no JSONObject marshaling==.\\n\\n**Servlet**\\n\\nJava Servlets functionally take an incoming HTTP request, process it, and return some data.  Often a servlet will ==function as a controller==, assembling data before passing it along to a JSP (JavaServer Page) for rendering.  The most direct equivalent in Node.js would likely be something along the lines of [routes](http://expressjs.com/guide/routing.html) in [Express](http://expressjs.com).\\n\\n**JavaServer Faces**\\n\\nThe Web brings with it an inherent ==statelessness==, which JavaServer Faces aims to abstract away from the developer.  At the same time, JSF provides a bundling of more complex ==component types== not generally provided for by Web standards.\\n\\n> I once heard JSF referred to as a \\\"Flash killer\\\".  Turns out that the true undoing of Flash was Apple.  Not that JSF is without its [critics](https://assets.thoughtworks.com/assets/technology-radar-jan-2014-en.pdf) and [champions](http://blog.primefaces.org/?p=3035).\\n\\nSolving these two problems with Node.js would tend more towards the ==client side== (browser).  Application frameworks such as [AngularJS](https://angularjs.org), and [React](https://facebook.github.io/react/) (from Facebook), seek to manage the statelessness, while modern standards such as [Shadow DOM](http://www.w3.org/TR/shadow-dom/) offer encapsulation of more complex interactions which surface in frameworks such as [Polymer](https://www.polymer-project.org/1.0/).\\n\\n**JavaServer Pages**\\n\\nAssuming that a Node.js Express route has been run, and data assembled, the terminology for JavaServer Pages would be a \\\"==template==\\\".  The most common equivalent then of JavaServer Pages in Node.js would be [Jade](http://jade-lang.com).  Everything from conditionals and includes, to iteration and inline code can be found in Jade.\\n\\n**JSP Tag Library**\\n\\nSCRIPT tag?  Components?\\n\\n###Enterprise Application Technologies\\n\\nWithout the enterprise, Java EE would just be Java SE (har, har).  It is in the enterprise that Java EE currently reigns supreme.  However, as Node.js continues to mature with offerings such as [StrongLoop](https://strongloop.com), and ==Oracle== continues to [lay off Java Evangelists](http://www.infoq.com/news/2015/09/oracle-purges-java-evangelists), the future of the enterprise is up for grabs.\\n\\n***Enterprise JavaBeans***\\n\\nEJB (Enterprise JavaBeans) are one of those technologies most Java EE developers love to hate.  The complexity of the early versions of the specification made it all but impractical for most applications.  To be sure, where it fit however, it fit great.\\n\\nEssentially an approach to distributed components, like [DCOM](https://en.wikipedia.org/wiki/Distributed_Component_Object_Model) before it, or [CORBA](https://en.wikipedia.org/wiki/Common_Object_Request_Broker_Architecture) before that, in Node.js this architectural approach most closely maps to the modern trend of [microservices](http://martinfowler.com/articles/microservices.html) - specifically containerization of services using technologies such as [Docker](https://www.docker.com).\\n\\n***Java Message Service***\\n\\nJMS (Java Message Service) is a bit of a misnomer, as it is really more of a protocol, than a true messaging service.  In Node.js the preferred equivalent technology would be to integrate to a ==message broker== using open standard protocols.  \\n\\n> Node.js is actually capable of implementing a message broker, in addition to a client, however specialized open source brokers are generally preferred.\\n\\nThe closest mapping between the functionality of JMS and an open standard would probably be [AMQP](https://en.wikipedia.org/wiki/Advanced_Message_Queuing_Protocol) (Advanced Message Queueing Protocol).  There are a number of brokers (both open and closed) that support AMQP, such as [RabbitMQ](http://www.rabbitmq.com).\\n\\nOnce free of JMS proper, you will find protocols designed for specific applications such as chat ([XMPP](https://en.wikipedia.org/wiki/XMPP)) or IoT ([MQTT](https://en.wikipedia.org/wiki/MQTT)).  Brokers also generally speak more than one protocol, and can even integrate with other open standards such as WebSocket.\\n\\n###Web Services Technologies\\n\\nOne might postulate that bringing JavaScript to the server was born out of the need to deliver web services, and the desire to do it with the same language that was used on the client.  With its asynchronous nature, Node.js makes a great fit.\\n\\n**JAX-RS**\\n\\nAt a basic level, JAX-RS would be similar to ==Express routing== done in a more formal manner.  This might include breaking the routes out to their own subclasses, and perhaps even breaking out model classes for each data type being represented.  Many of the ==JAX-RS annotations== will map very closely to Express routing.\\n\\n###Java EE-related Specifications in Java SE\\n\\nNot everything that exists in Java EE is solely Java EE.  Many Java SE applications need to parse XML, so you find that capability in the SE side of the house.  Today, Node.js does not make a differentiation between enterprise and non-enterprise use-cases, but you will still find support for XML and more.\\n\\n**XML Binding (JAXB)**\\n\\nBinding an XML tree to a JavaScript Object instance can be done using \\\"[xml2js](https://github.com/Leonidas-from-XIV/node-xml2js)\\\".  This package is particularly useful for Windows developers as it does not depend on \\\"node-gyp\\\" and therefore will not require you to install Visual Studio.\\n\\nIt is worth noting that \\\"xml2js\\\" uses \\\"[xmlbuilder](https://github.com/oozcitak/xmlbuilder-js/)\\\" for building an XML document from a JavaScript Object instance.  The \\\"xmlbuilder\\\" package is a useful utility in and of its own right, should you want to use it alone.\\n\\n**XML Processing (JAXP)**\\n\\nJava SE provides two main ways to process XML - ==SAX and DOM==.  \\n\\nThe ==SAX== approach iterates through an XML document, firing off events of various types as certain matches are found.  This results is far less memory consumption, but more tedious marshaling.  \\n\\nThe ==DOM== approach by comparison, takes the entire XML document, and loads it into memory.  It then gives you an API by which you can query the document nodes, regardless of where they may be in the document.\\n\\nIn Node.js the equivalent of SAX and DOM would be \\\"[sax js](https://github.com/isaacs/sax-js/)\\\" and \\\"[jsdom](https://github.com/isaacs/sax-js/)\\\".\\n\\n**Database Connectivity**\\n\\nDatabase connectivity in Node.js is not standardized.  There is also no common delineation of driver \\\"types\\\".  To most Node.js developers, a database package is chosen for the functionality that it provides on top of accessing the database proper, not by the means through which the database is accessed.  \\n\\n* Type 1: JDBC-ODBC Bridge\\n* Type 2: JDBC-Native API\\n* Type 3: JDBC-Pure Net Java \\n* Type 4: 100% Pure Java\\n\\n\\n\\nImplementation varies widely from Type 1 (JDBC-ODBC Bridge) to Type 4 (100% Pure Java).\\n\\nIn the interest of mapping to known Java SE terminology, what follows is a variety of database access packages grouped by their approach to connectivity.\\n\\n***Type 1: JDBC-ODBC Bridge***\\n\\nI was shocked (not judging), but somebody had actually created an ODBC package for Node.js called \\\"[node-odbc](https://github.com/wankdanker/node-odbc)\\\".\\n\\n***Type 2: JDBC-Native API***\\n\\nThis is where most Node.js database drivers fall in my experience.\\n\\n- [ODBC](https://github.com/wankdanker/node-odbc)\\n- [MS SQL](https://github.com/patriksimek/node-mssql)\\n- [MySQL](https://github.com/felixge/node-mysql)\\n- [Oracle](https://github.com/oracle/node-oracledb)\\n- [Postgres](https://github.com/brianc/node-postgres)\\n- [MongoDB](https://github.com/mongodb/node-mongodb-native)\\n- [Redis](https://github.com/NodeRedis/node_redis)\\n\\n\\n- Type 3: JDBC-Net Pure Java\\n- Type 4: 100% Pure Java\\n\\n- Web Application Technologies\\n - WebSocket (JSR 356)\\n - JSON Processing (JSR 353)\\n - Servlet (JSR 340)\\n - JavaServer Faces (JSR 344)\\n - JavaServer Pages (JSR 245)\\n - JSP Tag Library (JSR 52)\\n\\n- Enterprise Application Technologies\\n - Batch Applications (JSR 352)\\n - Concurrency Utilities (JSR 236)\\n - Dependency Injection (JSR 346)\\n - Bean Validation (JSR 349)\\n - Enterprise JavaBeans (JSR 345)\\n - Interceptors (JSR 318)\\n - Connector Architecture (JSR 322)\\n - Persistence (JSR 338)\\n - Common Annotations (JSR 250)\\n - Java Message Service (JSR 343)\\n - Java Transaction API (JSR 907)\\n - JavaMail (JSR 919)\\n\\n- Web Services Technologies\\n - JAX-RS (JSR 339)\\n - Enterprise Web Services (JSR 109)\\n - Web Services Metadata (JSR 161)\\n - JAX-RPC (JSR 101)\\n - XML Messaging (JSR 67)\\n - XML Registries - JAXR (JSR 93)\\n\\n- Management and Security Technologies\\n - Authentication Service Providers (JSR 196)\\n - Authorization Contract for Containers (JSR 115)\\n - Application Deployment (JSR 88)\\n - J2EE Management (JSR 77)\\n - Debugging Other Languages (JSR 45)\\n\\n- Java EE-related Specifications in Java SE\\n - XML Binding - JAXB (JSR 222)\\n - XML Processing - JAXP (JSR 206)\\n - Database Connectivity (JSR 221)\\n - Management Extensions - JMX (JSR 003)\\n - JavaBean Activation Framework (JSR 925)\\n - Streaming API for XML (JSR 173)\\n\\n\\n\"}]],\"sections\":[[10,0]],\"ghostVersion\":\"3.0\"}","html":"<!--kg-card-begin: markdown--><p><a href=\"http://www.oracle.com/technetwork/java/javaee/tech/index.html\">http://www.oracle.com/technetwork/java/javaee/tech/index.html</a></p>\n<p>As a Developer Advocate for IBM, I spend a lot of time at conferences.  In this role, you do not even have to attend the sessions to see where the technology landscape is shifting (to be clear, I thoroughly enjoy attending sessions as well).  Of late, sessions around <mark>Node.js</mark> are bursting at the seams.</p>\n<p>One can speculate on why this is, but it led me to thinking of the Java EE development I have done in the past.  Jumping stacks can be <mark>intimidating</mark>, especially when your stack is as <mark>thorough</mark> as Java EE, or as <mark>community-driven</mark> as Node.js.  With that in mind, I put together the following thoughts on where Java EE developers might look for Node.js <mark>equivalents</mark>.</p>\n<p><em>Before I even start, let me be clear that this list is <mark>not all-inclusive</mark>.  I am calling out the most prominent parts that I recall from my days of Java EE.  If you have additions or corrections, please leave a comment below, and I will update the post accordingly.</em></p>\n<h3 id=\"webapplicationtechnologies\">Web Application Technologies</h3>\n<p>Given that Node.js is JavaScript, and JavaScript originated in the browser, it should be no surprise that it does pretty much anything Web related particularly well.</p>\n<p><strong>WebSocket</strong></p>\n<p>Just like JavaScript, <mark>WebSocket is born of the Web</mark>, and there are many WebSocket implementations available for Node.js.  Probably the most popular is <a href=\"http://socket.io\">Socket.IO</a>, which layers robust functionality on top of plain WebSocket.  If WebSocket is all you want however, then I personally like &quot;<a href=\"https://einaros.github.io/ws/\">ws</a>&quot;.</p>\n<p><strong>JSON Processing</strong></p>\n<p>The &quot;JS&quot; part of &quot;JSON&quot; stands for JavaScript, so it should be no surprise that JSON is natively supported in Node.js.  You will use <mark>JSON.parse()</mark> and <mark>JSON.stringify()</mark> to achieve this task.  The result is a JavaScript &quot;Object&quot; instance.  Properties are publicly exposed on that object, so there is <mark>no JSONObject marshaling</mark>.</p>\n<p><strong>Servlet</strong></p>\n<p>Java Servlets functionally take an incoming HTTP request, process it, and return some data.  Often a servlet will <mark>function as a controller</mark>, assembling data before passing it along to a JSP (JavaServer Page) for rendering.  The most direct equivalent in Node.js would likely be something along the lines of <a href=\"http://expressjs.com/guide/routing.html\">routes</a> in <a href=\"http://expressjs.com\">Express</a>.</p>\n<p><strong>JavaServer Faces</strong></p>\n<p>The Web brings with it an inherent <mark>statelessness</mark>, which JavaServer Faces aims to abstract away from the developer.  At the same time, JSF provides a bundling of more complex <mark>component types</mark> not generally provided for by Web standards.</p>\n<blockquote>\n<p>I once heard JSF referred to as a &quot;Flash killer&quot;.  Turns out that the true undoing of Flash was Apple.  Not that JSF is without its <a href=\"https://assets.thoughtworks.com/assets/technology-radar-jan-2014-en.pdf\">critics</a> and <a href=\"http://blog.primefaces.org/?p=3035\">champions</a>.</p>\n</blockquote>\n<p>Solving these two problems with Node.js would tend more towards the <mark>client side</mark> (browser).  Application frameworks such as <a href=\"https://angularjs.org\">AngularJS</a>, and <a href=\"https://facebook.github.io/react/\">React</a> (from Facebook), seek to manage the statelessness, while modern standards such as <a href=\"http://www.w3.org/TR/shadow-dom/\">Shadow DOM</a> offer encapsulation of more complex interactions which surface in frameworks such as <a href=\"https://www.polymer-project.org/1.0/\">Polymer</a>.</p>\n<p><strong>JavaServer Pages</strong></p>\n<p>Assuming that a Node.js Express route has been run, and data assembled, the terminology for JavaServer Pages would be a &quot;<mark>template</mark>&quot;.  The most common equivalent then of JavaServer Pages in Node.js would be <a href=\"http://jade-lang.com\">Jade</a>.  Everything from conditionals and includes, to iteration and inline code can be found in Jade.</p>\n<p><strong>JSP Tag Library</strong></p>\n<p>SCRIPT tag?  Components?</p>\n<h3 id=\"enterpriseapplicationtechnologies\">Enterprise Application Technologies</h3>\n<p>Without the enterprise, Java EE would just be Java SE (har, har).  It is in the enterprise that Java EE currently reigns supreme.  However, as Node.js continues to mature with offerings such as <a href=\"https://strongloop.com\">StrongLoop</a>, and <mark>Oracle</mark> continues to <a href=\"http://www.infoq.com/news/2015/09/oracle-purges-java-evangelists\">lay off Java Evangelists</a>, the future of the enterprise is up for grabs.</p>\n<p><em><strong>Enterprise JavaBeans</strong></em></p>\n<p>EJB (Enterprise JavaBeans) are one of those technologies most Java EE developers love to hate.  The complexity of the early versions of the specification made it all but impractical for most applications.  To be sure, where it fit however, it fit great.</p>\n<p>Essentially an approach to distributed components, like <a href=\"https://en.wikipedia.org/wiki/Distributed_Component_Object_Model\">DCOM</a> before it, or <a href=\"https://en.wikipedia.org/wiki/Common_Object_Request_Broker_Architecture\">CORBA</a> before that, in Node.js this architectural approach most closely maps to the modern trend of <a href=\"http://martinfowler.com/articles/microservices.html\">microservices</a> - specifically containerization of services using technologies such as <a href=\"https://www.docker.com\">Docker</a>.</p>\n<p><em><strong>Java Message Service</strong></em></p>\n<p>JMS (Java Message Service) is a bit of a misnomer, as it is really more of a protocol, than a true messaging service.  In Node.js the preferred equivalent technology would be to integrate to a <mark>message broker</mark> using open standard protocols.</p>\n<blockquote>\n<p>Node.js is actually capable of implementing a message broker, in addition to a client, however specialized open source brokers are generally preferred.</p>\n</blockquote>\n<p>The closest mapping between the functionality of JMS and an open standard would probably be <a href=\"https://en.wikipedia.org/wiki/Advanced_Message_Queuing_Protocol\">AMQP</a> (Advanced Message Queueing Protocol).  There are a number of brokers (both open and closed) that support AMQP, such as <a href=\"http://www.rabbitmq.com\">RabbitMQ</a>.</p>\n<p>Once free of JMS proper, you will find protocols designed for specific applications such as chat (<a href=\"https://en.wikipedia.org/wiki/XMPP\">XMPP</a>) or IoT (<a href=\"https://en.wikipedia.org/wiki/MQTT\">MQTT</a>).  Brokers also generally speak more than one protocol, and can even integrate with other open standards such as WebSocket.</p>\n<h3 id=\"webservicestechnologies\">Web Services Technologies</h3>\n<p>One might postulate that bringing JavaScript to the server was born out of the need to deliver web services, and the desire to do it with the same language that was used on the client.  With its asynchronous nature, Node.js makes a great fit.</p>\n<p><strong>JAX-RS</strong></p>\n<p>At a basic level, JAX-RS would be similar to <mark>Express routing</mark> done in a more formal manner.  This might include breaking the routes out to their own subclasses, and perhaps even breaking out model classes for each data type being represented.  Many of the <mark>JAX-RS annotations</mark> will map very closely to Express routing.</p>\n<h3 id=\"javaeerelatedspecificationsinjavase\">Java EE-related Specifications in Java SE</h3>\n<p>Not everything that exists in Java EE is solely Java EE.  Many Java SE applications need to parse XML, so you find that capability in the SE side of the house.  Today, Node.js does not make a differentiation between enterprise and non-enterprise use-cases, but you will still find support for XML and more.</p>\n<p><strong>XML Binding (JAXB)</strong></p>\n<p>Binding an XML tree to a JavaScript Object instance can be done using &quot;<a href=\"https://github.com/Leonidas-from-XIV/node-xml2js\">xml2js</a>&quot;.  This package is particularly useful for Windows developers as it does not depend on &quot;node-gyp&quot; and therefore will not require you to install Visual Studio.</p>\n<p>It is worth noting that &quot;xml2js&quot; uses &quot;<a href=\"https://github.com/oozcitak/xmlbuilder-js/\">xmlbuilder</a>&quot; for building an XML document from a JavaScript Object instance.  The &quot;xmlbuilder&quot; package is a useful utility in and of its own right, should you want to use it alone.</p>\n<p><strong>XML Processing (JAXP)</strong></p>\n<p>Java SE provides two main ways to process XML - <mark>SAX and DOM</mark>.</p>\n<p>The <mark>SAX</mark> approach iterates through an XML document, firing off events of various types as certain matches are found.  This results is far less memory consumption, but more tedious marshaling.</p>\n<p>The <mark>DOM</mark> approach by comparison, takes the entire XML document, and loads it into memory.  It then gives you an API by which you can query the document nodes, regardless of where they may be in the document.</p>\n<p>In Node.js the equivalent of SAX and DOM would be &quot;<a href=\"https://github.com/isaacs/sax-js/\">sax js</a>&quot; and &quot;<a href=\"https://github.com/isaacs/sax-js/\">jsdom</a>&quot;.</p>\n<p><strong>Database Connectivity</strong></p>\n<p>Database connectivity in Node.js is not standardized.  There is also no common delineation of driver &quot;types&quot;.  To most Node.js developers, a database package is chosen for the functionality that it provides on top of accessing the database proper, not by the means through which the database is accessed.</p>\n<ul>\n<li>Type 1: JDBC-ODBC Bridge</li>\n<li>Type 2: JDBC-Native API</li>\n<li>Type 3: JDBC-Pure Net Java</li>\n<li>Type 4: 100% Pure Java</li>\n</ul>\n<p>Implementation varies widely from Type 1 (JDBC-ODBC Bridge) to Type 4 (100% Pure Java).</p>\n<p>In the interest of mapping to known Java SE terminology, what follows is a variety of database access packages grouped by their approach to connectivity.</p>\n<p><em><strong>Type 1: JDBC-ODBC Bridge</strong></em></p>\n<p>I was shocked (not judging), but somebody had actually created an ODBC package for Node.js called &quot;<a href=\"https://github.com/wankdanker/node-odbc\">node-odbc</a>&quot;.</p>\n<p><em><strong>Type 2: JDBC-Native API</strong></em></p>\n<p>This is where most Node.js database drivers fall in my experience.</p>\n<ul>\n<li>\n<p><a href=\"https://github.com/wankdanker/node-odbc\">ODBC</a></p>\n</li>\n<li>\n<p><a href=\"https://github.com/patriksimek/node-mssql\">MS SQL</a></p>\n</li>\n<li>\n<p><a href=\"https://github.com/felixge/node-mysql\">MySQL</a></p>\n</li>\n<li>\n<p><a href=\"https://github.com/oracle/node-oracledb\">Oracle</a></p>\n</li>\n<li>\n<p><a href=\"https://github.com/brianc/node-postgres\">Postgres</a></p>\n</li>\n<li>\n<p><a href=\"https://github.com/mongodb/node-mongodb-native\">MongoDB</a></p>\n</li>\n<li>\n<p><a href=\"https://github.com/NodeRedis/node_redis\">Redis</a></p>\n</li>\n<li>\n<p>Type 3: JDBC-Net Pure Java</p>\n</li>\n<li>\n<p>Type 4: 100% Pure Java</p>\n</li>\n<li>\n<p>Web Application Technologies</p>\n</li>\n<li>\n<p>WebSocket (JSR 356)</p>\n</li>\n<li>\n<p>JSON Processing (JSR 353)</p>\n</li>\n<li>\n<p>Servlet (JSR 340)</p>\n</li>\n<li>\n<p>JavaServer Faces (JSR 344)</p>\n</li>\n<li>\n<p>JavaServer Pages (JSR 245)</p>\n</li>\n<li>\n<p>JSP Tag Library (JSR 52)</p>\n</li>\n<li>\n<p>Enterprise Application Technologies</p>\n</li>\n<li>\n<p>Batch Applications (JSR 352)</p>\n</li>\n<li>\n<p>Concurrency Utilities (JSR 236)</p>\n</li>\n<li>\n<p>Dependency Injection (JSR 346)</p>\n</li>\n<li>\n<p>Bean Validation (JSR 349)</p>\n</li>\n<li>\n<p>Enterprise JavaBeans (JSR 345)</p>\n</li>\n<li>\n<p>Interceptors (JSR 318)</p>\n</li>\n<li>\n<p>Connector Architecture (JSR 322)</p>\n</li>\n<li>\n<p>Persistence (JSR 338)</p>\n</li>\n<li>\n<p>Common Annotations (JSR 250)</p>\n</li>\n<li>\n<p>Java Message Service (JSR 343)</p>\n</li>\n<li>\n<p>Java Transaction API (JSR 907)</p>\n</li>\n<li>\n<p>JavaMail (JSR 919)</p>\n</li>\n<li>\n<p>Web Services Technologies</p>\n</li>\n<li>\n<p>JAX-RS (JSR 339)</p>\n</li>\n<li>\n<p>Enterprise Web Services (JSR 109)</p>\n</li>\n<li>\n<p>Web Services Metadata (JSR 161)</p>\n</li>\n<li>\n<p>JAX-RPC (JSR 101)</p>\n</li>\n<li>\n<p>XML Messaging (JSR 67)</p>\n</li>\n<li>\n<p>XML Registries - JAXR (JSR 93)</p>\n</li>\n<li>\n<p>Management and Security Technologies</p>\n</li>\n<li>\n<p>Authentication Service Providers (JSR 196)</p>\n</li>\n<li>\n<p>Authorization Contract for Containers (JSR 115)</p>\n</li>\n<li>\n<p>Application Deployment (JSR 88)</p>\n</li>\n<li>\n<p>J2EE Management (JSR 77)</p>\n</li>\n<li>\n<p>Debugging Other Languages (JSR 45)</p>\n</li>\n<li>\n<p>Java EE-related Specifications in Java SE</p>\n</li>\n<li>\n<p>XML Binding - JAXB (JSR 222)</p>\n</li>\n<li>\n<p>XML Processing - JAXP (JSR 206)</p>\n</li>\n<li>\n<p>Database Connectivity (JSR 221)</p>\n</li>\n<li>\n<p>Management Extensions - JMX (JSR 003)</p>\n</li>\n<li>\n<p>JavaBean Activation Framework (JSR 925)</p>\n</li>\n<li>\n<p>Streaming API for XML (JSR 173)</p>\n</li>\n</ul>\n<!--kg-card-end: markdown-->","comment_id":"48","plaintext":"http://www.oracle.com/technetwork/java/javaee/tech/index.html\n\nAs a Developer Advocate for IBM, I spend a lot of time at conferences. In this\nrole, you do not even have to attend the sessions to see where the technology\nlandscape is shifting (to be clear, I thoroughly enjoy attending sessions as\nwell). Of late, sessions around Node.js are bursting at the seams.\n\nOne can speculate on why this is, but it led me to thinking of the Java EE\ndevelopment I have done in the past. Jumping stacks can be intimidating,\nespecially when your stack is as thorough as Java EE, or as community-driven as\nNode.js. With that in mind, I put together the following thoughts on where Java\nEE developers might look for Node.js equivalents.\n\nBefore I even start, let me be clear that this list is not all-inclusive. I am\ncalling out the most prominent parts that I recall from my days of Java EE. If\nyou have additions or corrections, please leave a comment below, and I will\nupdate the post accordingly.\n\nWeb Application Technologies\nGiven that Node.js is JavaScript, and JavaScript originated in the browser, it\nshould be no surprise that it does pretty much anything Web related particularly\nwell.\n\nWebSocket\n\nJust like JavaScript, WebSocket is born of the Web, and there are many WebSocket\nimplementations available for Node.js. Probably the most popular is Socket.IO\n[http://socket.io], which layers robust functionality on top of plain WebSocket.\nIf WebSocket is all you want however, then I personally like \"ws\n[https://einaros.github.io/ws/]\".\n\nJSON Processing\n\nThe \"JS\" part of \"JSON\" stands for JavaScript, so it should be no surprise that\nJSON is natively supported in Node.js. You will use JSON.parse() and \nJSON.stringify() to achieve this task. The result is a JavaScript \"Object\"\ninstance. Properties are publicly exposed on that object, so there is no\nJSONObject marshaling.\n\nServlet\n\nJava Servlets functionally take an incoming HTTP request, process it, and return\nsome data. Often a servlet will function as a controller, assembling data before\npassing it along to a JSP (JavaServer Page) for rendering. The most direct\nequivalent in Node.js would likely be something along the lines of routes\n[http://expressjs.com/guide/routing.html] in Express [http://expressjs.com].\n\nJavaServer Faces\n\nThe Web brings with it an inherent statelessness, which JavaServer Faces aims to\nabstract away from the developer. At the same time, JSF provides a bundling of\nmore complex component types not generally provided for by Web standards.\n\n> I once heard JSF referred to as a \"Flash killer\". Turns out that the true\nundoing of Flash was Apple. Not that JSF is without its critics\n[https://assets.thoughtworks.com/assets/technology-radar-jan-2014-en.pdf] and \nchampions [http://blog.primefaces.org/?p=3035].\n\n\nSolving these two problems with Node.js would tend more towards the client side \n(browser). Application frameworks such as AngularJS [https://angularjs.org], and \nReact [https://facebook.github.io/react/] (from Facebook), seek to manage the\nstatelessness, while modern standards such as Shadow DOM\n[http://www.w3.org/TR/shadow-dom/] offer encapsulation of more complex\ninteractions which surface in frameworks such as Polymer\n[https://www.polymer-project.org/1.0/].\n\nJavaServer Pages\n\nAssuming that a Node.js Express route has been run, and data assembled, the\nterminology for JavaServer Pages would be a \"template\". The most common\nequivalent then of JavaServer Pages in Node.js would be Jade\n[http://jade-lang.com]. Everything from conditionals and includes, to iteration\nand inline code can be found in Jade.\n\nJSP Tag Library\n\nSCRIPT tag? Components?\n\nEnterprise Application Technologies\nWithout the enterprise, Java EE would just be Java SE (har, har). It is in the\nenterprise that Java EE currently reigns supreme. However, as Node.js continues\nto mature with offerings such as StrongLoop [https://strongloop.com], and Oracle \ncontinues to lay off Java Evangelists\n[http://www.infoq.com/news/2015/09/oracle-purges-java-evangelists], the future\nof the enterprise is up for grabs.\n\nEnterprise JavaBeans\n\nEJB (Enterprise JavaBeans) are one of those technologies most Java EE developers\nlove to hate. The complexity of the early versions of the specification made it\nall but impractical for most applications. To be sure, where it fit however, it\nfit great.\n\nEssentially an approach to distributed components, like DCOM\n[https://en.wikipedia.org/wiki/Distributed_Component_Object_Model] before it, or \nCORBA [https://en.wikipedia.org/wiki/Common_Object_Request_Broker_Architecture] \nbefore that, in Node.js this architectural approach most closely maps to the\nmodern trend of microservices\n[http://martinfowler.com/articles/microservices.html] - specifically\ncontainerization of services using technologies such as Docker\n[https://www.docker.com].\n\nJava Message Service\n\nJMS (Java Message Service) is a bit of a misnomer, as it is really more of a\nprotocol, than a true messaging service. In Node.js the preferred equivalent\ntechnology would be to integrate to a message broker using open standard\nprotocols.\n\n> Node.js is actually capable of implementing a message broker, in addition to a\nclient, however specialized open source brokers are generally preferred.\n\n\nThe closest mapping between the functionality of JMS and an open standard would\nprobably be AMQP\n[https://en.wikipedia.org/wiki/Advanced_Message_Queuing_Protocol] (Advanced\nMessage Queueing Protocol). There are a number of brokers (both open and closed)\nthat support AMQP, such as RabbitMQ [http://www.rabbitmq.com].\n\nOnce free of JMS proper, you will find protocols designed for specific\napplications such as chat (XMPP [https://en.wikipedia.org/wiki/XMPP]) or IoT (\nMQTT [https://en.wikipedia.org/wiki/MQTT]). Brokers also generally speak more\nthan one protocol, and can even integrate with other open standards such as\nWebSocket.\n\nWeb Services Technologies\nOne might postulate that bringing JavaScript to the server was born out of the\nneed to deliver web services, and the desire to do it with the same language\nthat was used on the client. With its asynchronous nature, Node.js makes a great\nfit.\n\nJAX-RS\n\nAt a basic level, JAX-RS would be similar to Express routing done in a more\nformal manner. This might include breaking the routes out to their own\nsubclasses, and perhaps even breaking out model classes for each data type being\nrepresented. Many of the JAX-RS annotations will map very closely to Express\nrouting.\n\nJava EE-related Specifications in Java SE\nNot everything that exists in Java EE is solely Java EE. Many Java SE\napplications need to parse XML, so you find that capability in the SE side of\nthe house. Today, Node.js does not make a differentiation between enterprise and\nnon-enterprise use-cases, but you will still find support for XML and more.\n\nXML Binding (JAXB)\n\nBinding an XML tree to a JavaScript Object instance can be done using \"xml2js\n[https://github.com/Leonidas-from-XIV/node-xml2js]\". This package is\nparticularly useful for Windows developers as it does not depend on \"node-gyp\"\nand therefore will not require you to install Visual Studio.\n\nIt is worth noting that \"xml2js\" uses \"xmlbuilder\n[https://github.com/oozcitak/xmlbuilder-js/]\" for building an XML document from\na JavaScript Object instance. The \"xmlbuilder\" package is a useful utility in\nand of its own right, should you want to use it alone.\n\nXML Processing (JAXP)\n\nJava SE provides two main ways to process XML - SAX and DOM.\n\nThe SAX approach iterates through an XML document, firing off events of various\ntypes as certain matches are found. This results is far less memory consumption,\nbut more tedious marshaling.\n\nThe DOM approach by comparison, takes the entire XML document, and loads it into\nmemory. It then gives you an API by which you can query the document nodes,\nregardless of where they may be in the document.\n\nIn Node.js the equivalent of SAX and DOM would be \"sax js\n[https://github.com/isaacs/sax-js/]\" and \"jsdom\n[https://github.com/isaacs/sax-js/]\".\n\nDatabase Connectivity\n\nDatabase connectivity in Node.js is not standardized. There is also no common\ndelineation of driver \"types\". To most Node.js developers, a database package is\nchosen for the functionality that it provides on top of accessing the database\nproper, not by the means through which the database is accessed.\n\n * Type 1: JDBC-ODBC Bridge\n * Type 2: JDBC-Native API\n * Type 3: JDBC-Pure Net Java\n * Type 4: 100% Pure Java\n\nImplementation varies widely from Type 1 (JDBC-ODBC Bridge) to Type 4 (100% Pure\nJava).\n\nIn the interest of mapping to known Java SE terminology, what follows is a\nvariety of database access packages grouped by their approach to connectivity.\n\nType 1: JDBC-ODBC Bridge\n\nI was shocked (not judging), but somebody had actually created an ODBC package\nfor Node.js called \"node-odbc [https://github.com/wankdanker/node-odbc]\".\n\nType 2: JDBC-Native API\n\nThis is where most Node.js database drivers fall in my experience.\n\n * ODBC [https://github.com/wankdanker/node-odbc]\n   \n   \n * MS SQL [https://github.com/patriksimek/node-mssql]\n   \n   \n * MySQL [https://github.com/felixge/node-mysql]\n   \n   \n * Oracle [https://github.com/oracle/node-oracledb]\n   \n   \n * Postgres [https://github.com/brianc/node-postgres]\n   \n   \n * MongoDB [https://github.com/mongodb/node-mongodb-native]\n   \n   \n * Redis [https://github.com/NodeRedis/node_redis]\n   \n   \n * Type 3: JDBC-Net Pure Java\n   \n   \n * Type 4: 100% Pure Java\n   \n   \n * Web Application Technologies\n   \n   \n * WebSocket (JSR 356)\n   \n   \n * JSON Processing (JSR 353)\n   \n   \n * Servlet (JSR 340)\n   \n   \n * JavaServer Faces (JSR 344)\n   \n   \n * JavaServer Pages (JSR 245)\n   \n   \n * JSP Tag Library (JSR 52)\n   \n   \n * Enterprise Application Technologies\n   \n   \n * Batch Applications (JSR 352)\n   \n   \n * Concurrency Utilities (JSR 236)\n   \n   \n * Dependency Injection (JSR 346)\n   \n   \n * Bean Validation (JSR 349)\n   \n   \n * Enterprise JavaBeans (JSR 345)\n   \n   \n * Interceptors (JSR 318)\n   \n   \n * Connector Architecture (JSR 322)\n   \n   \n * Persistence (JSR 338)\n   \n   \n * Common Annotations (JSR 250)\n   \n   \n * Java Message Service (JSR 343)\n   \n   \n * Java Transaction API (JSR 907)\n   \n   \n * JavaMail (JSR 919)\n   \n   \n * Web Services Technologies\n   \n   \n * JAX-RS (JSR 339)\n   \n   \n * Enterprise Web Services (JSR 109)\n   \n   \n * Web Services Metadata (JSR 161)\n   \n   \n * JAX-RPC (JSR 101)\n   \n   \n * XML Messaging (JSR 67)\n   \n   \n * XML Registries - JAXR (JSR 93)\n   \n   \n * Management and Security Technologies\n   \n   \n * Authentication Service Providers (JSR 196)\n   \n   \n * Authorization Contract for Containers (JSR 115)\n   \n   \n * Application Deployment (JSR 88)\n   \n   \n * J2EE Management (JSR 77)\n   \n   \n * Debugging Other Languages (JSR 45)\n   \n   \n * Java EE-related Specifications in Java SE\n   \n   \n * XML Binding - JAXB (JSR 222)\n   \n   \n * XML Processing - JAXP (JSR 206)\n   \n   \n * Database Connectivity (JSR 221)\n   \n   \n * Management Extensions - JMX (JSR 003)\n   \n   \n * JavaBean Activation Framework (JSR 925)\n   \n   \n * Streaming API for XML (JSR 173)","feature_image":null,"featured":0,"status":"draft","locale":null,"visibility":"public","author_id":"1","created_at":"2015-09-22T21:15:08.000Z","updated_at":"2015-09-24T01:06:09.000Z","published_at":null,"custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"type":"post","email_recipient_filter":"none"},{"id":"5adb8922351ffe0018a5773a","uuid":"6e4bb743-3f8a-41f5-8aea-7a5b21fedc89","title":"Reading Light: Origins","slug":"project-reading-light","mobiledoc":"{\"version\":\"0.3.1\",\"markups\":[],\"atoms\":[],\"cards\":[[\"markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\"Some time ago (more than a year), my then 9 year-old daughter asked for a reading light.  She likes to read before bed, but prefers paper books over digital.  You can pick up reading lights in all forms on the cheap, but I saw this as a teachable opportunity.  So we broke out the soldering iron, and went to work.\\n\\n###Electronics\\n\\nReading lights are basic circuits.  There is a ==power source==, that is hooked up to a ==switch==.  When the switch is closed, the current runs through a suitable ==resistor==, into an ==LED==, and then to ground.  When the switch is open, no current flows, and the reading light is effectively \\\"off\\\".\\n\\nThe project needed enough light to read by, and I was not sure that a standard 5mm white LED would be enough.  I had [10mm diffused white LEDs](https://www.sparkfun.com/products/11121) lying around, but again, I was concerned about the brightness (or lack thereof).  In the end, I went with a [10mm super bright white LED](https://www.sparkfun.com/products/11118).\\n\\n![10mm super bright white LED.](http://images.kevinhoyt.com/led.super.bright.10mm.jpg)\\n\\nFor power, I had a [4xAA battery cube](https://www.sparkfun.com/products/retired/550) from an old robot project that I figured would provide enough power for extended reading.  The batteries provide ==6V==, while the LED has a ==forward voltage of 3.4V== and a ==forward current of 80mA==.  Pop those values into [Ohm's Law](https://en.wikipedia.org/wiki/Ohm%27s_law) (or an [LED calculator](http://led.linear1.org/1led.wiz)), and we need a ==33 Ohm== resistor.  Rounding up to what I had on hand, landed me with a ==47 Ohm== resistor.\\n\\n![LED calculator results.](http://images.kevinhoyt.com/reading.light.original.led.png)\\n\\n###Enclosure\\n\\nThe reading light will also need an enclosure.  It needed to be ==big enough== to hold the electrical components, but ==small enough== to be easily portable.  It also had to be relatively ==easy to modify== to be able to mount the switch and LED.  As I order parts from [SparkFun](http://sparkfun.com) all the time, I had one of their ==small cardboard boxes== on hand, which fit all my criteria.\\n\\n![Behold the original reading light.](http://images.kevinhoyt.com/reading.light.original.box.jpg)\\n\\nI placed the LED in the top third of the lid of box, using an [Xacto knife](http://www.amazon.com/gp/product/B00004Z2UB?keywords=xacto%20knife&qid=1451925544&ref_=sr_1_1&sr=8-1) to make small holes for the leads.  On the underside of the lid, I used ==glue== to hold the LED in place.  I placed the [switch](https://www.radioshack.com/products/spst-neon-rocker-switch?variant=5717521541) in the lid, by cutting the recommended hole size of ==3/4 of an inch==.  With the outer components in place, I ==soldered== the necessary wires in place.  A flip of the switch, and we were in business.\\n\\n###Problems\\n\\nWell, really just one problem.  The battery cube was ==free floating== inside the box.  This meant that as the box/reading light itself was transported around, that the ==battery cube would move==.  This is not a problem for the battery, but over time, as we found out, it is a problem for the wires - namely the thin leads for the resistor.  Enough back and forth, and the ==connections would break==, rendering the light useless.\\n\\n![Inside the box.](http://images.kevinhoyt.com/reading.light.original.inside.jpg)\\n\\nI figured this would be coming ahead of time, and [shrink-wrapped](https://www.sparkfun.com/products/9353) the entire resistor, including wire connections.  Unfortunately, the movement of the battery was ==still too much==.  To remedy this problem, I ==taped== the battery cube to the inside of the box, as well as the resistor.  No more movement.\\n\\nThe reading light has powered the LED for quite some time now.  The reading light has made its way, along with my daughter of course, to several countries.  It is so ==well traveled== in fact that the box is beginning to deteriorate, and will ==need replacement== soon.\\n\\n###Customer Feedback\\n\\nAs I look forward to the next generation of the reading light, it makes sense to leverage the software development lifecycle ([SDLC](https://en.wikipedia.org/wiki/Systems_development_life_cycle), really just a software-specific [Scientific Method](https://en.wikipedia.org/wiki/Scientific_method)), and gather some feedback.  After discussing possible improvements to the reading light with my now 11 year-old daughter, here is what I learned.\\n\\n* **Too bright** - It turns out that the 10mm super bright white LED, that produces 16,000 mcd is just too bright under most circumstances.\\n* **Constant** - I say under \\\"most\\\" circumstances, because there are conditions under which she needs all the light the LED can produce.\\n* **Placement** - Because there is no clip, the reading light generally gets placed on her pillow, next to her head.  Also, the light is used as a flashlight in unfamiliar surroundings (camping, hotel).\\n\\nTo this list, I have my own upgrades that I would like to make for the next iteration.\\n\\n* **Power source** - It turns out that the 4xAA cube has lasted a lot longer than I expected.  I could probably go with a bit more refined source such as a LiPo battery.  This of course means I will need a charging circuit as well.\\n* **Enclosure** - I am a stickler for custom enclosures.  Anybody can show you a bunch of wires soldered together, it is the enclosure that makes the project.  A smaller, more rugged package would be a good improvement.\\n\\n###Next Steps\\n\\nWith feedback collected, I am ready to embark on the next generation of my daughter's reading light.  The five points of feedback actually represent a pretty sizable leap forward.  Over the next several blog posts, I will document the process en route to the second version.\"}]],\"sections\":[[10,0]],\"ghostVersion\":\"3.0\"}","html":"<!--kg-card-begin: markdown--><p>Some time ago (more than a year), my then 9 year-old daughter asked for a reading light.  She likes to read before bed, but prefers paper books over digital.  You can pick up reading lights in all forms on the cheap, but I saw this as a teachable opportunity.  So we broke out the soldering iron, and went to work.</p>\n<h3 id=\"electronics\">Electronics</h3>\n<p>Reading lights are basic circuits.  There is a <mark>power source</mark>, that is hooked up to a <mark>switch</mark>.  When the switch is closed, the current runs through a suitable <mark>resistor</mark>, into an <mark>LED</mark>, and then to ground.  When the switch is open, no current flows, and the reading light is effectively &quot;off&quot;.</p>\n<p>The project needed enough light to read by, and I was not sure that a standard 5mm white LED would be enough.  I had <a href=\"https://www.sparkfun.com/products/11121\">10mm diffused white LEDs</a> lying around, but again, I was concerned about the brightness (or lack thereof).  In the end, I went with a <a href=\"https://www.sparkfun.com/products/11118\">10mm super bright white LED</a>.</p>\n<p><img src=\"http://images.kevinhoyt.com/led.super.bright.10mm.jpg\" alt=\"10mm super bright white LED.\" loading=\"lazy\"></p>\n<p>For power, I had a <a href=\"https://www.sparkfun.com/products/retired/550\">4xAA battery cube</a> from an old robot project that I figured would provide enough power for extended reading.  The batteries provide <mark>6V</mark>, while the LED has a <mark>forward voltage of 3.4V</mark> and a <mark>forward current of 80mA</mark>.  Pop those values into <a href=\"https://en.wikipedia.org/wiki/Ohm%27s_law\">Ohm's Law</a> (or an <a href=\"http://led.linear1.org/1led.wiz\">LED calculator</a>), and we need a <mark>33 Ohm</mark> resistor.  Rounding up to what I had on hand, landed me with a <mark>47 Ohm</mark> resistor.</p>\n<p><img src=\"http://images.kevinhoyt.com/reading.light.original.led.png\" alt=\"LED calculator results.\" loading=\"lazy\"></p>\n<h3 id=\"enclosure\">Enclosure</h3>\n<p>The reading light will also need an enclosure.  It needed to be <mark>big enough</mark> to hold the electrical components, but <mark>small enough</mark> to be easily portable.  It also had to be relatively <mark>easy to modify</mark> to be able to mount the switch and LED.  As I order parts from <a href=\"http://sparkfun.com\">SparkFun</a> all the time, I had one of their <mark>small cardboard boxes</mark> on hand, which fit all my criteria.</p>\n<p><img src=\"http://images.kevinhoyt.com/reading.light.original.box.jpg\" alt=\"Behold the original reading light.\" loading=\"lazy\"></p>\n<p>I placed the LED in the top third of the lid of box, using an <a href=\"http://www.amazon.com/gp/product/B00004Z2UB?keywords=xacto%20knife&amp;qid=1451925544&amp;ref_=sr_1_1&amp;sr=8-1\">Xacto knife</a> to make small holes for the leads.  On the underside of the lid, I used <mark>glue</mark> to hold the LED in place.  I placed the <a href=\"https://www.radioshack.com/products/spst-neon-rocker-switch?variant=5717521541\">switch</a> in the lid, by cutting the recommended hole size of <mark>3/4 of an inch</mark>.  With the outer components in place, I <mark>soldered</mark> the necessary wires in place.  A flip of the switch, and we were in business.</p>\n<h3 id=\"problems\">Problems</h3>\n<p>Well, really just one problem.  The battery cube was <mark>free floating</mark> inside the box.  This meant that as the box/reading light itself was transported around, that the <mark>battery cube would move</mark>.  This is not a problem for the battery, but over time, as we found out, it is a problem for the wires - namely the thin leads for the resistor.  Enough back and forth, and the <mark>connections would break</mark>, rendering the light useless.</p>\n<p><img src=\"http://images.kevinhoyt.com/reading.light.original.inside.jpg\" alt=\"Inside the box.\" loading=\"lazy\"></p>\n<p>I figured this would be coming ahead of time, and <a href=\"https://www.sparkfun.com/products/9353\">shrink-wrapped</a> the entire resistor, including wire connections.  Unfortunately, the movement of the battery was <mark>still too much</mark>.  To remedy this problem, I <mark>taped</mark> the battery cube to the inside of the box, as well as the resistor.  No more movement.</p>\n<p>The reading light has powered the LED for quite some time now.  The reading light has made its way, along with my daughter of course, to several countries.  It is so <mark>well traveled</mark> in fact that the box is beginning to deteriorate, and will <mark>need replacement</mark> soon.</p>\n<h3 id=\"customerfeedback\">Customer Feedback</h3>\n<p>As I look forward to the next generation of the reading light, it makes sense to leverage the software development lifecycle (<a href=\"https://en.wikipedia.org/wiki/Systems_development_life_cycle\">SDLC</a>, really just a software-specific <a href=\"https://en.wikipedia.org/wiki/Scientific_method\">Scientific Method</a>), and gather some feedback.  After discussing possible improvements to the reading light with my now 11 year-old daughter, here is what I learned.</p>\n<ul>\n<li><strong>Too bright</strong> - It turns out that the 10mm super bright white LED, that produces 16,000 mcd is just too bright under most circumstances.</li>\n<li><strong>Constant</strong> - I say under &quot;most&quot; circumstances, because there are conditions under which she needs all the light the LED can produce.</li>\n<li><strong>Placement</strong> - Because there is no clip, the reading light generally gets placed on her pillow, next to her head.  Also, the light is used as a flashlight in unfamiliar surroundings (camping, hotel).</li>\n</ul>\n<p>To this list, I have my own upgrades that I would like to make for the next iteration.</p>\n<ul>\n<li><strong>Power source</strong> - It turns out that the 4xAA cube has lasted a lot longer than I expected.  I could probably go with a bit more refined source such as a LiPo battery.  This of course means I will need a charging circuit as well.</li>\n<li><strong>Enclosure</strong> - I am a stickler for custom enclosures.  Anybody can show you a bunch of wires soldered together, it is the enclosure that makes the project.  A smaller, more rugged package would be a good improvement.</li>\n</ul>\n<h3 id=\"nextsteps\">Next Steps</h3>\n<p>With feedback collected, I am ready to embark on the next generation of my daughter's reading light.  The five points of feedback actually represent a pretty sizable leap forward.  Over the next several blog posts, I will document the process en route to the second version.</p>\n<!--kg-card-end: markdown-->","comment_id":"49","plaintext":"Some time ago (more than a year), my then 9 year-old daughter asked for a\nreading light. She likes to read before bed, but prefers paper books over\ndigital. You can pick up reading lights in all forms on the cheap, but I saw\nthis as a teachable opportunity. So we broke out the soldering iron, and went to\nwork.\n\nElectronics\nReading lights are basic circuits. There is a power source, that is hooked up to\na switch. When the switch is closed, the current runs through a suitable \nresistor, into an LED, and then to ground. When the switch is open, no current\nflows, and the reading light is effectively \"off\".\n\nThe project needed enough light to read by, and I was not sure that a standard\n5mm white LED would be enough. I had 10mm diffused white LEDs\n[https://www.sparkfun.com/products/11121] lying around, but again, I was\nconcerned about the brightness (or lack thereof). In the end, I went with a \n10mm\nsuper bright white LED [https://www.sparkfun.com/products/11118].\n\n\n\nFor power, I had a 4xAA battery cube\n[https://www.sparkfun.com/products/retired/550] from an old robot project that I\nfigured would provide enough power for extended reading. The batteries provide \n6V, while the LED has a forward voltage of 3.4V and a forward current of 80mA.\nPop those values into Ohm's Law [https://en.wikipedia.org/wiki/Ohm%27s_law] (or\nan LED calculator [http://led.linear1.org/1led.wiz]), and we need a 33 Ohm \nresistor. Rounding up to what I had on hand, landed me with a 47 Ohm resistor.\n\n\n\nEnclosure\nThe reading light will also need an enclosure. It needed to be big enough to\nhold the electrical components, but small enough to be easily portable. It also\nhad to be relatively easy to modify to be able to mount the switch and LED. As I\norder parts from SparkFun [http://sparkfun.com] all the time, I had one of their \nsmall cardboard boxes on hand, which fit all my criteria.\n\n\n\nI placed the LED in the top third of the lid of box, using an Xacto knife\n[http://www.amazon.com/gp/product/B00004Z2UB?keywords=xacto%20knife&qid=1451925544&ref_=sr_1_1&sr=8-1] \nto make small holes for the leads. On the underside of the lid, I used glue to\nhold the LED in place. I placed the switch\n[https://www.radioshack.com/products/spst-neon-rocker-switch?variant=5717521541] \nin the lid, by cutting the recommended hole size of 3/4 of an inch. With the\nouter components in place, I soldered the necessary wires in place. A flip of\nthe switch, and we were in business.\n\nProblems\nWell, really just one problem. The battery cube was free floating inside the\nbox. This meant that as the box/reading light itself was transported around,\nthat the battery cube would move. This is not a problem for the battery, but\nover time, as we found out, it is a problem for the wires - namely the thin\nleads for the resistor. Enough back and forth, and the connections would break,\nrendering the light useless.\n\n\n\nI figured this would be coming ahead of time, and shrink-wrapped\n[https://www.sparkfun.com/products/9353] the entire resistor, including wire\nconnections. Unfortunately, the movement of the battery was still too much. To\nremedy this problem, I taped the battery cube to the inside of the box, as well\nas the resistor. No more movement.\n\nThe reading light has powered the LED for quite some time now. The reading light\nhas made its way, along with my daughter of course, to several countries. It is\nso well traveled in fact that the box is beginning to deteriorate, and will need\nreplacement soon.\n\nCustomer Feedback\nAs I look forward to the next generation of the reading light, it makes sense to\nleverage the software development lifecycle (SDLC\n[https://en.wikipedia.org/wiki/Systems_development_life_cycle], really just a\nsoftware-specific Scientific Method\n[https://en.wikipedia.org/wiki/Scientific_method]), and gather some feedback.\nAfter discussing possible improvements to the reading light with my now 11\nyear-old daughter, here is what I learned.\n\n * Too bright - It turns out that the 10mm super bright white LED, that produces\n   16,000 mcd is just too bright under most circumstances.\n * Constant - I say under \"most\" circumstances, because there are conditions\n   under which she needs all the light the LED can produce.\n * Placement - Because there is no clip, the reading light generally gets placed\n   on her pillow, next to her head. Also, the light is used as a flashlight in\n   unfamiliar surroundings (camping, hotel).\n\nTo this list, I have my own upgrades that I would like to make for the next\niteration.\n\n * Power source - It turns out that the 4xAA cube has lasted a lot longer than I\n   expected. I could probably go with a bit more refined source such as a LiPo\n   battery. This of course means I will need a charging circuit as well.\n * Enclosure - I am a stickler for custom enclosures. Anybody can show you a\n   bunch of wires soldered together, it is the enclosure that makes the project.\n   A smaller, more rugged package would be a good improvement.\n\nNext Steps\nWith feedback collected, I am ready to embark on the next generation of my\ndaughter's reading light. The five points of feedback actually represent a\npretty sizable leap forward. Over the next several blog posts, I will document\nthe process en route to the second version.","feature_image":"http://images.kevinhoyt.com/reading.light.original.paige.jpg","featured":0,"status":"published","locale":null,"visibility":"public","author_id":"1","created_at":"2016-01-04T15:45:54.000Z","updated_at":"2016-01-04T17:36:59.000Z","published_at":"2016-01-04T17:36:59.000Z","custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"type":"post","email_recipient_filter":"none"},{"id":"5adb8922351ffe0018a5773b","uuid":"c2a4bf4d-c812-4da5-b810-acad9f6ca524","title":"Reading Light: Dawn of an Iteration","slug":"reading-light-dawn-of-an-iteration","mobiledoc":"{\"version\":\"0.3.1\",\"markups\":[],\"atoms\":[],\"cards\":[[\"markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\"In my last blog [post](http://www.kevinhoyt.com/2016/01/04/project-reading-light/), I talked about the origins of a handmade reading light my 11 year-old daughter has been using for years.  At the end of that post, there was some \\\"customer feedback\\\" which has now turned into the inspiration for the next iteration.  In this post I will look at how I intend to implement those features in the next version.\\n\\n###Too Bright\\n\\nThe original reading light used one 10mm super bright white LED.  Since I am powering it at full strength however, the 16,000 mcd was actually too much for reading.  Clearly I need to change the light source.\\n\\nMy first thought was that an array of smallish surface mount LEDs would do the trick.  Unfortunately, most of them come in at just hundreds of milli-candela.  Would that be bright enough?  I also looked at diffused white LEDs, but they too were only in the hundreds range.\\n\\n==The jump in LED packages seems to skip over the lower range, and the next stop is 8,000 mcd to 10,000 mcd.==\\n\\n###Constant\\n\\nThe original reading light had two states - on and off.  The feedback was to make the brightness adjustable.  A wide variety of options danced through my head.\\n\\nA potentiometer (dial to the layman) mounted to the side of the box seemed like it would be a fix.  But if I was going to do one for brightness, maybe I should stick an RGB LED in the box, and offer three more dials to control the color.\\n\\nOkay, wait, this is a reading light.  White is good.  ==I was letting my penchant for the electronics get in the way of solving the problem.==\\n\\n###Placement\\n\\nThe original reading light is often placed on the pillow next to my daughters head.  When she travels, the box gets tossed in her backpack and frequently bumped around.  There is potential here for a brightness dial to get bumped.  ==I need to be more clever about how I overcome this problem.==\\n\\n###Pieces Parts\\n\\nIf clear white LED packages go from the hundreds, and jump to around ==8,000 mcd, that is still half== of the 16,000 mcd that the 10mm package the reading light currently creates.  I think that will work, but I will need to buy some to test.\\n\\n![Accelerometer](http://images.kevinhoyt.com/adxl362.accelerometer.jpg)\\n\\nBrightness may not even be a factor if I can solve the \\\"constant\\\" problem.  To do that ==I am going to introduce an accelerometer== to the reading light.  This also means that I will ==need an MCU== to interact with it.  That MCU needs to be small enough to keep the reading light compact.\\n\\n![Arduino Pro Mini 3.3V](http://images.kevinhoyt.com/arduino.pro.mini.3.3v.jpg)\\n\\nThere are all variety of accelerometer out there, including very expensive options for drones and robots.  I just need basic tilt sensitivity, so I picked the cheapest from the SparkFun catalog, the [ADXL362](https://www.sparkfun.com/products/11446).\\n\\nThe accelerometer runs in the range of 1.6V to 3.5V, so it would be easiest to have an MCU that ran at the same levels.  Keeping in mind the need for compactness, I chose the [Arduino Pro Mini 3.3V](https://www.sparkfun.com/products/11114).\\n\\n###Power Source\\n\\nWhile my daughter had no complaints, I wanted to trim up the power supply from a giant 4xAA cube, to something a little more sleek.  With an MCU that runs at 3.3V introduced to the mix, a polymer lithium (LiPo) battery, which generally run at around 3.7V, seemed like it would be ideal.\\n\\n![Polymer Lithium battery.](http://images.kevinhoyt.com/lipo.2000.mah.jpg)\\n\\n==At 2,000 mAh and about 2\\\" wide by 2\\\" long, this particular battery makes an ideal fit.==  It will power the MCU, and the LED, for a long period of time.  At 0.25\\\" inches thick, it also maps well to a custom enclosure made from slices of 1/4\\\" acrylic.\\n\\n###Enclosure\\n\\nDesktop fabrication (laser cutting, CNC, 3D printing) has become a favorite hobby of mine.  I was never very good at power tools, but computer controlled tools fit me perfectly.  While I usually ship my projects to [Ponoko](http://www.ponoko.com), I got a [laser cutter](https://glowforge.com) for Christmas this year!\\n\\n![Glowforge](http://images.kevinhoyt.com/glowforge.jpg)\\n\\nGiven that the battery is a quarter-inch, and the electronics will be about the same, I think I will use ==two sheets of clear acrylic, sandwiched between two 3mm sheets of birch plywood==.  I thought the birch plywood would be a nice touch, as it would allow me to engrave custom art personalized to my daughter.\\n\\n###Next Steps\\n\\nThe use of a polymer lithium battery introduces one really big challenge - ==it has be be recharged==.  Ideally, this needs to happen without opening the case.  \\n\\nWhile both Adafruit and SparkFun make battery chargers, they both place the USB connector ==flush with the edge of circuit board==.  Like a product you might buy from a commercial vendor, I want to expose only the USB port for charging.\\n\\nIn the end, this means that I am going to have to create a custom circuit board - ==my first==.\"}]],\"sections\":[[10,0]],\"ghostVersion\":\"3.0\"}","html":"<!--kg-card-begin: markdown--><p>In my last blog <a href=\"http://www.kevinhoyt.com/2016/01/04/project-reading-light/\">post</a>, I talked about the origins of a handmade reading light my 11 year-old daughter has been using for years.  At the end of that post, there was some &quot;customer feedback&quot; which has now turned into the inspiration for the next iteration.  In this post I will look at how I intend to implement those features in the next version.</p>\n<h3 id=\"toobright\">Too Bright</h3>\n<p>The original reading light used one 10mm super bright white LED.  Since I am powering it at full strength however, the 16,000 mcd was actually too much for reading.  Clearly I need to change the light source.</p>\n<p>My first thought was that an array of smallish surface mount LEDs would do the trick.  Unfortunately, most of them come in at just hundreds of milli-candela.  Would that be bright enough?  I also looked at diffused white LEDs, but they too were only in the hundreds range.</p>\n<p><mark>The jump in LED packages seems to skip over the lower range, and the next stop is 8,000 mcd to 10,000 mcd.</mark></p>\n<h3 id=\"constant\">Constant</h3>\n<p>The original reading light had two states - on and off.  The feedback was to make the brightness adjustable.  A wide variety of options danced through my head.</p>\n<p>A potentiometer (dial to the layman) mounted to the side of the box seemed like it would be a fix.  But if I was going to do one for brightness, maybe I should stick an RGB LED in the box, and offer three more dials to control the color.</p>\n<p>Okay, wait, this is a reading light.  White is good.  <mark>I was letting my penchant for the electronics get in the way of solving the problem.</mark></p>\n<h3 id=\"placement\">Placement</h3>\n<p>The original reading light is often placed on the pillow next to my daughters head.  When she travels, the box gets tossed in her backpack and frequently bumped around.  There is potential here for a brightness dial to get bumped.  <mark>I need to be more clever about how I overcome this problem.</mark></p>\n<h3 id=\"piecesparts\">Pieces Parts</h3>\n<p>If clear white LED packages go from the hundreds, and jump to around <mark>8,000 mcd, that is still half</mark> of the 16,000 mcd that the 10mm package the reading light currently creates.  I think that will work, but I will need to buy some to test.</p>\n<p><img src=\"http://images.kevinhoyt.com/adxl362.accelerometer.jpg\" alt=\"Accelerometer\" loading=\"lazy\"></p>\n<p>Brightness may not even be a factor if I can solve the &quot;constant&quot; problem.  To do that <mark>I am going to introduce an accelerometer</mark> to the reading light.  This also means that I will <mark>need an MCU</mark> to interact with it.  That MCU needs to be small enough to keep the reading light compact.</p>\n<p><img src=\"http://images.kevinhoyt.com/arduino.pro.mini.3.3v.jpg\" alt=\"Arduino Pro Mini 3.3V\" loading=\"lazy\"></p>\n<p>There are all variety of accelerometer out there, including very expensive options for drones and robots.  I just need basic tilt sensitivity, so I picked the cheapest from the SparkFun catalog, the <a href=\"https://www.sparkfun.com/products/11446\">ADXL362</a>.</p>\n<p>The accelerometer runs in the range of 1.6V to 3.5V, so it would be easiest to have an MCU that ran at the same levels.  Keeping in mind the need for compactness, I chose the <a href=\"https://www.sparkfun.com/products/11114\">Arduino Pro Mini 3.3V</a>.</p>\n<h3 id=\"powersource\">Power Source</h3>\n<p>While my daughter had no complaints, I wanted to trim up the power supply from a giant 4xAA cube, to something a little more sleek.  With an MCU that runs at 3.3V introduced to the mix, a polymer lithium (LiPo) battery, which generally run at around 3.7V, seemed like it would be ideal.</p>\n<p><img src=\"http://images.kevinhoyt.com/lipo.2000.mah.jpg\" alt=\"Polymer Lithium battery.\" loading=\"lazy\"></p>\n<p><mark>At 2,000 mAh and about 2&quot; wide by 2&quot; long, this particular battery makes an ideal fit.</mark>  It will power the MCU, and the LED, for a long period of time.  At 0.25&quot; inches thick, it also maps well to a custom enclosure made from slices of 1/4&quot; acrylic.</p>\n<h3 id=\"enclosure\">Enclosure</h3>\n<p>Desktop fabrication (laser cutting, CNC, 3D printing) has become a favorite hobby of mine.  I was never very good at power tools, but computer controlled tools fit me perfectly.  While I usually ship my projects to <a href=\"http://www.ponoko.com\">Ponoko</a>, I got a <a href=\"https://glowforge.com\">laser cutter</a> for Christmas this year!</p>\n<p><img src=\"http://images.kevinhoyt.com/glowforge.jpg\" alt=\"Glowforge\" loading=\"lazy\"></p>\n<p>Given that the battery is a quarter-inch, and the electronics will be about the same, I think I will use <mark>two sheets of clear acrylic, sandwiched between two 3mm sheets of birch plywood</mark>.  I thought the birch plywood would be a nice touch, as it would allow me to engrave custom art personalized to my daughter.</p>\n<h3 id=\"nextsteps\">Next Steps</h3>\n<p>The use of a polymer lithium battery introduces one really big challenge - <mark>it has be be recharged</mark>.  Ideally, this needs to happen without opening the case.</p>\n<p>While both Adafruit and SparkFun make battery chargers, they both place the USB connector <mark>flush with the edge of circuit board</mark>.  Like a product you might buy from a commercial vendor, I want to expose only the USB port for charging.</p>\n<p>In the end, this means that I am going to have to create a custom circuit board - <mark>my first</mark>.</p>\n<!--kg-card-end: markdown-->","comment_id":"50","plaintext":"In my last blog post\n[http://www.kevinhoyt.com/2016/01/04/project-reading-light/], I talked about the\norigins of a handmade reading light my 11 year-old daughter has been using for\nyears. At the end of that post, there was some \"customer feedback\" which has now\nturned into the inspiration for the next iteration. In this post I will look at\nhow I intend to implement those features in the next version.\n\nToo Bright\nThe original reading light used one 10mm super bright white LED. Since I am\npowering it at full strength however, the 16,000 mcd was actually too much for\nreading. Clearly I need to change the light source.\n\nMy first thought was that an array of smallish surface mount LEDs would do the\ntrick. Unfortunately, most of them come in at just hundreds of milli-candela.\nWould that be bright enough? I also looked at diffused white LEDs, but they too\nwere only in the hundreds range.\n\nThe jump in LED packages seems to skip over the lower range, and the next stop\nis 8,000 mcd to 10,000 mcd.\n\nConstant\nThe original reading light had two states - on and off. The feedback was to make\nthe brightness adjustable. A wide variety of options danced through my head.\n\nA potentiometer (dial to the layman) mounted to the side of the box seemed like\nit would be a fix. But if I was going to do one for brightness, maybe I should\nstick an RGB LED in the box, and offer three more dials to control the color.\n\nOkay, wait, this is a reading light. White is good. I was letting my penchant\nfor the electronics get in the way of solving the problem.\n\nPlacement\nThe original reading light is often placed on the pillow next to my daughters\nhead. When she travels, the box gets tossed in her backpack and frequently\nbumped around. There is potential here for a brightness dial to get bumped. I\nneed to be more clever about how I overcome this problem.\n\nPieces Parts\nIf clear white LED packages go from the hundreds, and jump to around 8,000 mcd,\nthat is still half of the 16,000 mcd that the 10mm package the reading light\ncurrently creates. I think that will work, but I will need to buy some to test.\n\n\n\nBrightness may not even be a factor if I can solve the \"constant\" problem. To do\nthat I am going to introduce an accelerometer to the reading light. This also\nmeans that I will need an MCU to interact with it. That MCU needs to be small\nenough to keep the reading light compact.\n\n\n\nThere are all variety of accelerometer out there, including very expensive\noptions for drones and robots. I just need basic tilt sensitivity, so I picked\nthe cheapest from the SparkFun catalog, the ADXL362\n[https://www.sparkfun.com/products/11446].\n\nThe accelerometer runs in the range of 1.6V to 3.5V, so it would be easiest to\nhave an MCU that ran at the same levels. Keeping in mind the need for\ncompactness, I chose the Arduino Pro Mini 3.3V\n[https://www.sparkfun.com/products/11114].\n\nPower Source\nWhile my daughter had no complaints, I wanted to trim up the power supply from a\ngiant 4xAA cube, to something a little more sleek. With an MCU that runs at 3.3V\nintroduced to the mix, a polymer lithium (LiPo) battery, which generally run at\naround 3.7V, seemed like it would be ideal.\n\n\n\nAt 2,000 mAh and about 2\" wide by 2\" long, this particular battery makes an\nideal fit. It will power the MCU, and the LED, for a long period of time. At\n0.25\" inches thick, it also maps well to a custom enclosure made from slices of\n1/4\" acrylic.\n\nEnclosure\nDesktop fabrication (laser cutting, CNC, 3D printing) has become a favorite\nhobby of mine. I was never very good at power tools, but computer controlled\ntools fit me perfectly. While I usually ship my projects to Ponoko\n[http://www.ponoko.com], I got a laser cutter [https://glowforge.com] for\nChristmas this year!\n\n\n\nGiven that the battery is a quarter-inch, and the electronics will be about the\nsame, I think I will use two sheets of clear acrylic, sandwiched between two 3mm\nsheets of birch plywood. I thought the birch plywood would be a nice touch, as\nit would allow me to engrave custom art personalized to my daughter.\n\nNext Steps\nThe use of a polymer lithium battery introduces one really big challenge - it\nhas be be recharged. Ideally, this needs to happen without opening the case.\n\nWhile both Adafruit and SparkFun make battery chargers, they both place the USB\nconnector flush with the edge of circuit board. Like a product you might buy\nfrom a commercial vendor, I want to expose only the USB port for charging.\n\nIn the end, this means that I am going to have to create a custom circuit board\n- my first.","feature_image":"http://images.kevinhoyt.com/jayne.dawn.day.jpg","featured":0,"status":"published","locale":null,"visibility":"public","author_id":"1","created_at":"2016-01-06T15:08:14.000Z","updated_at":"2016-01-12T15:26:09.000Z","published_at":"2016-01-12T15:26:09.000Z","custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"type":"post","email_recipient_filter":"none"},{"id":"5adb8922351ffe0018a5773c","uuid":"207859d6-7dae-4784-9e39-b2d73d8aac29","title":"IBM Cube: Software","slug":"ibm-cube-software","mobiledoc":"{\"version\":\"0.3.1\",\"markups\":[],\"atoms\":[],\"cards\":[[\"markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\"When I originally designed the IBM Cube, I really wanted to have Qi wireless charging in the top.  The concept was that I could lay my phone on the top of the cube when it needed a charge - ==utility==.  When I dropped that feature, the IBM Cube became largely a wood box with lights.  That means it is up to the software to breath utility back into the IBM Cube.\\n\\nThere are three parts of software to consider for the IBM Cube - the firmware, the server, and the user interface.  I am going to start at the user interface.\\n\\n###User Interface\\n\\nThe UI for the IBM Cube is particularly important since the project itself has ==no screen, no buttons ... no way to directly interact== with it.  The functionality of the UI is also where the utility of the project will live until I revise it and add wireless charging.  When looking for inspiration, I settled on a ==circular interface==.  The design currently has ==four \\\"modes\\\"==.\\n\\n**Finance**\\n\\nIn the financial mode, the UI gives an assortment of ==stocks that I can choose to monitor==.  I went with some general technology stocks to start.  In this mode, the IBM Cube will look up the stock price change, and show red, green, or blue for decrease in price, increase in price, or no change in price, respectively.\\n\\n![Financial Mode](http://images.kevinhoyt.com/cube-financial-mode.png)\\n\\n**Color**\\n\\nIn the color mode, I am presented with an arrangement of colors from which to choose.  Selecting a color will turn the IBM Cube to that color.  I like to think of the utility of this mode as ==mood lighting==.  My daughter likes the pink mood light the best.\\n\\n![Color Mode](http://images.kevinhoyt.com/cube-color-mode.png)\\n\\n**Alarm Mode**\\n\\nThere is not an actual speaker in the IBM Cube, so this is not an alarm in the typical sense.  In this mode, the IBM Cube will ==start lighting up at 30-minutes prior to the set alarm time==.  The UI then allows me to pick the time I want to set as the alarm.\\n\\n![Alarm Mode](http://images.kevinhoyt.com/cube-alarm-mode.png)\\n\\n**Power Mode**\\n\\nThis was originally going to be a \\\"weather mode\\\" in which the color of the IBM Cube would change based on the ==outside temperature==.  I could never decide on a color palette however, so this ended up being \\\"power mode\\\".  In power mode, I can ==turn the IBM Cube off, full brightness, or some measure in-between==.\\n\\n![Power Mode](http://images.kevinhoyt.com/cube-power-mode.png)\\n\\nThe user interface was ==implemented for the browser==, allowing me to quickly and easy change settings from any device I may be holding.  ==SVG== plays a pretty important part throughout the design, as well as rotation with ==CSS transforms==.\\n\\n###Server\\n\\nAll the changes in the user interface ==interact with a server==.  The reason for this is mostly the ==size of the data from related services== such as stock prices (and originally weather).  While these services could be access by the IBM Cube directly, managing memory with the large amount of returned data would have proved quite challenging (if not impossible).\\n\\nThe other reason for the server is to be able to ==save settings==.  \\n\\nWhen the IBM Cube is unplugged, it  loses whatever settings it had.  I could have solved this by putting the settings into ==non-volatile memory==, but then that means the user interface would have to ==poll the device== directly for the settings whenever it was loaded.  This is possible, but it just seemed easier to ==put the settings on the server and share them== across whatever device (IBM Cube or browser) would be accessing them.\\n\\nYou might think I need a server to talk from the browser to the IBM Cube, but that is not actually the case thanks to the [Particle JS SDK](https://docs.particle.io/reference/javascript/).  It is entirely possible to talk directly from the browser, to the Particle Cloud, and into the IBM Cube.  This requires loading of security credentials (not an API key, but your full-on ==account username and password==) onto the client, which was something I did not want to do.\\n\\nWhen a mode is selected, or a setting within a mode is changed (from the browser), the server stores those changes, and then uses the Particle Cloud API to communicate the changes to the IBM Cube.  ==The data presented to the IBM Cube is a greatly simplified, CSV format== of the settings which takes up minimal memory, and is easy to parse.\\n\\n###Firmware\\n\\nThe firmware has two primary functions - ==listen== for incoming setting changes, and ==change== the state of the IBM Cube.  Settings changes can trigger changing the state, but the state of the IBM Cube also needs to change by itself over time.  For example, when a stock price changes.\\n\\n**Settings Changes**\\n\\nSettings changes are handled by a function that is ==exposed== to the [Particle Cloud API](https://docs.particle.io/reference/api/).  This effectively allows the server to call into the IBM Cube in a secure manner.  Settings are then ==parsed== into volatile memory, and a routine is called to ==take action== on those settings.  Here is an example of what the settings string looks like before it is parsed.\\n\\n`F,158,31,99,IBM,8:30,255,255,255`\\n\\nThis is the \\\"financial mode\\\" watching the IBM stock price.  Which as of this writing is up, so the color of the IBM Cube is green.  In \\\"color mode\\\" the IBM Cube would be showing rgb( 158, 31, 99 ).  In \\\"alarm mode\\\" the IBM Cube will start to light up at 8:00 AM, and be completely lit up at 8:30 AM.  And finally, in \\\"power mode\\\" the IBM Cube will be full brightness with a white color.\\n\\n> All said and done, it cost me a maximum of 36 bytes to communicate all that information.\\n\\n**Automatic Changes**\\n\\nFor all the given modes, an interval of about ==one update per minute== seemed more than acceptable.  Among the thirty levels of brightness in the \\\"alarm mode\\\", I am not sure my eye would notice anything more.  In the \\\"financial mode\\\" I am not trading based on the color of the IBM Cube, just glancing at it once in a while to see if the selected stock is up or down.  One minute worked fine here.\\n\\n```\\nvoid loop() {\\n  long now;\\n    \\n  // Seconds since epoch\\n  now = Time.now();\\n    \\n  // Non-blocking delay\\n  if( ( now - last ) > UPDATE_RATE ) {\\n    // Next timer\\n    last = now;\\n\\n    // Request settings from server\\n    request.hostname = CUBE_HOSTNAME;\\n    request.port = CUBE_PORT;\\n    request.path = CUBE_SETTINGS;        \\n        \\n    http.get( request, response, headers );\\n                \\n    // Parse settings\\n    settings_parse( response.body );\\n\\n    // Update cube\\n    refresh();\\n  }\\n}\\n```\\n\\nThe main loop of the firmware, uses a ==non-blocking== approach to the one minute timer.  I could have used delay() or even sleep() but then I would have been unable ==to handle incoming requests== from the user interface.  I did not want to make a change in the UI, only to have to wait up to a minute for the change to take effect.\\n\\nI use the ==HTTPClient library== on the Particle Photon to make REST requests from the firmware.  When requests are made from the IBM Cube, they go back to the same server where the settings are stored first, are then proxied out to the respective services as needed.\\n\\nAs mentioned earlier, the reason for this is in ==keeping tight control over the amount of data that needs to be handled by the firmware==.  Using the stock price service as an example, there is a wide variety of data returned that is likely pertinent to most applications.  All the IBM Cube wants to know however is if the price is up, down, or no change has occurred.  The resulting response to the IBM Cube then is ==boiled down to \\\"+\\\"== for a price increase.  ==A single byte.==\\n\\n###Next Steps\\n\\nThe first iteration is now complete, and sits on the night stand next to my bed.  I have already started on the ==next version==, which will include a ==3D printed USB micro port== for power, and ==Qi wireless charging==.  There will also be some optimization of the circuitry inside the IBM Cube, which should keep the ==wire clutter== inside the cube to a minimum.\\n\\nIf you have any questions about the IBM Cube, or would fancy a similar item for your business, with your logo, then please feel free to email me directly, or reach out to me on [Twitter](http://twitter.com/krhoyt).\"}]],\"sections\":[[10,0]],\"ghostVersion\":\"3.0\"}","html":"<!--kg-card-begin: markdown--><p>When I originally designed the IBM Cube, I really wanted to have Qi wireless charging in the top.  The concept was that I could lay my phone on the top of the cube when it needed a charge - <mark>utility</mark>.  When I dropped that feature, the IBM Cube became largely a wood box with lights.  That means it is up to the software to breath utility back into the IBM Cube.</p>\n<p>There are three parts of software to consider for the IBM Cube - the firmware, the server, and the user interface.  I am going to start at the user interface.</p>\n<h3 id=\"userinterface\">User Interface</h3>\n<p>The UI for the IBM Cube is particularly important since the project itself has <mark>no screen, no buttons ... no way to directly interact</mark> with it.  The functionality of the UI is also where the utility of the project will live until I revise it and add wireless charging.  When looking for inspiration, I settled on a <mark>circular interface</mark>.  The design currently has <mark>four &quot;modes&quot;</mark>.</p>\n<p><strong>Finance</strong></p>\n<p>In the financial mode, the UI gives an assortment of <mark>stocks that I can choose to monitor</mark>.  I went with some general technology stocks to start.  In this mode, the IBM Cube will look up the stock price change, and show red, green, or blue for decrease in price, increase in price, or no change in price, respectively.</p>\n<p><img src=\"http://images.kevinhoyt.com/cube-financial-mode.png\" alt=\"Financial Mode\" loading=\"lazy\"></p>\n<p><strong>Color</strong></p>\n<p>In the color mode, I am presented with an arrangement of colors from which to choose.  Selecting a color will turn the IBM Cube to that color.  I like to think of the utility of this mode as <mark>mood lighting</mark>.  My daughter likes the pink mood light the best.</p>\n<p><img src=\"http://images.kevinhoyt.com/cube-color-mode.png\" alt=\"Color Mode\" loading=\"lazy\"></p>\n<p><strong>Alarm Mode</strong></p>\n<p>There is not an actual speaker in the IBM Cube, so this is not an alarm in the typical sense.  In this mode, the IBM Cube will <mark>start lighting up at 30-minutes prior to the set alarm time</mark>.  The UI then allows me to pick the time I want to set as the alarm.</p>\n<p><img src=\"http://images.kevinhoyt.com/cube-alarm-mode.png\" alt=\"Alarm Mode\" loading=\"lazy\"></p>\n<p><strong>Power Mode</strong></p>\n<p>This was originally going to be a &quot;weather mode&quot; in which the color of the IBM Cube would change based on the <mark>outside temperature</mark>.  I could never decide on a color palette however, so this ended up being &quot;power mode&quot;.  In power mode, I can <mark>turn the IBM Cube off, full brightness, or some measure in-between</mark>.</p>\n<p><img src=\"http://images.kevinhoyt.com/cube-power-mode.png\" alt=\"Power Mode\" loading=\"lazy\"></p>\n<p>The user interface was <mark>implemented for the browser</mark>, allowing me to quickly and easy change settings from any device I may be holding.  <mark>SVG</mark> plays a pretty important part throughout the design, as well as rotation with <mark>CSS transforms</mark>.</p>\n<h3 id=\"server\">Server</h3>\n<p>All the changes in the user interface <mark>interact with a server</mark>.  The reason for this is mostly the <mark>size of the data from related services</mark> such as stock prices (and originally weather).  While these services could be access by the IBM Cube directly, managing memory with the large amount of returned data would have proved quite challenging (if not impossible).</p>\n<p>The other reason for the server is to be able to <mark>save settings</mark>.</p>\n<p>When the IBM Cube is unplugged, it  loses whatever settings it had.  I could have solved this by putting the settings into <mark>non-volatile memory</mark>, but then that means the user interface would have to <mark>poll the device</mark> directly for the settings whenever it was loaded.  This is possible, but it just seemed easier to <mark>put the settings on the server and share them</mark> across whatever device (IBM Cube or browser) would be accessing them.</p>\n<p>You might think I need a server to talk from the browser to the IBM Cube, but that is not actually the case thanks to the <a href=\"https://docs.particle.io/reference/javascript/\">Particle JS SDK</a>.  It is entirely possible to talk directly from the browser, to the Particle Cloud, and into the IBM Cube.  This requires loading of security credentials (not an API key, but your full-on <mark>account username and password</mark>) onto the client, which was something I did not want to do.</p>\n<p>When a mode is selected, or a setting within a mode is changed (from the browser), the server stores those changes, and then uses the Particle Cloud API to communicate the changes to the IBM Cube.  <mark>The data presented to the IBM Cube is a greatly simplified, CSV format</mark> of the settings which takes up minimal memory, and is easy to parse.</p>\n<h3 id=\"firmware\">Firmware</h3>\n<p>The firmware has two primary functions - <mark>listen</mark> for incoming setting changes, and <mark>change</mark> the state of the IBM Cube.  Settings changes can trigger changing the state, but the state of the IBM Cube also needs to change by itself over time.  For example, when a stock price changes.</p>\n<p><strong>Settings Changes</strong></p>\n<p>Settings changes are handled by a function that is <mark>exposed</mark> to the <a href=\"https://docs.particle.io/reference/api/\">Particle Cloud API</a>.  This effectively allows the server to call into the IBM Cube in a secure manner.  Settings are then <mark>parsed</mark> into volatile memory, and a routine is called to <mark>take action</mark> on those settings.  Here is an example of what the settings string looks like before it is parsed.</p>\n<p><code>F,158,31,99,IBM,8:30,255,255,255</code></p>\n<p>This is the &quot;financial mode&quot; watching the IBM stock price.  Which as of this writing is up, so the color of the IBM Cube is green.  In &quot;color mode&quot; the IBM Cube would be showing rgb( 158, 31, 99 ).  In &quot;alarm mode&quot; the IBM Cube will start to light up at 8:00 AM, and be completely lit up at 8:30 AM.  And finally, in &quot;power mode&quot; the IBM Cube will be full brightness with a white color.</p>\n<blockquote>\n<p>All said and done, it cost me a maximum of 36 bytes to communicate all that information.</p>\n</blockquote>\n<p><strong>Automatic Changes</strong></p>\n<p>For all the given modes, an interval of about <mark>one update per minute</mark> seemed more than acceptable.  Among the thirty levels of brightness in the &quot;alarm mode&quot;, I am not sure my eye would notice anything more.  In the &quot;financial mode&quot; I am not trading based on the color of the IBM Cube, just glancing at it once in a while to see if the selected stock is up or down.  One minute worked fine here.</p>\n<pre><code>void loop() {\n  long now;\n    \n  // Seconds since epoch\n  now = Time.now();\n    \n  // Non-blocking delay\n  if( ( now - last ) &gt; UPDATE_RATE ) {\n    // Next timer\n    last = now;\n\n    // Request settings from server\n    request.hostname = CUBE_HOSTNAME;\n    request.port = CUBE_PORT;\n    request.path = CUBE_SETTINGS;        \n        \n    http.get( request, response, headers );\n                \n    // Parse settings\n    settings_parse( response.body );\n\n    // Update cube\n    refresh();\n  }\n}\n</code></pre>\n<p>The main loop of the firmware, uses a <mark>non-blocking</mark> approach to the one minute timer.  I could have used delay() or even sleep() but then I would have been unable <mark>to handle incoming requests</mark> from the user interface.  I did not want to make a change in the UI, only to have to wait up to a minute for the change to take effect.</p>\n<p>I use the <mark>HTTPClient library</mark> on the Particle Photon to make REST requests from the firmware.  When requests are made from the IBM Cube, they go back to the same server where the settings are stored first, are then proxied out to the respective services as needed.</p>\n<p>As mentioned earlier, the reason for this is in <mark>keeping tight control over the amount of data that needs to be handled by the firmware</mark>.  Using the stock price service as an example, there is a wide variety of data returned that is likely pertinent to most applications.  All the IBM Cube wants to know however is if the price is up, down, or no change has occurred.  The resulting response to the IBM Cube then is <mark>boiled down to &quot;+&quot;</mark> for a price increase.  <mark>A single byte.</mark></p>\n<h3 id=\"nextsteps\">Next Steps</h3>\n<p>The first iteration is now complete, and sits on the night stand next to my bed.  I have already started on the <mark>next version</mark>, which will include a <mark>3D printed USB micro port</mark> for power, and <mark>Qi wireless charging</mark>.  There will also be some optimization of the circuitry inside the IBM Cube, which should keep the <mark>wire clutter</mark> inside the cube to a minimum.</p>\n<p>If you have any questions about the IBM Cube, or would fancy a similar item for your business, with your logo, then please feel free to email me directly, or reach out to me on <a href=\"http://twitter.com/krhoyt\">Twitter</a>.</p>\n<!--kg-card-end: markdown-->","comment_id":"51","plaintext":"When I originally designed the IBM Cube, I really wanted to have Qi wireless\ncharging in the top. The concept was that I could lay my phone on the top of the\ncube when it needed a charge - utility. When I dropped that feature, the IBM\nCube became largely a wood box with lights. That means it is up to the software\nto breath utility back into the IBM Cube.\n\nThere are three parts of software to consider for the IBM Cube - the firmware,\nthe server, and the user interface. I am going to start at the user interface.\n\nUser Interface\nThe UI for the IBM Cube is particularly important since the project itself has \nno screen, no buttons ... no way to directly interact with it. The functionality\nof the UI is also where the utility of the project will live until I revise it\nand add wireless charging. When looking for inspiration, I settled on a circular\ninterface. The design currently has four \"modes\".\n\nFinance\n\nIn the financial mode, the UI gives an assortment of stocks that I can choose to\nmonitor. I went with some general technology stocks to start. In this mode, the\nIBM Cube will look up the stock price change, and show red, green, or blue for\ndecrease in price, increase in price, or no change in price, respectively.\n\n\n\nColor\n\nIn the color mode, I am presented with an arrangement of colors from which to\nchoose. Selecting a color will turn the IBM Cube to that color. I like to think\nof the utility of this mode as mood lighting. My daughter likes the pink mood\nlight the best.\n\n\n\nAlarm Mode\n\nThere is not an actual speaker in the IBM Cube, so this is not an alarm in the\ntypical sense. In this mode, the IBM Cube will start lighting up at 30-minutes\nprior to the set alarm time. The UI then allows me to pick the time I want to\nset as the alarm.\n\n\n\nPower Mode\n\nThis was originally going to be a \"weather mode\" in which the color of the IBM\nCube would change based on the outside temperature. I could never decide on a\ncolor palette however, so this ended up being \"power mode\". In power mode, I can \nturn the IBM Cube off, full brightness, or some measure in-between.\n\n\n\nThe user interface was implemented for the browser, allowing me to quickly and\neasy change settings from any device I may be holding. SVG plays a pretty\nimportant part throughout the design, as well as rotation with CSS transforms.\n\nServer\nAll the changes in the user interface interact with a server. The reason for\nthis is mostly the size of the data from related services such as stock prices\n(and originally weather). While these services could be access by the IBM Cube\ndirectly, managing memory with the large amount of returned data would have\nproved quite challenging (if not impossible).\n\nThe other reason for the server is to be able to save settings.\n\nWhen the IBM Cube is unplugged, it loses whatever settings it had. I could have\nsolved this by putting the settings into non-volatile memory, but then that\nmeans the user interface would have to poll the device directly for the settings\nwhenever it was loaded. This is possible, but it just seemed easier to put the\nsettings on the server and share them across whatever device (IBM Cube or\nbrowser) would be accessing them.\n\nYou might think I need a server to talk from the browser to the IBM Cube, but\nthat is not actually the case thanks to the Particle JS SDK\n[https://docs.particle.io/reference/javascript/]. It is entirely possible to\ntalk directly from the browser, to the Particle Cloud, and into the IBM Cube.\nThis requires loading of security credentials (not an API key, but your full-on \naccount username and password) onto the client, which was something I did not\nwant to do.\n\nWhen a mode is selected, or a setting within a mode is changed (from the\nbrowser), the server stores those changes, and then uses the Particle Cloud API\nto communicate the changes to the IBM Cube. The data presented to the IBM Cube\nis a greatly simplified, CSV format of the settings which takes up minimal\nmemory, and is easy to parse.\n\nFirmware\nThe firmware has two primary functions - listen for incoming setting changes,\nand change the state of the IBM Cube. Settings changes can trigger changing the\nstate, but the state of the IBM Cube also needs to change by itself over time.\nFor example, when a stock price changes.\n\nSettings Changes\n\nSettings changes are handled by a function that is exposed to the Particle\nCloud\nAPI [https://docs.particle.io/reference/api/]. This effectively allows the\nserver to call into the IBM Cube in a secure manner. Settings are then parsed \ninto volatile memory, and a routine is called to take action on those settings.\nHere is an example of what the settings string looks like before it is parsed.\n\nF,158,31,99,IBM,8:30,255,255,255\n\nThis is the \"financial mode\" watching the IBM stock price. Which as of this\nwriting is up, so the color of the IBM Cube is green. In \"color mode\" the IBM\nCube would be showing rgb( 158, 31, 99 ). In \"alarm mode\" the IBM Cube will\nstart to light up at 8:00 AM, and be completely lit up at 8:30 AM. And finally,\nin \"power mode\" the IBM Cube will be full brightness with a white color.\n\n> All said and done, it cost me a maximum of 36 bytes to communicate all that\ninformation.\n\n\nAutomatic Changes\n\nFor all the given modes, an interval of about one update per minute seemed more\nthan acceptable. Among the thirty levels of brightness in the \"alarm mode\", I am\nnot sure my eye would notice anything more. In the \"financial mode\" I am not\ntrading based on the color of the IBM Cube, just glancing at it once in a while\nto see if the selected stock is up or down. One minute worked fine here.\n\nvoid loop() {\n  long now;\n    \n  // Seconds since epoch\n  now = Time.now();\n    \n  // Non-blocking delay\n  if( ( now - last ) > UPDATE_RATE ) {\n    // Next timer\n    last = now;\n\n    // Request settings from server\n    request.hostname = CUBE_HOSTNAME;\n    request.port = CUBE_PORT;\n    request.path = CUBE_SETTINGS;        \n        \n    http.get( request, response, headers );\n                \n    // Parse settings\n    settings_parse( response.body );\n\n    // Update cube\n    refresh();\n  }\n}\n\n\nThe main loop of the firmware, uses a non-blocking approach to the one minute\ntimer. I could have used delay() or even sleep() but then I would have been\nunable to handle incoming requests from the user interface. I did not want to\nmake a change in the UI, only to have to wait up to a minute for the change to\ntake effect.\n\nI use the HTTPClient library on the Particle Photon to make REST requests from\nthe firmware. When requests are made from the IBM Cube, they go back to the same\nserver where the settings are stored first, are then proxied out to the\nrespective services as needed.\n\nAs mentioned earlier, the reason for this is in keeping tight control over the\namount of data that needs to be handled by the firmware. Using the stock price\nservice as an example, there is a wide variety of data returned that is likely\npertinent to most applications. All the IBM Cube wants to know however is if the\nprice is up, down, or no change has occurred. The resulting response to the IBM\nCube then is boiled down to \"+\" for a price increase. A single byte.\n\nNext Steps\nThe first iteration is now complete, and sits on the night stand next to my bed.\nI have already started on the next version, which will include a 3D printed USB\nmicro port for power, and Qi wireless charging. There will also be some\noptimization of the circuitry inside the IBM Cube, which should keep the wire\nclutter inside the cube to a minimum.\n\nIf you have any questions about the IBM Cube, or would fancy a similar item for\nyour business, with your logo, then please feel free to email me directly, or\nreach out to me on Twitter [http://twitter.com/krhoyt].","feature_image":"__GHOST_URL__/content/images/2019/07/cube-completed-day-1.jpg","featured":0,"status":"published","locale":null,"visibility":"public","author_id":"1","created_at":"2016-02-29T16:24:16.000Z","updated_at":"2019-07-03T20:55:24.000Z","published_at":"2016-02-29T18:54:38.000Z","custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"type":"post","email_recipient_filter":"none"},{"id":"5adb8922351ffe0018a5773d","uuid":"70520d4f-27ec-4dba-8cc4-644205364069","title":"Particle Photon on Watson IoT","slug":"particle-photon-on-watson-iot","mobiledoc":"{\"version\":\"0.3.1\",\"markups\":[],\"atoms\":[],\"cards\":[[\"markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\"==IBM== has had an ==IoT platform== offering for a while now in various forms.  The most recent evolution falls under the Watson brand, and includes ==device management, and authentication==.  Marketed as the [Watson IoT Platform](https://console.ng.bluemix.net/catalog/services/internet-of-things-platform/), the messaging layer is ==MQTT== supported by [IBM MessageSight](http://www-03.ibm.com/software/products/en/messagesight).\\n\\n==MQTT== is an [ISO standard](http://docs.oasis-open.org/mqtt/mqtt/v3.1.1/mqtt-v3.1.1.html) largely conceived at IBM, and you can find implementations of it for just about any platform or language.  There is even support for MQTT over WebSocket via the Eclipse Paho JavaScript [library](https://www.eclipse.org/paho/clients/js/).\\n\\nOn the device side, there are MQTT implementations for [Electric Imp](https://electricimp.com/) and [Arduino](http://pubsubclient.knolleary.net/), as well as Linux-based systems such as [Raspberry Pi](https://www.npmjs.com/package/mqtt) and the [Intel Edison](https://www.eclipse.org/paho/clients/java/).  Today I will take a look at connecting a [Particle Photon](https://www.particle.io/) to Watson IoT.\\n\\n###Watson IoT Setup\\n\\n==Watson IoT runs on IBM Bluemix==.  The details of setting up a Bluemix account, and creating an IoT instance are beyond the scope of this blog post - mostly because the Bluemix user interface is constantly changing, and any pictures posted here would likely be out of date in just a months time.\\n\\n![IoT Platform inside the Bluemix dashboard.](http://images.kevinhoyt.com/watson.iot.dashboard.png)\\n\\nAt a high level however, once you have a Bluemix account, you can search the \\\"Catalog\\\" for \\\"iot\\\" and find the \\\"==Internet of Things Platform==\\\" service.  Create an instance of that service, which you can leave unbound.  Bringing it up in the Bluemix dashboard will reveal an option to \\\"Launch Dashboard\\\" which is where we will pick up.\\n\\n**Device Type**\\n\\nOnce you have reached the Watson IoT Platform management console, you will find a series of icons down the lefthand side of the screen.  You are looking for one labeled \\\"==Devices==\\\" which as of this writing is the second icon down.\\n\\n![Watson IoT Plaform overview screen](http://images.kevinhoyt.com/watson.iot.overview.png)\\n\\nBecause (a) we do not want just random devices connecting to our service and (b) we want to be able to interact with specific devices, it is here where we can define devices types, and then specific instances.  Click on the \\\"==Add Device==\\\" button to define your first device.\\n\\n![Creating a device type.](http://images.kevinhoyt.com/watson.iot.create.type.png)\\n\\nWhen prompted for a name, enter \\\"Photon\\\" as we will define additional attributes in subsequent steps.\\n\\nInitially you will have no device types defined, so select \\\"==Create a device type==\\\".  You will then be walked through a series of steps to setup information about this device type.  You can even define attributes common to this device type to help you find deployed devices later.\\n\\n![Specify a Watson IoT manufacturer](http://images.kevinhoyt.com/watson.iot.manufacturer.png)\\n\\nSelect \\\"Manufacturer\\\" here, and supply \\\"Particle\\\" for demonstration purposes.  \\n\\nIf you have any additional information about this device type that is not already captured, you can enter that in the metadata.  We will leave this blank.\\n\\n**Device Instance**\\n\\nThis completes the creation of the device type.  Now it is time to create an instance of the device.  You will now find \\\"Photon\\\" in the device type list.  Select it and click the \\\"Next\\\" button.\\n\\nThe second screen will ask you for the ==device ID==.  How you use this field is pretty open.  When I present at conferences, I like to use the conference name as the device ID.  In an industrial setting, you might use building numbers, warehouse sections, or other data.\\n\\n> Note that the device ID will come up again, as will the device type.  These are key pieces of information that will be used in publish/subscribe topic configuration.  You will want them to be simple, yet descriptive.\\n\\nAfter adding any additional metadata for this device instance, you will be asked about ==security==.    Clicking \\\"Next\\\" here will automatically generate a security token, which is what I usually do.  You will not get another chance to view the token, so you will want to store it somewhere safe - it will come up again as we program the Photon to connect over MQTT.\\n\\n![Watson IoT device instance security settings.](http://images.kevinhoyt.com/watson.iot.device.security.png)\\n\\nNow you have an instance of the Particle Photon device type created.  At the end of the creation process, you will be presented with a ==summary screen==.  You can also access this screen by clicking on a device instance in the device listing.  Notice the \\\"==Recent Events==\\\" and \\\"==Sensor Information==\\\" are currently blank.  Once we connect our Photon, these will come to life.\\n\\n###Particle Photon\\n\\nJust as this blog post does not cover using Bluemix, this blog post assumes that you know how to configure your Photon, and use Particle tooling.  I will be using the online editor, but a similar workflow can be applied should you be using the desktop IDE.\\n\\nAlong the lefthand side of the IDE screen, one of the icons represents the available ==libraries== for the Particle platform.  If you search for \\\"MQTT\\\" you will find an ==MQTT== implementation, which you can add to your application.  There is even an \\\"mqtttest.ino\\\" example file to use.  We only need to tweak the settings to point to our Watson IoT instance.\\n\\n```\\n#include \\\"MQTT/MQTT.h\\\"\\n\\nchar *IOT_CLIENT = \\\"d:ts200f:Photon:WebVisions\\\";\\nchar *IOT_HOST = \\\"_your_organization_here_.messaging.internetofthings.ibmcloud.com\\\";\\nchar *IOT_PASSWORD = \\\"_device_token_here_\\\";\\nchar *IOT_PUBLISH = \\\"iot-2/evt/count/fmt/json\\\";\\nchar *IOT_USERNAME = \\\"use-token-auth\\\";\\n\\nint count = 0;\\n\\nMQTT client( IOT_HOST, 1883, callback );\\n\\nvoid setup() {\\n  Serial.begin( 9600 );\\n\\n  while( !Serial.available() ) {\\n    Particle.process();\\n  }   \\n\\n  client.connect( \\n    IOT_CLIENT, \\n    IOT_USERNAME, \\n    IOT_PASSWORD \\n  );\\n\\n  if( client.isConnected() ) {\\n    Serial.println( \\\"Connected.\\\" );\\n    // client.subscribe( IOT_SUBSCRIBE );\\n  }\\n}\\n\\nvoid loop() {\\n  count = count + 1;\\n\\n  client.publish( \\n    IOT_PUBLISH, \\n    \\\"{\\\\\\\"count\\\\\\\": \\\" + count + \\\"}\\\" \\n  );            \\n  client.loop();\\n\\n  Serial.print( \\\"Count: \\\" );\\n  Serial.println( count );\\n  delay( 1000 );\\n}\\n\\nvoid callback( char* topic, byte* payload, unsigned int length ) {\\n  char p[length + 1];\\n    \\n  memcpy( p, payload, length );\\n  p[length] = NULL;\\n    \\n  String message( p );\\n}\\n\\n```\\n\\n**Client ID**\\n\\nEach device will need to have a unique client ID when communicating to Watson IoT.  In the code above, I use \\\"d:ts200f:Photon:WebVisions\\\".  The format of this string is ==very specific== (and specified in the documentation).  If you get part of it wrong, you will be immediately disconnected from Watson IoT.\\n\\nThe \\\"d\\\" part of the client ID string indicates that this is a ==device== that is connecting.  Applications can also connect to Watson IoT, in which case this part of the client ID would be \\\"a\\\".\\n\\nThe \\\"ts200f\\\" is the automatically generated ==organization== name given to my instance of Watson IoT.  The easiest place to find the organization name of your instance, is to look in the address bar of your browser.  Your organization name will be the first part of the domain.\\n\\nThe \\\"Photon\\\" part of the client ID is the ==device type== (not the instance).  This is the device type we created earlier.\\n\\nThe \\\"WebVisions\\\" part of the client ID is the ==device ID==.  In deployment, you would want to have the \\\"WebVisions\\\" part of the string be unique across all possible device instances for this device type.  ==You might accomplish this with a random number, UUID, MAC address, serial number, timestamp, or any other approach.==\\n\\n**Host**\\n\\nThe host in the code above is \\\"_your_organization_here_.messaging.internetofthings.ibmcloud.com\\\".  I talked about where to get the organization name of your Watson IoT instance in the previous section.  Just in case, you can find it as the ==first part of the domain in the address bar of your browser==.\\n\\n**Password**\\n\\nRemember that token we generated when we created an instance of a device type?  The token I warned you that you would need again, and only be able to see that once?  Yeah, that token is your password when connecting with devices.  When connecting as an application, you will use an API key and token from \\\"Access\\\" screen of Watson IoT.\\n\\n**Publish Topic**\\n\\nJust as the client ID is very specific, so is the topic naming when it comes to Watson IoT.  In the above code, I use \\\"iot-2/evt/count/fmt/json\\\".\\n\\n> Watson IoT devices have the option of publishing events, or subscribing to commands (e.g. update).  Applications by contrast can publish and subscribe to events and commands (and more).  \\n\\nWhat is the difference between an ==application== and a ==device==?  Largely the ==format of the client ID==, and the ==username/password== provided.  A Particle Photon could just as effectively behave as an application.  We are treating it as a device for the example, which is why the \\\"subscribe\\\" line is commented out.\\n\\nSince we are treating this Photon as a ==device==, it can only ==publish events==, so the first two parts of our topic name will always be \\\"iot-2/evt/\\\".  The next part is an event name, which you can name to match your specific needs.  And then finally, we will be publishing ==JSON== formatted messages, so the last part of the topic name is \\\"/fmt/json\\\".\\n\\nYou do not have to use JSON, and can find other options in the documentation, but JSON messages can be stored directly by Watson IoT (optional).\\n\\n**Username**\\n\\nWhen connecting as a \\\"==device==\\\" the username will always be \\\"==use-token-auth==\\\".  Combined with the specific client ID, and the token used for the password, this allows Watson IoT to authenticate your specific device.\\n\\nWhen ==authenticating as an application==, an API key and additional token value will be used as username and password.  This is useful if for example you are using [Node-RED](http://nodered.org/) to handle messages, or have a browser-based application using the Eclipse Paho MQTT library for JavaScript.\\n\\n**Instantiate Client**\\n\\nThe remainder of the code should be self explanatory.  We will create an instance of the MQTT client, connect to Watson IoT, and publish a message once every second.  \\n\\nThe ==client instantiation requires a callback== function for when messages arrive from a subscribed topic.  Since \\\"devices\\\" cannot subscribe to topics within the context of Watson IoT, the callback is ==never used==.  If you were treating this Photon as an \\\"application\\\" then you would be able to subscribe to events and callback handler would be used.\\n\\n###Next Steps\\n\\nWith the Photon code flashed to the physical device, and connected to Watson IoT, head back to the management console.  If you still have the \\\"Photon\\\" instance selected, you will now see messages start showing up under the \\\"==Recent Events==\\\" and \\\"==Sensor Information==\\\" sections.  If there was an error connecting, you can scroll down and check out the connection log.\\n\\nFrom here you would likely connect some other client to Watson IoT - probably as an application.  The client ID, username, and password connection strings would need to be changed, and then you could event connect the Photon to Watson IoT.  ==Anything that can connect to an MQTT broker can participate in the conversation.==\\n\\nUsing a publish/subscribe pattern is an ==immensely powerful== foundation for ==high-performance, event-driven== application - IoT or otherwise.  The pattern allows you full decoupling of ==microservices==, and can take you down the path of ==reactive programming==.  Skills that once mastered make testable, Web-scale applications, a snap.\\n\"}]],\"sections\":[[10,0]],\"ghostVersion\":\"3.0\"}","html":"<!--kg-card-begin: markdown--><p><mark>IBM</mark> has had an <mark>IoT platform</mark> offering for a while now in various forms.  The most recent evolution falls under the Watson brand, and includes <mark>device management, and authentication</mark>.  Marketed as the <a href=\"https://console.ng.bluemix.net/catalog/services/internet-of-things-platform/\">Watson IoT Platform</a>, the messaging layer is <mark>MQTT</mark> supported by <a href=\"http://www-03.ibm.com/software/products/en/messagesight\">IBM MessageSight</a>.</p>\n<p><mark>MQTT</mark> is an <a href=\"http://docs.oasis-open.org/mqtt/mqtt/v3.1.1/mqtt-v3.1.1.html\">ISO standard</a> largely conceived at IBM, and you can find implementations of it for just about any platform or language.  There is even support for MQTT over WebSocket via the Eclipse Paho JavaScript <a href=\"https://www.eclipse.org/paho/clients/js/\">library</a>.</p>\n<p>On the device side, there are MQTT implementations for <a href=\"https://electricimp.com/\">Electric Imp</a> and <a href=\"http://pubsubclient.knolleary.net/\">Arduino</a>, as well as Linux-based systems such as <a href=\"https://www.npmjs.com/package/mqtt\">Raspberry Pi</a> and the <a href=\"https://www.eclipse.org/paho/clients/java/\">Intel Edison</a>.  Today I will take a look at connecting a <a href=\"https://www.particle.io/\">Particle Photon</a> to Watson IoT.</p>\n<h3 id=\"watsoniotsetup\">Watson IoT Setup</h3>\n<p><mark>Watson IoT runs on IBM Bluemix</mark>.  The details of setting up a Bluemix account, and creating an IoT instance are beyond the scope of this blog post - mostly because the Bluemix user interface is constantly changing, and any pictures posted here would likely be out of date in just a months time.</p>\n<p><img src=\"http://images.kevinhoyt.com/watson.iot.dashboard.png\" alt=\"IoT Platform inside the Bluemix dashboard.\" loading=\"lazy\"></p>\n<p>At a high level however, once you have a Bluemix account, you can search the &quot;Catalog&quot; for &quot;iot&quot; and find the &quot;<mark>Internet of Things Platform</mark>&quot; service.  Create an instance of that service, which you can leave unbound.  Bringing it up in the Bluemix dashboard will reveal an option to &quot;Launch Dashboard&quot; which is where we will pick up.</p>\n<p><strong>Device Type</strong></p>\n<p>Once you have reached the Watson IoT Platform management console, you will find a series of icons down the lefthand side of the screen.  You are looking for one labeled &quot;<mark>Devices</mark>&quot; which as of this writing is the second icon down.</p>\n<p><img src=\"http://images.kevinhoyt.com/watson.iot.overview.png\" alt=\"Watson IoT Plaform overview screen\" loading=\"lazy\"></p>\n<p>Because (a) we do not want just random devices connecting to our service and (b) we want to be able to interact with specific devices, it is here where we can define devices types, and then specific instances.  Click on the &quot;<mark>Add Device</mark>&quot; button to define your first device.</p>\n<p><img src=\"http://images.kevinhoyt.com/watson.iot.create.type.png\" alt=\"Creating a device type.\" loading=\"lazy\"></p>\n<p>When prompted for a name, enter &quot;Photon&quot; as we will define additional attributes in subsequent steps.</p>\n<p>Initially you will have no device types defined, so select &quot;<mark>Create a device type</mark>&quot;.  You will then be walked through a series of steps to setup information about this device type.  You can even define attributes common to this device type to help you find deployed devices later.</p>\n<p><img src=\"http://images.kevinhoyt.com/watson.iot.manufacturer.png\" alt=\"Specify a Watson IoT manufacturer\" loading=\"lazy\"></p>\n<p>Select &quot;Manufacturer&quot; here, and supply &quot;Particle&quot; for demonstration purposes.</p>\n<p>If you have any additional information about this device type that is not already captured, you can enter that in the metadata.  We will leave this blank.</p>\n<p><strong>Device Instance</strong></p>\n<p>This completes the creation of the device type.  Now it is time to create an instance of the device.  You will now find &quot;Photon&quot; in the device type list.  Select it and click the &quot;Next&quot; button.</p>\n<p>The second screen will ask you for the <mark>device ID</mark>.  How you use this field is pretty open.  When I present at conferences, I like to use the conference name as the device ID.  In an industrial setting, you might use building numbers, warehouse sections, or other data.</p>\n<blockquote>\n<p>Note that the device ID will come up again, as will the device type.  These are key pieces of information that will be used in publish/subscribe topic configuration.  You will want them to be simple, yet descriptive.</p>\n</blockquote>\n<p>After adding any additional metadata for this device instance, you will be asked about <mark>security</mark>.    Clicking &quot;Next&quot; here will automatically generate a security token, which is what I usually do.  You will not get another chance to view the token, so you will want to store it somewhere safe - it will come up again as we program the Photon to connect over MQTT.</p>\n<p><img src=\"http://images.kevinhoyt.com/watson.iot.device.security.png\" alt=\"Watson IoT device instance security settings.\" loading=\"lazy\"></p>\n<p>Now you have an instance of the Particle Photon device type created.  At the end of the creation process, you will be presented with a <mark>summary screen</mark>.  You can also access this screen by clicking on a device instance in the device listing.  Notice the &quot;<mark>Recent Events</mark>&quot; and &quot;<mark>Sensor Information</mark>&quot; are currently blank.  Once we connect our Photon, these will come to life.</p>\n<h3 id=\"particlephoton\">Particle Photon</h3>\n<p>Just as this blog post does not cover using Bluemix, this blog post assumes that you know how to configure your Photon, and use Particle tooling.  I will be using the online editor, but a similar workflow can be applied should you be using the desktop IDE.</p>\n<p>Along the lefthand side of the IDE screen, one of the icons represents the available <mark>libraries</mark> for the Particle platform.  If you search for &quot;MQTT&quot; you will find an <mark>MQTT</mark> implementation, which you can add to your application.  There is even an &quot;mqtttest.ino&quot; example file to use.  We only need to tweak the settings to point to our Watson IoT instance.</p>\n<pre><code>#include &quot;MQTT/MQTT.h&quot;\n\nchar *IOT_CLIENT = &quot;d:ts200f:Photon:WebVisions&quot;;\nchar *IOT_HOST = &quot;_your_organization_here_.messaging.internetofthings.ibmcloud.com&quot;;\nchar *IOT_PASSWORD = &quot;_device_token_here_&quot;;\nchar *IOT_PUBLISH = &quot;iot-2/evt/count/fmt/json&quot;;\nchar *IOT_USERNAME = &quot;use-token-auth&quot;;\n\nint count = 0;\n\nMQTT client( IOT_HOST, 1883, callback );\n\nvoid setup() {\n  Serial.begin( 9600 );\n\n  while( !Serial.available() ) {\n    Particle.process();\n  }   \n\n  client.connect( \n    IOT_CLIENT, \n    IOT_USERNAME, \n    IOT_PASSWORD \n  );\n\n  if( client.isConnected() ) {\n    Serial.println( &quot;Connected.&quot; );\n    // client.subscribe( IOT_SUBSCRIBE );\n  }\n}\n\nvoid loop() {\n  count = count + 1;\n\n  client.publish( \n    IOT_PUBLISH, \n    &quot;{\\&quot;count\\&quot;: &quot; + count + &quot;}&quot; \n  );            \n  client.loop();\n\n  Serial.print( &quot;Count: &quot; );\n  Serial.println( count );\n  delay( 1000 );\n}\n\nvoid callback( char* topic, byte* payload, unsigned int length ) {\n  char p[length + 1];\n    \n  memcpy( p, payload, length );\n  p[length] = NULL;\n    \n  String message( p );\n}\n\n</code></pre>\n<p><strong>Client ID</strong></p>\n<p>Each device will need to have a unique client ID when communicating to Watson IoT.  In the code above, I use &quot;d:ts200f:Photon:WebVisions&quot;.  The format of this string is <mark>very specific</mark> (and specified in the documentation).  If you get part of it wrong, you will be immediately disconnected from Watson IoT.</p>\n<p>The &quot;d&quot; part of the client ID string indicates that this is a <mark>device</mark> that is connecting.  Applications can also connect to Watson IoT, in which case this part of the client ID would be &quot;a&quot;.</p>\n<p>The &quot;ts200f&quot; is the automatically generated <mark>organization</mark> name given to my instance of Watson IoT.  The easiest place to find the organization name of your instance, is to look in the address bar of your browser.  Your organization name will be the first part of the domain.</p>\n<p>The &quot;Photon&quot; part of the client ID is the <mark>device type</mark> (not the instance).  This is the device type we created earlier.</p>\n<p>The &quot;WebVisions&quot; part of the client ID is the <mark>device ID</mark>.  In deployment, you would want to have the &quot;WebVisions&quot; part of the string be unique across all possible device instances for this device type.  <mark>You might accomplish this with a random number, UUID, MAC address, serial number, timestamp, or any other approach.</mark></p>\n<p><strong>Host</strong></p>\n<p>The host in the code above is &quot;<em>your_organization_here</em>.messaging.internetofthings.ibmcloud.com&quot;.  I talked about where to get the organization name of your Watson IoT instance in the previous section.  Just in case, you can find it as the <mark>first part of the domain in the address bar of your browser</mark>.</p>\n<p><strong>Password</strong></p>\n<p>Remember that token we generated when we created an instance of a device type?  The token I warned you that you would need again, and only be able to see that once?  Yeah, that token is your password when connecting with devices.  When connecting as an application, you will use an API key and token from &quot;Access&quot; screen of Watson IoT.</p>\n<p><strong>Publish Topic</strong></p>\n<p>Just as the client ID is very specific, so is the topic naming when it comes to Watson IoT.  In the above code, I use &quot;iot-2/evt/count/fmt/json&quot;.</p>\n<blockquote>\n<p>Watson IoT devices have the option of publishing events, or subscribing to commands (e.g. update).  Applications by contrast can publish and subscribe to events and commands (and more).</p>\n</blockquote>\n<p>What is the difference between an <mark>application</mark> and a <mark>device</mark>?  Largely the <mark>format of the client ID</mark>, and the <mark>username/password</mark> provided.  A Particle Photon could just as effectively behave as an application.  We are treating it as a device for the example, which is why the &quot;subscribe&quot; line is commented out.</p>\n<p>Since we are treating this Photon as a <mark>device</mark>, it can only <mark>publish events</mark>, so the first two parts of our topic name will always be &quot;iot-2/evt/&quot;.  The next part is an event name, which you can name to match your specific needs.  And then finally, we will be publishing <mark>JSON</mark> formatted messages, so the last part of the topic name is &quot;/fmt/json&quot;.</p>\n<p>You do not have to use JSON, and can find other options in the documentation, but JSON messages can be stored directly by Watson IoT (optional).</p>\n<p><strong>Username</strong></p>\n<p>When connecting as a &quot;<mark>device</mark>&quot; the username will always be &quot;<mark>use-token-auth</mark>&quot;.  Combined with the specific client ID, and the token used for the password, this allows Watson IoT to authenticate your specific device.</p>\n<p>When <mark>authenticating as an application</mark>, an API key and additional token value will be used as username and password.  This is useful if for example you are using <a href=\"http://nodered.org/\">Node-RED</a> to handle messages, or have a browser-based application using the Eclipse Paho MQTT library for JavaScript.</p>\n<p><strong>Instantiate Client</strong></p>\n<p>The remainder of the code should be self explanatory.  We will create an instance of the MQTT client, connect to Watson IoT, and publish a message once every second.</p>\n<p>The <mark>client instantiation requires a callback</mark> function for when messages arrive from a subscribed topic.  Since &quot;devices&quot; cannot subscribe to topics within the context of Watson IoT, the callback is <mark>never used</mark>.  If you were treating this Photon as an &quot;application&quot; then you would be able to subscribe to events and callback handler would be used.</p>\n<h3 id=\"nextsteps\">Next Steps</h3>\n<p>With the Photon code flashed to the physical device, and connected to Watson IoT, head back to the management console.  If you still have the &quot;Photon&quot; instance selected, you will now see messages start showing up under the &quot;<mark>Recent Events</mark>&quot; and &quot;<mark>Sensor Information</mark>&quot; sections.  If there was an error connecting, you can scroll down and check out the connection log.</p>\n<p>From here you would likely connect some other client to Watson IoT - probably as an application.  The client ID, username, and password connection strings would need to be changed, and then you could event connect the Photon to Watson IoT.  <mark>Anything that can connect to an MQTT broker can participate in the conversation.</mark></p>\n<p>Using a publish/subscribe pattern is an <mark>immensely powerful</mark> foundation for <mark>high-performance, event-driven</mark> application - IoT or otherwise.  The pattern allows you full decoupling of <mark>microservices</mark>, and can take you down the path of <mark>reactive programming</mark>.  Skills that once mastered make testable, Web-scale applications, a snap.</p>\n<!--kg-card-end: markdown-->","comment_id":"52","plaintext":"IBM has had an IoT platform offering for a while now in various forms. The most\nrecent evolution falls under the Watson brand, and includes device management,\nand authentication. Marketed as the Watson IoT Platform\n[https://console.ng.bluemix.net/catalog/services/internet-of-things-platform/],\nthe messaging layer is MQTT supported by IBM MessageSight\n[http://www-03.ibm.com/software/products/en/messagesight].\n\nMQTT is an ISO standard\n[http://docs.oasis-open.org/mqtt/mqtt/v3.1.1/mqtt-v3.1.1.html] largely conceived\nat IBM, and you can find implementations of it for just about any platform or\nlanguage. There is even support for MQTT over WebSocket via the Eclipse Paho\nJavaScript library [https://www.eclipse.org/paho/clients/js/].\n\nOn the device side, there are MQTT implementations for Electric Imp\n[https://electricimp.com/] and Arduino [http://pubsubclient.knolleary.net/], as\nwell as Linux-based systems such as Raspberry Pi\n[https://www.npmjs.com/package/mqtt] and the Intel Edison\n[https://www.eclipse.org/paho/clients/java/]. Today I will take a look at\nconnecting a Particle Photon [https://www.particle.io/] to Watson IoT.\n\nWatson IoT Setup\nWatson IoT runs on IBM Bluemix. The details of setting up a Bluemix account, and\ncreating an IoT instance are beyond the scope of this blog post - mostly because\nthe Bluemix user interface is constantly changing, and any pictures posted here\nwould likely be out of date in just a months time.\n\n\n\nAt a high level however, once you have a Bluemix account, you can search the\n\"Catalog\" for \"iot\" and find the \"Internet of Things Platform\" service. Create\nan instance of that service, which you can leave unbound. Bringing it up in the\nBluemix dashboard will reveal an option to \"Launch Dashboard\" which is where we\nwill pick up.\n\nDevice Type\n\nOnce you have reached the Watson IoT Platform management console, you will find\na series of icons down the lefthand side of the screen. You are looking for one\nlabeled \"Devices\" which as of this writing is the second icon down.\n\n\n\nBecause (a) we do not want just random devices connecting to our service and (b)\nwe want to be able to interact with specific devices, it is here where we can\ndefine devices types, and then specific instances. Click on the \"Add Device\"\nbutton to define your first device.\n\n\n\nWhen prompted for a name, enter \"Photon\" as we will define additional attributes\nin subsequent steps.\n\nInitially you will have no device types defined, so select \"Create a device type\n\". You will then be walked through a series of steps to setup information about\nthis device type. You can even define attributes common to this device type to\nhelp you find deployed devices later.\n\n\n\nSelect \"Manufacturer\" here, and supply \"Particle\" for demonstration purposes.\n\nIf you have any additional information about this device type that is not\nalready captured, you can enter that in the metadata. We will leave this blank.\n\nDevice Instance\n\nThis completes the creation of the device type. Now it is time to create an\ninstance of the device. You will now find \"Photon\" in the device type list.\nSelect it and click the \"Next\" button.\n\nThe second screen will ask you for the device ID. How you use this field is\npretty open. When I present at conferences, I like to use the conference name as\nthe device ID. In an industrial setting, you might use building numbers,\nwarehouse sections, or other data.\n\n> Note that the device ID will come up again, as will the device type. These are\nkey pieces of information that will be used in publish/subscribe topic\nconfiguration. You will want them to be simple, yet descriptive.\n\n\nAfter adding any additional metadata for this device instance, you will be asked\nabout security. Clicking \"Next\" here will automatically generate a security\ntoken, which is what I usually do. You will not get another chance to view the\ntoken, so you will want to store it somewhere safe - it will come up again as we\nprogram the Photon to connect over MQTT.\n\n\n\nNow you have an instance of the Particle Photon device type created. At the end\nof the creation process, you will be presented with a summary screen. You can\nalso access this screen by clicking on a device instance in the device listing.\nNotice the \"Recent Events\" and \"Sensor Information\" are currently blank. Once we\nconnect our Photon, these will come to life.\n\nParticle Photon\nJust as this blog post does not cover using Bluemix, this blog post assumes that\nyou know how to configure your Photon, and use Particle tooling. I will be using\nthe online editor, but a similar workflow can be applied should you be using the\ndesktop IDE.\n\nAlong the lefthand side of the IDE screen, one of the icons represents the\navailable libraries for the Particle platform. If you search for \"MQTT\" you will\nfind an MQTT implementation, which you can add to your application. There is\neven an \"mqtttest.ino\" example file to use. We only need to tweak the settings\nto point to our Watson IoT instance.\n\n#include \"MQTT/MQTT.h\"\n\nchar *IOT_CLIENT = \"d:ts200f:Photon:WebVisions\";\nchar *IOT_HOST = \"_your_organization_here_.messaging.internetofthings.ibmcloud.com\";\nchar *IOT_PASSWORD = \"_device_token_here_\";\nchar *IOT_PUBLISH = \"iot-2/evt/count/fmt/json\";\nchar *IOT_USERNAME = \"use-token-auth\";\n\nint count = 0;\n\nMQTT client( IOT_HOST, 1883, callback );\n\nvoid setup() {\n  Serial.begin( 9600 );\n\n  while( !Serial.available() ) {\n    Particle.process();\n  }   \n\n  client.connect( \n    IOT_CLIENT, \n    IOT_USERNAME, \n    IOT_PASSWORD \n  );\n\n  if( client.isConnected() ) {\n    Serial.println( \"Connected.\" );\n    // client.subscribe( IOT_SUBSCRIBE );\n  }\n}\n\nvoid loop() {\n  count = count + 1;\n\n  client.publish( \n    IOT_PUBLISH, \n    \"{\\\"count\\\": \" + count + \"}\" \n  );            \n  client.loop();\n\n  Serial.print( \"Count: \" );\n  Serial.println( count );\n  delay( 1000 );\n}\n\nvoid callback( char* topic, byte* payload, unsigned int length ) {\n  char p[length + 1];\n    \n  memcpy( p, payload, length );\n  p[length] = NULL;\n    \n  String message( p );\n}\n\n\n\nClient ID\n\nEach device will need to have a unique client ID when communicating to Watson\nIoT. In the code above, I use \"d:ts200f:Photon:WebVisions\". The format of this\nstring is very specific (and specified in the documentation). If you get part of\nit wrong, you will be immediately disconnected from Watson IoT.\n\nThe \"d\" part of the client ID string indicates that this is a device that is\nconnecting. Applications can also connect to Watson IoT, in which case this part\nof the client ID would be \"a\".\n\nThe \"ts200f\" is the automatically generated organization name given to my\ninstance of Watson IoT. The easiest place to find the organization name of your\ninstance, is to look in the address bar of your browser. Your organization name\nwill be the first part of the domain.\n\nThe \"Photon\" part of the client ID is the device type (not the instance). This\nis the device type we created earlier.\n\nThe \"WebVisions\" part of the client ID is the device ID. In deployment, you\nwould want to have the \"WebVisions\" part of the string be unique across all\npossible device instances for this device type. You might accomplish this with a\nrandom number, UUID, MAC address, serial number, timestamp, or any other\napproach.\n\nHost\n\nThe host in the code above is \"your_organization_here\n.messaging.internetofthings.ibmcloud.com\". I talked about where to get the\norganization name of your Watson IoT instance in the previous section. Just in\ncase, you can find it as the first part of the domain in the address bar of your\nbrowser.\n\nPassword\n\nRemember that token we generated when we created an instance of a device type?\nThe token I warned you that you would need again, and only be able to see that\nonce? Yeah, that token is your password when connecting with devices. When\nconnecting as an application, you will use an API key and token from \"Access\"\nscreen of Watson IoT.\n\nPublish Topic\n\nJust as the client ID is very specific, so is the topic naming when it comes to\nWatson IoT. In the above code, I use \"iot-2/evt/count/fmt/json\".\n\n> Watson IoT devices have the option of publishing events, or subscribing to\ncommands (e.g. update). Applications by contrast can publish and subscribe to\nevents and commands (and more).\n\n\nWhat is the difference between an application and a device? Largely the format\nof the client ID, and the username/password provided. A Particle Photon could\njust as effectively behave as an application. We are treating it as a device for\nthe example, which is why the \"subscribe\" line is commented out.\n\nSince we are treating this Photon as a device, it can only publish events, so\nthe first two parts of our topic name will always be \"iot-2/evt/\". The next part\nis an event name, which you can name to match your specific needs. And then\nfinally, we will be publishing JSON formatted messages, so the last part of the\ntopic name is \"/fmt/json\".\n\nYou do not have to use JSON, and can find other options in the documentation,\nbut JSON messages can be stored directly by Watson IoT (optional).\n\nUsername\n\nWhen connecting as a \"device\" the username will always be \"use-token-auth\".\nCombined with the specific client ID, and the token used for the password, this\nallows Watson IoT to authenticate your specific device.\n\nWhen authenticating as an application, an API key and additional token value\nwill be used as username and password. This is useful if for example you are\nusing Node-RED [http://nodered.org/] to handle messages, or have a browser-based\napplication using the Eclipse Paho MQTT library for JavaScript.\n\nInstantiate Client\n\nThe remainder of the code should be self explanatory. We will create an instance\nof the MQTT client, connect to Watson IoT, and publish a message once every\nsecond.\n\nThe client instantiation requires a callback function for when messages arrive\nfrom a subscribed topic. Since \"devices\" cannot subscribe to topics within the\ncontext of Watson IoT, the callback is never used. If you were treating this\nPhoton as an \"application\" then you would be able to subscribe to events and\ncallback handler would be used.\n\nNext Steps\nWith the Photon code flashed to the physical device, and connected to Watson\nIoT, head back to the management console. If you still have the \"Photon\"\ninstance selected, you will now see messages start showing up under the \"Recent\nEvents\" and \"Sensor Information\" sections. If there was an error connecting, you\ncan scroll down and check out the connection log.\n\nFrom here you would likely connect some other client to Watson IoT - probably as\nan application. The client ID, username, and password connection strings would\nneed to be changed, and then you could event connect the Photon to Watson IoT. \nAnything that can connect to an MQTT broker can participate in the conversation.\n\nUsing a publish/subscribe pattern is an immensely powerful foundation for \nhigh-performance, event-driven application - IoT or otherwise. The pattern\nallows you full decoupling of microservices, and can take you down the path of \nreactive programming. Skills that once mastered make testable, Web-scale\napplications, a snap.","feature_image":"http://images.kevinhoyt.com/watson.jpg","featured":0,"status":"published","locale":null,"visibility":"public","author_id":"1","created_at":"2016-04-27T15:16:38.000Z","updated_at":"2016-04-27T17:25:25.000Z","published_at":"2016-04-27T17:25:25.000Z","custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"type":"post","email_recipient_filter":"none"},{"id":"5adb8922351ffe0018a5773e","uuid":"66c4c7d0-9bd4-420e-9ace-e3d6149c16d6","title":"Your Browser on Watson IoT","slug":"your-browser-on-watson-iot","mobiledoc":"{\"version\":\"0.3.1\",\"markups\":[],\"atoms\":[],\"cards\":[[\"markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\"In my [previous](http://www.kevinhoyt.com/2016/04/27/particle-photon-on-watson-iot/) blog post, I connected a [Particle Photon](https://www.particle.io/) to [Watson IoT](https://console.ng.bluemix.net/catalog/services/internet-of-things-platform/).  At several points during that post I mentioned the subtleties of device connectivity versus application connectivity, but I did not really show the application side.  In this post, I will pick up the messages from the Photon in the browser, as an ==application==, and as delivered by Watson IoT.\\n\\n###Particle Photon\\n\\nJust to review, I have a ==Particle Photon== connected to ==Watson IoT== as a ==device==.  The Photon does not do anything more than increment an integer once per second - ==counting==.  As it increments that value, it also publishes the value to Watson IoT on the topic \\\"==iot-2/evt/count/fmt/json==\\\".\\n\\n```\\n{count: 1}\\n```\\n\\nThe Photon device is of type \\\"==Particle==\\\" with an ID of \\\"==Photon==\\\".  It uses a \\\"==WebVisions==\\\" seed for the client ID.  The organization, as provided by the Watson IoT instance on Bluemix is \\\"==ts200f==\\\".  That is a lot of details to track, but they will be needed on the application side in the browser.\\n\\n###API Key\\n\\nUnlike connecting as a device, where only a token specific to that device is needed, connecting as ==an application requires an API key and token pair==.  To get an API key, we need to head to our Watson IoT Platform instance on ==Bluemix==, then click on the \\\"==Access==\\\" icon.\\n\\n![Get an API key and authentication token from Watson IoT.](http://images.kevinhoyt.com/watson.iot.api.key.png)\\n\\nOnce you are on that screen, click on the \\\"==api keys==\\\" link just below the screen title.  From there you will see a button to \\\"==Generate an API Key==\\\".  Unlike configuring a device type or instance, you will only get one screen to capture the token value - it will not be presented again.  ==You want to record both the API key and authentication token.==\\n\\n###Browser\\n\\nTo be able to connect from the browser, which we will treat as an application, to Watson IoT, we will need a JavaScript library that can connect using ==MQTT==.  The Eclipse Paho project has just such a [client](https://www.eclipse.org/paho/clients/js/), based on ==WebSocket==.\\n\\n```\\n<script \\n  src=\\\"mqttws31.js\\\" \\n  type=\\\"text/javascript\\\">\\n</script>        \\n\\n...\\n\\nvar client;\\n\\nwindow.addEventListener( 'load', function() {\\n  try {\\n    client = new Paho.MQTT.Client(\\n      'ts200f.messaging.internetofthings.ibmcloud.com', \\n      1883, \\n      'Counter' + Math.round( Math.random() * 1000 )\\n    );\\n  } catch( error ) {\\n    console.log( 'Error: ' + error );\\n  }    \\n\\n  client.connect( {\\n    userName: 'a-ts200f-lp7shitt3q',\\n    password: 'ghTN2UIV48aUEyjj1s',\\n    onSuccess: doClientConnect,\\n    onFailure: doClientFailure\\n  } );\\n\\n} );\\n```\\n\\nBecause you will want the ==client== to be ==long lived==, you will want to make sure to declare it at an appropriate ==scope== (global).  Then when you are ready, in this case the page load event, we can ==instantiate the client== instance.  Note that the host is \\\"ts200f.messaging.internetofthings.ibmcloud.com\\\" where \\\"==ts200f==\\\" is the Watson IoT ==organization== name.\\n\\nWith the client instantiated, we can now ==connect== to Watson IoT.  The ==username== is the ==API key== from the previous step.  The ==password== is the ==authentication token== from the previous step.  Optionally, I have specified ==listeners== to know when the client connects (or does not).  The failure event is especially useful when learning Watson IoT.\\n\\n```\\nfunction doClientConnect( context ) {\\n  console.log( 'Connected.' );\\n\\n  client.onMessageArrived = doMessageArrived;\\n  client.subscribe( 'iot-2/type/Particle/id/Photon/evt/count/fmt/json' );    \\n}    \\n\\nfunction doMessageArrived( message ) {\\n  var data = null;\\n    \\n  data = JSON.parse( message.payloadString );    \\n  console.log( 'Count: ' + data.count );\\n}\\n```\\n\\nWhen the client has connected we add an event listener for ==message handling==.  From there we ==subscribe== to the topic that will surface the count event created on the Particle Photon.\\n\\nThe topic name always starts with \\\"==iot-2/==\\\".  From there we specify the ==device type== with \\\"type/Particle\\\".  Then the ==device ID== we are interested in with \\\"id/Photon\\\".  Next up is the ==event== we are interested in using \\\"evt/count\\\".  And finally is the message ==format== type of \\\"fmt/json\\\".  \\n\\n> An asterisk (\\\\*) can be used for device type, device ID, and event type as a wildcard.  For example, if the Photon creates a \\\"temperature\\\" and \\\"humidity\\\" event, then you might use \\\"evt/\\\\*\\\".\\n\\nWhen a message arrives, the data will be the \\\"==payloadString==\\\" on the message object.  To get at the data then we will need to ==parse the JSON== to a JavaScript object.  What you do with the data from there is up to you.  Keep in mind that MQTT is really ==optimized for devices==, so you should not be passing large volumes of data in your messages.\\n\\n###Next Steps\\n\\nIn this situation we have connected the browser to Watson IoT as an application.  \\n\\nKeep in mind that the ==differentiation== between devices and applications on Watson IoT is really a matter of the ==client ID==, and the method used to ==authenticate==.  In this sense, there is nothing stopping you from connecting another Photon as an application.  Perhaps it displays the count on a 7-segment LED display.\\n\\nYou are now well on your way to building a Watson IoT application using real-time messaging.  \\n\\nFor a handful of Photons this works great.  At ==scale==, say thousands of deployed devices, the data is going to come so fast that visualizing it could prove a challenge.  From here you might roll in [Apache Spark](https://console.ng.bluemix.net/catalog/services/apache-spark/) at the edge to process messages, and raise ==exceptions== or perform ==analytics==.\\n\"}]],\"sections\":[[10,0]],\"ghostVersion\":\"3.0\"}","html":"<!--kg-card-begin: markdown--><p>In my <a href=\"http://www.kevinhoyt.com/2016/04/27/particle-photon-on-watson-iot/\">previous</a> blog post, I connected a <a href=\"https://www.particle.io/\">Particle Photon</a> to <a href=\"https://console.ng.bluemix.net/catalog/services/internet-of-things-platform/\">Watson IoT</a>.  At several points during that post I mentioned the subtleties of device connectivity versus application connectivity, but I did not really show the application side.  In this post, I will pick up the messages from the Photon in the browser, as an <mark>application</mark>, and as delivered by Watson IoT.</p>\n<h3 id=\"particlephoton\">Particle Photon</h3>\n<p>Just to review, I have a <mark>Particle Photon</mark> connected to <mark>Watson IoT</mark> as a <mark>device</mark>.  The Photon does not do anything more than increment an integer once per second - <mark>counting</mark>.  As it increments that value, it also publishes the value to Watson IoT on the topic &quot;<mark>iot-2/evt/count/fmt/json</mark>&quot;.</p>\n<pre><code>{count: 1}\n</code></pre>\n<p>The Photon device is of type &quot;<mark>Particle</mark>&quot; with an ID of &quot;<mark>Photon</mark>&quot;.  It uses a &quot;<mark>WebVisions</mark>&quot; seed for the client ID.  The organization, as provided by the Watson IoT instance on Bluemix is &quot;<mark>ts200f</mark>&quot;.  That is a lot of details to track, but they will be needed on the application side in the browser.</p>\n<h3 id=\"apikey\">API Key</h3>\n<p>Unlike connecting as a device, where only a token specific to that device is needed, connecting as <mark>an application requires an API key and token pair</mark>.  To get an API key, we need to head to our Watson IoT Platform instance on <mark>Bluemix</mark>, then click on the &quot;<mark>Access</mark>&quot; icon.</p>\n<p><img src=\"http://images.kevinhoyt.com/watson.iot.api.key.png\" alt=\"Get an API key and authentication token from Watson IoT.\" loading=\"lazy\"></p>\n<p>Once you are on that screen, click on the &quot;<mark>api keys</mark>&quot; link just below the screen title.  From there you will see a button to &quot;<mark>Generate an API Key</mark>&quot;.  Unlike configuring a device type or instance, you will only get one screen to capture the token value - it will not be presented again.  <mark>You want to record both the API key and authentication token.</mark></p>\n<h3 id=\"browser\">Browser</h3>\n<p>To be able to connect from the browser, which we will treat as an application, to Watson IoT, we will need a JavaScript library that can connect using <mark>MQTT</mark>.  The Eclipse Paho project has just such a <a href=\"https://www.eclipse.org/paho/clients/js/\">client</a>, based on <mark>WebSocket</mark>.</p>\n<pre><code>&lt;script \n  src=&quot;mqttws31.js&quot; \n  type=&quot;text/javascript&quot;&gt;\n&lt;/script&gt;        \n\n...\n\nvar client;\n\nwindow.addEventListener( 'load', function() {\n  try {\n    client = new Paho.MQTT.Client(\n      'ts200f.messaging.internetofthings.ibmcloud.com', \n      1883, \n      'Counter' + Math.round( Math.random() * 1000 )\n    );\n  } catch( error ) {\n    console.log( 'Error: ' + error );\n  }    \n\n  client.connect( {\n    userName: 'a-ts200f-lp7shitt3q',\n    password: 'ghTN2UIV48aUEyjj1s',\n    onSuccess: doClientConnect,\n    onFailure: doClientFailure\n  } );\n\n} );\n</code></pre>\n<p>Because you will want the <mark>client</mark> to be <mark>long lived</mark>, you will want to make sure to declare it at an appropriate <mark>scope</mark> (global).  Then when you are ready, in this case the page load event, we can <mark>instantiate the client</mark> instance.  Note that the host is &quot;ts200f.messaging.internetofthings.ibmcloud.com&quot; where &quot;<mark>ts200f</mark>&quot; is the Watson IoT <mark>organization</mark> name.</p>\n<p>With the client instantiated, we can now <mark>connect</mark> to Watson IoT.  The <mark>username</mark> is the <mark>API key</mark> from the previous step.  The <mark>password</mark> is the <mark>authentication token</mark> from the previous step.  Optionally, I have specified <mark>listeners</mark> to know when the client connects (or does not).  The failure event is especially useful when learning Watson IoT.</p>\n<pre><code>function doClientConnect( context ) {\n  console.log( 'Connected.' );\n\n  client.onMessageArrived = doMessageArrived;\n  client.subscribe( 'iot-2/type/Particle/id/Photon/evt/count/fmt/json' );    \n}    \n\nfunction doMessageArrived( message ) {\n  var data = null;\n    \n  data = JSON.parse( message.payloadString );    \n  console.log( 'Count: ' + data.count );\n}\n</code></pre>\n<p>When the client has connected we add an event listener for <mark>message handling</mark>.  From there we <mark>subscribe</mark> to the topic that will surface the count event created on the Particle Photon.</p>\n<p>The topic name always starts with &quot;<mark>iot-2/</mark>&quot;.  From there we specify the <mark>device type</mark> with &quot;type/Particle&quot;.  Then the <mark>device ID</mark> we are interested in with &quot;id/Photon&quot;.  Next up is the <mark>event</mark> we are interested in using &quot;evt/count&quot;.  And finally is the message <mark>format</mark> type of &quot;fmt/json&quot;.</p>\n<blockquote>\n<p>An asterisk (*) can be used for device type, device ID, and event type as a wildcard.  For example, if the Photon creates a &quot;temperature&quot; and &quot;humidity&quot; event, then you might use &quot;evt/*&quot;.</p>\n</blockquote>\n<p>When a message arrives, the data will be the &quot;<mark>payloadString</mark>&quot; on the message object.  To get at the data then we will need to <mark>parse the JSON</mark> to a JavaScript object.  What you do with the data from there is up to you.  Keep in mind that MQTT is really <mark>optimized for devices</mark>, so you should not be passing large volumes of data in your messages.</p>\n<h3 id=\"nextsteps\">Next Steps</h3>\n<p>In this situation we have connected the browser to Watson IoT as an application.</p>\n<p>Keep in mind that the <mark>differentiation</mark> between devices and applications on Watson IoT is really a matter of the <mark>client ID</mark>, and the method used to <mark>authenticate</mark>.  In this sense, there is nothing stopping you from connecting another Photon as an application.  Perhaps it displays the count on a 7-segment LED display.</p>\n<p>You are now well on your way to building a Watson IoT application using real-time messaging.</p>\n<p>For a handful of Photons this works great.  At <mark>scale</mark>, say thousands of deployed devices, the data is going to come so fast that visualizing it could prove a challenge.  From here you might roll in <a href=\"https://console.ng.bluemix.net/catalog/services/apache-spark/\">Apache Spark</a> at the edge to process messages, and raise <mark>exceptions</mark> or perform <mark>analytics</mark>.</p>\n<!--kg-card-end: markdown-->","comment_id":"53","plaintext":"In my previous\n[http://www.kevinhoyt.com/2016/04/27/particle-photon-on-watson-iot/] blog post,\nI connected a Particle Photon [https://www.particle.io/] to Watson IoT\n[https://console.ng.bluemix.net/catalog/services/internet-of-things-platform/].\nAt several points during that post I mentioned the subtleties of device\nconnectivity versus application connectivity, but I did not really show the\napplication side. In this post, I will pick up the messages from the Photon in\nthe browser, as an application, and as delivered by Watson IoT.\n\nParticle Photon\nJust to review, I have a Particle Photon connected to Watson IoT as a device.\nThe Photon does not do anything more than increment an integer once per second - \ncounting. As it increments that value, it also publishes the value to Watson IoT\non the topic \"iot-2/evt/count/fmt/json\".\n\n{count: 1}\n\n\nThe Photon device is of type \"Particle\" with an ID of \"Photon\". It uses a \"\nWebVisions\" seed for the client ID. The organization, as provided by the Watson\nIoT instance on Bluemix is \"ts200f\". That is a lot of details to track, but they\nwill be needed on the application side in the browser.\n\nAPI Key\nUnlike connecting as a device, where only a token specific to that device is\nneeded, connecting as an application requires an API key and token pair. To get\nan API key, we need to head to our Watson IoT Platform instance on Bluemix, then\nclick on the \"Access\" icon.\n\n\n\nOnce you are on that screen, click on the \"api keys\" link just below the screen\ntitle. From there you will see a button to \"Generate an API Key\". Unlike\nconfiguring a device type or instance, you will only get one screen to capture\nthe token value - it will not be presented again. You want to record both the\nAPI key and authentication token.\n\nBrowser\nTo be able to connect from the browser, which we will treat as an application,\nto Watson IoT, we will need a JavaScript library that can connect using MQTT.\nThe Eclipse Paho project has just such a client\n[https://www.eclipse.org/paho/clients/js/], based on WebSocket.\n\n<script \n  src=\"mqttws31.js\" \n  type=\"text/javascript\">\n</script>        \n\n...\n\nvar client;\n\nwindow.addEventListener( 'load', function() {\n  try {\n    client = new Paho.MQTT.Client(\n      'ts200f.messaging.internetofthings.ibmcloud.com', \n      1883, \n      'Counter' + Math.round( Math.random() * 1000 )\n    );\n  } catch( error ) {\n    console.log( 'Error: ' + error );\n  }    \n\n  client.connect( {\n    userName: 'a-ts200f-lp7shitt3q',\n    password: 'ghTN2UIV48aUEyjj1s',\n    onSuccess: doClientConnect,\n    onFailure: doClientFailure\n  } );\n\n} );\n\n\nBecause you will want the client to be long lived, you will want to make sure to\ndeclare it at an appropriate scope (global). Then when you are ready, in this\ncase the page load event, we can instantiate the client instance. Note that the\nhost is \"ts200f.messaging.internetofthings.ibmcloud.com\" where \"ts200f\" is the\nWatson IoT organization name.\n\nWith the client instantiated, we can now connect to Watson IoT. The username is\nthe API key from the previous step. The password is the authentication token \nfrom the previous step. Optionally, I have specified listeners to know when the\nclient connects (or does not). The failure event is especially useful when\nlearning Watson IoT.\n\nfunction doClientConnect( context ) {\n  console.log( 'Connected.' );\n\n  client.onMessageArrived = doMessageArrived;\n  client.subscribe( 'iot-2/type/Particle/id/Photon/evt/count/fmt/json' );    \n}    \n\nfunction doMessageArrived( message ) {\n  var data = null;\n    \n  data = JSON.parse( message.payloadString );    \n  console.log( 'Count: ' + data.count );\n}\n\n\nWhen the client has connected we add an event listener for message handling.\nFrom there we subscribe to the topic that will surface the count event created\non the Particle Photon.\n\nThe topic name always starts with \"iot-2/\". From there we specify the device\ntype with \"type/Particle\". Then the device ID we are interested in with\n\"id/Photon\". Next up is the event we are interested in using \"evt/count\". And\nfinally is the message format type of \"fmt/json\".\n\n> An asterisk (*) can be used for device type, device ID, and event type as a\nwildcard. For example, if the Photon creates a \"temperature\" and \"humidity\"\nevent, then you might use \"evt/*\".\n\n\nWhen a message arrives, the data will be the \"payloadString\" on the message\nobject. To get at the data then we will need to parse the JSON to a JavaScript\nobject. What you do with the data from there is up to you. Keep in mind that\nMQTT is really optimized for devices, so you should not be passing large volumes\nof data in your messages.\n\nNext Steps\nIn this situation we have connected the browser to Watson IoT as an application.\n\nKeep in mind that the differentiation between devices and applications on Watson\nIoT is really a matter of the client ID, and the method used to authenticate. In\nthis sense, there is nothing stopping you from connecting another Photon as an\napplication. Perhaps it displays the count on a 7-segment LED display.\n\nYou are now well on your way to building a Watson IoT application using\nreal-time messaging.\n\nFor a handful of Photons this works great. At scale, say thousands of deployed\ndevices, the data is going to come so fast that visualizing it could prove a\nchallenge. From here you might roll in Apache Spark\n[https://console.ng.bluemix.net/catalog/services/apache-spark/] at the edge to\nprocess messages, and raise exceptions or perform analytics.","feature_image":"http://images.kevinhoyt.com/watson.jpg","featured":0,"status":"published","locale":null,"visibility":"public","author_id":"1","created_at":"2016-04-27T19:23:47.000Z","updated_at":"2016-04-28T20:27:20.000Z","published_at":"2016-04-28T20:27:20.000Z","custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"type":"post","email_recipient_filter":"none"},{"id":"5adb8922351ffe0018a5773f","uuid":"5c104541-06ef-4c63-9e5e-6e53de6abaf7","title":"Electric Imp on Watson IoT","slug":"electric-imp-on-watson-iot","mobiledoc":"{\"version\":\"0.3.1\",\"markups\":[],\"atoms\":[],\"cards\":[[\"markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\"I recently posted about connecting a [Particle Photon](http://www.kevinhoyt.com/2016/04/27/particle-photon-on-watson-iot/) and your [browser](http://www.kevinhoyt.com/2016/04/28/your-browser-on-watson-iot/) to the [Watson IoT Platform](http://www.ibm.com/internet-of-things/).  Both of these systems easily supports publish/subscribe using MQTT.  What happens when you have a device that does not support MQTT, yet still needs to communicate with Watson IoT?  In this post we will take a look at connecting an [Electric Imp](https://electricimp.com/) to Watson IoT.\\n\\n###Electric Imp\\n\\n![Electric Imp Developer Edition courtesy SparkFun.](https://cdn.sparkfun.com//assets/parts/7/1/7/2/11395-03.jpg)\\n\\nI got my first Electric Imp around 2011-2012 timeframe.  At the time, ==the hardest part of IoT was in easily setting the SSID/password on a device with no screen or keyboard==.  Sure as a developer, you could code it, but that does not work so well in a commercial setting - you are not going to visit every customers house personally and configure their shiny new IoT device.\\n\\nElectric Imp is a ==wireless platform with an ARM Cortex M3== at it's heart.  The developer edition comes in an [SD card](https://www.sparkfun.com/products/11395) form factor, and SparkFun makes a [breakout board](https://www.sparkfun.com/products/12886) for prototyping.  This innovative packaging allows you to use SMD SD card products to hold and power the Imp.  \\n\\n![Electric Imp breakout board sold by SparkFun (image courtesy of SparkFun).](https://cdn.sparkfun.com//assets/parts/9/7/7/6/12886-00.jpg)\\n\\nPerhaps the most innovative feature of the Imp is the (patent pending) ==BlinkUp==.  You can think of BlinkUp like ==Morse code==, where your smartphone flashes the screen from black to white, at a very rapid pace, ==to communicate the SSID/password== to a photocell receiver on the Imp.\\n\\nUnfortunately, in my (pretty extensive) testing across devices, the ==BlinkUp is unreliable==, or just flat out unsupported.  I was working on a product for a while where my very first tester was using a Windows Phone ... ==Windows Phone is not supported== by Electric Imp.  It did not take me long to figure out that this was not going to work in production.\\n\\nHowever, if you have a controlled environment, or know that your end user will have a supported device, then the Electric Imp makes for a fantastic wireless platform on a number of fronts.\\n\\n###Device and Agent\\n\\nWhen developing on the Imp, there are ==two parts== to consider.  The first is the \\\"==device==\\\" where you interact with any electrical components.  This is code running locally on the Imp.  The second part is the \\\"==agent==\\\" which is code running in the Electric Imp cloud.  ==Devices can only communicate with their agents==.\\n\\nThis may seem tedious at first - if you want to make an HTTP request, your device needs to tell the agent to do the work, and then pass the response back to the device.  With any other device you would make the request directly from the wireless chip.  In practice, this configuration, and ==separation of responsibility==, provide a thorough layer of ==security== for your IoT device.\\n\\nIt does however put a damper on any publish/subscribe pattern implementations.  For example, at the time of this writing, ==WebSocket== was being considered for future releases, and an ==MQTT== implementation was ==not available==.  Robust, secure request/response?  You bet!  Event-driven publish/subscribe?  Nope.\\n\\n###Watson IoT\\n\\nRecently added to the Watson IoT Platform is the ability to ==communicate via HTTPS== from devices and applications.  The Imp agent is already very good at HTTPS, so this works out perfectly.  We will treat the Imp as a Watson IoT device, and then follow the general pattern of making an ==HTTP POST== request to your Watson IoT instance.\\n\\n```\\nhttps://${orgid}.internetofthings.ibmcloud.com/api/v0002/device/types/${typeId}/devices/${deviceId} \\n```\\n\\nThe request requires ==two headers==.  The first the the \\\"==Authentication==\\\" header, which as a device we will specify \\\"==use-token-auth==\\\" as the user name, and your ==device token== as the password.  For more on what a device token is, and where to obtain it, view the first post in the series on using a [Particle Photon](http://www.kevinhoyt.com/2016/04/27/particle-photon-on-watson-iot/).\\n\\nThe second header is \\\"==Content-Type==\\\" which we will set to \\\"==application/json==\\\".  If you recall from my previous Watson IoT posts, the message format is usually on the topic string.  Note that in the URL above, there is no \\\"fmt/json\\\" at the end.\\n\\n**Imp Device**\\n\\nThe Electric Imp is programmed using [Squirrel](https://electricimp.com/docs/squirrel/).  Squirrel is a loosely-typed language very ==similar in nature to JavaScript==.  In my experience, Squirrel has a bit more focus on data structures, which comes in handy.  Translating our Watson IoT requirements to the Imp device looks like the following code.\\n\\n```\\n// Track count\\n// Will reset if Imp is reset\\nlocal count = 0;\\n\\n// Called to count\\n// Increments, sends, sleeps\\nfunction counting() {\\n  // Increment\\n  count = count + 1;\\n  \\n  // Send to Imp agent  \\n  agent.send( \\\"count\\\", {\\n    \\\"count\\\": count\\n  } );\\n\\n  // Sleep for one second\\n  // Call this function again upon waking\\n  imp.wakeup( 1.0, counting );    \\n}\\n\\n// Start counting\\ncounting();\\n```\\n\\nYou can see how the data structure for counting, which effectively looks like JSON (or a hash), is sent to the Imp agent.  There is a ==lookup== for the function to call at the Imp agent, but ==no destination URL== is given, or any other type of authorization/authentication credentials.  ==The call to the Imp agent, living in the Electric Imp cloud, is secure.==  \\n\\n> Compare this to the Particle Photon, where we outright embedded our credentials in the code, and had no robust HTTPS class to otherwise secure our communication/credentials.\\n\\n**Imp Agent**\\n\\nThe Imp agent side of things is ==where the magic happens== - where we actually reach out to Watson IoT.  The trickiest part of this exchange is in making sure the ==URL is correct==.  Like the MQTT topic string, the URL is lengthy, and generally flows from ==device type, to device ID, to event type==.\\n\\n```\\n// Registered agent function\\ndevice.on( \\\"count\\\", function( data ) {\\n  // URL for HTTP POST\\n  // Maps to specific device type\\n  // Specific device instance\\n  // Specific event\\n  local url = \\\"https://\\\" +\\n    \\\"ts200f\\\" + \\n    \\\".internetofthings.ibmcloud.com\\\" +\\n    \\\"/api/v0002/device/types/\\\" +\\n    \\\"Imp\\\" + \\n    \\\"/devices/\\\" +\\n    \\\"WebVisions\\\" +\\n    \\\"/events/\\\" +\\n    \\\"count\\\";\\n    \\n  // Make the request\\n  // Headers for authorization and format\\n  // JSON-encoded data\\n  local watson = http.post(\\n    url,\\n    {\\n      \\\"Authorization\\\": \\\"Basic \\\" + http.base64encode( \\n        \\\"use-auth-token:_YOUR_TOKEN_HERE_\\\" \\n      ),\\n      \\\"Content-Type\\\": \\\"application/json\\\"\\n    },\\n    http.jsonencode( data )\\n  ).sendasync(\\n    // No response body from Watson IoT\\n    // Log request as finished\\n    function( response ) {\\n      server.log( \\\"Counted.\\\" );\\n    }\\n  );\\n} );\\n```\\n\\nOne thing that bothered me about the response from Watson IoT was that there is ==no body content==.  It would be nice to at least get some fashion of acknowledgement - similar in nature to what would happen with MQTT.  In this case, I just log the end of the request/response exchange to the server log, and move on.\\n\\nYou could also send data back from the agent to the device.\\n\\n###Next Steps\\n\\nAs you may have been able to tell, I have a ==love/hate== relationship with the Imp.  I love the packaging and form factor, as well as the CPU inside.  I hate the BlinkUp - having tested it across hundreds of technology savvy end-users and failed repeatedly.  I love the security of the separation between device and agent.  I hate the limitations that places on me for real-time data applications.\\n\\nBeing able to merge the Imp's tendency towards ==request/response== with the real-time implementation of ==Watson IoT== is a big step forward.  While the device may still be request/response, my applications can handle that data in real-time as it arrives from Watson IoT.  Now if we could just solve that BlinkUp dependency.\"}]],\"sections\":[[10,0]],\"ghostVersion\":\"3.0\"}","html":"<!--kg-card-begin: markdown--><p>I recently posted about connecting a <a href=\"http://www.kevinhoyt.com/2016/04/27/particle-photon-on-watson-iot/\">Particle Photon</a> and your <a href=\"http://www.kevinhoyt.com/2016/04/28/your-browser-on-watson-iot/\">browser</a> to the <a href=\"http://www.ibm.com/internet-of-things/\">Watson IoT Platform</a>.  Both of these systems easily supports publish/subscribe using MQTT.  What happens when you have a device that does not support MQTT, yet still needs to communicate with Watson IoT?  In this post we will take a look at connecting an <a href=\"https://electricimp.com/\">Electric Imp</a> to Watson IoT.</p>\n<h3 id=\"electricimp\">Electric Imp</h3>\n<p><img src=\"https://cdn.sparkfun.com//assets/parts/7/1/7/2/11395-03.jpg\" alt=\"Electric Imp Developer Edition courtesy SparkFun.\" loading=\"lazy\"></p>\n<p>I got my first Electric Imp around 2011-2012 timeframe.  At the time, <mark>the hardest part of IoT was in easily setting the SSID/password on a device with no screen or keyboard</mark>.  Sure as a developer, you could code it, but that does not work so well in a commercial setting - you are not going to visit every customers house personally and configure their shiny new IoT device.</p>\n<p>Electric Imp is a <mark>wireless platform with an ARM Cortex M3</mark> at it's heart.  The developer edition comes in an <a href=\"https://www.sparkfun.com/products/11395\">SD card</a> form factor, and SparkFun makes a <a href=\"https://www.sparkfun.com/products/12886\">breakout board</a> for prototyping.  This innovative packaging allows you to use SMD SD card products to hold and power the Imp.</p>\n<p><img src=\"https://cdn.sparkfun.com//assets/parts/9/7/7/6/12886-00.jpg\" alt=\"Electric Imp breakout board sold by SparkFun (image courtesy of SparkFun).\" loading=\"lazy\"></p>\n<p>Perhaps the most innovative feature of the Imp is the (patent pending) <mark>BlinkUp</mark>.  You can think of BlinkUp like <mark>Morse code</mark>, where your smartphone flashes the screen from black to white, at a very rapid pace, <mark>to communicate the SSID/password</mark> to a photocell receiver on the Imp.</p>\n<p>Unfortunately, in my (pretty extensive) testing across devices, the <mark>BlinkUp is unreliable</mark>, or just flat out unsupported.  I was working on a product for a while where my very first tester was using a Windows Phone ... <mark>Windows Phone is not supported</mark> by Electric Imp.  It did not take me long to figure out that this was not going to work in production.</p>\n<p>However, if you have a controlled environment, or know that your end user will have a supported device, then the Electric Imp makes for a fantastic wireless platform on a number of fronts.</p>\n<h3 id=\"deviceandagent\">Device and Agent</h3>\n<p>When developing on the Imp, there are <mark>two parts</mark> to consider.  The first is the &quot;<mark>device</mark>&quot; where you interact with any electrical components.  This is code running locally on the Imp.  The second part is the &quot;<mark>agent</mark>&quot; which is code running in the Electric Imp cloud.  <mark>Devices can only communicate with their agents</mark>.</p>\n<p>This may seem tedious at first - if you want to make an HTTP request, your device needs to tell the agent to do the work, and then pass the response back to the device.  With any other device you would make the request directly from the wireless chip.  In practice, this configuration, and <mark>separation of responsibility</mark>, provide a thorough layer of <mark>security</mark> for your IoT device.</p>\n<p>It does however put a damper on any publish/subscribe pattern implementations.  For example, at the time of this writing, <mark>WebSocket</mark> was being considered for future releases, and an <mark>MQTT</mark> implementation was <mark>not available</mark>.  Robust, secure request/response?  You bet!  Event-driven publish/subscribe?  Nope.</p>\n<h3 id=\"watsoniot\">Watson IoT</h3>\n<p>Recently added to the Watson IoT Platform is the ability to <mark>communicate via HTTPS</mark> from devices and applications.  The Imp agent is already very good at HTTPS, so this works out perfectly.  We will treat the Imp as a Watson IoT device, and then follow the general pattern of making an <mark>HTTP POST</mark> request to your Watson IoT instance.</p>\n<pre><code>https://${orgid}.internetofthings.ibmcloud.com/api/v0002/device/types/${typeId}/devices/${deviceId} \n</code></pre>\n<p>The request requires <mark>two headers</mark>.  The first the the &quot;<mark>Authentication</mark>&quot; header, which as a device we will specify &quot;<mark>use-token-auth</mark>&quot; as the user name, and your <mark>device token</mark> as the password.  For more on what a device token is, and where to obtain it, view the first post in the series on using a <a href=\"http://www.kevinhoyt.com/2016/04/27/particle-photon-on-watson-iot/\">Particle Photon</a>.</p>\n<p>The second header is &quot;<mark>Content-Type</mark>&quot; which we will set to &quot;<mark>application/json</mark>&quot;.  If you recall from my previous Watson IoT posts, the message format is usually on the topic string.  Note that in the URL above, there is no &quot;fmt/json&quot; at the end.</p>\n<p><strong>Imp Device</strong></p>\n<p>The Electric Imp is programmed using <a href=\"https://electricimp.com/docs/squirrel/\">Squirrel</a>.  Squirrel is a loosely-typed language very <mark>similar in nature to JavaScript</mark>.  In my experience, Squirrel has a bit more focus on data structures, which comes in handy.  Translating our Watson IoT requirements to the Imp device looks like the following code.</p>\n<pre><code>// Track count\n// Will reset if Imp is reset\nlocal count = 0;\n\n// Called to count\n// Increments, sends, sleeps\nfunction counting() {\n  // Increment\n  count = count + 1;\n  \n  // Send to Imp agent  \n  agent.send( &quot;count&quot;, {\n    &quot;count&quot;: count\n  } );\n\n  // Sleep for one second\n  // Call this function again upon waking\n  imp.wakeup( 1.0, counting );    \n}\n\n// Start counting\ncounting();\n</code></pre>\n<p>You can see how the data structure for counting, which effectively looks like JSON (or a hash), is sent to the Imp agent.  There is a <mark>lookup</mark> for the function to call at the Imp agent, but <mark>no destination URL</mark> is given, or any other type of authorization/authentication credentials.  <mark>The call to the Imp agent, living in the Electric Imp cloud, is secure.</mark></p>\n<blockquote>\n<p>Compare this to the Particle Photon, where we outright embedded our credentials in the code, and had no robust HTTPS class to otherwise secure our communication/credentials.</p>\n</blockquote>\n<p><strong>Imp Agent</strong></p>\n<p>The Imp agent side of things is <mark>where the magic happens</mark> - where we actually reach out to Watson IoT.  The trickiest part of this exchange is in making sure the <mark>URL is correct</mark>.  Like the MQTT topic string, the URL is lengthy, and generally flows from <mark>device type, to device ID, to event type</mark>.</p>\n<pre><code>// Registered agent function\ndevice.on( &quot;count&quot;, function( data ) {\n  // URL for HTTP POST\n  // Maps to specific device type\n  // Specific device instance\n  // Specific event\n  local url = &quot;https://&quot; +\n    &quot;ts200f&quot; + \n    &quot;.internetofthings.ibmcloud.com&quot; +\n    &quot;/api/v0002/device/types/&quot; +\n    &quot;Imp&quot; + \n    &quot;/devices/&quot; +\n    &quot;WebVisions&quot; +\n    &quot;/events/&quot; +\n    &quot;count&quot;;\n    \n  // Make the request\n  // Headers for authorization and format\n  // JSON-encoded data\n  local watson = http.post(\n    url,\n    {\n      &quot;Authorization&quot;: &quot;Basic &quot; + http.base64encode( \n        &quot;use-auth-token:_YOUR_TOKEN_HERE_&quot; \n      ),\n      &quot;Content-Type&quot;: &quot;application/json&quot;\n    },\n    http.jsonencode( data )\n  ).sendasync(\n    // No response body from Watson IoT\n    // Log request as finished\n    function( response ) {\n      server.log( &quot;Counted.&quot; );\n    }\n  );\n} );\n</code></pre>\n<p>One thing that bothered me about the response from Watson IoT was that there is <mark>no body content</mark>.  It would be nice to at least get some fashion of acknowledgement - similar in nature to what would happen with MQTT.  In this case, I just log the end of the request/response exchange to the server log, and move on.</p>\n<p>You could also send data back from the agent to the device.</p>\n<h3 id=\"nextsteps\">Next Steps</h3>\n<p>As you may have been able to tell, I have a <mark>love/hate</mark> relationship with the Imp.  I love the packaging and form factor, as well as the CPU inside.  I hate the BlinkUp - having tested it across hundreds of technology savvy end-users and failed repeatedly.  I love the security of the separation between device and agent.  I hate the limitations that places on me for real-time data applications.</p>\n<p>Being able to merge the Imp's tendency towards <mark>request/response</mark> with the real-time implementation of <mark>Watson IoT</mark> is a big step forward.  While the device may still be request/response, my applications can handle that data in real-time as it arrives from Watson IoT.  Now if we could just solve that BlinkUp dependency.</p>\n<!--kg-card-end: markdown-->","comment_id":"54","plaintext":"I recently posted about connecting a Particle Photon\n[http://www.kevinhoyt.com/2016/04/27/particle-photon-on-watson-iot/] and your \nbrowser [http://www.kevinhoyt.com/2016/04/28/your-browser-on-watson-iot/] to the \nWatson IoT Platform [http://www.ibm.com/internet-of-things/]. Both of these\nsystems easily supports publish/subscribe using MQTT. What happens when you have\na device that does not support MQTT, yet still needs to communicate with Watson\nIoT? In this post we will take a look at connecting an Electric Imp\n[https://electricimp.com/] to Watson IoT.\n\nElectric Imp\n\n\nI got my first Electric Imp around 2011-2012 timeframe. At the time, the hardest\npart of IoT was in easily setting the SSID/password on a device with no screen\nor keyboard. Sure as a developer, you could code it, but that does not work so\nwell in a commercial setting - you are not going to visit every customers house\npersonally and configure their shiny new IoT device.\n\nElectric Imp is a wireless platform with an ARM Cortex M3 at it's heart. The\ndeveloper edition comes in an SD card [https://www.sparkfun.com/products/11395] \nform factor, and SparkFun makes a breakout board\n[https://www.sparkfun.com/products/12886] for prototyping. This innovative\npackaging allows you to use SMD SD card products to hold and power the Imp.\n\n\n\nPerhaps the most innovative feature of the Imp is the (patent pending) BlinkUp.\nYou can think of BlinkUp like Morse code, where your smartphone flashes the\nscreen from black to white, at a very rapid pace, to communicate the\nSSID/password to a photocell receiver on the Imp.\n\nUnfortunately, in my (pretty extensive) testing across devices, the BlinkUp is\nunreliable, or just flat out unsupported. I was working on a product for a while\nwhere my very first tester was using a Windows Phone ... Windows Phone is not\nsupported by Electric Imp. It did not take me long to figure out that this was\nnot going to work in production.\n\nHowever, if you have a controlled environment, or know that your end user will\nhave a supported device, then the Electric Imp makes for a fantastic wireless\nplatform on a number of fronts.\n\nDevice and Agent\nWhen developing on the Imp, there are two parts to consider. The first is the \"\ndevice\" where you interact with any electrical components. This is code running\nlocally on the Imp. The second part is the \"agent\" which is code running in the\nElectric Imp cloud. Devices can only communicate with their agents.\n\nThis may seem tedious at first - if you want to make an HTTP request, your\ndevice needs to tell the agent to do the work, and then pass the response back\nto the device. With any other device you would make the request directly from\nthe wireless chip. In practice, this configuration, and separation of\nresponsibility, provide a thorough layer of security for your IoT device.\n\nIt does however put a damper on any publish/subscribe pattern implementations.\nFor example, at the time of this writing, WebSocket was being considered for\nfuture releases, and an MQTT implementation was not available. Robust, secure\nrequest/response? You bet! Event-driven publish/subscribe? Nope.\n\nWatson IoT\nRecently added to the Watson IoT Platform is the ability to communicate via\nHTTPS from devices and applications. The Imp agent is already very good at\nHTTPS, so this works out perfectly. We will treat the Imp as a Watson IoT\ndevice, and then follow the general pattern of making an HTTP POST request to\nyour Watson IoT instance.\n\nhttps://${orgid}.internetofthings.ibmcloud.com/api/v0002/device/types/${typeId}/devices/${deviceId} \n\n\nThe request requires two headers. The first the the \"Authentication\" header,\nwhich as a device we will specify \"use-token-auth\" as the user name, and your \ndevice token as the password. For more on what a device token is, and where to\nobtain it, view the first post in the series on using a Particle Photon\n[http://www.kevinhoyt.com/2016/04/27/particle-photon-on-watson-iot/].\n\nThe second header is \"Content-Type\" which we will set to \"application/json\". If\nyou recall from my previous Watson IoT posts, the message format is usually on\nthe topic string. Note that in the URL above, there is no \"fmt/json\" at the end.\n\nImp Device\n\nThe Electric Imp is programmed using Squirrel\n[https://electricimp.com/docs/squirrel/]. Squirrel is a loosely-typed language\nvery similar in nature to JavaScript. In my experience, Squirrel has a bit more\nfocus on data structures, which comes in handy. Translating our Watson IoT\nrequirements to the Imp device looks like the following code.\n\n// Track count\n// Will reset if Imp is reset\nlocal count = 0;\n\n// Called to count\n// Increments, sends, sleeps\nfunction counting() {\n  // Increment\n  count = count + 1;\n  \n  // Send to Imp agent  \n  agent.send( \"count\", {\n    \"count\": count\n  } );\n\n  // Sleep for one second\n  // Call this function again upon waking\n  imp.wakeup( 1.0, counting );    \n}\n\n// Start counting\ncounting();\n\n\nYou can see how the data structure for counting, which effectively looks like\nJSON (or a hash), is sent to the Imp agent. There is a lookup for the function\nto call at the Imp agent, but no destination URL is given, or any other type of\nauthorization/authentication credentials. The call to the Imp agent, living in\nthe Electric Imp cloud, is secure.\n\n> Compare this to the Particle Photon, where we outright embedded our credentials\nin the code, and had no robust HTTPS class to otherwise secure our\ncommunication/credentials.\n\n\nImp Agent\n\nThe Imp agent side of things is where the magic happens - where we actually\nreach out to Watson IoT. The trickiest part of this exchange is in making sure\nthe URL is correct. Like the MQTT topic string, the URL is lengthy, and\ngenerally flows from device type, to device ID, to event type.\n\n// Registered agent function\ndevice.on( \"count\", function( data ) {\n  // URL for HTTP POST\n  // Maps to specific device type\n  // Specific device instance\n  // Specific event\n  local url = \"https://\" +\n    \"ts200f\" + \n    \".internetofthings.ibmcloud.com\" +\n    \"/api/v0002/device/types/\" +\n    \"Imp\" + \n    \"/devices/\" +\n    \"WebVisions\" +\n    \"/events/\" +\n    \"count\";\n    \n  // Make the request\n  // Headers for authorization and format\n  // JSON-encoded data\n  local watson = http.post(\n    url,\n    {\n      \"Authorization\": \"Basic \" + http.base64encode( \n        \"use-auth-token:_YOUR_TOKEN_HERE_\" \n      ),\n      \"Content-Type\": \"application/json\"\n    },\n    http.jsonencode( data )\n  ).sendasync(\n    // No response body from Watson IoT\n    // Log request as finished\n    function( response ) {\n      server.log( \"Counted.\" );\n    }\n  );\n} );\n\n\nOne thing that bothered me about the response from Watson IoT was that there is \nno body content. It would be nice to at least get some fashion of\nacknowledgement - similar in nature to what would happen with MQTT. In this\ncase, I just log the end of the request/response exchange to the server log, and\nmove on.\n\nYou could also send data back from the agent to the device.\n\nNext Steps\nAs you may have been able to tell, I have a love/hate relationship with the Imp.\nI love the packaging and form factor, as well as the CPU inside. I hate the\nBlinkUp - having tested it across hundreds of technology savvy end-users and\nfailed repeatedly. I love the security of the separation between device and\nagent. I hate the limitations that places on me for real-time data applications.\n\nBeing able to merge the Imp's tendency towards request/response with the\nreal-time implementation of Watson IoT is a big step forward. While the device\nmay still be request/response, my applications can handle that data in real-time\nas it arrives from Watson IoT. Now if we could just solve that BlinkUp\ndependency.","feature_image":"http://images.kevinhoyt.com/watson.jpg","featured":0,"status":"published","locale":null,"visibility":"public","author_id":"1","created_at":"2016-05-03T15:50:55.000Z","updated_at":"2016-05-03T17:37:59.000Z","published_at":"2016-05-03T17:37:59.000Z","custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"type":"post","email_recipient_filter":"none"},{"id":"5adb8922351ffe0018a57740","uuid":"2083421f-dd57-4880-becd-28c09b6631ec","title":"The 12 Steps of Bluetooth (Swift)","slug":"the-12-steps-of-bluetooth-swift","mobiledoc":"{\"version\":\"0.3.1\",\"markups\":[],\"atoms\":[],\"cards\":[[\"markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\"There are a lot of tutorials out there for using Bluetooth (Low Energy) on iOS.  Many of those are written using Swift.  Some of them cover the mechanics of Bluetooth, while others focus on just the code.  This post will definitely be of the later variety.\\n\\nWhy this post if it is already thoroughly covered in other areas?  Mostly because I recently ran across I project on which I needed BLE, had to really get my head around it, and now want to write it down in case I need it again.  And, hey, maybe it will be valuable to others.\\n\\n>While the process is the same, the interface to the CoreBluetooth API changes with pretty much every new version of Swift. As of Oct 2017, a version using Swift 3 can be found on my IBM GitHub [repository](https://github.com/krhoyt/IBM/blob/master/iot/bluetooth/cloud/ios/Bean/Bean.swift).\\n\\n###(1) Import\\n\\nUnlike beacons, which use Core Location, if you are communicating to a BLE device, you will use CoreBluetooth.\\n\\n```\\nimport CoreBluetooth\\n```\\n\\n###(2) Delegates\\n\\nEventually you are going to want to get callbacks from some functionality.  There are two delegates to implement: CBCentralManagerDelegate, and CBPeripheralDelegate.\\n\\n```\\nclass ViewController: \\n  UIViewController, \\n  CBCentralManagerDelegate, \\n  CBPeripheralDelegate {\\n\\n  // Moar code\\n\\n}\\n```\\n\\n###(3) Declare Manager and Peripheral\\n\\nThe CBCentralManager install will be what you use to find, connect, and manage BLE devices.  Once you are connected, and are working with a specific service, the peripheral will help you iterate characteristics and interacting with them.\\n\\n```\\nvar manager:CBCentralManager!\\nvar peripheral:CBPeripheral!\\n```\\n\\n###(4) UUID and Service Name\\n\\nYou will need UUID for the BLE service, and a UUID for the specific characteristic.  In some cases, you will need additional UUIDs.  They get used repeatedly throughout the code, so having constants for them will keep the code cleaner, and easier to maintain.  There are also many service/characteristic pairs called out in the [specification](https://developer.bluetooth.org/gatt/services/Pages/ServicesHome.aspx).\\n\\n```\\nlet BEAN_NAME = \\\"Robu\\\"\\nlet BEAN_SCRATCH_UUID = \\n  CBUUID(string: \\\"a495ff21-c5b1-4b44-b512-1370f02d74de\\\")\\nlet BEAN_SERVICE_UUID = \\n  CBUUID(string: \\\"a495ff20-c5b1-4b44-b512-1370f02d74de\\\")\\n```\\n\\n###(5) Instantiate Manager\\n\\nOne-liner to create an instance of CBCentralManager.  It takes the delegate as an argument, and options, which in most cases are not needed.  This is also the jumping off point for what effectively becomes a chain of the remaining seven waterfall steps.\\n\\n```\\noverride func viewDidLoad() {\\n  super.viewDidLoad()        \\n  manager = CBCentralManager(delegate: self, queue: nil)\\n}\\n```\\n\\n###(6) Scan for Devices\\n\\nOnce the CBCentralManager instance is finished creating, it will call centralManagerDidUpdateState on the delegate class.  From there, if Bluetooth is available (as in \\\"turned on\\\"), you can start scanning for devices.\\n\\n```\\nfunc centralManagerDidUpdateState(central: CBCentralManager) {\\n  if central.state == CBCentralManagerState.PoweredOn {\\n    central.scanForPeripheralsWithServices(nil, options: nil)\\n  } else {\\n    print(\\\"Bluetooth not available.\\\")\\n  }\\n}\\n```\\n\\n###(7) Connect to a Device\\n\\nWhen you find the device you are interested in interacting with, you will want to connect to it.  This is the only place where the device name shows up in the code, but I still like to declare it as a constant with the UUIDs.\\n\\n```\\nfunc centralManager(\\n  central: CBCentralManager, \\n  didDiscoverPeripheral peripheral: CBPeripheral, \\n  advertisementData: [String : AnyObject], \\n  RSSI: NSNumber) {\\n  let device = (advertisementData as NSDictionary)\\n    .objectForKey(CBAdvertisementDataLocalNameKey) \\n    as? NSString\\n        \\n  if device?.containsString(BEAN_NAME) == true {\\n    self.manager.stopScan()\\n            \\n    self.peripheral = peripheral\\n    self.peripheral.delegate = self\\n            \\n    manager.connectPeripheral(peripheral, options: nil)\\n  }\\n}\\n```\\n\\n###(8) Get Services\\n\\nOnce you are connected to a device, you can get a list of services on that device.\\n\\n```\\nfunc centralManager(\\n  central: CBCentralManager, \\n  didConnectPeripheral peripheral: CBPeripheral) {\\n  peripheral.discoverServices(nil)\\n}\\n```\\n\\n###(9) Get Characteristics\\n\\nOnce you get a list of the services offered by the device, you will want to get a list of the characteristics.  You can get crazy here, or limit listing of characteristics to just a specific service.  If you go crazy watch for threading issues.\\n\\n```\\nfunc peripheral(\\n  peripheral: CBPeripheral, \\n  didDiscoverServices error: NSError?) {\\n  for service in peripheral.services! {\\n    let thisService = service as CBService\\n\\n    if service.UUID == BEAN_SERVICE_UUID {\\n      peripheral.discoverCharacteristics(\\n        nil, \\n        forService: thisService\\n      )\\n    }\\n  }\\n}\\n```\\n\\n###(10) Setup Notifications\\n\\nThere are different ways to approach getting data from the BLE device.  One approach would be to read changes incrementally.  Another approach, the approach I used in my application, would be to have the BLE device notify you whenever a characteristic value has changed.\\n\\n```\\nfunc peripheral(\\n  peripheral: CBPeripheral, \\n  didDiscoverCharacteristicsForService service: CBService, \\n  error: NSError?) {\\n  for characteristic in service.characteristics! {\\n    let thisCharacteristic = characteristic as CBCharacteristic\\n\\n    if thisCharacteristic.UUID == BEAN_SCRATCH_UUID {\\n      self.peripheral.setNotifyValue(\\n        true, \\n        forCharacteristic: thisCharacteristic\\n      )\\n    }\\n  }\\n}\\n```\\n\\n###(11) Changes Are Coming\\n\\nAny characteristic changes you have setup to receive notifications for will call this delegate method.  You will want to be sure and filter them out to take the appropriate action for the specific change.\\n\\n```\\nfunc peripheral(\\n  peripheral: CBPeripheral, \\n  didUpdateValueForCharacteristic characteristic: CBCharacteristic, \\n  error: NSError?) {\\n  var count:UInt32 = 0;\\n\\n  if characteristic.UUID == BEAN_SCRATCH_UUID {\\n    characteristic.value!.getBytes(&count, length: sizeof(UInt32))\\n    labelCount.text = \\n      NSString(format: \\\"%llu\\\", count) as String\\n  }\\n}\\n```\\n\\n###(12) Disconnect and Try Again\\n\\nThis is an optional step, but hey, let us be good programmers and clean up after ourselves.  Also a good place to start scanning all over again.\\n\\n```\\nfunc centralManager(\\n  central: CBCentralManager, \\n  didDisconnectPeripheral peripheral: CBPeripheral, \\n  error: NSError?) {\\n  central.scanForPeripheralsWithServices(nil, options: nil)\\n}\\n```\\n\\n###Next Steps\\n\\nYes, twelve, count them, twelve steps to working with BLE devices - and this is just a single service and single characteristic.  What makes them particularly tricky is that they cascade.  After step &num;5 the everything is one long chain.  Watch your ... step (ha!).\\n\"}]],\"sections\":[[10,0]],\"ghostVersion\":\"3.0\"}","html":"<!--kg-card-begin: markdown--><p>There are a lot of tutorials out there for using Bluetooth (Low Energy) on iOS.  Many of those are written using Swift.  Some of them cover the mechanics of Bluetooth, while others focus on just the code.  This post will definitely be of the later variety.</p>\n<p>Why this post if it is already thoroughly covered in other areas?  Mostly because I recently ran across I project on which I needed BLE, had to really get my head around it, and now want to write it down in case I need it again.  And, hey, maybe it will be valuable to others.</p>\n<blockquote>\n<p>While the process is the same, the interface to the CoreBluetooth API changes with pretty much every new version of Swift. As of Oct 2017, a version using Swift 3 can be found on my IBM GitHub <a href=\"https://github.com/krhoyt/IBM/blob/master/iot/bluetooth/cloud/ios/Bean/Bean.swift\">repository</a>.</p>\n</blockquote>\n<h3 id=\"1import\">(1) Import</h3>\n<p>Unlike beacons, which use Core Location, if you are communicating to a BLE device, you will use CoreBluetooth.</p>\n<pre><code>import CoreBluetooth\n</code></pre>\n<h3 id=\"2delegates\">(2) Delegates</h3>\n<p>Eventually you are going to want to get callbacks from some functionality.  There are two delegates to implement: CBCentralManagerDelegate, and CBPeripheralDelegate.</p>\n<pre><code>class ViewController: \n  UIViewController, \n  CBCentralManagerDelegate, \n  CBPeripheralDelegate {\n\n  // Moar code\n\n}\n</code></pre>\n<h3 id=\"3declaremanagerandperipheral\">(3) Declare Manager and Peripheral</h3>\n<p>The CBCentralManager install will be what you use to find, connect, and manage BLE devices.  Once you are connected, and are working with a specific service, the peripheral will help you iterate characteristics and interacting with them.</p>\n<pre><code>var manager:CBCentralManager!\nvar peripheral:CBPeripheral!\n</code></pre>\n<h3 id=\"4uuidandservicename\">(4) UUID and Service Name</h3>\n<p>You will need UUID for the BLE service, and a UUID for the specific characteristic.  In some cases, you will need additional UUIDs.  They get used repeatedly throughout the code, so having constants for them will keep the code cleaner, and easier to maintain.  There are also many service/characteristic pairs called out in the <a href=\"https://developer.bluetooth.org/gatt/services/Pages/ServicesHome.aspx\">specification</a>.</p>\n<pre><code>let BEAN_NAME = &quot;Robu&quot;\nlet BEAN_SCRATCH_UUID = \n  CBUUID(string: &quot;a495ff21-c5b1-4b44-b512-1370f02d74de&quot;)\nlet BEAN_SERVICE_UUID = \n  CBUUID(string: &quot;a495ff20-c5b1-4b44-b512-1370f02d74de&quot;)\n</code></pre>\n<h3 id=\"5instantiatemanager\">(5) Instantiate Manager</h3>\n<p>One-liner to create an instance of CBCentralManager.  It takes the delegate as an argument, and options, which in most cases are not needed.  This is also the jumping off point for what effectively becomes a chain of the remaining seven waterfall steps.</p>\n<pre><code>override func viewDidLoad() {\n  super.viewDidLoad()        \n  manager = CBCentralManager(delegate: self, queue: nil)\n}\n</code></pre>\n<h3 id=\"6scanfordevices\">(6) Scan for Devices</h3>\n<p>Once the CBCentralManager instance is finished creating, it will call centralManagerDidUpdateState on the delegate class.  From there, if Bluetooth is available (as in &quot;turned on&quot;), you can start scanning for devices.</p>\n<pre><code>func centralManagerDidUpdateState(central: CBCentralManager) {\n  if central.state == CBCentralManagerState.PoweredOn {\n    central.scanForPeripheralsWithServices(nil, options: nil)\n  } else {\n    print(&quot;Bluetooth not available.&quot;)\n  }\n}\n</code></pre>\n<h3 id=\"7connecttoadevice\">(7) Connect to a Device</h3>\n<p>When you find the device you are interested in interacting with, you will want to connect to it.  This is the only place where the device name shows up in the code, but I still like to declare it as a constant with the UUIDs.</p>\n<pre><code>func centralManager(\n  central: CBCentralManager, \n  didDiscoverPeripheral peripheral: CBPeripheral, \n  advertisementData: [String : AnyObject], \n  RSSI: NSNumber) {\n  let device = (advertisementData as NSDictionary)\n    .objectForKey(CBAdvertisementDataLocalNameKey) \n    as? NSString\n        \n  if device?.containsString(BEAN_NAME) == true {\n    self.manager.stopScan()\n            \n    self.peripheral = peripheral\n    self.peripheral.delegate = self\n            \n    manager.connectPeripheral(peripheral, options: nil)\n  }\n}\n</code></pre>\n<h3 id=\"8getservices\">(8) Get Services</h3>\n<p>Once you are connected to a device, you can get a list of services on that device.</p>\n<pre><code>func centralManager(\n  central: CBCentralManager, \n  didConnectPeripheral peripheral: CBPeripheral) {\n  peripheral.discoverServices(nil)\n}\n</code></pre>\n<h3 id=\"9getcharacteristics\">(9) Get Characteristics</h3>\n<p>Once you get a list of the services offered by the device, you will want to get a list of the characteristics.  You can get crazy here, or limit listing of characteristics to just a specific service.  If you go crazy watch for threading issues.</p>\n<pre><code>func peripheral(\n  peripheral: CBPeripheral, \n  didDiscoverServices error: NSError?) {\n  for service in peripheral.services! {\n    let thisService = service as CBService\n\n    if service.UUID == BEAN_SERVICE_UUID {\n      peripheral.discoverCharacteristics(\n        nil, \n        forService: thisService\n      )\n    }\n  }\n}\n</code></pre>\n<h3 id=\"10setupnotifications\">(10) Setup Notifications</h3>\n<p>There are different ways to approach getting data from the BLE device.  One approach would be to read changes incrementally.  Another approach, the approach I used in my application, would be to have the BLE device notify you whenever a characteristic value has changed.</p>\n<pre><code>func peripheral(\n  peripheral: CBPeripheral, \n  didDiscoverCharacteristicsForService service: CBService, \n  error: NSError?) {\n  for characteristic in service.characteristics! {\n    let thisCharacteristic = characteristic as CBCharacteristic\n\n    if thisCharacteristic.UUID == BEAN_SCRATCH_UUID {\n      self.peripheral.setNotifyValue(\n        true, \n        forCharacteristic: thisCharacteristic\n      )\n    }\n  }\n}\n</code></pre>\n<h3 id=\"11changesarecoming\">(11) Changes Are Coming</h3>\n<p>Any characteristic changes you have setup to receive notifications for will call this delegate method.  You will want to be sure and filter them out to take the appropriate action for the specific change.</p>\n<pre><code>func peripheral(\n  peripheral: CBPeripheral, \n  didUpdateValueForCharacteristic characteristic: CBCharacteristic, \n  error: NSError?) {\n  var count:UInt32 = 0;\n\n  if characteristic.UUID == BEAN_SCRATCH_UUID {\n    characteristic.value!.getBytes(&amp;count, length: sizeof(UInt32))\n    labelCount.text = \n      NSString(format: &quot;%llu&quot;, count) as String\n  }\n}\n</code></pre>\n<h3 id=\"12disconnectandtryagain\">(12) Disconnect and Try Again</h3>\n<p>This is an optional step, but hey, let us be good programmers and clean up after ourselves.  Also a good place to start scanning all over again.</p>\n<pre><code>func centralManager(\n  central: CBCentralManager, \n  didDisconnectPeripheral peripheral: CBPeripheral, \n  error: NSError?) {\n  central.scanForPeripheralsWithServices(nil, options: nil)\n}\n</code></pre>\n<h3 id=\"nextsteps\">Next Steps</h3>\n<p>Yes, twelve, count them, twelve steps to working with BLE devices - and this is just a single service and single characteristic.  What makes them particularly tricky is that they cascade.  After step #5 the everything is one long chain.  Watch your ... step (ha!).</p>\n<!--kg-card-end: markdown-->","comment_id":"55","plaintext":"There are a lot of tutorials out there for using Bluetooth (Low Energy) on iOS.\nMany of those are written using Swift. Some of them cover the mechanics of\nBluetooth, while others focus on just the code. This post will definitely be of\nthe later variety.\n\nWhy this post if it is already thoroughly covered in other areas? Mostly because\nI recently ran across I project on which I needed BLE, had to really get my head\naround it, and now want to write it down in case I need it again. And, hey,\nmaybe it will be valuable to others.\n\n> While the process is the same, the interface to the CoreBluetooth API changes\nwith pretty much every new version of Swift. As of Oct 2017, a version using\nSwift 3 can be found on my IBM GitHub repository\n[https://github.com/krhoyt/IBM/blob/master/iot/bluetooth/cloud/ios/Bean/Bean.swift]\n.\n\n\n(1) Import\nUnlike beacons, which use Core Location, if you are communicating to a BLE\ndevice, you will use CoreBluetooth.\n\nimport CoreBluetooth\n\n\n(2) Delegates\nEventually you are going to want to get callbacks from some functionality. There\nare two delegates to implement: CBCentralManagerDelegate, and\nCBPeripheralDelegate.\n\nclass ViewController: \n  UIViewController, \n  CBCentralManagerDelegate, \n  CBPeripheralDelegate {\n\n  // Moar code\n\n}\n\n\n(3) Declare Manager and Peripheral\nThe CBCentralManager install will be what you use to find, connect, and manage\nBLE devices. Once you are connected, and are working with a specific service,\nthe peripheral will help you iterate characteristics and interacting with them.\n\nvar manager:CBCentralManager!\nvar peripheral:CBPeripheral!\n\n\n(4) UUID and Service Name\nYou will need UUID for the BLE service, and a UUID for the specific\ncharacteristic. In some cases, you will need additional UUIDs. They get used\nrepeatedly throughout the code, so having constants for them will keep the code\ncleaner, and easier to maintain. There are also many service/characteristic\npairs called out in the specification\n[https://developer.bluetooth.org/gatt/services/Pages/ServicesHome.aspx].\n\nlet BEAN_NAME = \"Robu\"\nlet BEAN_SCRATCH_UUID = \n  CBUUID(string: \"a495ff21-c5b1-4b44-b512-1370f02d74de\")\nlet BEAN_SERVICE_UUID = \n  CBUUID(string: \"a495ff20-c5b1-4b44-b512-1370f02d74de\")\n\n\n(5) Instantiate Manager\nOne-liner to create an instance of CBCentralManager. It takes the delegate as an\nargument, and options, which in most cases are not needed. This is also the\njumping off point for what effectively becomes a chain of the remaining seven\nwaterfall steps.\n\noverride func viewDidLoad() {\n  super.viewDidLoad()        \n  manager = CBCentralManager(delegate: self, queue: nil)\n}\n\n\n(6) Scan for Devices\nOnce the CBCentralManager instance is finished creating, it will call\ncentralManagerDidUpdateState on the delegate class. From there, if Bluetooth is\navailable (as in \"turned on\"), you can start scanning for devices.\n\nfunc centralManagerDidUpdateState(central: CBCentralManager) {\n  if central.state == CBCentralManagerState.PoweredOn {\n    central.scanForPeripheralsWithServices(nil, options: nil)\n  } else {\n    print(\"Bluetooth not available.\")\n  }\n}\n\n\n(7) Connect to a Device\nWhen you find the device you are interested in interacting with, you will want\nto connect to it. This is the only place where the device name shows up in the\ncode, but I still like to declare it as a constant with the UUIDs.\n\nfunc centralManager(\n  central: CBCentralManager, \n  didDiscoverPeripheral peripheral: CBPeripheral, \n  advertisementData: [String : AnyObject], \n  RSSI: NSNumber) {\n  let device = (advertisementData as NSDictionary)\n    .objectForKey(CBAdvertisementDataLocalNameKey) \n    as? NSString\n        \n  if device?.containsString(BEAN_NAME) == true {\n    self.manager.stopScan()\n            \n    self.peripheral = peripheral\n    self.peripheral.delegate = self\n            \n    manager.connectPeripheral(peripheral, options: nil)\n  }\n}\n\n\n(8) Get Services\nOnce you are connected to a device, you can get a list of services on that\ndevice.\n\nfunc centralManager(\n  central: CBCentralManager, \n  didConnectPeripheral peripheral: CBPeripheral) {\n  peripheral.discoverServices(nil)\n}\n\n\n(9) Get Characteristics\nOnce you get a list of the services offered by the device, you will want to get\na list of the characteristics. You can get crazy here, or limit listing of\ncharacteristics to just a specific service. If you go crazy watch for threading\nissues.\n\nfunc peripheral(\n  peripheral: CBPeripheral, \n  didDiscoverServices error: NSError?) {\n  for service in peripheral.services! {\n    let thisService = service as CBService\n\n    if service.UUID == BEAN_SERVICE_UUID {\n      peripheral.discoverCharacteristics(\n        nil, \n        forService: thisService\n      )\n    }\n  }\n}\n\n\n(10) Setup Notifications\nThere are different ways to approach getting data from the BLE device. One\napproach would be to read changes incrementally. Another approach, the approach\nI used in my application, would be to have the BLE device notify you whenever a\ncharacteristic value has changed.\n\nfunc peripheral(\n  peripheral: CBPeripheral, \n  didDiscoverCharacteristicsForService service: CBService, \n  error: NSError?) {\n  for characteristic in service.characteristics! {\n    let thisCharacteristic = characteristic as CBCharacteristic\n\n    if thisCharacteristic.UUID == BEAN_SCRATCH_UUID {\n      self.peripheral.setNotifyValue(\n        true, \n        forCharacteristic: thisCharacteristic\n      )\n    }\n  }\n}\n\n\n(11) Changes Are Coming\nAny characteristic changes you have setup to receive notifications for will call\nthis delegate method. You will want to be sure and filter them out to take the\nappropriate action for the specific change.\n\nfunc peripheral(\n  peripheral: CBPeripheral, \n  didUpdateValueForCharacteristic characteristic: CBCharacteristic, \n  error: NSError?) {\n  var count:UInt32 = 0;\n\n  if characteristic.UUID == BEAN_SCRATCH_UUID {\n    characteristic.value!.getBytes(&count, length: sizeof(UInt32))\n    labelCount.text = \n      NSString(format: \"%llu\", count) as String\n  }\n}\n\n\n(12) Disconnect and Try Again\nThis is an optional step, but hey, let us be good programmers and clean up after\nourselves. Also a good place to start scanning all over again.\n\nfunc centralManager(\n  central: CBCentralManager, \n  didDisconnectPeripheral peripheral: CBPeripheral, \n  error: NSError?) {\n  central.scanForPeripheralsWithServices(nil, options: nil)\n}\n\n\nNext Steps\nYes, twelve, count them, twelve steps to working with BLE devices - and this is\njust a single service and single characteristic. What makes them particularly\ntricky is that they cascade. After step #5 the everything is one long chain.\nWatch your ... step (ha!).","feature_image":"http://images.kevinhoyt.com/blue.teeth.jpg","featured":0,"status":"published","locale":null,"visibility":"public","author_id":"1","created_at":"2016-05-06T04:21:58.000Z","updated_at":"2017-10-12T18:05:26.000Z","published_at":"2016-05-20T00:59:14.000Z","custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"type":"post","email_recipient_filter":"none"},{"id":"5adb8922351ffe0018a57741","uuid":"d7a0c67b-eae1-455c-9c02-f3fb20fbd221","title":"Discovering Beacons (Swift)","slug":"discovering-beacons-swift","mobiledoc":"{\"version\":\"0.3.1\",\"markups\":[],\"atoms\":[],\"cards\":[[\"markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\"When it comes to Bluetooth Smart (often called Bluetooth Low Energy, Bluetooth LE, or just BLE), there is a lot for a developer to like.  Long battery life, connectivity without crazy pairing processes, and of course the beacon profile.  This is a quick overview of how your iOS application can detect beacons.\\n\\n###Profiles and Modes\\n\\nWhen it comes to Bluetooth LE, ==devices will be in one of two different states==.  The first state is called \\\"==connected==\\\" or \\\"normal\\\".  This is a ==one-to-one== communication state between the BLE device, and your smartphone (or other device).  This mode is commonly found in consumer BLE devices such as ==heart rate monitors==.\\n\\nThe other mode is called \\\"==advertising==\\\" mode, and is generally the backbone of how ==beacons== work.  In advertising mode, a BLE device sends out three key pieces of information for your application to leverage.  This is a ==many-to-many== mode of operation.  The pieces of information are:\\n\\n- **UUID** - This is how you look for and get notifications in your application, for your specific beacon.  The most common way to think about UUID in a beacon setting is that it is roughly analogous to your brand.\\n- **Major** - This is a number consisting of two bytes, and is roughly analogous to a store for a specific brand.\\n- **Minor** - Another number with two bytes will specify a specific shelf or location within a store.\\n\\nThe analogies made here of ==brand, store, shelf==, are just a way to think about how to choose and assign these pieces of information.  There is no hard rule that says you have to use them in this fashion.  ==UUID will generally be consistent== within a set of set of BLE devices used for a specific application.  After that, you can use major and minor to describe whatever is appropriate for your application.\\n\\n###(1) Core Location\\n\\nWhen you connect to a BLE device in the \\\"connected\\\" mode, you use Core Blueooth.  Beacons are considering an extension of location - indoors.  As such, beacons fall under Core Location on iOS.\\n\\n```\\nimport CoreLocation\\n```\\n\\n###(2) Delegate\\n\\nYou will want your application to be notified when a beacon is discovered.  Like most event-driven APIs in iOS, this means you will need to implement a delegate.  For beacons, as a location service, you will need to implement CLlocationManagerDelegate.\\n\\n```\\nclass ViewController: UIViewController, CLLocationManagerDelegate {\\n\\n  // Teh codez\\n\\n}\\n```\\n\\n###(3) Brand and UUID\\n\\nMost beacons will allow you to set the UUID you want to use, that works for your application, in some fashion of management console.  Your iOS application will want to know this value.  You will also define a \\\"brand identifier\\\" which I have never used outside of the fact that Core Location seems to want one.\\n\\n```\\nlet BRAND_IDENTIFIER = \\\"com.ibm\\\"\\nlet BRAND_UUID: String = \\\"A495DE30-C5B1-4B44-B512-1370F02D74DF\\\"\\n```\\n\\n###(4) Region and Manager\\n\\nSpecifying what beacons to look for is called a \\\"region\\\" in iOS.  You create a beacon region, and assign it to a Location Manager instance.  These are values we will want to have around, so declare them with a broad scope visibility.\\n\\n```\\nvar beaconRegion: CLBeaconRegion!\\nvar locationManager: CLLocationManager!\\n```\\n\\n###(5) Create the Manager\\n\\nI generally instantiate my Location Manager instance in \\\"viewDidLoad\\\".  It does not take any arguments at this point, you just instantiate the instance.  We will strap on additional details in a moment.\\n\\n```\\noverride func viewDidLoad() {\\n  locationManager = CLLocationManager()\\n  locationManager.delegate = self  \\n}\\n```\\n\\nNote that the delegate is assigned to \\\"self\\\" which is why we have implemented the CLLocationManagerDelegate on the view controller.\\n\\n###(6) Request Authorization\\n\\nIn order to use location features of iOS, you must first ask the user for permission.  This is one of those steps that can really trip you up if you are not careful.  You will run your application, and it will run without a problem, but beacon ranging simply will not do anything.  You will scratch you head, set breakpoints everywhere, look hard, and find nothing.\\n\\n```\\noverride func viewDidLoad() {\\n  locationManager = CLLocationManager()\\n  locationManager.delegate = self\\n\\n  locationManager.requestWhenInUseAuthorization()\\n}\\n```\\n\\nYou can also check to see if the application already has been given access, or if the user has declined access, and prompt again.  For brevity, I use the one-liner above, but it is worth spending some time here to make sure you check/catch authorization.\\n\\n> Even without checking, this one-liner will only prompt the user one, regardless of how often it is executed.  iOS will just check the system to see if it has authorization.  If it does, it will just move on.\\n\\nThe other part of this, that does not show up in code, is an additional property that must be specified in your \\\"Info.plist\\\" file.  The property is \\\"\\\" which is a string, and you can set it to whatever you want.  That string will be presented to the user when they are prompted for authorization.  You will want to use verbiage here that encourages the user to agree to access to location.\\n\\n###(7) Region\\n\\nUp next we will instantiate the beacon region we are interested in monitoring.  This is where that UUID and brand identifier come into play.\\n\\n```\\noverride func viewDidLoad() {\\n  locationManager = CLLocationManager()\\n  locationManager.delegate = self\\n\\n  locationManager.requestWhenInUseAuthorization()\\n\\n  let uuid = NSUUID(UUIDString: BRAND_UUID)\\n  beaconRegion = CLBeaconRegion(\\n    proximityUUID: uuid!, \\n    identifier: BRAND_IDENTIFIER\\n  )\\n  beaconRegion.notifyOnEntry = true\\n  beaconRegion.notifyOnExit = true\\n}\\n```\\n\\nCore Location gives you methods to implement for notifications when the phone enters or leaves a specific region.  In my experience, these methods are called sporadically, so I generally do not implement them.  You need to specify \\\"true\\\" for the notifications, but you do not need to implement the respective methods.\\n\\n###(8) Monitoring\\n\\nUp next we tell the iOS to start looking for beacons.  If you have done everything correctly up to this point, iOS will start the magic of finding beacons.\\n\\n```\\noverride func viewDidLoad() {\\n  locationManager = CLLocationManager()\\n  locationManager.delegate = self\\n\\n  locationManager.requestWhenInUseAuthorization()\\n\\n  let uuid = NSUUID(UUIDString: BRAND_UUID)\\n  beaconRegion = CLBeaconRegion(\\n    proximityUUID: uuid!, \\n    identifier: BRAND_IDENTIFIER\\n  )\\n  beaconRegion.notifyOnEntry = true\\n  beaconRegion.notifyOnExit = true\\n\\n  locationManager.startMonitoringForRegion(beaconRegion)\\n  locationManager.startRangingBeaconsInRegion(beaconRegion)\\n}\\n```\\n\\n###(9) Did Range Beacons\\n\\nWhen beacons with the specified UUID are discovered, the \\\"didRangeBeacons\\\" method will be called by iOS.  Keep in mind that more than one beacon may be present, and reported.\\n\\n```\\nfunc locationManager(\\n  manager: CLLocationManager, \\n  didRangeBeacons beacons: [CLBeacon], \\n  inRegion region: CLBeaconRegion) {\\n\\n  if beacons.count > 0 {\\n    // Beacons found\\n  } else {\\n    // No beacons found\\n  }\\n}\\n```\\n\\nWhen the iOS device leaves the beacon range, the \\\"didRangeBeacons\\\" method will be called as well.  It is important then to catch that situation, and adjust the user interface as needed.\\n\\n###(10) How Close?\\n\\nWhile beacons do not give you specific distances, they do present a value called RSSI (Relative Signal Strength Indicator).  iOS abstracts this value into a series of constants on the CLPromitity class.  You can then in turn determine which beacons are closest and take whatever action your application requires.\\n\\n```\\nfunc locationManager(\\n  manager: CLLocationManager, \\n  didRangeBeacons beacons: [CLBeacon], \\n  inRegion region: CLBeaconRegion) {\\n\\n  if beacons.count > 0 {\\n    for beacon:CLBeacon in beacons {\\n      if beacon.proximity == CLProximity.Far {\\n        // Far away, whatever than means\\n      } else if beacon.proximity == CLProximity.Near {\\n        // Getting closer\\n      } else if beacon.proximity == CLProximity.Immediate {\\n        // Right on top of the beacon\\n      }\\n    }\\n  } else {\\n    // No beacons found\\n  }\\n}\\n```\\n\\nThe values of \\\"Far\\\", \\\"Near\\\", and \\\"Immediate\\\" are pretty arbitrary.  In my testing, \\\"Far\\\" is about the last couple of feet of the beacon range (which may vary depending on how the radio is set on your beacon).  \\\"Near\\\" seems to be a broad range somewhere between a foot away from the iOS device, up to around three feet away.  \\\"Immediate\\\" seems to match when the iOS device is literally right next to the beacon - within inches.  ==YMMV==.\\n\\n###Next Steps\\n\\nThat is all there is to it.  While it looks like a lot of work, once you put it into your code, you will find that you can get going very quickly.  From here you might want to invoke a REST call to a service to store the fact that the device encountered the beacon.  Or you might track the time a device spent near the beacon.  The possibilities are endless.\"}]],\"sections\":[[10,0]],\"ghostVersion\":\"3.0\"}","html":"<!--kg-card-begin: markdown--><p>When it comes to Bluetooth Smart (often called Bluetooth Low Energy, Bluetooth LE, or just BLE), there is a lot for a developer to like.  Long battery life, connectivity without crazy pairing processes, and of course the beacon profile.  This is a quick overview of how your iOS application can detect beacons.</p>\n<h3 id=\"profilesandmodes\">Profiles and Modes</h3>\n<p>When it comes to Bluetooth LE, <mark>devices will be in one of two different states</mark>.  The first state is called &quot;<mark>connected</mark>&quot; or &quot;normal&quot;.  This is a <mark>one-to-one</mark> communication state between the BLE device, and your smartphone (or other device).  This mode is commonly found in consumer BLE devices such as <mark>heart rate monitors</mark>.</p>\n<p>The other mode is called &quot;<mark>advertising</mark>&quot; mode, and is generally the backbone of how <mark>beacons</mark> work.  In advertising mode, a BLE device sends out three key pieces of information for your application to leverage.  This is a <mark>many-to-many</mark> mode of operation.  The pieces of information are:</p>\n<ul>\n<li><strong>UUID</strong> - This is how you look for and get notifications in your application, for your specific beacon.  The most common way to think about UUID in a beacon setting is that it is roughly analogous to your brand.</li>\n<li><strong>Major</strong> - This is a number consisting of two bytes, and is roughly analogous to a store for a specific brand.</li>\n<li><strong>Minor</strong> - Another number with two bytes will specify a specific shelf or location within a store.</li>\n</ul>\n<p>The analogies made here of <mark>brand, store, shelf</mark>, are just a way to think about how to choose and assign these pieces of information.  There is no hard rule that says you have to use them in this fashion.  <mark>UUID will generally be consistent</mark> within a set of set of BLE devices used for a specific application.  After that, you can use major and minor to describe whatever is appropriate for your application.</p>\n<h3 id=\"1corelocation\">(1) Core Location</h3>\n<p>When you connect to a BLE device in the &quot;connected&quot; mode, you use Core Blueooth.  Beacons are considering an extension of location - indoors.  As such, beacons fall under Core Location on iOS.</p>\n<pre><code>import CoreLocation\n</code></pre>\n<h3 id=\"2delegate\">(2) Delegate</h3>\n<p>You will want your application to be notified when a beacon is discovered.  Like most event-driven APIs in iOS, this means you will need to implement a delegate.  For beacons, as a location service, you will need to implement CLlocationManagerDelegate.</p>\n<pre><code>class ViewController: UIViewController, CLLocationManagerDelegate {\n\n  // Teh codez\n\n}\n</code></pre>\n<h3 id=\"3brandanduuid\">(3) Brand and UUID</h3>\n<p>Most beacons will allow you to set the UUID you want to use, that works for your application, in some fashion of management console.  Your iOS application will want to know this value.  You will also define a &quot;brand identifier&quot; which I have never used outside of the fact that Core Location seems to want one.</p>\n<pre><code>let BRAND_IDENTIFIER = &quot;com.ibm&quot;\nlet BRAND_UUID: String = &quot;A495DE30-C5B1-4B44-B512-1370F02D74DF&quot;\n</code></pre>\n<h3 id=\"4regionandmanager\">(4) Region and Manager</h3>\n<p>Specifying what beacons to look for is called a &quot;region&quot; in iOS.  You create a beacon region, and assign it to a Location Manager instance.  These are values we will want to have around, so declare them with a broad scope visibility.</p>\n<pre><code>var beaconRegion: CLBeaconRegion!\nvar locationManager: CLLocationManager!\n</code></pre>\n<h3 id=\"5createthemanager\">(5) Create the Manager</h3>\n<p>I generally instantiate my Location Manager instance in &quot;viewDidLoad&quot;.  It does not take any arguments at this point, you just instantiate the instance.  We will strap on additional details in a moment.</p>\n<pre><code>override func viewDidLoad() {\n  locationManager = CLLocationManager()\n  locationManager.delegate = self  \n}\n</code></pre>\n<p>Note that the delegate is assigned to &quot;self&quot; which is why we have implemented the CLLocationManagerDelegate on the view controller.</p>\n<h3 id=\"6requestauthorization\">(6) Request Authorization</h3>\n<p>In order to use location features of iOS, you must first ask the user for permission.  This is one of those steps that can really trip you up if you are not careful.  You will run your application, and it will run without a problem, but beacon ranging simply will not do anything.  You will scratch you head, set breakpoints everywhere, look hard, and find nothing.</p>\n<pre><code>override func viewDidLoad() {\n  locationManager = CLLocationManager()\n  locationManager.delegate = self\n\n  locationManager.requestWhenInUseAuthorization()\n}\n</code></pre>\n<p>You can also check to see if the application already has been given access, or if the user has declined access, and prompt again.  For brevity, I use the one-liner above, but it is worth spending some time here to make sure you check/catch authorization.</p>\n<blockquote>\n<p>Even without checking, this one-liner will only prompt the user one, regardless of how often it is executed.  iOS will just check the system to see if it has authorization.  If it does, it will just move on.</p>\n</blockquote>\n<p>The other part of this, that does not show up in code, is an additional property that must be specified in your &quot;Info.plist&quot; file.  The property is &quot;&quot; which is a string, and you can set it to whatever you want.  That string will be presented to the user when they are prompted for authorization.  You will want to use verbiage here that encourages the user to agree to access to location.</p>\n<h3 id=\"7region\">(7) Region</h3>\n<p>Up next we will instantiate the beacon region we are interested in monitoring.  This is where that UUID and brand identifier come into play.</p>\n<pre><code>override func viewDidLoad() {\n  locationManager = CLLocationManager()\n  locationManager.delegate = self\n\n  locationManager.requestWhenInUseAuthorization()\n\n  let uuid = NSUUID(UUIDString: BRAND_UUID)\n  beaconRegion = CLBeaconRegion(\n    proximityUUID: uuid!, \n    identifier: BRAND_IDENTIFIER\n  )\n  beaconRegion.notifyOnEntry = true\n  beaconRegion.notifyOnExit = true\n}\n</code></pre>\n<p>Core Location gives you methods to implement for notifications when the phone enters or leaves a specific region.  In my experience, these methods are called sporadically, so I generally do not implement them.  You need to specify &quot;true&quot; for the notifications, but you do not need to implement the respective methods.</p>\n<h3 id=\"8monitoring\">(8) Monitoring</h3>\n<p>Up next we tell the iOS to start looking for beacons.  If you have done everything correctly up to this point, iOS will start the magic of finding beacons.</p>\n<pre><code>override func viewDidLoad() {\n  locationManager = CLLocationManager()\n  locationManager.delegate = self\n\n  locationManager.requestWhenInUseAuthorization()\n\n  let uuid = NSUUID(UUIDString: BRAND_UUID)\n  beaconRegion = CLBeaconRegion(\n    proximityUUID: uuid!, \n    identifier: BRAND_IDENTIFIER\n  )\n  beaconRegion.notifyOnEntry = true\n  beaconRegion.notifyOnExit = true\n\n  locationManager.startMonitoringForRegion(beaconRegion)\n  locationManager.startRangingBeaconsInRegion(beaconRegion)\n}\n</code></pre>\n<h3 id=\"9didrangebeacons\">(9) Did Range Beacons</h3>\n<p>When beacons with the specified UUID are discovered, the &quot;didRangeBeacons&quot; method will be called by iOS.  Keep in mind that more than one beacon may be present, and reported.</p>\n<pre><code>func locationManager(\n  manager: CLLocationManager, \n  didRangeBeacons beacons: [CLBeacon], \n  inRegion region: CLBeaconRegion) {\n\n  if beacons.count &gt; 0 {\n    // Beacons found\n  } else {\n    // No beacons found\n  }\n}\n</code></pre>\n<p>When the iOS device leaves the beacon range, the &quot;didRangeBeacons&quot; method will be called as well.  It is important then to catch that situation, and adjust the user interface as needed.</p>\n<h3 id=\"10howclose\">(10) How Close?</h3>\n<p>While beacons do not give you specific distances, they do present a value called RSSI (Relative Signal Strength Indicator).  iOS abstracts this value into a series of constants on the CLPromitity class.  You can then in turn determine which beacons are closest and take whatever action your application requires.</p>\n<pre><code>func locationManager(\n  manager: CLLocationManager, \n  didRangeBeacons beacons: [CLBeacon], \n  inRegion region: CLBeaconRegion) {\n\n  if beacons.count &gt; 0 {\n    for beacon:CLBeacon in beacons {\n      if beacon.proximity == CLProximity.Far {\n        // Far away, whatever than means\n      } else if beacon.proximity == CLProximity.Near {\n        // Getting closer\n      } else if beacon.proximity == CLProximity.Immediate {\n        // Right on top of the beacon\n      }\n    }\n  } else {\n    // No beacons found\n  }\n}\n</code></pre>\n<p>The values of &quot;Far&quot;, &quot;Near&quot;, and &quot;Immediate&quot; are pretty arbitrary.  In my testing, &quot;Far&quot; is about the last couple of feet of the beacon range (which may vary depending on how the radio is set on your beacon).  &quot;Near&quot; seems to be a broad range somewhere between a foot away from the iOS device, up to around three feet away.  &quot;Immediate&quot; seems to match when the iOS device is literally right next to the beacon - within inches.  <mark>YMMV</mark>.</p>\n<h3 id=\"nextsteps\">Next Steps</h3>\n<p>That is all there is to it.  While it looks like a lot of work, once you put it into your code, you will find that you can get going very quickly.  From here you might want to invoke a REST call to a service to store the fact that the device encountered the beacon.  Or you might track the time a device spent near the beacon.  The possibilities are endless.</p>\n<!--kg-card-end: markdown-->","comment_id":"56","plaintext":"When it comes to Bluetooth Smart (often called Bluetooth Low Energy, Bluetooth\nLE, or just BLE), there is a lot for a developer to like. Long battery life,\nconnectivity without crazy pairing processes, and of course the beacon profile.\nThis is a quick overview of how your iOS application can detect beacons.\n\nProfiles and Modes\nWhen it comes to Bluetooth LE, devices will be in one of two different states.\nThe first state is called \"connected\" or \"normal\". This is a one-to-one \ncommunication state between the BLE device, and your smartphone (or other\ndevice). This mode is commonly found in consumer BLE devices such as heart rate\nmonitors.\n\nThe other mode is called \"advertising\" mode, and is generally the backbone of\nhow beacons work. In advertising mode, a BLE device sends out three key pieces\nof information for your application to leverage. This is a many-to-many mode of\noperation. The pieces of information are:\n\n * UUID - This is how you look for and get notifications in your application,\n   for your specific beacon. The most common way to think about UUID in a beacon\n   setting is that it is roughly analogous to your brand.\n * Major - This is a number consisting of two bytes, and is roughly analogous to\n   a store for a specific brand.\n * Minor - Another number with two bytes will specify a specific shelf or\n   location within a store.\n\nThe analogies made here of brand, store, shelf, are just a way to think about\nhow to choose and assign these pieces of information. There is no hard rule that\nsays you have to use them in this fashion. UUID will generally be consistent \nwithin a set of set of BLE devices used for a specific application. After that,\nyou can use major and minor to describe whatever is appropriate for your\napplication.\n\n(1) Core Location\nWhen you connect to a BLE device in the \"connected\" mode, you use Core Blueooth.\nBeacons are considering an extension of location - indoors. As such, beacons\nfall under Core Location on iOS.\n\nimport CoreLocation\n\n\n(2) Delegate\nYou will want your application to be notified when a beacon is discovered. Like\nmost event-driven APIs in iOS, this means you will need to implement a delegate.\nFor beacons, as a location service, you will need to implement\nCLlocationManagerDelegate.\n\nclass ViewController: UIViewController, CLLocationManagerDelegate {\n\n  // Teh codez\n\n}\n\n\n(3) Brand and UUID\nMost beacons will allow you to set the UUID you want to use, that works for your\napplication, in some fashion of management console. Your iOS application will\nwant to know this value. You will also define a \"brand identifier\" which I have\nnever used outside of the fact that Core Location seems to want one.\n\nlet BRAND_IDENTIFIER = \"com.ibm\"\nlet BRAND_UUID: String = \"A495DE30-C5B1-4B44-B512-1370F02D74DF\"\n\n\n(4) Region and Manager\nSpecifying what beacons to look for is called a \"region\" in iOS. You create a\nbeacon region, and assign it to a Location Manager instance. These are values we\nwill want to have around, so declare them with a broad scope visibility.\n\nvar beaconRegion: CLBeaconRegion!\nvar locationManager: CLLocationManager!\n\n\n(5) Create the Manager\nI generally instantiate my Location Manager instance in \"viewDidLoad\". It does\nnot take any arguments at this point, you just instantiate the instance. We will\nstrap on additional details in a moment.\n\noverride func viewDidLoad() {\n  locationManager = CLLocationManager()\n  locationManager.delegate = self  \n}\n\n\nNote that the delegate is assigned to \"self\" which is why we have implemented\nthe CLLocationManagerDelegate on the view controller.\n\n(6) Request Authorization\nIn order to use location features of iOS, you must first ask the user for\npermission. This is one of those steps that can really trip you up if you are\nnot careful. You will run your application, and it will run without a problem,\nbut beacon ranging simply will not do anything. You will scratch you head, set\nbreakpoints everywhere, look hard, and find nothing.\n\noverride func viewDidLoad() {\n  locationManager = CLLocationManager()\n  locationManager.delegate = self\n\n  locationManager.requestWhenInUseAuthorization()\n}\n\n\nYou can also check to see if the application already has been given access, or\nif the user has declined access, and prompt again. For brevity, I use the\none-liner above, but it is worth spending some time here to make sure you\ncheck/catch authorization.\n\n> Even without checking, this one-liner will only prompt the user one, regardless\nof how often it is executed. iOS will just check the system to see if it has\nauthorization. If it does, it will just move on.\n\n\nThe other part of this, that does not show up in code, is an additional property\nthat must be specified in your \"Info.plist\" file. The property is \"\" which is a\nstring, and you can set it to whatever you want. That string will be presented\nto the user when they are prompted for authorization. You will want to use\nverbiage here that encourages the user to agree to access to location.\n\n(7) Region\nUp next we will instantiate the beacon region we are interested in monitoring.\nThis is where that UUID and brand identifier come into play.\n\noverride func viewDidLoad() {\n  locationManager = CLLocationManager()\n  locationManager.delegate = self\n\n  locationManager.requestWhenInUseAuthorization()\n\n  let uuid = NSUUID(UUIDString: BRAND_UUID)\n  beaconRegion = CLBeaconRegion(\n    proximityUUID: uuid!, \n    identifier: BRAND_IDENTIFIER\n  )\n  beaconRegion.notifyOnEntry = true\n  beaconRegion.notifyOnExit = true\n}\n\n\nCore Location gives you methods to implement for notifications when the phone\nenters or leaves a specific region. In my experience, these methods are called\nsporadically, so I generally do not implement them. You need to specify \"true\"\nfor the notifications, but you do not need to implement the respective methods.\n\n(8) Monitoring\nUp next we tell the iOS to start looking for beacons. If you have done\neverything correctly up to this point, iOS will start the magic of finding\nbeacons.\n\noverride func viewDidLoad() {\n  locationManager = CLLocationManager()\n  locationManager.delegate = self\n\n  locationManager.requestWhenInUseAuthorization()\n\n  let uuid = NSUUID(UUIDString: BRAND_UUID)\n  beaconRegion = CLBeaconRegion(\n    proximityUUID: uuid!, \n    identifier: BRAND_IDENTIFIER\n  )\n  beaconRegion.notifyOnEntry = true\n  beaconRegion.notifyOnExit = true\n\n  locationManager.startMonitoringForRegion(beaconRegion)\n  locationManager.startRangingBeaconsInRegion(beaconRegion)\n}\n\n\n(9) Did Range Beacons\nWhen beacons with the specified UUID are discovered, the \"didRangeBeacons\"\nmethod will be called by iOS. Keep in mind that more than one beacon may be\npresent, and reported.\n\nfunc locationManager(\n  manager: CLLocationManager, \n  didRangeBeacons beacons: [CLBeacon], \n  inRegion region: CLBeaconRegion) {\n\n  if beacons.count > 0 {\n    // Beacons found\n  } else {\n    // No beacons found\n  }\n}\n\n\nWhen the iOS device leaves the beacon range, the \"didRangeBeacons\" method will\nbe called as well. It is important then to catch that situation, and adjust the\nuser interface as needed.\n\n(10) How Close?\nWhile beacons do not give you specific distances, they do present a value called\nRSSI (Relative Signal Strength Indicator). iOS abstracts this value into a\nseries of constants on the CLPromitity class. You can then in turn determine\nwhich beacons are closest and take whatever action your application requires.\n\nfunc locationManager(\n  manager: CLLocationManager, \n  didRangeBeacons beacons: [CLBeacon], \n  inRegion region: CLBeaconRegion) {\n\n  if beacons.count > 0 {\n    for beacon:CLBeacon in beacons {\n      if beacon.proximity == CLProximity.Far {\n        // Far away, whatever than means\n      } else if beacon.proximity == CLProximity.Near {\n        // Getting closer\n      } else if beacon.proximity == CLProximity.Immediate {\n        // Right on top of the beacon\n      }\n    }\n  } else {\n    // No beacons found\n  }\n}\n\n\nThe values of \"Far\", \"Near\", and \"Immediate\" are pretty arbitrary. In my\ntesting, \"Far\" is about the last couple of feet of the beacon range (which may\nvary depending on how the radio is set on your beacon). \"Near\" seems to be a\nbroad range somewhere between a foot away from the iOS device, up to around\nthree feet away. \"Immediate\" seems to match when the iOS device is literally\nright next to the beacon - within inches. YMMV.\n\nNext Steps\nThat is all there is to it. While it looks like a lot of work, once you put it\ninto your code, you will find that you can get going very quickly. From here you\nmight want to invoke a REST call to a service to store the fact that the device\nencountered the beacon. Or you might track the time a device spent near the\nbeacon. The possibilities are endless.","feature_image":"http://images.kevinhoyt.com/lighthouse.jpg","featured":0,"status":"published","locale":null,"visibility":"public","author_id":"1","created_at":"2016-05-20T01:11:15.000Z","updated_at":"2016-05-23T15:56:17.000Z","published_at":"2016-05-23T15:56:17.000Z","custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"type":"post","email_recipient_filter":"none"},{"id":"5adb8922351ffe0018a57742","uuid":"125c51e5-4b8b-454a-974e-956c8afdc47e","title":"Tessel on Watson IoT","slug":"tessel-on-watson-iot","mobiledoc":"{\"version\":\"0.3.1\",\"markups\":[],\"atoms\":[],\"cards\":[[\"markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\"Do you know JavaScript?  Do you like IoT?  Lean towards open standards?  Then the Tessel should be right up your alley!  Tessel is a hardware development platform.  It has both Ethernet and 802.11 b/g/n support (and more), and you develop for it using Node.js (or Rust, Python).  Based on OpenWRT, it is open source, and open governance.\\n\\n###Moar Details\\n\\n![Tessel 2 courtesy Makezine](http://images.kevinhoyt.com/tessel.2.jpg)\\n\\nThe Tessel team has made interesting hardware choices along the way.  On top of the connectivity I already mentioned, the Tessel includes ==two USB ports==, and ==two module ports==.  With a ==580MHz CPU, 64Mb of RAM, and 32Mb of flash==, this thing was made to deliver on your IoT dreams.\\n\\nWait, what is a module port?\\n\\nIf you are a JavaScript developer, chances are good that you are not an Electrical Engineer.  To ease that learning curve, Tessel produces a line of \\\"modules\\\" that plug into the Tessel module ports.  These include ==climate sensors, relay, accelerometer, and more==.  There are even ==USB modules== designed specifically for the Tessel, that let you add ==BLE, camera, and more==.\\n\\n![Tessel climate module.](https://s3.amazonaws.com/technicalmachine-assets/product+pics/2014+05+15+production+modules/climate.jpg)\\n\\nEach of the Tessel-developed modules have been tested, and include a Node.js library for easy control of the hardware.  You run \\\"==npm==\\\" to get the packages, ==require== them in your code, and use them just like any other Node.js program.  You can even hook your own sensor up to the modules ports, and control ==GPIO, talk SPI and I2C, and more==.\\n\\n> One of my first projects with the Tessel had me wiring up a standard 5mm LED to the pins on a modules port.  From there I controlled the GPIO from Node.js.\\n\\n###Tessel Configuration\\n\\nThere is Node.js support for ==MQTT==, so does that mean you can just load that module and connect to ==Watson IoT==?  ==Exactly==!  I covered getting started with Watson IoT in [another post](http://www.kevinhoyt.com/2016/04/27/particle-photon-on-watson-iot/).  In this post I will focus on the code and workflow of developing for the Tessel.\\n\\nThe ==Tessel CLI== is the way you will configure and program the device (at least initially).  If you have Node.js installed on your development machine, you can use \\\"npm\\\" to get the CLI installed.\\n\\n```\\nnpm install -g t2-cli\\n```\\n\\nWith the Tessel connected to your machine, and the CLI installed, you can ==setup wireless== information on the device.\\n\\n```\\nt2 wifi -n <network-name> -p <password>\\n```\\n\\nWorking on the same LAN as your Tessel, and want to ==push code to it wirelessly==?  Or maybe you need to free up a USB port on your development machine?\\n\\n```\\nt2 provision\\n```\\n\\nIt all really is that easy.  Do not forget to ==update== the firmware on the Tessel before we get going with Watson IoT.\\n\\n```\\nt2 update\\n```\\n\\n###First Project\\n\\nJust like you might use \\\"==npm init==\\\" to initialize a workspace for a Node.js project, you can use the CLI to help you out there with the Tessel.\\n\\n```\\nt2 init\\n```\\n\\nThis creates a basic \\\"==index.js==\\\" file which will act as the ==entry point== for your application, as well as a \\\"==package.json==\\\" file.  If you add Node.js modules to your project, do not forget to make sure they get into the \\\"package.json\\\".  ==Installing something like MQTT== would then look like this:\\n\\n```\\nnpm install --save mqtt\\n```\\n\\nWait!  ==That looks just like Node.js!==  Yes.  Yes it does.  That is because to use Node.js modules, you use the normal Node.js development workflow.  Pretty cool, right?\\n\\n###Start Counting\\n\\nTo make a counter in Node.js is easy enough - ==setInterval()== takes care of that.  And that is exactly what we will use here.\\n\\n```\\nvar count = 0;\\nvar interval = setInterval( function() {\\n  count = count + 1;\\n  console.log( 'Count: ' + count );\\n}, 1000 );\\n```\\n\\nNote the use of \\\"==console.log()==\\\" in this code.  When you run it on the Tessel, you will get ==output to the terminal== - just like you would expect when using Node.js for a server application.  \\n\\n```\\nt2 run index.js\\n```\\n\\nOne more command to get our code over to the Tessel and ==running==.  This is a \\\"==tethered==\\\" mode good for development.  When you are ready to have the Tessel ==run by itself==, you use the \\\"==push==\\\" command.\\n\\n```\\nt2 push index.js\\n```\\n\\n###Watson IoT\\n\\nOkay, remember earlier when I told you how to install an MQTT Node.js module?  Yeah, do that.  ==MQTT is the protocol that Watson IoT speaks== best, and since there are already Node.js modules for MQTT, we can turn to them for connectivity.\\n\\n```\\n// MQTT for Watson IoT\\nvar mqtt = require( 'mqtt' );\\n\\n// Connect to Watson IoT\\nvar client = mqtt.connect(\\n  'mqtt://_ORG_.messaging.internetofthings.ibmcloud.com', {\\n  clientId: 'd:_ORG_:_TYPE_:_ID_',\\n  password: '_TOKEN_',\\n  username: 'use-token-auth'\\n} );\\nvar count = 0;\\nvar interval;\\n\\n// Connected\\nclient.on( 'connect', function() {\\n  console.log( 'Connected.' );\\n\\t\\n  // Start counting\\n  interval = setInterval( function() {\\n    count = count + 1;\\n\\t\\t\\n    // Publish value to Watson IoT\\n    client.publish( \\n      'iot-2/evt/count/fmt/json', \\n      JSON.stringify( {\\n        count: count\\n      } ) \\n    );\\n\\t\\t\\n    console.log( 'Count: ' + count );\\n  }, \\n    // Wait a second\\n    1000 \\n  );\\n} );\\n```\\n\\nFrankly, this would look exactly the same if you were running it on a Node.js server.  There is nothing distinctly Tessel here.  We use the MQTT library, connect to Watson IoT, start counting once we are connected, and publish the count value on each iteration.\\n\\n> While there is no difference from what this would look like on a Node.js server, there is a healthy amount of Watson IoT specifics.  You will need your \\\"organization\\\" ID, the device type and device name you setup in the management console, and the token for that device.\\n\\n###Next Steps\\n\\nThere is a wealth of extras to explore with the Tessel.  ==Blinking the on-board LEDs== is a common \\\"Hello World\\\" for hardware.  To do that you would ==include the Tessel hardware library== in your code.  You do not need to load it via \\\"npm\\\" into your \\\"package.json\\\" as it will ==already be waiting for you on the Tessel==.\\n\\n```\\nvar tessel = require( 'tessel' );\\n```\\n\\nThere are ==two things== that bother me about the Tessel ...  \\n\\nThe ==first== is that it is a pretty ==chunky board==.  Not compared to an Arduino Uno or Raspberry Pi, but certainly compared to an Electric Imp or Particle Photon.  Even the original Tessel, with the module ports hanging off both sides, fit perfectly in an Altoids tin.\\n\\n![Original Tessel in Altoids tin.](http://images.kevinhoyt.com/tessel.altoids.jpg)\\n\\nThe ==other thing== that bothers me is the ==price - at $45.00 USD==, the Tessel is not the cheapest board on the market by a long shot.  Does it have enough feature differentiation being all JavaScript-y from a Raspberry Pi to be worth the extra cash?  I am not sure.  How would that price point work at scale?  There is a *big* difference between $12,000 USD for 10,000 units (Photon P1), and $450,000 USD (Tessel).\\n\\nAs far as development goes however, and the pile of hobbyist projects I have stacking up, the Tessel is a great fit with just about the easiest, most comfortable, workflow out there.\"}]],\"sections\":[[10,0]],\"ghostVersion\":\"3.0\"}","html":"<!--kg-card-begin: markdown--><p>Do you know JavaScript?  Do you like IoT?  Lean towards open standards?  Then the Tessel should be right up your alley!  Tessel is a hardware development platform.  It has both Ethernet and 802.11 b/g/n support (and more), and you develop for it using Node.js (or Rust, Python).  Based on OpenWRT, it is open source, and open governance.</p>\n<h3 id=\"moardetails\">Moar Details</h3>\n<p><img src=\"http://images.kevinhoyt.com/tessel.2.jpg\" alt=\"Tessel 2 courtesy Makezine\" loading=\"lazy\"></p>\n<p>The Tessel team has made interesting hardware choices along the way.  On top of the connectivity I already mentioned, the Tessel includes <mark>two USB ports</mark>, and <mark>two module ports</mark>.  With a <mark>580MHz CPU, 64Mb of RAM, and 32Mb of flash</mark>, this thing was made to deliver on your IoT dreams.</p>\n<p>Wait, what is a module port?</p>\n<p>If you are a JavaScript developer, chances are good that you are not an Electrical Engineer.  To ease that learning curve, Tessel produces a line of &quot;modules&quot; that plug into the Tessel module ports.  These include <mark>climate sensors, relay, accelerometer, and more</mark>.  There are even <mark>USB modules</mark> designed specifically for the Tessel, that let you add <mark>BLE, camera, and more</mark>.</p>\n<p><img src=\"https://s3.amazonaws.com/technicalmachine-assets/product+pics/2014+05+15+production+modules/climate.jpg\" alt=\"Tessel climate module.\" loading=\"lazy\"></p>\n<p>Each of the Tessel-developed modules have been tested, and include a Node.js library for easy control of the hardware.  You run &quot;<mark>npm</mark>&quot; to get the packages, <mark>require</mark> them in your code, and use them just like any other Node.js program.  You can even hook your own sensor up to the modules ports, and control <mark>GPIO, talk SPI and I2C, and more</mark>.</p>\n<blockquote>\n<p>One of my first projects with the Tessel had me wiring up a standard 5mm LED to the pins on a modules port.  From there I controlled the GPIO from Node.js.</p>\n</blockquote>\n<h3 id=\"tesselconfiguration\">Tessel Configuration</h3>\n<p>There is Node.js support for <mark>MQTT</mark>, so does that mean you can just load that module and connect to <mark>Watson IoT</mark>?  <mark>Exactly</mark>!  I covered getting started with Watson IoT in <a href=\"http://www.kevinhoyt.com/2016/04/27/particle-photon-on-watson-iot/\">another post</a>.  In this post I will focus on the code and workflow of developing for the Tessel.</p>\n<p>The <mark>Tessel CLI</mark> is the way you will configure and program the device (at least initially).  If you have Node.js installed on your development machine, you can use &quot;npm&quot; to get the CLI installed.</p>\n<pre><code>npm install -g t2-cli\n</code></pre>\n<p>With the Tessel connected to your machine, and the CLI installed, you can <mark>setup wireless</mark> information on the device.</p>\n<pre><code>t2 wifi -n &lt;network-name&gt; -p &lt;password&gt;\n</code></pre>\n<p>Working on the same LAN as your Tessel, and want to <mark>push code to it wirelessly</mark>?  Or maybe you need to free up a USB port on your development machine?</p>\n<pre><code>t2 provision\n</code></pre>\n<p>It all really is that easy.  Do not forget to <mark>update</mark> the firmware on the Tessel before we get going with Watson IoT.</p>\n<pre><code>t2 update\n</code></pre>\n<h3 id=\"firstproject\">First Project</h3>\n<p>Just like you might use &quot;<mark>npm init</mark>&quot; to initialize a workspace for a Node.js project, you can use the CLI to help you out there with the Tessel.</p>\n<pre><code>t2 init\n</code></pre>\n<p>This creates a basic &quot;<mark>index.js</mark>&quot; file which will act as the <mark>entry point</mark> for your application, as well as a &quot;<mark>package.json</mark>&quot; file.  If you add Node.js modules to your project, do not forget to make sure they get into the &quot;package.json&quot;.  <mark>Installing something like MQTT</mark> would then look like this:</p>\n<pre><code>npm install --save mqtt\n</code></pre>\n<p>Wait!  <mark>That looks just like Node.js!</mark>  Yes.  Yes it does.  That is because to use Node.js modules, you use the normal Node.js development workflow.  Pretty cool, right?</p>\n<h3 id=\"startcounting\">Start Counting</h3>\n<p>To make a counter in Node.js is easy enough - <mark>setInterval()</mark> takes care of that.  And that is exactly what we will use here.</p>\n<pre><code>var count = 0;\nvar interval = setInterval( function() {\n  count = count + 1;\n  console.log( 'Count: ' + count );\n}, 1000 );\n</code></pre>\n<p>Note the use of &quot;<mark>console.log()</mark>&quot; in this code.  When you run it on the Tessel, you will get <mark>output to the terminal</mark> - just like you would expect when using Node.js for a server application.</p>\n<pre><code>t2 run index.js\n</code></pre>\n<p>One more command to get our code over to the Tessel and <mark>running</mark>.  This is a &quot;<mark>tethered</mark>&quot; mode good for development.  When you are ready to have the Tessel <mark>run by itself</mark>, you use the &quot;<mark>push</mark>&quot; command.</p>\n<pre><code>t2 push index.js\n</code></pre>\n<h3 id=\"watsoniot\">Watson IoT</h3>\n<p>Okay, remember earlier when I told you how to install an MQTT Node.js module?  Yeah, do that.  <mark>MQTT is the protocol that Watson IoT speaks</mark> best, and since there are already Node.js modules for MQTT, we can turn to them for connectivity.</p>\n<pre><code>// MQTT for Watson IoT\nvar mqtt = require( 'mqtt' );\n\n// Connect to Watson IoT\nvar client = mqtt.connect(\n  'mqtt://_ORG_.messaging.internetofthings.ibmcloud.com', {\n  clientId: 'd:_ORG_:_TYPE_:_ID_',\n  password: '_TOKEN_',\n  username: 'use-token-auth'\n} );\nvar count = 0;\nvar interval;\n\n// Connected\nclient.on( 'connect', function() {\n  console.log( 'Connected.' );\n\t\n  // Start counting\n  interval = setInterval( function() {\n    count = count + 1;\n\t\t\n    // Publish value to Watson IoT\n    client.publish( \n      'iot-2/evt/count/fmt/json', \n      JSON.stringify( {\n        count: count\n      } ) \n    );\n\t\t\n    console.log( 'Count: ' + count );\n  }, \n    // Wait a second\n    1000 \n  );\n} );\n</code></pre>\n<p>Frankly, this would look exactly the same if you were running it on a Node.js server.  There is nothing distinctly Tessel here.  We use the MQTT library, connect to Watson IoT, start counting once we are connected, and publish the count value on each iteration.</p>\n<blockquote>\n<p>While there is no difference from what this would look like on a Node.js server, there is a healthy amount of Watson IoT specifics.  You will need your &quot;organization&quot; ID, the device type and device name you setup in the management console, and the token for that device.</p>\n</blockquote>\n<h3 id=\"nextsteps\">Next Steps</h3>\n<p>There is a wealth of extras to explore with the Tessel.  <mark>Blinking the on-board LEDs</mark> is a common &quot;Hello World&quot; for hardware.  To do that you would <mark>include the Tessel hardware library</mark> in your code.  You do not need to load it via &quot;npm&quot; into your &quot;package.json&quot; as it will <mark>already be waiting for you on the Tessel</mark>.</p>\n<pre><code>var tessel = require( 'tessel' );\n</code></pre>\n<p>There are <mark>two things</mark> that bother me about the Tessel ...</p>\n<p>The <mark>first</mark> is that it is a pretty <mark>chunky board</mark>.  Not compared to an Arduino Uno or Raspberry Pi, but certainly compared to an Electric Imp or Particle Photon.  Even the original Tessel, with the module ports hanging off both sides, fit perfectly in an Altoids tin.</p>\n<p><img src=\"http://images.kevinhoyt.com/tessel.altoids.jpg\" alt=\"Original Tessel in Altoids tin.\" loading=\"lazy\"></p>\n<p>The <mark>other thing</mark> that bothers me is the <mark>price - at $45.00 USD</mark>, the Tessel is not the cheapest board on the market by a long shot.  Does it have enough feature differentiation being all JavaScript-y from a Raspberry Pi to be worth the extra cash?  I am not sure.  How would that price point work at scale?  There is a <em>big</em> difference between $12,000 USD for 10,000 units (Photon P1), and $450,000 USD (Tessel).</p>\n<p>As far as development goes however, and the pile of hobbyist projects I have stacking up, the Tessel is a great fit with just about the easiest, most comfortable, workflow out there.</p>\n<!--kg-card-end: markdown-->","comment_id":"57","plaintext":"Do you know JavaScript? Do you like IoT? Lean towards open standards? Then the\nTessel should be right up your alley! Tessel is a hardware development platform.\nIt has both Ethernet and 802.11 b/g/n support (and more), and you develop for it\nusing Node.js (or Rust, Python). Based on OpenWRT, it is open source, and open\ngovernance.\n\nMoar Details\n\n\nThe Tessel team has made interesting hardware choices along the way. On top of\nthe connectivity I already mentioned, the Tessel includes two USB ports, and two\nmodule ports. With a 580MHz CPU, 64Mb of RAM, and 32Mb of flash, this thing was\nmade to deliver on your IoT dreams.\n\nWait, what is a module port?\n\nIf you are a JavaScript developer, chances are good that you are not an\nElectrical Engineer. To ease that learning curve, Tessel produces a line of\n\"modules\" that plug into the Tessel module ports. These include climate sensors,\nrelay, accelerometer, and more. There are even USB modules designed specifically\nfor the Tessel, that let you add BLE, camera, and more.\n\n\n\nEach of the Tessel-developed modules have been tested, and include a Node.js\nlibrary for easy control of the hardware. You run \"npm\" to get the packages, \nrequire them in your code, and use them just like any other Node.js program. You\ncan even hook your own sensor up to the modules ports, and control GPIO, talk\nSPI and I2C, and more.\n\n> One of my first projects with the Tessel had me wiring up a standard 5mm LED to\nthe pins on a modules port. From there I controlled the GPIO from Node.js.\n\n\nTessel Configuration\nThere is Node.js support for MQTT, so does that mean you can just load that\nmodule and connect to Watson IoT? Exactly! I covered getting started with Watson\nIoT in another post\n[http://www.kevinhoyt.com/2016/04/27/particle-photon-on-watson-iot/]. In this\npost I will focus on the code and workflow of developing for the Tessel.\n\nThe Tessel CLI is the way you will configure and program the device (at least\ninitially). If you have Node.js installed on your development machine, you can\nuse \"npm\" to get the CLI installed.\n\nnpm install -g t2-cli\n\n\nWith the Tessel connected to your machine, and the CLI installed, you can setup\nwireless information on the device.\n\nt2 wifi -n <network-name> -p <password>\n\n\nWorking on the same LAN as your Tessel, and want to push code to it wirelessly?\nOr maybe you need to free up a USB port on your development machine?\n\nt2 provision\n\n\nIt all really is that easy. Do not forget to update the firmware on the Tessel\nbefore we get going with Watson IoT.\n\nt2 update\n\n\nFirst Project\nJust like you might use \"npm init\" to initialize a workspace for a Node.js\nproject, you can use the CLI to help you out there with the Tessel.\n\nt2 init\n\n\nThis creates a basic \"index.js\" file which will act as the entry point for your\napplication, as well as a \"package.json\" file. If you add Node.js modules to\nyour project, do not forget to make sure they get into the \"package.json\". \nInstalling something like MQTT would then look like this:\n\nnpm install --save mqtt\n\n\nWait! That looks just like Node.js! Yes. Yes it does. That is because to use\nNode.js modules, you use the normal Node.js development workflow. Pretty cool,\nright?\n\nStart Counting\nTo make a counter in Node.js is easy enough - setInterval() takes care of that.\nAnd that is exactly what we will use here.\n\nvar count = 0;\nvar interval = setInterval( function() {\n  count = count + 1;\n  console.log( 'Count: ' + count );\n}, 1000 );\n\n\nNote the use of \"console.log()\" in this code. When you run it on the Tessel, you\nwill get output to the terminal - just like you would expect when using Node.js\nfor a server application.\n\nt2 run index.js\n\n\nOne more command to get our code over to the Tessel and running. This is a \"\ntethered\" mode good for development. When you are ready to have the Tessel run\nby itself, you use the \"push\" command.\n\nt2 push index.js\n\n\nWatson IoT\nOkay, remember earlier when I told you how to install an MQTT Node.js module?\nYeah, do that. MQTT is the protocol that Watson IoT speaks best, and since there\nare already Node.js modules for MQTT, we can turn to them for connectivity.\n\n// MQTT for Watson IoT\nvar mqtt = require( 'mqtt' );\n\n// Connect to Watson IoT\nvar client = mqtt.connect(\n  'mqtt://_ORG_.messaging.internetofthings.ibmcloud.com', {\n  clientId: 'd:_ORG_:_TYPE_:_ID_',\n  password: '_TOKEN_',\n  username: 'use-token-auth'\n} );\nvar count = 0;\nvar interval;\n\n// Connected\nclient.on( 'connect', function() {\n  console.log( 'Connected.' );\n\t\n  // Start counting\n  interval = setInterval( function() {\n    count = count + 1;\n\t\t\n    // Publish value to Watson IoT\n    client.publish( \n      'iot-2/evt/count/fmt/json', \n      JSON.stringify( {\n        count: count\n      } ) \n    );\n\t\t\n    console.log( 'Count: ' + count );\n  }, \n    // Wait a second\n    1000 \n  );\n} );\n\n\nFrankly, this would look exactly the same if you were running it on a Node.js\nserver. There is nothing distinctly Tessel here. We use the MQTT library,\nconnect to Watson IoT, start counting once we are connected, and publish the\ncount value on each iteration.\n\n> While there is no difference from what this would look like on a Node.js server,\nthere is a healthy amount of Watson IoT specifics. You will need your\n\"organization\" ID, the device type and device name you setup in the management\nconsole, and the token for that device.\n\n\nNext Steps\nThere is a wealth of extras to explore with the Tessel. Blinking the on-board\nLEDs is a common \"Hello World\" for hardware. To do that you would include the\nTessel hardware library in your code. You do not need to load it via \"npm\" into\nyour \"package.json\" as it will already be waiting for you on the Tessel.\n\nvar tessel = require( 'tessel' );\n\n\nThere are two things that bother me about the Tessel ...\n\nThe first is that it is a pretty chunky board. Not compared to an Arduino Uno or\nRaspberry Pi, but certainly compared to an Electric Imp or Particle Photon. Even\nthe original Tessel, with the module ports hanging off both sides, fit perfectly\nin an Altoids tin.\n\n\n\nThe other thing that bothers me is the price - at $45.00 USD, the Tessel is not\nthe cheapest board on the market by a long shot. Does it have enough feature\ndifferentiation being all JavaScript-y from a Raspberry Pi to be worth the extra\ncash? I am not sure. How would that price point work at scale? There is a big \ndifference between $12,000 USD for 10,000 units (Photon P1), and $450,000 USD\n(Tessel).\n\nAs far as development goes however, and the pile of hobbyist projects I have\nstacking up, the Tessel is a great fit with just about the easiest, most\ncomfortable, workflow out there.","feature_image":"http://images.kevinhoyt.com/watson.jpg","featured":0,"status":"published","locale":null,"visibility":"public","author_id":"1","created_at":"2016-05-20T22:41:21.000Z","updated_at":"2016-09-01T21:36:25.000Z","published_at":"2016-09-01T21:36:25.000Z","custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"type":"post","email_recipient_filter":"none"},{"id":"5adb8922351ffe0018a57743","uuid":"852f4dcd-85fe-429b-b0c6-e3e480f11af5","title":"Evangelist, Advocate, Community","slug":"evangelist-advocate-community","mobiledoc":"{\"version\":\"0.3.1\",\"markups\":[],\"atoms\":[],\"cards\":[[\"markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\"I have been directly involved in developer relations for almost a decade.  Indirectly, for about another five years on top of that.  When I first started, there was only the term \\\"evangelist\\\".  Over the years, for one reason or another, the term \\\"advocate\\\" has become more mainstream.  And all along there has been some aspect of \\\"community\\\".  In this post, I will set out my views on these roles as I have come to relate them to others.\\n\\n###Hype Cycle\\n\\nIt is difficult to tell when exactly the \\\"==hype cycle==\\\" came into being, but it is most commonly associated with ==Gartner==, and has become a prominent (albeit not without flaws) model in the Information Technology industry.  The hype cycle consists of ==five phases==.\\n\\n![Hype Cycle](http://images.kevinhoyt.com/devrel.playbook.curve.png)\\n\\n**Innovation Trigger**\\n\\nThe first phase is the \\\"innovation trigger\\\".  This is when an ==idea== first manifests itself on the market.  Note the keyword there of \\\"idea\\\".  While there may be no shortage of examples of given technology in action, the business model remains relatively unproven at this point.\\n\\n**Peak of Inflated Expectations**\\n\\nNext up is the \\\"peak of inflated expectations\\\" phase.  Eventually an idea takes hold, somebody, somewhere, runs with it, and before long there are a handful of individuals/companies touting how great the idea can be.\\n\\n**Trough of Disillusionment**\\n\\nNot every idea can be a winner however, and as such comes the \\\"trough of disillusionment\\\" phase.  If supporters of the idea cannot bear actual fruit, the idea will likely ==die off== here ... Or potentially ==slip back== on the overall curve.\\n\\n**Slope of Enlightenment**\\n\\nNow we are getting somewhere.  The idea has \\\"==crystalized==\\\" (as [Wikipedia](https://en.wikipedia.org/wiki/Hype_cycle) puts it), has likely seen a second or third iteration, and companies are starting to understand how to use it to their benefit.  This is the \\\"slope of enlightenment\\\" phase.  The idea has become mainstream, but implementations still lack broad adoption.\\n\\n**Plateau of Productivity**\\n\\nAt the \\\"plateau of productivity\\\" phase, a business model has been established - or at least a definitive ==return on investment==.  At this point you can talk about the idea at pretty much any industry event, and people will at least know of it, if not have specific opinions about it.\\n\\n###Developer Relations\\n\\nWhile the hype cycle (curve, really) can be applied to just about any market, in the context of IT, we are talking about ==technologies in use by developers==.  In that respect, providers of the idea will be looking to get their implementation into the hands of developers.  In my view, developer relations roles ==cover the entire hype cycle==.\\n\\n![The span of developer relations.](http://images.kevinhoyt.com/devrel.playbook.overview.png)\\n\\nWhat differs however is how vendors will want to relate to developers in the ==context== of where their implementation sits on the hype cycle.  Companies driving their idea up the peak of inflated expectations will relate to developers differently from those whose idea has become an industry standard.  This is the foundation of how I break down the roles of evangelist, advocate, and community manager.\\n\\n###Evangelist\\n\\nThe term \\\"evangelist\\\" borrows heavily from a religious context - specifically the evangelistic model found in Christianity.  In the religious context, an evangelist is primarily interested in getting ==as many people as possible to hear their message==.  Ideally, an evangelist creates converts.  Converts then in turn evangelize others, and so on and so forth.\\n\\n![Evangelist at the start of the hype cycle.](http://images.kevinhoyt.com/devrel.playbook.evangelist.png)\\n\\nWithin the context of technology, specifically the innovation trigger and peak of inflated expectations phases, this is exactly the type of role you want on your developer relations team.  You want to ==achieve as much mindshare== for your idea as possible.  \\n\\nYou do not have a community at this point (or not much of one), and the technology simply is not mature enough (or not adopted broadly enough) to shoot for much more.  As such this will be largely a ==one-way conversation==, and you should be okay with that.\\n\\n>Do not mistake creating mindshare through evangelism with sales.  All developer relations staff should be deeply technical, motivated by the idea itself, and not be coin-operated.  I will circle back with more thoughts on this in another post.\\n\\n###Advocate\\n\\nAs your idea progresses up the peak of inflated expectations, and moves into the trough of disillusionment, you are going to need more than a one-way conversation to get through the tough times ahead.  You are going to need staff that can ==interpret feedback from a growing customer base, into actionable terms== for product management - an advocate.  \\n\\n![Getting through the tough times with advocates.](http://images.kevinhoyt.com/devrel.playbook.advocate.png)\\n\\nNote that while there is now a two-way conversation happening, that the primary goal of an advocate should be to ==champion the needs of the developer== (customer), not the technology (company).  An advocate needs something to advocate for - this is the developer, hence \\\"developer advocate\\\".\\n\\nThat being said, product management will likely also have ideas of their own that they wish to vet against developers using, or looking to use, their technology.  As product management has the job of running a product, there is only so much of this work that they can do themselves.  A developer advocate can ==help scale the reach of the product team==.\\n\\nIf the product team has a feature they are considering, an advocate will know just the right customer for them to talk to, and know how to make that connection.  Product teams should not only embrace the function of the advocate, but also relish the feedback and chance to connect with customers.\\n\\nKeep in mind here that at this stage of the hype cycle, an idea is still working towards mainstream adoption.  It may be that the way customers want to use the technology differs from how the company views that technology being used.  This can cause ==friction between advocates and the product team==.  Senior management would do well to remember that the advocate is representing the customer (read: where the money is at).\\n\\n###Evangelist (Again)\\n\\nAs an idea pushes up the plateau of productivity, and becomes mainstream, there is potentially (usually) less work for an advocate to do.  The idea has become mainstream.  ==Everybody is using it.==  Communities have formed.  The business model has largely solidified.  An advocate can provide customer feedback all they want, but the idea has really taken on a life of its own at this point.\\n\\nHere is where I see the evangelist role resurfacing.\\n\\n![Keeping interest alive, and community informed with evangelists (again).](http://images.kevinhoyt.com/devrel.playbook.evangelist.again.png)\\n\\nBorrowing again from Christianity, nothing gets a congregation (those with the religion already) more excited than a visit from the evangelist out in the field.  The flock is excited to hear the latest news about the growth of their religion from the front lines.  First hand.\\n\\nIn technology terms, when it comes to an established product, one that is mainstream, the role of the evangelist is important to ==keep people informed about said technology, and to keep the interest alive==.  Evangelists at this stage of the hype cycle will largely communicate specifically what their company is doing, from a features perspective, within the context of the overall technology market.\\n\\n###Community\\n\\nLast, but certainly not least, is the role of community management.  As it relates to the hype cycle, communities will generally begin to form organically (on their own, without funding from the vendor), roughly around the trough of disillusionment.  This work will continue through the plateau of productivity.\\n\\n![Managing community formation and corporate participation.](http://images.kevinhoyt.com/devrel.playbook.full.png)\\n\\nThis is about the same time as the ==advocate== comes on the scene, and continues through the resurfacing of the ==evangelist==.  Both of these roles however have their work already cut out for them.  Expecting them to additionally ==manage a community== will result in a diminished ability to do either role.\\n\\nPresentations take time to create.  Demonstrations of the latest features, especially on software still under development, can be especially time consuming.  Travel takes time.  As communities grow to the tens of thousands, or even millions, it will simply be ==beyond the scope of the evangelist and/or advocate== to adequately manage the day-to-day interactions.\\n\\nWhen a social media outcry surfaces because of feature/pricing/business changes, it is critical to staff community managers to ==keep the lines of communication open==.\\n\\n###Next Steps\\n\\nThis is a high-level outline of how I view developer relations roles and responsibilities.  I have not said too much about the day-to-day motivation, operation, and management of these different roles at this point.  I intend to cover those topics in future posts.  This is a broad brush stroke designed to ==resolve ambiguity== in these roles, and where they most add value.\\n\\nI will close with a thought to consider further...\\n\\nDeveloper relations is a ==long-term, strategic, investment==.  Make no mistake about the investment required to do it correctly (time and money).  In general however, an operational charter of the evangelist/advocate should be to ==plot where the technology will be in a year, and start seeding those concepts in the present==.  If you are making that investment, then talking about what is already in the market is a day late and a dollar short.\"}]],\"sections\":[[10,0]],\"ghostVersion\":\"3.0\"}","html":"<!--kg-card-begin: markdown--><p>I have been directly involved in developer relations for almost a decade.  Indirectly, for about another five years on top of that.  When I first started, there was only the term &quot;evangelist&quot;.  Over the years, for one reason or another, the term &quot;advocate&quot; has become more mainstream.  And all along there has been some aspect of &quot;community&quot;.  In this post, I will set out my views on these roles as I have come to relate them to others.</p>\n<h3 id=\"hypecycle\">Hype Cycle</h3>\n<p>It is difficult to tell when exactly the &quot;<mark>hype cycle</mark>&quot; came into being, but it is most commonly associated with <mark>Gartner</mark>, and has become a prominent (albeit not without flaws) model in the Information Technology industry.  The hype cycle consists of <mark>five phases</mark>.</p>\n<p><img src=\"http://images.kevinhoyt.com/devrel.playbook.curve.png\" alt=\"Hype Cycle\" loading=\"lazy\"></p>\n<p><strong>Innovation Trigger</strong></p>\n<p>The first phase is the &quot;innovation trigger&quot;.  This is when an <mark>idea</mark> first manifests itself on the market.  Note the keyword there of &quot;idea&quot;.  While there may be no shortage of examples of given technology in action, the business model remains relatively unproven at this point.</p>\n<p><strong>Peak of Inflated Expectations</strong></p>\n<p>Next up is the &quot;peak of inflated expectations&quot; phase.  Eventually an idea takes hold, somebody, somewhere, runs with it, and before long there are a handful of individuals/companies touting how great the idea can be.</p>\n<p><strong>Trough of Disillusionment</strong></p>\n<p>Not every idea can be a winner however, and as such comes the &quot;trough of disillusionment&quot; phase.  If supporters of the idea cannot bear actual fruit, the idea will likely <mark>die off</mark> here ... Or potentially <mark>slip back</mark> on the overall curve.</p>\n<p><strong>Slope of Enlightenment</strong></p>\n<p>Now we are getting somewhere.  The idea has &quot;<mark>crystalized</mark>&quot; (as <a href=\"https://en.wikipedia.org/wiki/Hype_cycle\">Wikipedia</a> puts it), has likely seen a second or third iteration, and companies are starting to understand how to use it to their benefit.  This is the &quot;slope of enlightenment&quot; phase.  The idea has become mainstream, but implementations still lack broad adoption.</p>\n<p><strong>Plateau of Productivity</strong></p>\n<p>At the &quot;plateau of productivity&quot; phase, a business model has been established - or at least a definitive <mark>return on investment</mark>.  At this point you can talk about the idea at pretty much any industry event, and people will at least know of it, if not have specific opinions about it.</p>\n<h3 id=\"developerrelations\">Developer Relations</h3>\n<p>While the hype cycle (curve, really) can be applied to just about any market, in the context of IT, we are talking about <mark>technologies in use by developers</mark>.  In that respect, providers of the idea will be looking to get their implementation into the hands of developers.  In my view, developer relations roles <mark>cover the entire hype cycle</mark>.</p>\n<p><img src=\"http://images.kevinhoyt.com/devrel.playbook.overview.png\" alt=\"The span of developer relations.\" loading=\"lazy\"></p>\n<p>What differs however is how vendors will want to relate to developers in the <mark>context</mark> of where their implementation sits on the hype cycle.  Companies driving their idea up the peak of inflated expectations will relate to developers differently from those whose idea has become an industry standard.  This is the foundation of how I break down the roles of evangelist, advocate, and community manager.</p>\n<h3 id=\"evangelist\">Evangelist</h3>\n<p>The term &quot;evangelist&quot; borrows heavily from a religious context - specifically the evangelistic model found in Christianity.  In the religious context, an evangelist is primarily interested in getting <mark>as many people as possible to hear their message</mark>.  Ideally, an evangelist creates converts.  Converts then in turn evangelize others, and so on and so forth.</p>\n<p><img src=\"http://images.kevinhoyt.com/devrel.playbook.evangelist.png\" alt=\"Evangelist at the start of the hype cycle.\" loading=\"lazy\"></p>\n<p>Within the context of technology, specifically the innovation trigger and peak of inflated expectations phases, this is exactly the type of role you want on your developer relations team.  You want to <mark>achieve as much mindshare</mark> for your idea as possible.</p>\n<p>You do not have a community at this point (or not much of one), and the technology simply is not mature enough (or not adopted broadly enough) to shoot for much more.  As such this will be largely a <mark>one-way conversation</mark>, and you should be okay with that.</p>\n<blockquote>\n<p>Do not mistake creating mindshare through evangelism with sales.  All developer relations staff should be deeply technical, motivated by the idea itself, and not be coin-operated.  I will circle back with more thoughts on this in another post.</p>\n</blockquote>\n<h3 id=\"advocate\">Advocate</h3>\n<p>As your idea progresses up the peak of inflated expectations, and moves into the trough of disillusionment, you are going to need more than a one-way conversation to get through the tough times ahead.  You are going to need staff that can <mark>interpret feedback from a growing customer base, into actionable terms</mark> for product management - an advocate.</p>\n<p><img src=\"http://images.kevinhoyt.com/devrel.playbook.advocate.png\" alt=\"Getting through the tough times with advocates.\" loading=\"lazy\"></p>\n<p>Note that while there is now a two-way conversation happening, that the primary goal of an advocate should be to <mark>champion the needs of the developer</mark> (customer), not the technology (company).  An advocate needs something to advocate for - this is the developer, hence &quot;developer advocate&quot;.</p>\n<p>That being said, product management will likely also have ideas of their own that they wish to vet against developers using, or looking to use, their technology.  As product management has the job of running a product, there is only so much of this work that they can do themselves.  A developer advocate can <mark>help scale the reach of the product team</mark>.</p>\n<p>If the product team has a feature they are considering, an advocate will know just the right customer for them to talk to, and know how to make that connection.  Product teams should not only embrace the function of the advocate, but also relish the feedback and chance to connect with customers.</p>\n<p>Keep in mind here that at this stage of the hype cycle, an idea is still working towards mainstream adoption.  It may be that the way customers want to use the technology differs from how the company views that technology being used.  This can cause <mark>friction between advocates and the product team</mark>.  Senior management would do well to remember that the advocate is representing the customer (read: where the money is at).</p>\n<h3 id=\"evangelistagain\">Evangelist (Again)</h3>\n<p>As an idea pushes up the plateau of productivity, and becomes mainstream, there is potentially (usually) less work for an advocate to do.  The idea has become mainstream.  <mark>Everybody is using it.</mark>  Communities have formed.  The business model has largely solidified.  An advocate can provide customer feedback all they want, but the idea has really taken on a life of its own at this point.</p>\n<p>Here is where I see the evangelist role resurfacing.</p>\n<p><img src=\"http://images.kevinhoyt.com/devrel.playbook.evangelist.again.png\" alt=\"Keeping interest alive, and community informed with evangelists (again).\" loading=\"lazy\"></p>\n<p>Borrowing again from Christianity, nothing gets a congregation (those with the religion already) more excited than a visit from the evangelist out in the field.  The flock is excited to hear the latest news about the growth of their religion from the front lines.  First hand.</p>\n<p>In technology terms, when it comes to an established product, one that is mainstream, the role of the evangelist is important to <mark>keep people informed about said technology, and to keep the interest alive</mark>.  Evangelists at this stage of the hype cycle will largely communicate specifically what their company is doing, from a features perspective, within the context of the overall technology market.</p>\n<h3 id=\"community\">Community</h3>\n<p>Last, but certainly not least, is the role of community management.  As it relates to the hype cycle, communities will generally begin to form organically (on their own, without funding from the vendor), roughly around the trough of disillusionment.  This work will continue through the plateau of productivity.</p>\n<p><img src=\"http://images.kevinhoyt.com/devrel.playbook.full.png\" alt=\"Managing community formation and corporate participation.\" loading=\"lazy\"></p>\n<p>This is about the same time as the <mark>advocate</mark> comes on the scene, and continues through the resurfacing of the <mark>evangelist</mark>.  Both of these roles however have their work already cut out for them.  Expecting them to additionally <mark>manage a community</mark> will result in a diminished ability to do either role.</p>\n<p>Presentations take time to create.  Demonstrations of the latest features, especially on software still under development, can be especially time consuming.  Travel takes time.  As communities grow to the tens of thousands, or even millions, it will simply be <mark>beyond the scope of the evangelist and/or advocate</mark> to adequately manage the day-to-day interactions.</p>\n<p>When a social media outcry surfaces because of feature/pricing/business changes, it is critical to staff community managers to <mark>keep the lines of communication open</mark>.</p>\n<h3 id=\"nextsteps\">Next Steps</h3>\n<p>This is a high-level outline of how I view developer relations roles and responsibilities.  I have not said too much about the day-to-day motivation, operation, and management of these different roles at this point.  I intend to cover those topics in future posts.  This is a broad brush stroke designed to <mark>resolve ambiguity</mark> in these roles, and where they most add value.</p>\n<p>I will close with a thought to consider further...</p>\n<p>Developer relations is a <mark>long-term, strategic, investment</mark>.  Make no mistake about the investment required to do it correctly (time and money).  In general however, an operational charter of the evangelist/advocate should be to <mark>plot where the technology will be in a year, and start seeding those concepts in the present</mark>.  If you are making that investment, then talking about what is already in the market is a day late and a dollar short.</p>\n<!--kg-card-end: markdown-->","comment_id":"58","plaintext":"I have been directly involved in developer relations for almost a decade.\nIndirectly, for about another five years on top of that. When I first started,\nthere was only the term \"evangelist\". Over the years, for one reason or another,\nthe term \"advocate\" has become more mainstream. And all along there has been\nsome aspect of \"community\". In this post, I will set out my views on these roles\nas I have come to relate them to others.\n\nHype Cycle\nIt is difficult to tell when exactly the \"hype cycle\" came into being, but it is\nmost commonly associated with Gartner, and has become a prominent (albeit not\nwithout flaws) model in the Information Technology industry. The hype cycle\nconsists of five phases.\n\n\n\nInnovation Trigger\n\nThe first phase is the \"innovation trigger\". This is when an idea first\nmanifests itself on the market. Note the keyword there of \"idea\". While there\nmay be no shortage of examples of given technology in action, the business model\nremains relatively unproven at this point.\n\nPeak of Inflated Expectations\n\nNext up is the \"peak of inflated expectations\" phase. Eventually an idea takes\nhold, somebody, somewhere, runs with it, and before long there are a handful of\nindividuals/companies touting how great the idea can be.\n\nTrough of Disillusionment\n\nNot every idea can be a winner however, and as such comes the \"trough of\ndisillusionment\" phase. If supporters of the idea cannot bear actual fruit, the\nidea will likely die off here ... Or potentially slip back on the overall curve.\n\nSlope of Enlightenment\n\nNow we are getting somewhere. The idea has \"crystalized\" (as Wikipedia\n[https://en.wikipedia.org/wiki/Hype_cycle] puts it), has likely seen a second or\nthird iteration, and companies are starting to understand how to use it to their\nbenefit. This is the \"slope of enlightenment\" phase. The idea has become\nmainstream, but implementations still lack broad adoption.\n\nPlateau of Productivity\n\nAt the \"plateau of productivity\" phase, a business model has been established -\nor at least a definitive return on investment. At this point you can talk about\nthe idea at pretty much any industry event, and people will at least know of it,\nif not have specific opinions about it.\n\nDeveloper Relations\nWhile the hype cycle (curve, really) can be applied to just about any market, in\nthe context of IT, we are talking about technologies in use by developers. In\nthat respect, providers of the idea will be looking to get their implementation\ninto the hands of developers. In my view, developer relations roles cover the\nentire hype cycle.\n\n\n\nWhat differs however is how vendors will want to relate to developers in the \ncontext of where their implementation sits on the hype cycle. Companies driving\ntheir idea up the peak of inflated expectations will relate to developers\ndifferently from those whose idea has become an industry standard. This is the\nfoundation of how I break down the roles of evangelist, advocate, and community\nmanager.\n\nEvangelist\nThe term \"evangelist\" borrows heavily from a religious context - specifically\nthe evangelistic model found in Christianity. In the religious context, an\nevangelist is primarily interested in getting as many people as possible to hear\ntheir message. Ideally, an evangelist creates converts. Converts then in turn\nevangelize others, and so on and so forth.\n\n\n\nWithin the context of technology, specifically the innovation trigger and peak\nof inflated expectations phases, this is exactly the type of role you want on\nyour developer relations team. You want to achieve as much mindshare for your\nidea as possible.\n\nYou do not have a community at this point (or not much of one), and the\ntechnology simply is not mature enough (or not adopted broadly enough) to shoot\nfor much more. As such this will be largely a one-way conversation, and you\nshould be okay with that.\n\n> Do not mistake creating mindshare through evangelism with sales. All developer\nrelations staff should be deeply technical, motivated by the idea itself, and\nnot be coin-operated. I will circle back with more thoughts on this in another\npost.\n\n\nAdvocate\nAs your idea progresses up the peak of inflated expectations, and moves into the\ntrough of disillusionment, you are going to need more than a one-way\nconversation to get through the tough times ahead. You are going to need staff\nthat can interpret feedback from a growing customer base, into actionable terms \nfor product management - an advocate.\n\n\n\nNote that while there is now a two-way conversation happening, that the primary\ngoal of an advocate should be to champion the needs of the developer (customer),\nnot the technology (company). An advocate needs something to advocate for - this\nis the developer, hence \"developer advocate\".\n\nThat being said, product management will likely also have ideas of their own\nthat they wish to vet against developers using, or looking to use, their\ntechnology. As product management has the job of running a product, there is\nonly so much of this work that they can do themselves. A developer advocate can \nhelp scale the reach of the product team.\n\nIf the product team has a feature they are considering, an advocate will know\njust the right customer for them to talk to, and know how to make that\nconnection. Product teams should not only embrace the function of the advocate,\nbut also relish the feedback and chance to connect with customers.\n\nKeep in mind here that at this stage of the hype cycle, an idea is still working\ntowards mainstream adoption. It may be that the way customers want to use the\ntechnology differs from how the company views that technology being used. This\ncan cause friction between advocates and the product team. Senior management\nwould do well to remember that the advocate is representing the customer (read:\nwhere the money is at).\n\nEvangelist (Again)\nAs an idea pushes up the plateau of productivity, and becomes mainstream, there\nis potentially (usually) less work for an advocate to do. The idea has become\nmainstream. Everybody is using it. Communities have formed. The business model\nhas largely solidified. An advocate can provide customer feedback all they want,\nbut the idea has really taken on a life of its own at this point.\n\nHere is where I see the evangelist role resurfacing.\n\n\n\nBorrowing again from Christianity, nothing gets a congregation (those with the\nreligion already) more excited than a visit from the evangelist out in the\nfield. The flock is excited to hear the latest news about the growth of their\nreligion from the front lines. First hand.\n\nIn technology terms, when it comes to an established product, one that is\nmainstream, the role of the evangelist is important to keep people informed\nabout said technology, and to keep the interest alive. Evangelists at this stage\nof the hype cycle will largely communicate specifically what their company is\ndoing, from a features perspective, within the context of the overall technology\nmarket.\n\nCommunity\nLast, but certainly not least, is the role of community management. As it\nrelates to the hype cycle, communities will generally begin to form organically\n(on their own, without funding from the vendor), roughly around the trough of\ndisillusionment. This work will continue through the plateau of productivity.\n\n\n\nThis is about the same time as the advocate comes on the scene, and continues\nthrough the resurfacing of the evangelist. Both of these roles however have\ntheir work already cut out for them. Expecting them to additionally manage a\ncommunity will result in a diminished ability to do either role.\n\nPresentations take time to create. Demonstrations of the latest features,\nespecially on software still under development, can be especially time\nconsuming. Travel takes time. As communities grow to the tens of thousands, or\neven millions, it will simply be beyond the scope of the evangelist and/or\nadvocate to adequately manage the day-to-day interactions.\n\nWhen a social media outcry surfaces because of feature/pricing/business changes,\nit is critical to staff community managers to keep the lines of communication\nopen.\n\nNext Steps\nThis is a high-level outline of how I view developer relations roles and\nresponsibilities. I have not said too much about the day-to-day motivation,\noperation, and management of these different roles at this point. I intend to\ncover those topics in future posts. This is a broad brush stroke designed to \nresolve ambiguity in these roles, and where they most add value.\n\nI will close with a thought to consider further...\n\nDeveloper relations is a long-term, strategic, investment. Make no mistake about\nthe investment required to do it correctly (time and money). In general however,\nan operational charter of the evangelist/advocate should be to plot where the\ntechnology will be in a year, and start seeding those concepts in the present.\nIf you are making that investment, then talking about what is already in the\nmarket is a day late and a dollar short.","feature_image":"http://images.kevinhoyt.com/webvisions.audience.jpg","featured":0,"status":"published","locale":null,"visibility":"public","author_id":"1","created_at":"2016-06-07T05:36:14.000Z","updated_at":"2016-06-08T15:19:45.000Z","published_at":"2016-06-08T15:19:45.000Z","custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"type":"post","email_recipient_filter":"none"},{"id":"5adb8922351ffe0018a57744","uuid":"aa155c16-38ae-4467-80c1-829acd57f227","title":"Advocating Conversations","slug":"advocating-conversations","mobiledoc":"{\"version\":\"0.3.1\",\"markups\":[],\"atoms\":[],\"cards\":[[\"markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\"Have you ever seen an old time gospel preacher?  Maybe even if just portrayed in a movie?  Big, boisterous, voice.  A tone of conviction that moves the congregation to its feet.  The choir starts signing, and people start dancing in the aisles.  Hallelujah!\\n\\n>You will have to bear with me here as this post is going to carry heavy religious overtones at first.  We will get to the business of developer relations along the way.\\n\\nNow imagine that preacher visiting your house for dinner.  Imagine them carrying the same voice and tone.  A few days later, you pass them in the grocery store, and again, the same boisterous voice and tone of conviction.  \\\"Gotta, gotta, gotta, buy some BANANAS!\\\"  At the PTA meeting, and so on.  Okay, this is starting to get old.\\n\\nIn many ways, the developer advocate is like a preacher.  You can sense the conviction they have about a technology.  They take the stage and give it to you straight, hoping to motivate you to action.  But just as the preacher is not constantly preaching, nor should a developer advocate.\\n\\n###Getting to Know You\\n\\nIn Christianity, preaching is usually a part of a broader role called \\\"pastor\\\".  The mostly unseen work of a pastor is in helping those in the congregation in time of need (or in celebration).  Visiting the ill in hospitals.  Helping people cope with a death in the family.  Welcoming a newborn baby into the world.\\n\\n>While every pastor would love for you to hear their Sunday message and be convicted enough to change your life, it is the countless hours spent in conversation with others that have the biggest impact.  So should it be with developer advocates.\\n\\nChances are that you have friends.  Some of these friends may be really close.  Maybe you have helped one through a hard time in their life.  Others may be more acquaintances that you see only at work.  Regardless of the spectrum, I am willing to bet that you could name your top ten friends.\\n\\nIf you looked closely at those relationships, I would further suggest that those most close and dear to you are the ones you trust the most.  If you gave them advice, they would certainly consider it, if not take action on it.  \\n\\nIf you are in developer relations, do you know the best developers using your technology?  Do you listen to what they say?  Do you take action?  If you give them technology advice, will they take action?  ==Do you know the names of their kids?==\\n\\n###Relationships Build Trust\\n\\n==Wait, wait, wait!  Do not run off just yet!==  I know that comes across as really creepy at first, but hear me out.  It is really only creepy if you are looking at it from a heartless, mechanical, CRM kind of perspective.  When you blur the line between your work as a developer advocate, and being a person with friends, the creepiness starts to fade.\\n\\nWhen I go to a conference, especially ones I have been to before, I get genuinely excited.  I am going to hang out with my friends.  And I am not talking just about \\\"birds of a feather flock together\\\" friends, but people with whom I would go on vacation.  People with whom I would stay after the conference, and go hiking on the weekend.\\n\\nTo be clear, just as we have friends who are really more just acquaintances we see at work, there will be just acquaintances I see at an event.  Sometimes I am even just shaking hands and remembering that a specific developer works for a company I want to get access to.  Sometimes it is just CRM-worthy business.  I always go into an event however with the mentality of making friends.  Relationships build trust, and only then can we move the industry forward.\\n\"}]],\"sections\":[[10,0]],\"ghostVersion\":\"3.0\"}","html":"<!--kg-card-begin: markdown--><p>Have you ever seen an old time gospel preacher?  Maybe even if just portrayed in a movie?  Big, boisterous, voice.  A tone of conviction that moves the congregation to its feet.  The choir starts signing, and people start dancing in the aisles.  Hallelujah!</p>\n<blockquote>\n<p>You will have to bear with me here as this post is going to carry heavy religious overtones at first.  We will get to the business of developer relations along the way.</p>\n</blockquote>\n<p>Now imagine that preacher visiting your house for dinner.  Imagine them carrying the same voice and tone.  A few days later, you pass them in the grocery store, and again, the same boisterous voice and tone of conviction.  &quot;Gotta, gotta, gotta, buy some BANANAS!&quot;  At the PTA meeting, and so on.  Okay, this is starting to get old.</p>\n<p>In many ways, the developer advocate is like a preacher.  You can sense the conviction they have about a technology.  They take the stage and give it to you straight, hoping to motivate you to action.  But just as the preacher is not constantly preaching, nor should a developer advocate.</p>\n<h3 id=\"gettingtoknowyou\">Getting to Know You</h3>\n<p>In Christianity, preaching is usually a part of a broader role called &quot;pastor&quot;.  The mostly unseen work of a pastor is in helping those in the congregation in time of need (or in celebration).  Visiting the ill in hospitals.  Helping people cope with a death in the family.  Welcoming a newborn baby into the world.</p>\n<blockquote>\n<p>While every pastor would love for you to hear their Sunday message and be convicted enough to change your life, it is the countless hours spent in conversation with others that have the biggest impact.  So should it be with developer advocates.</p>\n</blockquote>\n<p>Chances are that you have friends.  Some of these friends may be really close.  Maybe you have helped one through a hard time in their life.  Others may be more acquaintances that you see only at work.  Regardless of the spectrum, I am willing to bet that you could name your top ten friends.</p>\n<p>If you looked closely at those relationships, I would further suggest that those most close and dear to you are the ones you trust the most.  If you gave them advice, they would certainly consider it, if not take action on it.</p>\n<p>If you are in developer relations, do you know the best developers using your technology?  Do you listen to what they say?  Do you take action?  If you give them technology advice, will they take action?  <mark>Do you know the names of their kids?</mark></p>\n<h3 id=\"relationshipsbuildtrust\">Relationships Build Trust</h3>\n<p><mark>Wait, wait, wait!  Do not run off just yet!</mark>  I know that comes across as really creepy at first, but hear me out.  It is really only creepy if you are looking at it from a heartless, mechanical, CRM kind of perspective.  When you blur the line between your work as a developer advocate, and being a person with friends, the creepiness starts to fade.</p>\n<p>When I go to a conference, especially ones I have been to before, I get genuinely excited.  I am going to hang out with my friends.  And I am not talking just about &quot;birds of a feather flock together&quot; friends, but people with whom I would go on vacation.  People with whom I would stay after the conference, and go hiking on the weekend.</p>\n<p>To be clear, just as we have friends who are really more just acquaintances we see at work, there will be just acquaintances I see at an event.  Sometimes I am even just shaking hands and remembering that a specific developer works for a company I want to get access to.  Sometimes it is just CRM-worthy business.  I always go into an event however with the mentality of making friends.  Relationships build trust, and only then can we move the industry forward.</p>\n<!--kg-card-end: markdown-->","comment_id":"59","plaintext":"Have you ever seen an old time gospel preacher? Maybe even if just portrayed in\na movie? Big, boisterous, voice. A tone of conviction that moves the\ncongregation to its feet. The choir starts signing, and people start dancing in\nthe aisles. Hallelujah!\n\n> You will have to bear with me here as this post is going to carry heavy\nreligious overtones at first. We will get to the business of developer relations\nalong the way.\n\n\nNow imagine that preacher visiting your house for dinner. Imagine them carrying\nthe same voice and tone. A few days later, you pass them in the grocery store,\nand again, the same boisterous voice and tone of conviction. \"Gotta, gotta,\ngotta, buy some BANANAS!\" At the PTA meeting, and so on. Okay, this is starting\nto get old.\n\nIn many ways, the developer advocate is like a preacher. You can sense the\nconviction they have about a technology. They take the stage and give it to you\nstraight, hoping to motivate you to action. But just as the preacher is not\nconstantly preaching, nor should a developer advocate.\n\nGetting to Know You\nIn Christianity, preaching is usually a part of a broader role called \"pastor\".\nThe mostly unseen work of a pastor is in helping those in the congregation in\ntime of need (or in celebration). Visiting the ill in hospitals. Helping people\ncope with a death in the family. Welcoming a newborn baby into the world.\n\n> While every pastor would love for you to hear their Sunday message and be\nconvicted enough to change your life, it is the countless hours spent in\nconversation with others that have the biggest impact. So should it be with\ndeveloper advocates.\n\n\nChances are that you have friends. Some of these friends may be really close.\nMaybe you have helped one through a hard time in their life. Others may be more\nacquaintances that you see only at work. Regardless of the spectrum, I am\nwilling to bet that you could name your top ten friends.\n\nIf you looked closely at those relationships, I would further suggest that those\nmost close and dear to you are the ones you trust the most. If you gave them\nadvice, they would certainly consider it, if not take action on it.\n\nIf you are in developer relations, do you know the best developers using your\ntechnology? Do you listen to what they say? Do you take action? If you give them\ntechnology advice, will they take action? Do you know the names of their kids?\n\nRelationships Build Trust\nWait, wait, wait! Do not run off just yet! I know that comes across as really\ncreepy at first, but hear me out. It is really only creepy if you are looking at\nit from a heartless, mechanical, CRM kind of perspective. When you blur the line\nbetween your work as a developer advocate, and being a person with friends, the\ncreepiness starts to fade.\n\nWhen I go to a conference, especially ones I have been to before, I get\ngenuinely excited. I am going to hang out with my friends. And I am not talking\njust about \"birds of a feather flock together\" friends, but people with whom I\nwould go on vacation. People with whom I would stay after the conference, and go\nhiking on the weekend.\n\nTo be clear, just as we have friends who are really more just acquaintances we\nsee at work, there will be just acquaintances I see at an event. Sometimes I am\neven just shaking hands and remembering that a specific developer works for a\ncompany I want to get access to. Sometimes it is just CRM-worthy business. I\nalways go into an event however with the mentality of making friends.\nRelationships build trust, and only then can we move the industry forward.","feature_image":null,"featured":0,"status":"draft","locale":null,"visibility":"public","author_id":"1","created_at":"2016-06-08T15:44:34.000Z","updated_at":"2016-10-01T20:05:29.000Z","published_at":null,"custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"type":"post","email_recipient_filter":"none"},{"id":"5adb8922351ffe0018a57745","uuid":"05968b23-6374-4a1f-8fd9-0522b45cf721","title":"Measuring Developer Relations","slug":"measuring-developer-relations","mobiledoc":"{\"version\":\"0.3.1\",\"markups\":[],\"atoms\":[],\"cards\":[[\"markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\"If you are in a developer relations organization, this question will eventually come up. It has likely already come up when the team was first created. And it is a tricky to answer. What is the question? \\\"How do you measure developer relations?\\\"\\n\\n###Origins\\n\\nMost businesses seek to drive the behavior of their employees by setting certain incentives in place. For example, let us say that you are releasing a new product among your suite of products. How do you get the sales team to sell more of that product? You change the compensation plan - they get paid more for selling more of the new product.\\n\\nPutting incentives in place drives a change in behavior.\\n\\n###Leakage\\n\\nIt is not uncommon for this desire to drive behavior through incentives to leak over into the developer relations organization. Inevitably, somebody will suggest that the developer relations should be generating more tweets from the money spent sending them to \\\"have fun\\\" at conferences.\\n\\nSoon an incentive shows its ugly head. \\\"You need twenty (arbitrarily picked number) tweets per event.\\\" Or \\\"You need to get ten more followers.\\\" You get the idea, and if you have been in this fledgling career path long enough, I am sure you have run across this before.\\n\\nSomebody on the team will eventually respond by creating dummy accounts. Or by getting their relatives to follow them. Or by ... take your pick. Advocates are smart, and given the chance to game the system, just like anybody else, they will game the system.\\n\\nSo how do you measure developer relations?\\n\\n###Say What?\\n\\nI like to answer this question with \\\"You don't.\\\" This usually draws a pronounced reaction. Eyes roll. Jaws drop. I have even seen tempers flare.\\n\\nGood. Now that I have their attention, I can continue.\\n\\nMeasuring developer relations is actually a very good thing. You definitely want to know how many people were at a conference. How many people were in your session. How many social media imprints that session drew. How many social media imprints the conference drew. You want to know what it cost to get there, what it cost to be there, etc. The list goes on and on.\\n\\nYou want to know this because the facts help you make better business decisions in the future. You do not want to know these facts so you can say which advocate is better than the others, or which one will get the raise.\\n\\nFor example, when I ran a developer relations team at Adobe (for seven years), we found that up to a point, a ratio of about one advocate per 150 attendees would yield the best results. So if the conference was 600 people, we would generally send three or four advocates. Some would have sessions. Some would be attending sessions. All of them would be working to build relationships (making heroes).\\n\\nAbove 600, and certainly by 1,000 attendees, the impact of the individual advocate fell dramatically. We were entering the realm where corporate marketing was needed to really emphasize  a brand presence.\\n\\nMeasuring our work helped us to agree on this trend, and plan how to adjust in the future.\\n\\n###Battle vs. War\\n\\nAs an Army veteran, I like to think of this in the context of a battle vs. a war.  If you are trying to raise awareness of your brand, products, ideas, etc. then that is your marketing war. A conference, meetup, open source project, etc. are specific battles.\\n\\nIn that sense, a field commander wants to know if the battle was won or lost. That same commander however does not say \\\"You each need to kill twenty more enemies in this next battle.\\\" That is dark, I know, but the analogy holds true. The commander knows that he sent a certain number of soldiers against a certain number of enemies, and came away with a specific result. He can use that information to maximize his results further along the war.\\n\\nTelling an advocate to get more tweets would be like telling them to kill more enemies.\\n\\n###Revisiting Incentives\\n\\nAt the end of the day however, advocates are corporate employees. There will come a time, usually annually, when raises are due, promotions are considered, etc. This means that there does need to be incentives for individuals. You just want to be sure that the incentives do no compete with the altruistic behavior of the advocate.\\n\\nSo what does that look like?\\n\\nFor my team at Adobe, I very much focused on the individual. There are certain activities which an advocate must do by the very nature of the role. I wanted to setup incentives that drove them to be the best advocates they could be. That went something like the following.\\n\\n**Customer Visit** (once per quarter)\\n\\nAn advocate should be in touch with the customer. This means more than just going to conferences. Visiting our customers, and hearing their challenges and successes, is key to understanding how our product(s) is being received.\\n\\n**Conferences and Groups** (once per month)\\n\\nObviously, a speaking opportunity is great, but it is also important that advocates attend conferences. This gives them an opportunity to explore new content. To see how others present their product. When attending, it was expected that notes would be taken, and insights would be shared across the team.\\n\\n**Blogging and Video** (once per week)\\n\\nBeing a person, an individual, and having opinions is key. Understanding the context of the comments is also important. Developers do not trust corporations, they trust people, so be a person. Be you. See where that takes you.\\n\\nAdditionally, not every development shop moves in cadence with your product releases. Having material available for developers to review, when they come around to needing that information, is important. I like to think of this as the long tail of content, and would often tell my team that they needed to be presenting content (in whatever form) that was a year out from what most shops were currently using.\\n\\n**Individual Contribution** (once per quarter)\\n\\nIt is difficult to build relationships (make heroes) if all you are doing is talking. I wanted my team to be actively involved with helping customers succeed. If the opportunity presented itself for them to spend a week with a customer, helping them test out our product features, then that was a worthy endeavor.\\n\\nIn my experience, this usually starts at a conference. You get that rush of questions after a session for example. If one of those questions pulls you to take action, then you should be motivated to do so. You should also have the budget flexibility to make purchases if needed (an IoT project for example).\\n\\n**Personal Project** (per manager)\\n\\nI would use this as my wildcard. It allowed me to work with the individual advocates directly on their career growth. For example, it is not uncommon for new advocates to struggle with work/life balance. More mature advocates might want to dig deeper into a specific area of study. Or even take some classes. Reserving a personal project for my management discretion helped me keep a balanced team.\\n\\n###Conclusion\\n\\nAll of my incentives were setup to bring out the best in my team. They were setup to motivate behavior inline with what I expected of the advocacy role. Measurements were important as well, but not as a grade, rather as a means to assess if we were being as efficient as possible (maximizing ROI).\\n\\nSo does developer relations need to be measured? Yes! Absolutely. It is important however to not mix measurement with incentives.\"}]],\"sections\":[[10,0]],\"ghostVersion\":\"3.0\"}","html":"<!--kg-card-begin: markdown--><p>If you are in a developer relations organization, this question will eventually come up. It has likely already come up when the team was first created. And it is a tricky to answer. What is the question? &quot;How do you measure developer relations?&quot;</p>\n<h3 id=\"origins\">Origins</h3>\n<p>Most businesses seek to drive the behavior of their employees by setting certain incentives in place. For example, let us say that you are releasing a new product among your suite of products. How do you get the sales team to sell more of that product? You change the compensation plan - they get paid more for selling more of the new product.</p>\n<p>Putting incentives in place drives a change in behavior.</p>\n<h3 id=\"leakage\">Leakage</h3>\n<p>It is not uncommon for this desire to drive behavior through incentives to leak over into the developer relations organization. Inevitably, somebody will suggest that the developer relations should be generating more tweets from the money spent sending them to &quot;have fun&quot; at conferences.</p>\n<p>Soon an incentive shows its ugly head. &quot;You need twenty (arbitrarily picked number) tweets per event.&quot; Or &quot;You need to get ten more followers.&quot; You get the idea, and if you have been in this fledgling career path long enough, I am sure you have run across this before.</p>\n<p>Somebody on the team will eventually respond by creating dummy accounts. Or by getting their relatives to follow them. Or by ... take your pick. Advocates are smart, and given the chance to game the system, just like anybody else, they will game the system.</p>\n<p>So how do you measure developer relations?</p>\n<h3 id=\"saywhat\">Say What?</h3>\n<p>I like to answer this question with &quot;You don't.&quot; This usually draws a pronounced reaction. Eyes roll. Jaws drop. I have even seen tempers flare.</p>\n<p>Good. Now that I have their attention, I can continue.</p>\n<p>Measuring developer relations is actually a very good thing. You definitely want to know how many people were at a conference. How many people were in your session. How many social media imprints that session drew. How many social media imprints the conference drew. You want to know what it cost to get there, what it cost to be there, etc. The list goes on and on.</p>\n<p>You want to know this because the facts help you make better business decisions in the future. You do not want to know these facts so you can say which advocate is better than the others, or which one will get the raise.</p>\n<p>For example, when I ran a developer relations team at Adobe (for seven years), we found that up to a point, a ratio of about one advocate per 150 attendees would yield the best results. So if the conference was 600 people, we would generally send three or four advocates. Some would have sessions. Some would be attending sessions. All of them would be working to build relationships (making heroes).</p>\n<p>Above 600, and certainly by 1,000 attendees, the impact of the individual advocate fell dramatically. We were entering the realm where corporate marketing was needed to really emphasize  a brand presence.</p>\n<p>Measuring our work helped us to agree on this trend, and plan how to adjust in the future.</p>\n<h3 id=\"battlevswar\">Battle vs. War</h3>\n<p>As an Army veteran, I like to think of this in the context of a battle vs. a war.  If you are trying to raise awareness of your brand, products, ideas, etc. then that is your marketing war. A conference, meetup, open source project, etc. are specific battles.</p>\n<p>In that sense, a field commander wants to know if the battle was won or lost. That same commander however does not say &quot;You each need to kill twenty more enemies in this next battle.&quot; That is dark, I know, but the analogy holds true. The commander knows that he sent a certain number of soldiers against a certain number of enemies, and came away with a specific result. He can use that information to maximize his results further along the war.</p>\n<p>Telling an advocate to get more tweets would be like telling them to kill more enemies.</p>\n<h3 id=\"revisitingincentives\">Revisiting Incentives</h3>\n<p>At the end of the day however, advocates are corporate employees. There will come a time, usually annually, when raises are due, promotions are considered, etc. This means that there does need to be incentives for individuals. You just want to be sure that the incentives do no compete with the altruistic behavior of the advocate.</p>\n<p>So what does that look like?</p>\n<p>For my team at Adobe, I very much focused on the individual. There are certain activities which an advocate must do by the very nature of the role. I wanted to setup incentives that drove them to be the best advocates they could be. That went something like the following.</p>\n<p><strong>Customer Visit</strong> (once per quarter)</p>\n<p>An advocate should be in touch with the customer. This means more than just going to conferences. Visiting our customers, and hearing their challenges and successes, is key to understanding how our product(s) is being received.</p>\n<p><strong>Conferences and Groups</strong> (once per month)</p>\n<p>Obviously, a speaking opportunity is great, but it is also important that advocates attend conferences. This gives them an opportunity to explore new content. To see how others present their product. When attending, it was expected that notes would be taken, and insights would be shared across the team.</p>\n<p><strong>Blogging and Video</strong> (once per week)</p>\n<p>Being a person, an individual, and having opinions is key. Understanding the context of the comments is also important. Developers do not trust corporations, they trust people, so be a person. Be you. See where that takes you.</p>\n<p>Additionally, not every development shop moves in cadence with your product releases. Having material available for developers to review, when they come around to needing that information, is important. I like to think of this as the long tail of content, and would often tell my team that they needed to be presenting content (in whatever form) that was a year out from what most shops were currently using.</p>\n<p><strong>Individual Contribution</strong> (once per quarter)</p>\n<p>It is difficult to build relationships (make heroes) if all you are doing is talking. I wanted my team to be actively involved with helping customers succeed. If the opportunity presented itself for them to spend a week with a customer, helping them test out our product features, then that was a worthy endeavor.</p>\n<p>In my experience, this usually starts at a conference. You get that rush of questions after a session for example. If one of those questions pulls you to take action, then you should be motivated to do so. You should also have the budget flexibility to make purchases if needed (an IoT project for example).</p>\n<p><strong>Personal Project</strong> (per manager)</p>\n<p>I would use this as my wildcard. It allowed me to work with the individual advocates directly on their career growth. For example, it is not uncommon for new advocates to struggle with work/life balance. More mature advocates might want to dig deeper into a specific area of study. Or even take some classes. Reserving a personal project for my management discretion helped me keep a balanced team.</p>\n<h3 id=\"conclusion\">Conclusion</h3>\n<p>All of my incentives were setup to bring out the best in my team. They were setup to motivate behavior inline with what I expected of the advocacy role. Measurements were important as well, but not as a grade, rather as a means to assess if we were being as efficient as possible (maximizing ROI).</p>\n<p>So does developer relations need to be measured? Yes! Absolutely. It is important however to not mix measurement with incentives.</p>\n<!--kg-card-end: markdown-->","comment_id":"60","plaintext":"If you are in a developer relations organization, this question will eventually\ncome up. It has likely already come up when the team was first created. And it\nis a tricky to answer. What is the question? \"How do you measure developer\nrelations?\"\n\nOrigins\nMost businesses seek to drive the behavior of their employees by setting certain\nincentives in place. For example, let us say that you are releasing a new\nproduct among your suite of products. How do you get the sales team to sell more\nof that product? You change the compensation plan - they get paid more for\nselling more of the new product.\n\nPutting incentives in place drives a change in behavior.\n\nLeakage\nIt is not uncommon for this desire to drive behavior through incentives to leak\nover into the developer relations organization. Inevitably, somebody will\nsuggest that the developer relations should be generating more tweets from the\nmoney spent sending them to \"have fun\" at conferences.\n\nSoon an incentive shows its ugly head. \"You need twenty (arbitrarily picked\nnumber) tweets per event.\" Or \"You need to get ten more followers.\" You get the\nidea, and if you have been in this fledgling career path long enough, I am sure\nyou have run across this before.\n\nSomebody on the team will eventually respond by creating dummy accounts. Or by\ngetting their relatives to follow them. Or by ... take your pick. Advocates are\nsmart, and given the chance to game the system, just like anybody else, they\nwill game the system.\n\nSo how do you measure developer relations?\n\nSay What?\nI like to answer this question with \"You don't.\" This usually draws a pronounced\nreaction. Eyes roll. Jaws drop. I have even seen tempers flare.\n\nGood. Now that I have their attention, I can continue.\n\nMeasuring developer relations is actually a very good thing. You definitely want\nto know how many people were at a conference. How many people were in your\nsession. How many social media imprints that session drew. How many social media\nimprints the conference drew. You want to know what it cost to get there, what\nit cost to be there, etc. The list goes on and on.\n\nYou want to know this because the facts help you make better business decisions\nin the future. You do not want to know these facts so you can say which advocate\nis better than the others, or which one will get the raise.\n\nFor example, when I ran a developer relations team at Adobe (for seven years),\nwe found that up to a point, a ratio of about one advocate per 150 attendees\nwould yield the best results. So if the conference was 600 people, we would\ngenerally send three or four advocates. Some would have sessions. Some would be\nattending sessions. All of them would be working to build relationships (making\nheroes).\n\nAbove 600, and certainly by 1,000 attendees, the impact of the individual\nadvocate fell dramatically. We were entering the realm where corporate marketing\nwas needed to really emphasize a brand presence.\n\nMeasuring our work helped us to agree on this trend, and plan how to adjust in\nthe future.\n\nBattle vs. War\nAs an Army veteran, I like to think of this in the context of a battle vs. a\nwar. If you are trying to raise awareness of your brand, products, ideas, etc.\nthen that is your marketing war. A conference, meetup, open source project, etc.\nare specific battles.\n\nIn that sense, a field commander wants to know if the battle was won or lost.\nThat same commander however does not say \"You each need to kill twenty more\nenemies in this next battle.\" That is dark, I know, but the analogy holds true.\nThe commander knows that he sent a certain number of soldiers against a certain\nnumber of enemies, and came away with a specific result. He can use that\ninformation to maximize his results further along the war.\n\nTelling an advocate to get more tweets would be like telling them to kill more\nenemies.\n\nRevisiting Incentives\nAt the end of the day however, advocates are corporate employees. There will\ncome a time, usually annually, when raises are due, promotions are considered,\netc. This means that there does need to be incentives for individuals. You just\nwant to be sure that the incentives do no compete with the altruistic behavior\nof the advocate.\n\nSo what does that look like?\n\nFor my team at Adobe, I very much focused on the individual. There are certain\nactivities which an advocate must do by the very nature of the role. I wanted to\nsetup incentives that drove them to be the best advocates they could be. That\nwent something like the following.\n\nCustomer Visit (once per quarter)\n\nAn advocate should be in touch with the customer. This means more than just\ngoing to conferences. Visiting our customers, and hearing their challenges and\nsuccesses, is key to understanding how our product(s) is being received.\n\nConferences and Groups (once per month)\n\nObviously, a speaking opportunity is great, but it is also important that\nadvocates attend conferences. This gives them an opportunity to explore new\ncontent. To see how others present their product. When attending, it was\nexpected that notes would be taken, and insights would be shared across the\nteam.\n\nBlogging and Video (once per week)\n\nBeing a person, an individual, and having opinions is key. Understanding the\ncontext of the comments is also important. Developers do not trust corporations,\nthey trust people, so be a person. Be you. See where that takes you.\n\nAdditionally, not every development shop moves in cadence with your product\nreleases. Having material available for developers to review, when they come\naround to needing that information, is important. I like to think of this as the\nlong tail of content, and would often tell my team that they needed to be\npresenting content (in whatever form) that was a year out from what most shops\nwere currently using.\n\nIndividual Contribution (once per quarter)\n\nIt is difficult to build relationships (make heroes) if all you are doing is\ntalking. I wanted my team to be actively involved with helping customers\nsucceed. If the opportunity presented itself for them to spend a week with a\ncustomer, helping them test out our product features, then that was a worthy\nendeavor.\n\nIn my experience, this usually starts at a conference. You get that rush of\nquestions after a session for example. If one of those questions pulls you to\ntake action, then you should be motivated to do so. You should also have the\nbudget flexibility to make purchases if needed (an IoT project for example).\n\nPersonal Project (per manager)\n\nI would use this as my wildcard. It allowed me to work with the individual\nadvocates directly on their career growth. For example, it is not uncommon for\nnew advocates to struggle with work/life balance. More mature advocates might\nwant to dig deeper into a specific area of study. Or even take some classes.\nReserving a personal project for my management discretion helped me keep a\nbalanced team.\n\nConclusion\nAll of my incentives were setup to bring out the best in my team. They were\nsetup to motivate behavior inline with what I expected of the advocacy role.\nMeasurements were important as well, but not as a grade, rather as a means to\nassess if we were being as efficient as possible (maximizing ROI).\n\nSo does developer relations need to be measured? Yes! Absolutely. It is\nimportant however to not mix measurement with incentives.","feature_image":"http://images.kevinhoyt.com/webvisions.audience.jpg","featured":0,"status":"published","locale":null,"visibility":"public","author_id":"1","created_at":"2016-08-26T20:38:38.000Z","updated_at":"2016-08-26T21:44:00.000Z","published_at":"2016-08-26T21:44:00.000Z","custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"type":"post","email_recipient_filter":"none"},{"id":"5adb8922351ffe0018a57746","uuid":"f1c5fc19-f4ab-4fa0-99b8-53701464fa8d","title":"Custom Tessel Module","slug":"custom-tessel-module","mobiledoc":"{\"version\":\"0.3.1\",\"markups\":[],\"atoms\":[],\"cards\":[[\"markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\"Not too long ago now, I presented at Windy City Things 2016 in Chicago. Among the IoT boards I showed there was the Tessel. I have written about the [Tessel](http://www.kevinhoyt.com/2014/11/19/tessel-to-parse-com/) [before](http://www.kevinhoyt.com/2016/09/01/tessel-on-watson-iot/), and how approachable it is for a JavaScript developer. Do not let the approachability fool you though, there are still plenty of ways to get your hands dirty with electrical engineering.\\n\\n###Ports and Pins\\n\\nThe Tessel has ==two module ports== - labeled A and B. These are generally intended to be used by the module add-ons that Tessel sells. Nothing says that you have to use those module port pins for modules. They are, after all, just pins - ==like pins you would find on the Arduino or Raspberry Pi==.\\n\\nIn the case of [Windy City Things](http://windycitythings.com), I had a group of components I wanted to use in my demonstration. One was a ==button==, to trigger an event and show digital input. Another was an ==LED==, for an example of digital output. And the last was a ==photocell== for an example of analog input.\\n\\nI did not want to have to run all those wires and manage them while I was on stage presenting, so I decided to make a module of my own.\\n\\n==Each port has ten pins, which includes ground and power (3V3).== The remainder of the pins can be used for digital, analog, I2C, etc. Together with a little perfboard and 90-degree header pins, I was able to mount all the components in a compact space. One that did not mean I would have to worry about loose wires.\\n\\n![Custom Tessel Module](http://images.kevinhoyt.com/tessel.custom.module.jpg)\\n\\n###LED Control\\n\\nTo control the LED, I connected the Tessel to ==Watson IoT== (MQTT). This allowed me to subscribe for commands at the Tessel from any device or interface that supported MQTT. For this demonstration, that meant the Web. When a panel was clicked in the user interface, ==a command would be published==. The Tessel would then in turn pick that up, read the command, and take action with the LED (setting digital high or low).\\n\\n    // LED on pin 1\\n    // Treat as digital output\\n    // Turn off initially\\n    led = tessel.port.B.pin[1];\\n    led.output( 0 );\\n\\n    // ...\\n\\n    // Handle messages\\n    // LED command\\n    client.on( 'message', function( topic, message, packet ) {\\n      var data = null;\\n\\t\\n      // Object from JSON\\n      data = JSON.parse( message );\\n\\t\\n      // Set LED state\\n      led.output( parseInt( data.value ) );\\n\\t\\n      // Debug\\n      console.log( 'Message: ' + parseInt( data.value ) );\\n    } );\\n\\n###Photocell Reporting\\n\\nThe photocell then is effectively the ==inverse==. At the Tessel, I would sample the photocell analog value, and store that for reference. Then in at a separate interval, the Tessel would publish that value to Watson IoT.\\n\\n    // Photocell on pin 3\\n    // Treat as analog input\\n    photocell = tessel.port.B.pin[3];\\n\\n    bright = setInterval( function() {\\n      photocell.analogRead( function( err, value ) {\\n        light = value;\\n      } );\\n    }, 100 );\\n\\nI like to ==keep the sensor interaction as isolated== as possible in my designs. This means I do not have to worry about blocking, or race conditions, etc.\\n\\n    // Send light value to clients\\n    // Decoupled from reading\\t\\n    interval = setInterval( function() {\\n      client.publish( 'iot-2/evt/light/fmt/json', JSON.stringify( {\\n        light: map( light, 0, 3.3, 0, 100 )\\n      } ) );\\n\\n      console.log( 'Light: ' + light );\\n    }, 1000 );\\n\\n###Button Trigger\\n\\nI probably went overkill for my button implementation and accounted for ==debouncing==. A \\\"debounce\\\" is a situation where a component causes multiple signals when it opens or closes - effectively reporting twice. To solve this you ignore any additional signals for a period of time (microseconds) after you get the first change in signal.\\n\\nI do not know if it is really necessary for the Tessel, but I put it in there anyways.\\n\\n    // Work with button on pin 5\\n    button = tessel.port.B.pin[5];\\n    pressed = false;\\n\\n    // Monitor button pin\\n    bounce = setInterval( function() {\\n      // Asynchromouns button reading\\n      // Digital signal (on or off)\\n      button.read( function( err, value ) {\\n        // Up (off)\\n        if( value === 0 ) {\\n          pressed = false;\\n        // Down (on)\\n        } else if( value === 1 ) {\\n          // Only fire press once\\n          if( !pressed ) {\\n            pressed = true;\\n\\t\\t\\t\\t\\n            // Publish value to Watson IoT\\n            client.publish( 'iot-2/evt/button/fmt/json', JSON.stringify( {\\n              pressed: pressed\\n            } ) );\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\n            console.log( 'Pressed.' );\\n          }\\n        }\\n      } );\\t\\n    }, 100 );\\n\\nWhen the button is pressed, the Tessel publishes a message to Watson IoT. The web page picks this up and can reflect the button press by changing the user interface. In my case, I showed a white flash. With ==five different devices going at once==, this proved to be pretty fun to watch.\\n\\n###Next Steps\\n\\nCreating a ==custom Tessel module is not that hard== if you know your way around a breadboard. If you can wire up a circuit in a breadboard, then you can build your own custom Tessel module. And I have really only started getting into what else can be done. In a future post I will talk about using I2C.\\n\\nBy the way, if you want to watch the video of my presentation at ==Windy City Things==, it is [online](https://windycitythings.com/videos/2016/#kevin-hoyt). It is a 35-minute walkthrough of five different IoT boards and some of the details about how each of them works. The code for all of it is on [GitHub](https://github.com/krhoyt/WindyCityThings2016).\"}]],\"sections\":[[10,0]],\"ghostVersion\":\"3.0\"}","html":"<!--kg-card-begin: markdown--><p>Not too long ago now, I presented at Windy City Things 2016 in Chicago. Among the IoT boards I showed there was the Tessel. I have written about the <a href=\"http://www.kevinhoyt.com/2014/11/19/tessel-to-parse-com/\">Tessel</a> <a href=\"http://www.kevinhoyt.com/2016/09/01/tessel-on-watson-iot/\">before</a>, and how approachable it is for a JavaScript developer. Do not let the approachability fool you though, there are still plenty of ways to get your hands dirty with electrical engineering.</p>\n<h3 id=\"portsandpins\">Ports and Pins</h3>\n<p>The Tessel has <mark>two module ports</mark> - labeled A and B. These are generally intended to be used by the module add-ons that Tessel sells. Nothing says that you have to use those module port pins for modules. They are, after all, just pins - <mark>like pins you would find on the Arduino or Raspberry Pi</mark>.</p>\n<p>In the case of <a href=\"http://windycitythings.com\">Windy City Things</a>, I had a group of components I wanted to use in my demonstration. One was a <mark>button</mark>, to trigger an event and show digital input. Another was an <mark>LED</mark>, for an example of digital output. And the last was a <mark>photocell</mark> for an example of analog input.</p>\n<p>I did not want to have to run all those wires and manage them while I was on stage presenting, so I decided to make a module of my own.</p>\n<p><mark>Each port has ten pins, which includes ground and power (3V3).</mark> The remainder of the pins can be used for digital, analog, I2C, etc. Together with a little perfboard and 90-degree header pins, I was able to mount all the components in a compact space. One that did not mean I would have to worry about loose wires.</p>\n<p><img src=\"http://images.kevinhoyt.com/tessel.custom.module.jpg\" alt=\"Custom Tessel Module\" loading=\"lazy\"></p>\n<h3 id=\"ledcontrol\">LED Control</h3>\n<p>To control the LED, I connected the Tessel to <mark>Watson IoT</mark> (MQTT). This allowed me to subscribe for commands at the Tessel from any device or interface that supported MQTT. For this demonstration, that meant the Web. When a panel was clicked in the user interface, <mark>a command would be published</mark>. The Tessel would then in turn pick that up, read the command, and take action with the LED (setting digital high or low).</p>\n<pre><code>// LED on pin 1\n// Treat as digital output\n// Turn off initially\nled = tessel.port.B.pin[1];\nled.output( 0 );\n\n// ...\n\n// Handle messages\n// LED command\nclient.on( 'message', function( topic, message, packet ) {\n  var data = null;\n\n  // Object from JSON\n  data = JSON.parse( message );\n\n  // Set LED state\n  led.output( parseInt( data.value ) );\n\n  // Debug\n  console.log( 'Message: ' + parseInt( data.value ) );\n} );\n</code></pre>\n<h3 id=\"photocellreporting\">Photocell Reporting</h3>\n<p>The photocell then is effectively the <mark>inverse</mark>. At the Tessel, I would sample the photocell analog value, and store that for reference. Then in at a separate interval, the Tessel would publish that value to Watson IoT.</p>\n<pre><code>// Photocell on pin 3\n// Treat as analog input\nphotocell = tessel.port.B.pin[3];\n\nbright = setInterval( function() {\n  photocell.analogRead( function( err, value ) {\n    light = value;\n  } );\n}, 100 );\n</code></pre>\n<p>I like to <mark>keep the sensor interaction as isolated</mark> as possible in my designs. This means I do not have to worry about blocking, or race conditions, etc.</p>\n<pre><code>// Send light value to clients\n// Decoupled from reading\t\ninterval = setInterval( function() {\n  client.publish( 'iot-2/evt/light/fmt/json', JSON.stringify( {\n    light: map( light, 0, 3.3, 0, 100 )\n  } ) );\n\n  console.log( 'Light: ' + light );\n}, 1000 );\n</code></pre>\n<h3 id=\"buttontrigger\">Button Trigger</h3>\n<p>I probably went overkill for my button implementation and accounted for <mark>debouncing</mark>. A &quot;debounce&quot; is a situation where a component causes multiple signals when it opens or closes - effectively reporting twice. To solve this you ignore any additional signals for a period of time (microseconds) after you get the first change in signal.</p>\n<p>I do not know if it is really necessary for the Tessel, but I put it in there anyways.</p>\n<pre><code>// Work with button on pin 5\nbutton = tessel.port.B.pin[5];\npressed = false;\n\n// Monitor button pin\nbounce = setInterval( function() {\n  // Asynchromouns button reading\n  // Digital signal (on or off)\n  button.read( function( err, value ) {\n    // Up (off)\n    if( value === 0 ) {\n      pressed = false;\n    // Down (on)\n    } else if( value === 1 ) {\n      // Only fire press once\n      if( !pressed ) {\n        pressed = true;\n\t\t\t\n        // Publish value to Watson IoT\n        client.publish( 'iot-2/evt/button/fmt/json', JSON.stringify( {\n          pressed: pressed\n        } ) );\t\t\t\t\t\n\t\t\t\n        console.log( 'Pressed.' );\n      }\n    }\n  } );\t\n}, 100 );\n</code></pre>\n<p>When the button is pressed, the Tessel publishes a message to Watson IoT. The web page picks this up and can reflect the button press by changing the user interface. In my case, I showed a white flash. With <mark>five different devices going at once</mark>, this proved to be pretty fun to watch.</p>\n<h3 id=\"nextsteps\">Next Steps</h3>\n<p>Creating a <mark>custom Tessel module is not that hard</mark> if you know your way around a breadboard. If you can wire up a circuit in a breadboard, then you can build your own custom Tessel module. And I have really only started getting into what else can be done. In a future post I will talk about using I2C.</p>\n<p>By the way, if you want to watch the video of my presentation at <mark>Windy City Things</mark>, it is <a href=\"https://windycitythings.com/videos/2016/#kevin-hoyt\">online</a>. It is a 35-minute walkthrough of five different IoT boards and some of the details about how each of them works. The code for all of it is on <a href=\"https://github.com/krhoyt/WindyCityThings2016\">GitHub</a>.</p>\n<!--kg-card-end: markdown-->","comment_id":"61","plaintext":"Not too long ago now, I presented at Windy City Things 2016 in Chicago. Among\nthe IoT boards I showed there was the Tessel. I have written about the Tessel\n[http://www.kevinhoyt.com/2014/11/19/tessel-to-parse-com/] before\n[http://www.kevinhoyt.com/2016/09/01/tessel-on-watson-iot/], and how\napproachable it is for a JavaScript developer. Do not let the approachability\nfool you though, there are still plenty of ways to get your hands dirty with\nelectrical engineering.\n\nPorts and Pins\nThe Tessel has two module ports - labeled A and B. These are generally intended\nto be used by the module add-ons that Tessel sells. Nothing says that you have\nto use those module port pins for modules. They are, after all, just pins - like\npins you would find on the Arduino or Raspberry Pi.\n\nIn the case of Windy City Things [http://windycitythings.com], I had a group of\ncomponents I wanted to use in my demonstration. One was a button, to trigger an\nevent and show digital input. Another was an LED, for an example of digital\noutput. And the last was a photocell for an example of analog input.\n\nI did not want to have to run all those wires and manage them while I was on\nstage presenting, so I decided to make a module of my own.\n\nEach port has ten pins, which includes ground and power (3V3). The remainder of\nthe pins can be used for digital, analog, I2C, etc. Together with a little\nperfboard and 90-degree header pins, I was able to mount all the components in a\ncompact space. One that did not mean I would have to worry about loose wires.\n\n\n\nLED Control\nTo control the LED, I connected the Tessel to Watson IoT (MQTT). This allowed me\nto subscribe for commands at the Tessel from any device or interface that\nsupported MQTT. For this demonstration, that meant the Web. When a panel was\nclicked in the user interface, a command would be published. The Tessel would\nthen in turn pick that up, read the command, and take action with the LED\n(setting digital high or low).\n\n// LED on pin 1\n// Treat as digital output\n// Turn off initially\nled = tessel.port.B.pin[1];\nled.output( 0 );\n\n// ...\n\n// Handle messages\n// LED command\nclient.on( 'message', function( topic, message, packet ) {\n  var data = null;\n\n  // Object from JSON\n  data = JSON.parse( message );\n\n  // Set LED state\n  led.output( parseInt( data.value ) );\n\n  // Debug\n  console.log( 'Message: ' + parseInt( data.value ) );\n} );\n\n\nPhotocell Reporting\nThe photocell then is effectively the inverse. At the Tessel, I would sample the\nphotocell analog value, and store that for reference. Then in at a separate\ninterval, the Tessel would publish that value to Watson IoT.\n\n// Photocell on pin 3\n// Treat as analog input\nphotocell = tessel.port.B.pin[3];\n\nbright = setInterval( function() {\n  photocell.analogRead( function( err, value ) {\n    light = value;\n  } );\n}, 100 );\n\n\nI like to keep the sensor interaction as isolated as possible in my designs.\nThis means I do not have to worry about blocking, or race conditions, etc.\n\n// Send light value to clients\n// Decoupled from reading\t\ninterval = setInterval( function() {\n  client.publish( 'iot-2/evt/light/fmt/json', JSON.stringify( {\n    light: map( light, 0, 3.3, 0, 100 )\n  } ) );\n\n  console.log( 'Light: ' + light );\n}, 1000 );\n\n\nButton Trigger\nI probably went overkill for my button implementation and accounted for \ndebouncing. A \"debounce\" is a situation where a component causes multiple\nsignals when it opens or closes - effectively reporting twice. To solve this you\nignore any additional signals for a period of time (microseconds) after you get\nthe first change in signal.\n\nI do not know if it is really necessary for the Tessel, but I put it in there\nanyways.\n\n// Work with button on pin 5\nbutton = tessel.port.B.pin[5];\npressed = false;\n\n// Monitor button pin\nbounce = setInterval( function() {\n  // Asynchromouns button reading\n  // Digital signal (on or off)\n  button.read( function( err, value ) {\n    // Up (off)\n    if( value === 0 ) {\n      pressed = false;\n    // Down (on)\n    } else if( value === 1 ) {\n      // Only fire press once\n      if( !pressed ) {\n        pressed = true;\n\t\t\t\n        // Publish value to Watson IoT\n        client.publish( 'iot-2/evt/button/fmt/json', JSON.stringify( {\n          pressed: pressed\n        } ) );\t\t\t\t\t\n\t\t\t\n        console.log( 'Pressed.' );\n      }\n    }\n  } );\t\n}, 100 );\n\n\nWhen the button is pressed, the Tessel publishes a message to Watson IoT. The\nweb page picks this up and can reflect the button press by changing the user\ninterface. In my case, I showed a white flash. With five different devices going\nat once, this proved to be pretty fun to watch.\n\nNext Steps\nCreating a custom Tessel module is not that hard if you know your way around a\nbreadboard. If you can wire up a circuit in a breadboard, then you can build\nyour own custom Tessel module. And I have really only started getting into what\nelse can be done. In a future post I will talk about using I2C.\n\nBy the way, if you want to watch the video of my presentation at Windy City\nThings, it is online [https://windycitythings.com/videos/2016/#kevin-hoyt]. It\nis a 35-minute walkthrough of five different IoT boards and some of the details\nabout how each of them works. The code for all of it is on GitHub\n[https://github.com/krhoyt/WindyCityThings2016].","feature_image":"http://images.kevinhoyt.com/ceramic.tessellation.jpg","featured":0,"status":"published","locale":null,"visibility":"public","author_id":"1","created_at":"2016-09-01T21:37:35.000Z","updated_at":"2016-09-07T21:35:41.000Z","published_at":"2016-09-07T21:35:41.000Z","custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"type":"post","email_recipient_filter":"none"},{"id":"5adb8922351ffe0018a57747","uuid":"da2b16a9-b3cf-4567-a964-d2727b166e67","title":"Tessel I2C Climate","slug":"tessel-i2c-climate","mobiledoc":"{\"version\":\"0.3.1\",\"markups\":[],\"atoms\":[],\"cards\":[[\"markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\"The Tessel is expandable in several ways. One of those ways is pre-built modules. And one of those modules is called the \\\"climate\\\" module. This is effectively the Silicon Labs ==Si7020== temperature and humidity sensor. And for whatever reason, on the Tessel, it is ==horribly inaccurate==.\\n\\n###Heat\\n\\nThe general explanation found around the community is that the ==proximity to the processors== on the Tessel generates enough heat to throw off the temperature of the climate module (Si7020). Some folks have had slightly better results by wiring the module to be several inches away from the Tessel. \\n\\n![Tessel Climate Module wired to be away from the Tessel itself.](http://images.kevinhoyt.com/tessel.climate.wires.jpg)\\n\\nNow I will admit that there is always heat from processors to be considered, but we are talking an error in the range of ==ten degrees fahrenheit==. I have even run a small fan over the Tessel to try and balance out the temperature reading, with little reproducible success.\\n\\nAt some point you start suspecting the implementation itself. Wether there is a component on the PCB that should be at a different value, or perhaps the parsing of data from the sensor is wrong. To put this to the test, I used one of my favorite climate sensors - ==HIH6130== - on the Tessel. I even placed right next to the Tessel, the results were every bit as accurate as I had come to expect - within a single degree of the surrounding area (tested via IR).\\n\\n![HIH6130 from SparkFun](https://cdn.sparkfun.com//assets/parts/6/9/6/9/11295-01.jpg)\\n\\n###I2C\\n\\nThis post is not all rant though. I actually thoroughly enjoyed the process of testing everything. What is more, is that I learned how to implement a ==custom I2C sensor== on the Tessel! \\n\\n==I2C is a communication protocol== for microcontrollers. Each controller on the bus can be individually addressed, ==saving you valuable pins==. A series of bits will be sent on the bus, to a specific sensor, and will generally be interpreted as a command.  The sensor will then perform some operation, and respond with a series of bits.\\n\\nWhile the ==communication== between controller and device ==is standardized==, the ==formats== of the commands and messages ==are not==. You will often find vendors that claim I2C, only to be ever so slightly different. The good news is that the HIH6130 is I2C, so we just need to know how to send a command, and parse the response.\\n\\n###Command\\n\\nI2C implementations will need ==four pins==. There is the power and ground connections, and then a data line and a clock line. Since we are working in at very low levels, and extremely fast, the clock line keeps everything in sync. The commands and messages flow on the data line.\\n\\nThe Tessel has the data (SDA) and clock (SCL) lines clearly marked on the breadboard.  With everything connected, we are looking at (BOB to Tessel): \\n\\n- GND to GND\\n- VDD to 3V3\\n- SCL to SCL 0\\n- SDA to SDA 1\\n\\nIn our JavaScript code for the Tessel, we first need to get a ==reference to the I2C system==. The default address of the HIH6130 (per the documentation) is 0x27, and the sensor is wired up on Port B of the Tessel.\\n\\n    // Sensor\\n    // Default address\\n    hih = tessel.port['B'].I2C( 0x27 );\\n\\nThe Tessel takes care of the low-level I2C implementation, but the format of the data is up to us. This means getting really comfortable with several areas. The first is the documentation for the sensor you are using. The next is understanding bits and bytes (such as LSB/MSB, endianness). Finally, you have to shift those bits around in JavaScript to actually get some data.\\n\\nWhen it comes to the HIH6130, there are several commands we can send to the sensor. [Peter Anderson](http://www.phanderson.com/arduino/I2CCommunications.pdf) has the data sheet on his web site, which is the resource I found most useful in this experiment. The command to read the temperature and humidity breaks down something like this:\\n\\n- Two bytes (eight bits)\\n- Address of 0x27 (seven bits, 0010 0111)\\n- Read command of 1 in last bit\\n- 1010 1111 becomes 0x4F in hex\\n\\nTo issue the command to read the climate values then, we want to send 0x4F using a ==Buffer object==, which will keep our bits intact. We also tell it how many bytes we are expecting in return, and supply a function to parse that data. \\n\\n    hih.transfer( \\n      new Buffer( 0x4F ), \\n      4, \\n      function( error, data ) { ... } \\n    );\\n\\nThat all sounds great, and technically impressive. The reality of the matter is that it did not seem to matter what I sent the sensor, it always replied with the climate readings. The difference is lost in the ether between the Tessel JavaScript commands, and how the Tessel actually maps that to I2C.\\n\\n###Response\\n\\nIf you thought the value we are sending over to the HIH6130 as a command was black magic, it does not get much better on the response. \\n\\nTemperature and humidity are crammed into ==14 bits each==, for a total of 28 bits, or 3.5 bytes. We are not going to deal with half a byte here, so we request a full four bytes. Then, get this, we throw away last two bits of the response. That is what the documentation actually says to do. Why 14 bits for each value? Could they not just make it easy, and have 16 bits each? \\n\\n>Welcome to the land of I2C.\\n\\nThis means that for each value, temperature and humidity, we are working with seven bits of the first byte, then the whole byte for the second. Then we have to cram them together to get a float value. \\n\\nThat is not all either. Once we have the values, they are ==not representative of any actual temperature scale== (celsius, fahrenheit). From there we get to perform math with seemingly totally random values. For example, the temperature value needs to be multiplied by 165, and then 40 needs to be subtracted from the result. What?!\\n\\n    var raw_humidity = null;\\n    var raw_temperature = null;\\n\\t\\n    if( !error ) {\\n      // First two bytes for humidity\\n      // In the range ( 2^14 - 1 )\\n      raw_humidity = ( ( data[0] & 0x3f ) << 8 ) | data[1];\\n      humidity = raw_humidity / 16382;\\n\\n      // Second two bytes for temperature\\n      // In the range ( 2^14 - 1 )\\n      // Multiply by 165 and subtract 40\\n      // Because that is what the documentation says\\n      raw_temperature = ( ( data[2] << 8 ) | data[3] ) >> 2;\\n      temperature = ( raw_temperature / 16382 ) * 165 - 40;            \\n            \\n      console.log( temperature );\\n      console.log( humidity );\\n    } else {\\n      console.log( error );\\n    }\\n\\nI would love to tell you that this is where I stuck it out. Where everything came together. That the documentation was so clear that I was on my way in no time. The reality is that I banged bits around for hours, referencing various Arduino implementations along the way (most of which worked completely different from one another, or the documentation). So goes adventures in managing low-level I2C communication.\\n\\n###Next Steps\\n\\nI am generally a very mellow person, open to spending numerous hours pushing through experiments like this. If my tone comes across as bitter at times for this project, I suppose, it is a little. ==Black magic in programming is the worst.== \\n\\nIn the end, I have a ==highly accurate climate sensor==, and I am very excited by that. I have since gone on to experiment with sampling rates against the HIH6130, and sending the results to Watson IoT for storage and analytics. Next up is to put the sensor on a ==custom module==.\\n\"}]],\"sections\":[[10,0]],\"ghostVersion\":\"3.0\"}","html":"<!--kg-card-begin: markdown--><p>The Tessel is expandable in several ways. One of those ways is pre-built modules. And one of those modules is called the &quot;climate&quot; module. This is effectively the Silicon Labs <mark>Si7020</mark> temperature and humidity sensor. And for whatever reason, on the Tessel, it is <mark>horribly inaccurate</mark>.</p>\n<h3 id=\"heat\">Heat</h3>\n<p>The general explanation found around the community is that the <mark>proximity to the processors</mark> on the Tessel generates enough heat to throw off the temperature of the climate module (Si7020). Some folks have had slightly better results by wiring the module to be several inches away from the Tessel.</p>\n<p><img src=\"http://images.kevinhoyt.com/tessel.climate.wires.jpg\" alt=\"Tessel Climate Module wired to be away from the Tessel itself.\" loading=\"lazy\"></p>\n<p>Now I will admit that there is always heat from processors to be considered, but we are talking an error in the range of <mark>ten degrees fahrenheit</mark>. I have even run a small fan over the Tessel to try and balance out the temperature reading, with little reproducible success.</p>\n<p>At some point you start suspecting the implementation itself. Wether there is a component on the PCB that should be at a different value, or perhaps the parsing of data from the sensor is wrong. To put this to the test, I used one of my favorite climate sensors - <mark>HIH6130</mark> - on the Tessel. I even placed right next to the Tessel, the results were every bit as accurate as I had come to expect - within a single degree of the surrounding area (tested via IR).</p>\n<p><img src=\"https://cdn.sparkfun.com//assets/parts/6/9/6/9/11295-01.jpg\" alt=\"HIH6130 from SparkFun\" loading=\"lazy\"></p>\n<h3 id=\"i2c\">I2C</h3>\n<p>This post is not all rant though. I actually thoroughly enjoyed the process of testing everything. What is more, is that I learned how to implement a <mark>custom I2C sensor</mark> on the Tessel!</p>\n<p><mark>I2C is a communication protocol</mark> for microcontrollers. Each controller on the bus can be individually addressed, <mark>saving you valuable pins</mark>. A series of bits will be sent on the bus, to a specific sensor, and will generally be interpreted as a command.  The sensor will then perform some operation, and respond with a series of bits.</p>\n<p>While the <mark>communication</mark> between controller and device <mark>is standardized</mark>, the <mark>formats</mark> of the commands and messages <mark>are not</mark>. You will often find vendors that claim I2C, only to be ever so slightly different. The good news is that the HIH6130 is I2C, so we just need to know how to send a command, and parse the response.</p>\n<h3 id=\"command\">Command</h3>\n<p>I2C implementations will need <mark>four pins</mark>. There is the power and ground connections, and then a data line and a clock line. Since we are working in at very low levels, and extremely fast, the clock line keeps everything in sync. The commands and messages flow on the data line.</p>\n<p>The Tessel has the data (SDA) and clock (SCL) lines clearly marked on the breadboard.  With everything connected, we are looking at (BOB to Tessel):</p>\n<ul>\n<li>GND to GND</li>\n<li>VDD to 3V3</li>\n<li>SCL to SCL 0</li>\n<li>SDA to SDA 1</li>\n</ul>\n<p>In our JavaScript code for the Tessel, we first need to get a <mark>reference to the I2C system</mark>. The default address of the HIH6130 (per the documentation) is 0x27, and the sensor is wired up on Port B of the Tessel.</p>\n<pre><code>// Sensor\n// Default address\nhih = tessel.port['B'].I2C( 0x27 );\n</code></pre>\n<p>The Tessel takes care of the low-level I2C implementation, but the format of the data is up to us. This means getting really comfortable with several areas. The first is the documentation for the sensor you are using. The next is understanding bits and bytes (such as LSB/MSB, endianness). Finally, you have to shift those bits around in JavaScript to actually get some data.</p>\n<p>When it comes to the HIH6130, there are several commands we can send to the sensor. <a href=\"http://www.phanderson.com/arduino/I2CCommunications.pdf\">Peter Anderson</a> has the data sheet on his web site, which is the resource I found most useful in this experiment. The command to read the temperature and humidity breaks down something like this:</p>\n<ul>\n<li>Two bytes (eight bits)</li>\n<li>Address of 0x27 (seven bits, 0010 0111)</li>\n<li>Read command of 1 in last bit</li>\n<li>1010 1111 becomes 0x4F in hex</li>\n</ul>\n<p>To issue the command to read the climate values then, we want to send 0x4F using a <mark>Buffer object</mark>, which will keep our bits intact. We also tell it how many bytes we are expecting in return, and supply a function to parse that data.</p>\n<pre><code>hih.transfer( \n  new Buffer( 0x4F ), \n  4, \n  function( error, data ) { ... } \n);\n</code></pre>\n<p>That all sounds great, and technically impressive. The reality of the matter is that it did not seem to matter what I sent the sensor, it always replied with the climate readings. The difference is lost in the ether between the Tessel JavaScript commands, and how the Tessel actually maps that to I2C.</p>\n<h3 id=\"response\">Response</h3>\n<p>If you thought the value we are sending over to the HIH6130 as a command was black magic, it does not get much better on the response.</p>\n<p>Temperature and humidity are crammed into <mark>14 bits each</mark>, for a total of 28 bits, or 3.5 bytes. We are not going to deal with half a byte here, so we request a full four bytes. Then, get this, we throw away last two bits of the response. That is what the documentation actually says to do. Why 14 bits for each value? Could they not just make it easy, and have 16 bits each?</p>\n<blockquote>\n<p>Welcome to the land of I2C.</p>\n</blockquote>\n<p>This means that for each value, temperature and humidity, we are working with seven bits of the first byte, then the whole byte for the second. Then we have to cram them together to get a float value.</p>\n<p>That is not all either. Once we have the values, they are <mark>not representative of any actual temperature scale</mark> (celsius, fahrenheit). From there we get to perform math with seemingly totally random values. For example, the temperature value needs to be multiplied by 165, and then 40 needs to be subtracted from the result. What?!</p>\n<pre><code>var raw_humidity = null;\nvar raw_temperature = null;\n\nif( !error ) {\n  // First two bytes for humidity\n  // In the range ( 2^14 - 1 )\n  raw_humidity = ( ( data[0] &amp; 0x3f ) &lt;&lt; 8 ) | data[1];\n  humidity = raw_humidity / 16382;\n\n  // Second two bytes for temperature\n  // In the range ( 2^14 - 1 )\n  // Multiply by 165 and subtract 40\n  // Because that is what the documentation says\n  raw_temperature = ( ( data[2] &lt;&lt; 8 ) | data[3] ) &gt;&gt; 2;\n  temperature = ( raw_temperature / 16382 ) * 165 - 40;            \n        \n  console.log( temperature );\n  console.log( humidity );\n} else {\n  console.log( error );\n}\n</code></pre>\n<p>I would love to tell you that this is where I stuck it out. Where everything came together. That the documentation was so clear that I was on my way in no time. The reality is that I banged bits around for hours, referencing various Arduino implementations along the way (most of which worked completely different from one another, or the documentation). So goes adventures in managing low-level I2C communication.</p>\n<h3 id=\"nextsteps\">Next Steps</h3>\n<p>I am generally a very mellow person, open to spending numerous hours pushing through experiments like this. If my tone comes across as bitter at times for this project, I suppose, it is a little. <mark>Black magic in programming is the worst.</mark></p>\n<p>In the end, I have a <mark>highly accurate climate sensor</mark>, and I am very excited by that. I have since gone on to experiment with sampling rates against the HIH6130, and sending the results to Watson IoT for storage and analytics. Next up is to put the sensor on a <mark>custom module</mark>.</p>\n<!--kg-card-end: markdown-->","comment_id":"62","plaintext":"The Tessel is expandable in several ways. One of those ways is pre-built\nmodules. And one of those modules is called the \"climate\" module. This is\neffectively the Silicon Labs Si7020 temperature and humidity sensor. And for\nwhatever reason, on the Tessel, it is horribly inaccurate.\n\nHeat\nThe general explanation found around the community is that the proximity to the\nprocessors on the Tessel generates enough heat to throw off the temperature of\nthe climate module (Si7020). Some folks have had slightly better results by\nwiring the module to be several inches away from the Tessel.\n\n\n\nNow I will admit that there is always heat from processors to be considered, but\nwe are talking an error in the range of ten degrees fahrenheit. I have even run\na small fan over the Tessel to try and balance out the temperature reading, with\nlittle reproducible success.\n\nAt some point you start suspecting the implementation itself. Wether there is a\ncomponent on the PCB that should be at a different value, or perhaps the parsing\nof data from the sensor is wrong. To put this to the test, I used one of my\nfavorite climate sensors - HIH6130 - on the Tessel. I even placed right next to\nthe Tessel, the results were every bit as accurate as I had come to expect -\nwithin a single degree of the surrounding area (tested via IR).\n\n\n\nI2C\nThis post is not all rant though. I actually thoroughly enjoyed the process of\ntesting everything. What is more, is that I learned how to implement a custom\nI2C sensor on the Tessel!\n\nI2C is a communication protocol for microcontrollers. Each controller on the bus\ncan be individually addressed, saving you valuable pins. A series of bits will\nbe sent on the bus, to a specific sensor, and will generally be interpreted as a\ncommand. The sensor will then perform some operation, and respond with a series\nof bits.\n\nWhile the communication between controller and device is standardized, the \nformats of the commands and messages are not. You will often find vendors that\nclaim I2C, only to be ever so slightly different. The good news is that the\nHIH6130 is I2C, so we just need to know how to send a command, and parse the\nresponse.\n\nCommand\nI2C implementations will need four pins. There is the power and ground\nconnections, and then a data line and a clock line. Since we are working in at\nvery low levels, and extremely fast, the clock line keeps everything in sync.\nThe commands and messages flow on the data line.\n\nThe Tessel has the data (SDA) and clock (SCL) lines clearly marked on the\nbreadboard. With everything connected, we are looking at (BOB to Tessel):\n\n * GND to GND\n * VDD to 3V3\n * SCL to SCL 0\n * SDA to SDA 1\n\nIn our JavaScript code for the Tessel, we first need to get a reference to the\nI2C system. The default address of the HIH6130 (per the documentation) is 0x27,\nand the sensor is wired up on Port B of the Tessel.\n\n// Sensor\n// Default address\nhih = tessel.port['B'].I2C( 0x27 );\n\n\nThe Tessel takes care of the low-level I2C implementation, but the format of the\ndata is up to us. This means getting really comfortable with several areas. The\nfirst is the documentation for the sensor you are using. The next is\nunderstanding bits and bytes (such as LSB/MSB, endianness). Finally, you have to\nshift those bits around in JavaScript to actually get some data.\n\nWhen it comes to the HIH6130, there are several commands we can send to the\nsensor. Peter Anderson [http://www.phanderson.com/arduino/I2CCommunications.pdf] \nhas the data sheet on his web site, which is the resource I found most useful in\nthis experiment. The command to read the temperature and humidity breaks down\nsomething like this:\n\n * Two bytes (eight bits)\n * Address of 0x27 (seven bits, 0010 0111)\n * Read command of 1 in last bit\n * 1010 1111 becomes 0x4F in hex\n\nTo issue the command to read the climate values then, we want to send 0x4F using\na Buffer object, which will keep our bits intact. We also tell it how many bytes\nwe are expecting in return, and supply a function to parse that data.\n\nhih.transfer( \n  new Buffer( 0x4F ), \n  4, \n  function( error, data ) { ... } \n);\n\n\nThat all sounds great, and technically impressive. The reality of the matter is\nthat it did not seem to matter what I sent the sensor, it always replied with\nthe climate readings. The difference is lost in the ether between the Tessel\nJavaScript commands, and how the Tessel actually maps that to I2C.\n\nResponse\nIf you thought the value we are sending over to the HIH6130 as a command was\nblack magic, it does not get much better on the response.\n\nTemperature and humidity are crammed into 14 bits each, for a total of 28 bits,\nor 3.5 bytes. We are not going to deal with half a byte here, so we request a\nfull four bytes. Then, get this, we throw away last two bits of the response.\nThat is what the documentation actually says to do. Why 14 bits for each value?\nCould they not just make it easy, and have 16 bits each?\n\n> Welcome to the land of I2C.\n\n\nThis means that for each value, temperature and humidity, we are working with\nseven bits of the first byte, then the whole byte for the second. Then we have\nto cram them together to get a float value.\n\nThat is not all either. Once we have the values, they are not representative of\nany actual temperature scale (celsius, fahrenheit). From there we get to perform\nmath with seemingly totally random values. For example, the temperature value\nneeds to be multiplied by 165, and then 40 needs to be subtracted from the\nresult. What?!\n\nvar raw_humidity = null;\nvar raw_temperature = null;\n\nif( !error ) {\n  // First two bytes for humidity\n  // In the range ( 2^14 - 1 )\n  raw_humidity = ( ( data[0] & 0x3f ) << 8 ) | data[1];\n  humidity = raw_humidity / 16382;\n\n  // Second two bytes for temperature\n  // In the range ( 2^14 - 1 )\n  // Multiply by 165 and subtract 40\n  // Because that is what the documentation says\n  raw_temperature = ( ( data[2] << 8 ) | data[3] ) >> 2;\n  temperature = ( raw_temperature / 16382 ) * 165 - 40;            \n        \n  console.log( temperature );\n  console.log( humidity );\n} else {\n  console.log( error );\n}\n\n\nI would love to tell you that this is where I stuck it out. Where everything\ncame together. That the documentation was so clear that I was on my way in no\ntime. The reality is that I banged bits around for hours, referencing various\nArduino implementations along the way (most of which worked completely different\nfrom one another, or the documentation). So goes adventures in managing\nlow-level I2C communication.\n\nNext Steps\nI am generally a very mellow person, open to spending numerous hours pushing\nthrough experiments like this. If my tone comes across as bitter at times for\nthis project, I suppose, it is a little. Black magic in programming is the\nworst.\n\nIn the end, I have a highly accurate climate sensor, and I am very excited by\nthat. I have since gone on to experiment with sampling rates against the\nHIH6130, and sending the results to Watson IoT for storage and analytics. Next\nup is to put the sensor on a custom module.","feature_image":"http://images.kevinhoyt.com/ceramic.tessellation.jpg","featured":0,"status":"published","locale":null,"visibility":"public","author_id":"1","created_at":"2016-09-12T16:13:01.000Z","updated_at":"2016-09-12T17:52:57.000Z","published_at":"2016-09-12T17:52:57.000Z","custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"type":"post","email_recipient_filter":"none"},{"id":"5adb8922351ffe0018a57748","uuid":"456fecf8-7045-40b0-b260-caed415d9e51","title":"Tessel Barcode Scanner: Hardware","slug":"tessel-barcode-scanner-hardware","mobiledoc":"{\"version\":\"0.3.1\",\"markups\":[],\"atoms\":[],\"cards\":[[\"markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\"In addition to the two module ports on the [Tessel](https://tessel.io/), there are also two USB ports. USB, while big and bulky in many IoT projects, can provide a useful access point to hardware. One example used by Tessel is a [web camera](https://github.com/tessel/tessel-av). Many POS (point of sale) devices are also designed to work with USB. In this post we will take a look at using a barcode scanner.\\n\\n###Barcode Scanner\\n\\nI got the barcode scanner I use from [SparkFun](https://www.sparkfun.com/products/9166). This scanner is tough and rubbery, like you would expect to find at a POS system in a grocery store. It supports a wide range of barcodes, and can be used as a keyboard HID (human interface device). While this is useful for GUI applications, an IoT project rarely has that luxury.\\n\\n![USB Barcode Scanner](https://cdn.sparkfun.com//assets/parts/2/5/9/2/09166-03-L.jpg)\\n\\nLucky for us, this barcode scanner also supports a ==serial interface==. This is a common communication method for USB devices (and other hardware). For example, if you debug an [Arduino](http://arduino.cc), you are using the serial interface.\\n\\nTo ==configure the barcode scanner== to use the serial port, we have to reference the provided \\\"==user manual==\\\". While there is not much of a manual needed to plug in a USB device, also included is an array of barcodes used to configure the scanner. \\n\\n> While I have managed to keep my manual intact for several years, I am always worried about losing it. I have scanned the manual, and you can [download](http://images.kevinhoyt.com/barcode.scanner.user.manual.pdf) it should you (or I) need it.\\n\\nTo configure the scanner, with it attached to any USB port (for power), scan the \\\"==PROGRAM==\\\" barcode at the top of any page, then scan the \\\"==RS-232C==\\\" barcode, and then the \\\"==END==\\\" barcode at the bottom of any page. If you are working on platforms where you need more control (stop bits, baud, etc.) then you repeat the process accordingly with the associated barcodes.\\n\\n###Finding the Scanner\\n\\nThe Tessel documentation on using the USB ports quickly gets into all variety of native code, which is way deeper than we need to get for this application. Finding the scanner however, did take me some ==trial and error==.\\n\\nThe way I went about finding the scanner is first in knowing that the Tessel runs [OpenWRT](https://openwrt.org/). OpenWRT is a Linux-based operating system. Linux systems mount serial devices as a directory inside \\\"==/dev==\\\" and usually appear as \\\"==tty==\\\" with some additional labeling.\\n\\nThe Node.js \\\"==fs==\\\" package allows us to get a ==directory listing==. Without the barcode scanner attached to the Tessel, a directory listing of \\\"/dev\\\" will yield two \\\"tty\\\" endpoints - \\\"ttyS0\\\" and \\\"ttyS1\\\". Being that there are two endpoints, and two USB ports, I figured that it must be one of them.\\n\\n```\\nvar dirs = fs.readdirSync( '/dev' );\\n\\nfor( var d = 0; d < dirs.length; d++ ) {\\n  if( dirs[d] == 'ttyACM0' ) {\\n    console.log( 'Found barcode scanner.' );\\n    break;\\n  }\\n}\\n```\\n\\nWhile I was able to connect to both endpoints from Node.js on the Tessel, neither provided any output. Then I ==connected the barcode scanner== and ran the directory listing again. This time a \\\"==ttyACM0==\\\" appeared. When I tested that endpoint, I was able to get data from the scanner. We are in business!\\n\\n###Serial Ports\\n\\nThe go-to package for ==serial port communication== with Node.js is [Node Serialport](https://github.com/EmergingTechnologyAdvisors/node-serialport). Because some compilation is generally needed for Node Serialport, I was concerned at first that it would not work on the Tessel. To my delight, ==it just worked==.\\n\\n```\\n// Open barcode scanner serial port\\n// Use carriage return for terminator\\nvar port = new SerialPort( '/dev/ttyACM0', {\\n  parser: SerialPort.parsers.readline( '\\\\r' )\\n} );\\n\\nport.on( 'open', function() {\\n  // Debug\\n  console.log( 'Port connected.' );\\n} );\\n\\n// Barcode scanned\\nport.on( 'data', function( data ) {\\n  // Buffer to string\\n  var barcode = data.toString().trim();\\n    \\n  // Debug\\n  console.log( barcode );\\n} );\\n```\\n\\nThe first step to using Node Serialport is to ==open a connection to the port==. The barcode scanner will provide a ==carriage return== when a barcode value has been read, so we additionally tell Serialport to look for that signal.\\n\\nWhen a ==barcode is read== (or really any serial communication with a carriage return), the \\\"==data==\\\" event will be emitted. Not all serial systems speak character data, so the \\\"data\\\" property is a [Buffer](https://nodejs.org/dist/latest-v4.x/docs/api/buffer.html) object. In this case, we want the string value without the carriage return.\\n\\n###Next Steps\\n\\n==That is it!== Now you have a Tessel that can scan barcodes. While I have my scanner looking for one barcode per trigger pull, you can even configure the scanner for ==continuous scanning==. This might be useful in a production setting for fast, repeated, scanning.\\n\\nOther POS devices that speak serial over USB includes ==magnetic card readers==. The reader on SparkFun does not have a USB port, but you can find various options around the Internet. With a little more work, you could build your very own, custom, POS system using a Tessel.\\n\\nGetting the barcode value is useful, but it would be even better to ==find the product that barcode belongs to== an take some action such as manage inventory in a database. I will cover barcode lookup, and communication to other services, in my next post. \"}]],\"sections\":[[10,0]],\"ghostVersion\":\"3.0\"}","html":"<!--kg-card-begin: markdown--><p>In addition to the two module ports on the <a href=\"https://tessel.io/\">Tessel</a>, there are also two USB ports. USB, while big and bulky in many IoT projects, can provide a useful access point to hardware. One example used by Tessel is a <a href=\"https://github.com/tessel/tessel-av\">web camera</a>. Many POS (point of sale) devices are also designed to work with USB. In this post we will take a look at using a barcode scanner.</p>\n<h3 id=\"barcodescanner\">Barcode Scanner</h3>\n<p>I got the barcode scanner I use from <a href=\"https://www.sparkfun.com/products/9166\">SparkFun</a>. This scanner is tough and rubbery, like you would expect to find at a POS system in a grocery store. It supports a wide range of barcodes, and can be used as a keyboard HID (human interface device). While this is useful for GUI applications, an IoT project rarely has that luxury.</p>\n<p><img src=\"https://cdn.sparkfun.com//assets/parts/2/5/9/2/09166-03-L.jpg\" alt=\"USB Barcode Scanner\" loading=\"lazy\"></p>\n<p>Lucky for us, this barcode scanner also supports a <mark>serial interface</mark>. This is a common communication method for USB devices (and other hardware). For example, if you debug an <a href=\"http://arduino.cc\">Arduino</a>, you are using the serial interface.</p>\n<p>To <mark>configure the barcode scanner</mark> to use the serial port, we have to reference the provided &quot;<mark>user manual</mark>&quot;. While there is not much of a manual needed to plug in a USB device, also included is an array of barcodes used to configure the scanner.</p>\n<blockquote>\n<p>While I have managed to keep my manual intact for several years, I am always worried about losing it. I have scanned the manual, and you can <a href=\"http://images.kevinhoyt.com/barcode.scanner.user.manual.pdf\">download</a> it should you (or I) need it.</p>\n</blockquote>\n<p>To configure the scanner, with it attached to any USB port (for power), scan the &quot;<mark>PROGRAM</mark>&quot; barcode at the top of any page, then scan the &quot;<mark>RS-232C</mark>&quot; barcode, and then the &quot;<mark>END</mark>&quot; barcode at the bottom of any page. If you are working on platforms where you need more control (stop bits, baud, etc.) then you repeat the process accordingly with the associated barcodes.</p>\n<h3 id=\"findingthescanner\">Finding the Scanner</h3>\n<p>The Tessel documentation on using the USB ports quickly gets into all variety of native code, which is way deeper than we need to get for this application. Finding the scanner however, did take me some <mark>trial and error</mark>.</p>\n<p>The way I went about finding the scanner is first in knowing that the Tessel runs <a href=\"https://openwrt.org/\">OpenWRT</a>. OpenWRT is a Linux-based operating system. Linux systems mount serial devices as a directory inside &quot;<mark>/dev</mark>&quot; and usually appear as &quot;<mark>tty</mark>&quot; with some additional labeling.</p>\n<p>The Node.js &quot;<mark>fs</mark>&quot; package allows us to get a <mark>directory listing</mark>. Without the barcode scanner attached to the Tessel, a directory listing of &quot;/dev&quot; will yield two &quot;tty&quot; endpoints - &quot;ttyS0&quot; and &quot;ttyS1&quot;. Being that there are two endpoints, and two USB ports, I figured that it must be one of them.</p>\n<pre><code>var dirs = fs.readdirSync( '/dev' );\n\nfor( var d = 0; d &lt; dirs.length; d++ ) {\n  if( dirs[d] == 'ttyACM0' ) {\n    console.log( 'Found barcode scanner.' );\n    break;\n  }\n}\n</code></pre>\n<p>While I was able to connect to both endpoints from Node.js on the Tessel, neither provided any output. Then I <mark>connected the barcode scanner</mark> and ran the directory listing again. This time a &quot;<mark>ttyACM0</mark>&quot; appeared. When I tested that endpoint, I was able to get data from the scanner. We are in business!</p>\n<h3 id=\"serialports\">Serial Ports</h3>\n<p>The go-to package for <mark>serial port communication</mark> with Node.js is <a href=\"https://github.com/EmergingTechnologyAdvisors/node-serialport\">Node Serialport</a>. Because some compilation is generally needed for Node Serialport, I was concerned at first that it would not work on the Tessel. To my delight, <mark>it just worked</mark>.</p>\n<pre><code>// Open barcode scanner serial port\n// Use carriage return for terminator\nvar port = new SerialPort( '/dev/ttyACM0', {\n  parser: SerialPort.parsers.readline( '\\r' )\n} );\n\nport.on( 'open', function() {\n  // Debug\n  console.log( 'Port connected.' );\n} );\n\n// Barcode scanned\nport.on( 'data', function( data ) {\n  // Buffer to string\n  var barcode = data.toString().trim();\n    \n  // Debug\n  console.log( barcode );\n} );\n</code></pre>\n<p>The first step to using Node Serialport is to <mark>open a connection to the port</mark>. The barcode scanner will provide a <mark>carriage return</mark> when a barcode value has been read, so we additionally tell Serialport to look for that signal.</p>\n<p>When a <mark>barcode is read</mark> (or really any serial communication with a carriage return), the &quot;<mark>data</mark>&quot; event will be emitted. Not all serial systems speak character data, so the &quot;data&quot; property is a <a href=\"https://nodejs.org/dist/latest-v4.x/docs/api/buffer.html\">Buffer</a> object. In this case, we want the string value without the carriage return.</p>\n<h3 id=\"nextsteps\">Next Steps</h3>\n<p><mark>That is it!</mark> Now you have a Tessel that can scan barcodes. While I have my scanner looking for one barcode per trigger pull, you can even configure the scanner for <mark>continuous scanning</mark>. This might be useful in a production setting for fast, repeated, scanning.</p>\n<p>Other POS devices that speak serial over USB includes <mark>magnetic card readers</mark>. The reader on SparkFun does not have a USB port, but you can find various options around the Internet. With a little more work, you could build your very own, custom, POS system using a Tessel.</p>\n<p>Getting the barcode value is useful, but it would be even better to <mark>find the product that barcode belongs to</mark> an take some action such as manage inventory in a database. I will cover barcode lookup, and communication to other services, in my next post.</p>\n<!--kg-card-end: markdown-->","comment_id":"63","plaintext":"In addition to the two module ports on the Tessel [https://tessel.io/], there\nare also two USB ports. USB, while big and bulky in many IoT projects, can\nprovide a useful access point to hardware. One example used by Tessel is a web\ncamera [https://github.com/tessel/tessel-av]. Many POS (point of sale) devices\nare also designed to work with USB. In this post we will take a look at using a\nbarcode scanner.\n\nBarcode Scanner\nI got the barcode scanner I use from SparkFun\n[https://www.sparkfun.com/products/9166]. This scanner is tough and rubbery,\nlike you would expect to find at a POS system in a grocery store. It supports a\nwide range of barcodes, and can be used as a keyboard HID (human interface\ndevice). While this is useful for GUI applications, an IoT project rarely has\nthat luxury.\n\n\n\nLucky for us, this barcode scanner also supports a serial interface. This is a\ncommon communication method for USB devices (and other hardware). For example,\nif you debug an Arduino [http://arduino.cc], you are using the serial interface.\n\nTo configure the barcode scanner to use the serial port, we have to reference\nthe provided \"user manual\". While there is not much of a manual needed to plug\nin a USB device, also included is an array of barcodes used to configure the\nscanner.\n\n> While I have managed to keep my manual intact for several years, I am always\nworried about losing it. I have scanned the manual, and you can download\n[http://images.kevinhoyt.com/barcode.scanner.user.manual.pdf] it should you (or\nI) need it.\n\n\nTo configure the scanner, with it attached to any USB port (for power), scan the\n\"PROGRAM\" barcode at the top of any page, then scan the \"RS-232C\" barcode, and\nthen the \"END\" barcode at the bottom of any page. If you are working on\nplatforms where you need more control (stop bits, baud, etc.) then you repeat\nthe process accordingly with the associated barcodes.\n\nFinding the Scanner\nThe Tessel documentation on using the USB ports quickly gets into all variety of\nnative code, which is way deeper than we need to get for this application.\nFinding the scanner however, did take me some trial and error.\n\nThe way I went about finding the scanner is first in knowing that the Tessel\nruns OpenWRT [https://openwrt.org/]. OpenWRT is a Linux-based operating system.\nLinux systems mount serial devices as a directory inside \"/dev\" and usually\nappear as \"tty\" with some additional labeling.\n\nThe Node.js \"fs\" package allows us to get a directory listing. Without the\nbarcode scanner attached to the Tessel, a directory listing of \"/dev\" will yield\ntwo \"tty\" endpoints - \"ttyS0\" and \"ttyS1\". Being that there are two endpoints,\nand two USB ports, I figured that it must be one of them.\n\nvar dirs = fs.readdirSync( '/dev' );\n\nfor( var d = 0; d < dirs.length; d++ ) {\n  if( dirs[d] == 'ttyACM0' ) {\n    console.log( 'Found barcode scanner.' );\n    break;\n  }\n}\n\n\nWhile I was able to connect to both endpoints from Node.js on the Tessel,\nneither provided any output. Then I connected the barcode scanner and ran the\ndirectory listing again. This time a \"ttyACM0\" appeared. When I tested that\nendpoint, I was able to get data from the scanner. We are in business!\n\nSerial Ports\nThe go-to package for serial port communication with Node.js is Node Serialport\n[https://github.com/EmergingTechnologyAdvisors/node-serialport]. Because some\ncompilation is generally needed for Node Serialport, I was concerned at first\nthat it would not work on the Tessel. To my delight, it just worked.\n\n// Open barcode scanner serial port\n// Use carriage return for terminator\nvar port = new SerialPort( '/dev/ttyACM0', {\n  parser: SerialPort.parsers.readline( '\\r' )\n} );\n\nport.on( 'open', function() {\n  // Debug\n  console.log( 'Port connected.' );\n} );\n\n// Barcode scanned\nport.on( 'data', function( data ) {\n  // Buffer to string\n  var barcode = data.toString().trim();\n    \n  // Debug\n  console.log( barcode );\n} );\n\n\nThe first step to using Node Serialport is to open a connection to the port. The\nbarcode scanner will provide a carriage return when a barcode value has been\nread, so we additionally tell Serialport to look for that signal.\n\nWhen a barcode is read (or really any serial communication with a carriage\nreturn), the \"data\" event will be emitted. Not all serial systems speak\ncharacter data, so the \"data\" property is a Buffer\n[https://nodejs.org/dist/latest-v4.x/docs/api/buffer.html] object. In this case,\nwe want the string value without the carriage return.\n\nNext Steps\nThat is it! Now you have a Tessel that can scan barcodes. While I have my\nscanner looking for one barcode per trigger pull, you can even configure the\nscanner for continuous scanning. This might be useful in a production setting\nfor fast, repeated, scanning.\n\nOther POS devices that speak serial over USB includes magnetic card readers. The\nreader on SparkFun does not have a USB port, but you can find various options\naround the Internet. With a little more work, you could build your very own,\ncustom, POS system using a Tessel.\n\nGetting the barcode value is useful, but it would be even better to find the\nproduct that barcode belongs to an take some action such as manage inventory in\na database. I will cover barcode lookup, and communication to other services, in\nmy next post.","feature_image":"__GHOST_URL__/content/images/2019/01/shopping.carts-2.jpg","featured":0,"status":"published","locale":null,"visibility":"public","author_id":"1","created_at":"2016-10-22T07:28:32.000Z","updated_at":"2019-01-17T00:46:14.000Z","published_at":"2016-11-03T15:46:04.000Z","custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"type":"post","email_recipient_filter":"none"},{"id":"5adb8922351ffe0018a57749","uuid":"53a81244-67d6-440f-acb7-516a95eb96ad","title":"Tessel Barcode Scanner: Software","slug":"tessel-barcode-scanner-software","mobiledoc":"{\"version\":\"0.3.1\",\"markups\":[],\"atoms\":[],\"cards\":[[\"markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\"In my previous post, I show how you can connect a barcode scanner to a USB port on the Tessel, and read the scanned values. While that is interesting, what is more interesting is in doing something with that value. In this post we will take a look at finding a related product, and then communicating that information over Watson IoT.\\n\\n###The Barcode Wild West\\n\\nBarcodes are one of those addictive pieces of technology. Simple stripes, printed on a piece of paper, become digits. Amazing! And barcodes are everywhere. Once your scanner is connected to your Tessel, if you are like me, you will find yourself running around your house looking for things to scan.\\n\\nIn some cases, such as a barcode on Netflix DVD envelope, there is not much to do with this information. It is likely some internal routing number. Most commercial products, like things you buy at the grocery store, also have barcodes, and these uniquely identify the item in a database.\\n\\nWhat is more is that these barcodes are often unique for that product on a global (or at least national) scale. The barcode on a bag of Cheetos for example, needs to tell the POS (point of sale) system that it is a bag of Cheetos regardless as to where you are shopping. This means that there must be a universal database of these codes and their product mappings.\\n\\nNot so fast, cowboy! \\n\\nDatabases like this do exist, but they are generally expensive - mainly because they are expensive to maintain. Companies like Amazon, who you know has an extensive database of barcode mappings, provide lookup services, but they put tight restrictions around their usage. You did not expect them to let that costly database go for free did you?\\n\\nOver the years, many a crowd-sourced database has come and gone. It turns out it is really hard to make a business on barcode mappings. Which is yet another reason why the databases tend to be so expensive. Some databases also only cater to specific categories like electronics, or music CDs (what is that?).\\n\\n> Another business model around barcode databases is APIs around helping you manage your own inventory, taxes, shipping, etc. The [Google Content API for Shopping](https://developers.google.com/shopping-content/v2/quickstart) is an example of this approach.\\n\\nThe option I will be using for this demonstration is a crowdsourced offering from [ProductLayer](https://developer.productlayer.com/documentation?version=0.5). ProductLayer has a commercial side called [Prod.ly](https://prod.ly/home). I like to think of Prod.ly as a social network for people that geek out about scanning barcodes. While the database is not as extensive as Amazon, it is an easy-to-use starting point.\\n\\n###Using ProductLayer\\n\\nTo use the barcode database from ProductLayer, you first have to create a free account. From there you can create an application, which will result in an API key. This API key is used in the HTTP header when making requests against the ProductLayer services.\\n\\n```\\n// Look up product detail\\nrequest( {\\n  uri: config.prodly_url + '?gtin=' + barcode,\\n  headers: {\\n    'API-KEY': config.prodly_key\\n  }\\n}, function( error, response, body ) {\\n  var data = null;\\n  var message = null;\\n\\n  // Parse\\n  data = JSON.parse( body );\\n\\n  // Debug\\n  console.log( data[0]['pl-prod-name'];\\n} );\\n```\\n\\nThe ProductLayer service provides a GET endpoint to query their barcode database. When you run the query you can get multiple responses (JSON array) as sometimes barcodes map to multiple items. For this application I am going to take the first item returned and pass that to Watson IoT.\\n\\n>I do the product lookup on the Tessel because that is where it feels most natural. The Tessel has fast and stable wireless access, plenty of memory and CPU, and is where the physical action is taking place.\\n\\n###Watson IoT\\n\\nI have covered using the Tessel with Watson IoT in a [previous post](http://www.kevinhoyt.com/2016/09/01/tessel-on-watson-iot/), so I will not belabor the point here other than a high level review. You can get access to [Watson IoT](https://console.ng.bluemix.net/catalog/services/internet-of-things-platform/) from [IBM Bluemix](https://console.ng.bluemix.net/).\\n\\nWatson IoT, at its core is an [MQTT](https://en.wikipedia.org/wiki/MQTT) broker. Created at IBM, MQTT is a common pub-sub protocol for IoT devices. There are several Node.js implementations of MQTT in both client and broker form. I will use \\\"[MQTT.js](https://github.com/mqttjs/MQTT.js)\\\" for this project.\\n\\n```\\nvar mqtt = require( 'mqtt' );\\n\\n// Connect to Watson IoT\\nvar client = mqtt.connect( \\n  config.iot_host, \\n  {\\n    clientId: config.iot_client,\\n    username: config.iot_user,\\n    password: config.iot_password,\\n    port: config.iot_port\\n  }\\n);    \\n    \\nclient.on( 'connect', function() {\\n  // Debug\\n  console.log( 'IoT connected.' );\\n} );\\n```\\n\\nThe first step is to connect the Tessel to Watson IoT. Once connected, we will simply let it sit there waiting to be used. MQTT.js is robust enough to handle reconnecting should there be a network problem or client conflict.\\n\\n```\\nvar data = null;\\nvar message = null;\\n\\n// Parse\\ndata = JSON.parse( body );\\n\\n// Default message\\nmessage = {\\n  barcode: barcode\\n};\\n\\n// Look for match\\nif( data.length > 0 ) {\\n  message.product = data[0]['pl-prod-name'];\\n}\\n\\n// Debug\\n// Product\\nconsole.log( message.product );\\n\\n// Publish to Watson IoT\\nclient.publish( \\n  config.iot_topic, \\n  JSON.stringify( message ) \\n);  \\n```\\n\\nWhen a barcode value is read by Node Seriaport, and the lookup against ProductLayer is complete, I check to see if there was any match. If there was a match, then I return the first element in the resulting array, as well as the barcode itself. If there was no match, then I simply return the barcode.\\n\\nThese details are then published to Watson IoT.\\n\\n###Node.js on IBM Bluemix\\n\\nYou may or may not have heard of [Cloud Foundry](https://www.cloudfoundry.org/). Cloud Foundry is an open architecture for cloud applications. IBM is a Cloud Foundry provider (via Bluemix), and you can quickly and easily deploy applications running everything from Java, to ASP.NET, and Node.js.\\n\\nI created a Node.js application instance in Bluemix for this example. The Node.js application serves several purposes.\\n\\nFirst, it listens for barcode messages from Watson IoT. There is a [browser implementation](http://www.eclipse.org/paho/) of MQTT in JavaScript, but then I would have to provide credentials in the open.\\n\\n```\\n// Watson IoT\\nvar iot = mqtt.connect( config.iot_host, {\\n  clientId: config.iot_client,\\n  username: config.iot_user,\\n  password: config.iot_password\\n} );\\n\\n// Connected to broker\\niot.on( 'connect', function() {\\n  // Debug\\n  console.log( 'Connected to Watson.' );\\n\\n  // Subscribe to barcode scans\\n  iot.subscribe( config.iot_topic );  \\n} );    \\n\\n// Message from broker\\niot.on( 'message', function( topic, message ) {\\n  var data = null;\\n\\n  // Buffer to String\\n  data = message.toString();\\n\\n  // Debug\\n  console.log( data );\\n\\n  // Send to clients\\n  for( var c = 0; c < socket.clients.length; c++ ) {\\n    if( socket.clients[c] != this ) {\\n      socket.clients[c].send( data );\\n    }\\n  }\\n} );\\n```\\n\\nTo get around this, the Node.js application also hosts a [WebSocket](http://caniuse.com/#feat=websockets) server. This means that any platform that can communicate over WebSocket (which include iOS, Android, and many others), can securely listen for barcode messages.\\n\\n```\\n// Sockets\\nvar server = http.createServer();\\nvar socket = new ws.Server( {\\n  server: server\\n} );\\n\\n// Client connected\\nsocket.on( 'connection', function( client ) {\\n  // Debug\\n  console.log( 'WebSocket connection.' );\\n} );\\n```\\n\\n>I use the same MQTT package on the Tessel as I use for the server application - JavaScript FTW!\\n\\nFinally, the Node.js application runs [Express](http://expressjs.com/) for a web server.\\n\\n```\\n// Web\\nvar app = express();\\n\\n// Static for main files\\napp.use( '/', express.static( 'public' ) );\\n\\n// Cloud Foundry support\\n// Bluemix\\nvar env = cfenv.getAppEnv();\\n\\n// Listen\\nserver.on( 'request', app );\\nserver.listen( env.port, '0.0.0.0', function() {\\n  // Debug\\n  console.log( 'Started on: ' + env.port );\\n} );\\n```\\n\\n###Browser Display\\n\\nThe last part of this project then is to display the barcode, and product name if any, in the browser. When the browser application starts, it connects to the Node.js application via WebSocket. When a message arrives at the server, and is passed to the client (browser), the details are displayed.\\n\\n```\\nvar Barcode = ( function() {\\n\\n  var display = null;\\n  var socket = null;\\n\\n  var doSocketMessage = function( evt ) {\\n    var data = null;\\n\\n    // Parse\\n    data = JSON.parse( evt.data );\\n\\n    // Debug    \\n    console.log( data );\\n\\n    // Populate display\\n    // Name or raw barcode\\n    if( data.product ) {\\n      display.innerHTML = data.product;\\n    } else {\\n      display.innerHTML = data.barcode;\\n    }\\n  };\\n\\n  // Connected\\n  var doSocketOpen = function() {\\n    console.log( 'Socket open.' );\\n  };\\n\\n  // **\\n  // Initialize\\n  // **\\n  \\n  // References\\n  display = document.querySelector( '.display' );\\n\\n  // Socket\\n  socket = new WebSocket( 'ws://' + window.location.host );\\n  socket.addEventListener( 'open', doSocketOpen );\\n  socket.addEventListener( 'message', doSocketMessage );\\n\\n  // Reveal\\n  return {\\n\\n  };\\n  \\n} )();\\n```\\n\\n> I like to use the \\\"reveal pattern\\\" in my JavaScript to keep my application properties from polluting the global namespace.\\n\\nThis code will execute as soon as loaded, and will immediately create a WebSocket connection from the browser, back to the originating server (not a requirement for WebSocket, BTW). It then adds an event listener for any messages coming from the server. When a message arrives, it is parsed, and the content is displayed on the screen.\\n\\n###Next Steps\\n\\nNow we can scan a barcode using our Tessel, lookup any product specifics from a third-party database, pass that information to a Node.js server, and then push that information to a connected client via WebSocket. All the parts are decoupled from one another thanks to the publish-subscribe pattern, and the whole process executes in a fraction of a second.\\n\\nFrom here you might establish routes in the Node.js server application to handle additional shopping cart actions - perhaps mapping to the the Google Content API for Shopping. You can even reuse the WebSocket for removing items from the client, and propagating that back into an order database.\"}]],\"sections\":[[10,0]],\"ghostVersion\":\"3.0\"}","html":"<!--kg-card-begin: markdown--><p>In my previous post, I show how you can connect a barcode scanner to a USB port on the Tessel, and read the scanned values. While that is interesting, what is more interesting is in doing something with that value. In this post we will take a look at finding a related product, and then communicating that information over Watson IoT.</p>\n<h3 id=\"thebarcodewildwest\">The Barcode Wild West</h3>\n<p>Barcodes are one of those addictive pieces of technology. Simple stripes, printed on a piece of paper, become digits. Amazing! And barcodes are everywhere. Once your scanner is connected to your Tessel, if you are like me, you will find yourself running around your house looking for things to scan.</p>\n<p>In some cases, such as a barcode on Netflix DVD envelope, there is not much to do with this information. It is likely some internal routing number. Most commercial products, like things you buy at the grocery store, also have barcodes, and these uniquely identify the item in a database.</p>\n<p>What is more is that these barcodes are often unique for that product on a global (or at least national) scale. The barcode on a bag of Cheetos for example, needs to tell the POS (point of sale) system that it is a bag of Cheetos regardless as to where you are shopping. This means that there must be a universal database of these codes and their product mappings.</p>\n<p>Not so fast, cowboy!</p>\n<p>Databases like this do exist, but they are generally expensive - mainly because they are expensive to maintain. Companies like Amazon, who you know has an extensive database of barcode mappings, provide lookup services, but they put tight restrictions around their usage. You did not expect them to let that costly database go for free did you?</p>\n<p>Over the years, many a crowd-sourced database has come and gone. It turns out it is really hard to make a business on barcode mappings. Which is yet another reason why the databases tend to be so expensive. Some databases also only cater to specific categories like electronics, or music CDs (what is that?).</p>\n<blockquote>\n<p>Another business model around barcode databases is APIs around helping you manage your own inventory, taxes, shipping, etc. The <a href=\"https://developers.google.com/shopping-content/v2/quickstart\">Google Content API for Shopping</a> is an example of this approach.</p>\n</blockquote>\n<p>The option I will be using for this demonstration is a crowdsourced offering from <a href=\"https://developer.productlayer.com/documentation?version=0.5\">ProductLayer</a>. ProductLayer has a commercial side called <a href=\"https://prod.ly/home\">Prod.ly</a>. I like to think of Prod.ly as a social network for people that geek out about scanning barcodes. While the database is not as extensive as Amazon, it is an easy-to-use starting point.</p>\n<h3 id=\"usingproductlayer\">Using ProductLayer</h3>\n<p>To use the barcode database from ProductLayer, you first have to create a free account. From there you can create an application, which will result in an API key. This API key is used in the HTTP header when making requests against the ProductLayer services.</p>\n<pre><code>// Look up product detail\nrequest( {\n  uri: config.prodly_url + '?gtin=' + barcode,\n  headers: {\n    'API-KEY': config.prodly_key\n  }\n}, function( error, response, body ) {\n  var data = null;\n  var message = null;\n\n  // Parse\n  data = JSON.parse( body );\n\n  // Debug\n  console.log( data[0]['pl-prod-name'];\n} );\n</code></pre>\n<p>The ProductLayer service provides a GET endpoint to query their barcode database. When you run the query you can get multiple responses (JSON array) as sometimes barcodes map to multiple items. For this application I am going to take the first item returned and pass that to Watson IoT.</p>\n<blockquote>\n<p>I do the product lookup on the Tessel because that is where it feels most natural. The Tessel has fast and stable wireless access, plenty of memory and CPU, and is where the physical action is taking place.</p>\n</blockquote>\n<h3 id=\"watsoniot\">Watson IoT</h3>\n<p>I have covered using the Tessel with Watson IoT in a <a href=\"http://www.kevinhoyt.com/2016/09/01/tessel-on-watson-iot/\">previous post</a>, so I will not belabor the point here other than a high level review. You can get access to <a href=\"https://console.ng.bluemix.net/catalog/services/internet-of-things-platform/\">Watson IoT</a> from <a href=\"https://console.ng.bluemix.net/\">IBM Bluemix</a>.</p>\n<p>Watson IoT, at its core is an <a href=\"https://en.wikipedia.org/wiki/MQTT\">MQTT</a> broker. Created at IBM, MQTT is a common pub-sub protocol for IoT devices. There are several Node.js implementations of MQTT in both client and broker form. I will use &quot;<a href=\"https://github.com/mqttjs/MQTT.js\">MQTT.js</a>&quot; for this project.</p>\n<pre><code>var mqtt = require( 'mqtt' );\n\n// Connect to Watson IoT\nvar client = mqtt.connect( \n  config.iot_host, \n  {\n    clientId: config.iot_client,\n    username: config.iot_user,\n    password: config.iot_password,\n    port: config.iot_port\n  }\n);    \n    \nclient.on( 'connect', function() {\n  // Debug\n  console.log( 'IoT connected.' );\n} );\n</code></pre>\n<p>The first step is to connect the Tessel to Watson IoT. Once connected, we will simply let it sit there waiting to be used. MQTT.js is robust enough to handle reconnecting should there be a network problem or client conflict.</p>\n<pre><code>var data = null;\nvar message = null;\n\n// Parse\ndata = JSON.parse( body );\n\n// Default message\nmessage = {\n  barcode: barcode\n};\n\n// Look for match\nif( data.length &gt; 0 ) {\n  message.product = data[0]['pl-prod-name'];\n}\n\n// Debug\n// Product\nconsole.log( message.product );\n\n// Publish to Watson IoT\nclient.publish( \n  config.iot_topic, \n  JSON.stringify( message ) \n);  \n</code></pre>\n<p>When a barcode value is read by Node Seriaport, and the lookup against ProductLayer is complete, I check to see if there was any match. If there was a match, then I return the first element in the resulting array, as well as the barcode itself. If there was no match, then I simply return the barcode.</p>\n<p>These details are then published to Watson IoT.</p>\n<h3 id=\"nodejsonibmbluemix\">Node.js on IBM Bluemix</h3>\n<p>You may or may not have heard of <a href=\"https://www.cloudfoundry.org/\">Cloud Foundry</a>. Cloud Foundry is an open architecture for cloud applications. IBM is a Cloud Foundry provider (via Bluemix), and you can quickly and easily deploy applications running everything from Java, to ASP.NET, and Node.js.</p>\n<p>I created a Node.js application instance in Bluemix for this example. The Node.js application serves several purposes.</p>\n<p>First, it listens for barcode messages from Watson IoT. There is a <a href=\"http://www.eclipse.org/paho/\">browser implementation</a> of MQTT in JavaScript, but then I would have to provide credentials in the open.</p>\n<pre><code>// Watson IoT\nvar iot = mqtt.connect( config.iot_host, {\n  clientId: config.iot_client,\n  username: config.iot_user,\n  password: config.iot_password\n} );\n\n// Connected to broker\niot.on( 'connect', function() {\n  // Debug\n  console.log( 'Connected to Watson.' );\n\n  // Subscribe to barcode scans\n  iot.subscribe( config.iot_topic );  \n} );    \n\n// Message from broker\niot.on( 'message', function( topic, message ) {\n  var data = null;\n\n  // Buffer to String\n  data = message.toString();\n\n  // Debug\n  console.log( data );\n\n  // Send to clients\n  for( var c = 0; c &lt; socket.clients.length; c++ ) {\n    if( socket.clients[c] != this ) {\n      socket.clients[c].send( data );\n    }\n  }\n} );\n</code></pre>\n<p>To get around this, the Node.js application also hosts a <a href=\"http://caniuse.com/#feat=websockets\">WebSocket</a> server. This means that any platform that can communicate over WebSocket (which include iOS, Android, and many others), can securely listen for barcode messages.</p>\n<pre><code>// Sockets\nvar server = http.createServer();\nvar socket = new ws.Server( {\n  server: server\n} );\n\n// Client connected\nsocket.on( 'connection', function( client ) {\n  // Debug\n  console.log( 'WebSocket connection.' );\n} );\n</code></pre>\n<blockquote>\n<p>I use the same MQTT package on the Tessel as I use for the server application - JavaScript FTW!</p>\n</blockquote>\n<p>Finally, the Node.js application runs <a href=\"http://expressjs.com/\">Express</a> for a web server.</p>\n<pre><code>// Web\nvar app = express();\n\n// Static for main files\napp.use( '/', express.static( 'public' ) );\n\n// Cloud Foundry support\n// Bluemix\nvar env = cfenv.getAppEnv();\n\n// Listen\nserver.on( 'request', app );\nserver.listen( env.port, '0.0.0.0', function() {\n  // Debug\n  console.log( 'Started on: ' + env.port );\n} );\n</code></pre>\n<h3 id=\"browserdisplay\">Browser Display</h3>\n<p>The last part of this project then is to display the barcode, and product name if any, in the browser. When the browser application starts, it connects to the Node.js application via WebSocket. When a message arrives at the server, and is passed to the client (browser), the details are displayed.</p>\n<pre><code>var Barcode = ( function() {\n\n  var display = null;\n  var socket = null;\n\n  var doSocketMessage = function( evt ) {\n    var data = null;\n\n    // Parse\n    data = JSON.parse( evt.data );\n\n    // Debug    \n    console.log( data );\n\n    // Populate display\n    // Name or raw barcode\n    if( data.product ) {\n      display.innerHTML = data.product;\n    } else {\n      display.innerHTML = data.barcode;\n    }\n  };\n\n  // Connected\n  var doSocketOpen = function() {\n    console.log( 'Socket open.' );\n  };\n\n  // **\n  // Initialize\n  // **\n  \n  // References\n  display = document.querySelector( '.display' );\n\n  // Socket\n  socket = new WebSocket( 'ws://' + window.location.host );\n  socket.addEventListener( 'open', doSocketOpen );\n  socket.addEventListener( 'message', doSocketMessage );\n\n  // Reveal\n  return {\n\n  };\n  \n} )();\n</code></pre>\n<blockquote>\n<p>I like to use the &quot;reveal pattern&quot; in my JavaScript to keep my application properties from polluting the global namespace.</p>\n</blockquote>\n<p>This code will execute as soon as loaded, and will immediately create a WebSocket connection from the browser, back to the originating server (not a requirement for WebSocket, BTW). It then adds an event listener for any messages coming from the server. When a message arrives, it is parsed, and the content is displayed on the screen.</p>\n<h3 id=\"nextsteps\">Next Steps</h3>\n<p>Now we can scan a barcode using our Tessel, lookup any product specifics from a third-party database, pass that information to a Node.js server, and then push that information to a connected client via WebSocket. All the parts are decoupled from one another thanks to the publish-subscribe pattern, and the whole process executes in a fraction of a second.</p>\n<p>From here you might establish routes in the Node.js server application to handle additional shopping cart actions - perhaps mapping to the the Google Content API for Shopping. You can even reuse the WebSocket for removing items from the client, and propagating that back into an order database.</p>\n<!--kg-card-end: markdown-->","comment_id":"64","plaintext":"In my previous post, I show how you can connect a barcode scanner to a USB port\non the Tessel, and read the scanned values. While that is interesting, what is\nmore interesting is in doing something with that value. In this post we will\ntake a look at finding a related product, and then communicating that\ninformation over Watson IoT.\n\nThe Barcode Wild West\nBarcodes are one of those addictive pieces of technology. Simple stripes,\nprinted on a piece of paper, become digits. Amazing! And barcodes are\neverywhere. Once your scanner is connected to your Tessel, if you are like me,\nyou will find yourself running around your house looking for things to scan.\n\nIn some cases, such as a barcode on Netflix DVD envelope, there is not much to\ndo with this information. It is likely some internal routing number. Most\ncommercial products, like things you buy at the grocery store, also have\nbarcodes, and these uniquely identify the item in a database.\n\nWhat is more is that these barcodes are often unique for that product on a\nglobal (or at least national) scale. The barcode on a bag of Cheetos for\nexample, needs to tell the POS (point of sale) system that it is a bag of\nCheetos regardless as to where you are shopping. This means that there must be a\nuniversal database of these codes and their product mappings.\n\nNot so fast, cowboy!\n\nDatabases like this do exist, but they are generally expensive - mainly because\nthey are expensive to maintain. Companies like Amazon, who you know has an\nextensive database of barcode mappings, provide lookup services, but they put\ntight restrictions around their usage. You did not expect them to let that\ncostly database go for free did you?\n\nOver the years, many a crowd-sourced database has come and gone. It turns out it\nis really hard to make a business on barcode mappings. Which is yet another\nreason why the databases tend to be so expensive. Some databases also only cater\nto specific categories like electronics, or music CDs (what is that?).\n\n> Another business model around barcode databases is APIs around helping you\nmanage your own inventory, taxes, shipping, etc. The Google Content API for\nShopping [https://developers.google.com/shopping-content/v2/quickstart] is an\nexample of this approach.\n\n\nThe option I will be using for this demonstration is a crowdsourced offering\nfrom ProductLayer [https://developer.productlayer.com/documentation?version=0.5]\n. ProductLayer has a commercial side called Prod.ly [https://prod.ly/home]. I\nlike to think of Prod.ly as a social network for people that geek out about\nscanning barcodes. While the database is not as extensive as Amazon, it is an\neasy-to-use starting point.\n\nUsing ProductLayer\nTo use the barcode database from ProductLayer, you first have to create a free\naccount. From there you can create an application, which will result in an API\nkey. This API key is used in the HTTP header when making requests against the\nProductLayer services.\n\n// Look up product detail\nrequest( {\n  uri: config.prodly_url + '?gtin=' + barcode,\n  headers: {\n    'API-KEY': config.prodly_key\n  }\n}, function( error, response, body ) {\n  var data = null;\n  var message = null;\n\n  // Parse\n  data = JSON.parse( body );\n\n  // Debug\n  console.log( data[0]['pl-prod-name'];\n} );\n\n\nThe ProductLayer service provides a GET endpoint to query their barcode\ndatabase. When you run the query you can get multiple responses (JSON array) as\nsometimes barcodes map to multiple items. For this application I am going to\ntake the first item returned and pass that to Watson IoT.\n\n> I do the product lookup on the Tessel because that is where it feels most\nnatural. The Tessel has fast and stable wireless access, plenty of memory and\nCPU, and is where the physical action is taking place.\n\n\nWatson IoT\nI have covered using the Tessel with Watson IoT in a previous post\n[http://www.kevinhoyt.com/2016/09/01/tessel-on-watson-iot/], so I will not\nbelabor the point here other than a high level review. You can get access to \nWatson IoT\n[https://console.ng.bluemix.net/catalog/services/internet-of-things-platform/] \nfrom IBM Bluemix [https://console.ng.bluemix.net/].\n\nWatson IoT, at its core is an MQTT [https://en.wikipedia.org/wiki/MQTT] broker.\nCreated at IBM, MQTT is a common pub-sub protocol for IoT devices. There are\nseveral Node.js implementations of MQTT in both client and broker form. I will\nuse \"MQTT.js [https://github.com/mqttjs/MQTT.js]\" for this project.\n\nvar mqtt = require( 'mqtt' );\n\n// Connect to Watson IoT\nvar client = mqtt.connect( \n  config.iot_host, \n  {\n    clientId: config.iot_client,\n    username: config.iot_user,\n    password: config.iot_password,\n    port: config.iot_port\n  }\n);    \n    \nclient.on( 'connect', function() {\n  // Debug\n  console.log( 'IoT connected.' );\n} );\n\n\nThe first step is to connect the Tessel to Watson IoT. Once connected, we will\nsimply let it sit there waiting to be used. MQTT.js is robust enough to handle\nreconnecting should there be a network problem or client conflict.\n\nvar data = null;\nvar message = null;\n\n// Parse\ndata = JSON.parse( body );\n\n// Default message\nmessage = {\n  barcode: barcode\n};\n\n// Look for match\nif( data.length > 0 ) {\n  message.product = data[0]['pl-prod-name'];\n}\n\n// Debug\n// Product\nconsole.log( message.product );\n\n// Publish to Watson IoT\nclient.publish( \n  config.iot_topic, \n  JSON.stringify( message ) \n);  \n\n\nWhen a barcode value is read by Node Seriaport, and the lookup against\nProductLayer is complete, I check to see if there was any match. If there was a\nmatch, then I return the first element in the resulting array, as well as the\nbarcode itself. If there was no match, then I simply return the barcode.\n\nThese details are then published to Watson IoT.\n\nNode.js on IBM Bluemix\nYou may or may not have heard of Cloud Foundry [https://www.cloudfoundry.org/].\nCloud Foundry is an open architecture for cloud applications. IBM is a Cloud\nFoundry provider (via Bluemix), and you can quickly and easily deploy\napplications running everything from Java, to ASP.NET, and Node.js.\n\nI created a Node.js application instance in Bluemix for this example. The\nNode.js application serves several purposes.\n\nFirst, it listens for barcode messages from Watson IoT. There is a browser\nimplementation [http://www.eclipse.org/paho/] of MQTT in JavaScript, but then I\nwould have to provide credentials in the open.\n\n// Watson IoT\nvar iot = mqtt.connect( config.iot_host, {\n  clientId: config.iot_client,\n  username: config.iot_user,\n  password: config.iot_password\n} );\n\n// Connected to broker\niot.on( 'connect', function() {\n  // Debug\n  console.log( 'Connected to Watson.' );\n\n  // Subscribe to barcode scans\n  iot.subscribe( config.iot_topic );  \n} );    \n\n// Message from broker\niot.on( 'message', function( topic, message ) {\n  var data = null;\n\n  // Buffer to String\n  data = message.toString();\n\n  // Debug\n  console.log( data );\n\n  // Send to clients\n  for( var c = 0; c < socket.clients.length; c++ ) {\n    if( socket.clients[c] != this ) {\n      socket.clients[c].send( data );\n    }\n  }\n} );\n\n\nTo get around this, the Node.js application also hosts a WebSocket\n[http://caniuse.com/#feat=websockets] server. This means that any platform that\ncan communicate over WebSocket (which include iOS, Android, and many others),\ncan securely listen for barcode messages.\n\n// Sockets\nvar server = http.createServer();\nvar socket = new ws.Server( {\n  server: server\n} );\n\n// Client connected\nsocket.on( 'connection', function( client ) {\n  // Debug\n  console.log( 'WebSocket connection.' );\n} );\n\n\n> I use the same MQTT package on the Tessel as I use for the server application -\nJavaScript FTW!\n\n\nFinally, the Node.js application runs Express [http://expressjs.com/] for a web\nserver.\n\n// Web\nvar app = express();\n\n// Static for main files\napp.use( '/', express.static( 'public' ) );\n\n// Cloud Foundry support\n// Bluemix\nvar env = cfenv.getAppEnv();\n\n// Listen\nserver.on( 'request', app );\nserver.listen( env.port, '0.0.0.0', function() {\n  // Debug\n  console.log( 'Started on: ' + env.port );\n} );\n\n\nBrowser Display\nThe last part of this project then is to display the barcode, and product name\nif any, in the browser. When the browser application starts, it connects to the\nNode.js application via WebSocket. When a message arrives at the server, and is\npassed to the client (browser), the details are displayed.\n\nvar Barcode = ( function() {\n\n  var display = null;\n  var socket = null;\n\n  var doSocketMessage = function( evt ) {\n    var data = null;\n\n    // Parse\n    data = JSON.parse( evt.data );\n\n    // Debug    \n    console.log( data );\n\n    // Populate display\n    // Name or raw barcode\n    if( data.product ) {\n      display.innerHTML = data.product;\n    } else {\n      display.innerHTML = data.barcode;\n    }\n  };\n\n  // Connected\n  var doSocketOpen = function() {\n    console.log( 'Socket open.' );\n  };\n\n  // **\n  // Initialize\n  // **\n  \n  // References\n  display = document.querySelector( '.display' );\n\n  // Socket\n  socket = new WebSocket( 'ws://' + window.location.host );\n  socket.addEventListener( 'open', doSocketOpen );\n  socket.addEventListener( 'message', doSocketMessage );\n\n  // Reveal\n  return {\n\n  };\n  \n} )();\n\n\n> I like to use the \"reveal pattern\" in my JavaScript to keep my application\nproperties from polluting the global namespace.\n\n\nThis code will execute as soon as loaded, and will immediately create a\nWebSocket connection from the browser, back to the originating server (not a\nrequirement for WebSocket, BTW). It then adds an event listener for any messages\ncoming from the server. When a message arrives, it is parsed, and the content is\ndisplayed on the screen.\n\nNext Steps\nNow we can scan a barcode using our Tessel, lookup any product specifics from a\nthird-party database, pass that information to a Node.js server, and then push\nthat information to a connected client via WebSocket. All the parts are\ndecoupled from one another thanks to the publish-subscribe pattern, and the\nwhole process executes in a fraction of a second.\n\nFrom here you might establish routes in the Node.js server application to handle\nadditional shopping cart actions - perhaps mapping to the the Google Content API\nfor Shopping. You can even reuse the WebSocket for removing items from the\nclient, and propagating that back into an order database.","feature_image":"__GHOST_URL__/content/images/2019/01/shopping.carts.jpg","featured":0,"status":"published","locale":null,"visibility":"public","author_id":"1","created_at":"2016-10-22T08:16:50.000Z","updated_at":"2019-01-17T00:45:16.000Z","published_at":"2016-11-09T18:24:56.000Z","custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"type":"post","email_recipient_filter":"none"},{"id":"5adb8922351ffe0018a5774a","uuid":"95d193b7-fbf7-4c1d-8b23-eee78764dac3","title":"Everyday Carry","slug":"edc","mobiledoc":"{\"version\":\"0.3.1\",\"markups\":[],\"atoms\":[],\"cards\":[[\"markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\"[Apple 15\\\" MacBook Pro](http://www.apple.com/macbook-pro/)\\n\\nDespite sometimes leapfrogging the market, Apple makes a great development machine. The battery will get me through most of a days worth of meetings, while the metal body holds up to my regular commutes. My big hands prefer the 15\\\" model for the extra space.\\n\\n[Apple iPad](http://www.apple.com/ipad-air-2/)\\n\\nMy first iPad, the first iPad, was left behind in an airplane seat pocket, and I was never able to recover it. I got the next version when it was released, and use it almost daily. The 31-pin connector is starting to fail these days, so I may have to upgrade before too long. I use my tablet mostly for reading, with a couple of go-to games.\\n\\n[Amazon Kindle](https://kindle.amazon.com/)\\n\\nWhile the iPad serves as my main reading device, there are ample times when the brightness of the screen is something I want to avoid. The Kindle fills that gap, letting me read in bed without the side-effects of blue light keeping me awake. I mostly read non-fiction. Content usually revolves around business, economics, and theology.\\n\\n[Apple iPhone 5s](http://www.apple.com/iphone-se/)\\n\\nYup, still using an iPhone 5s. I just have not found the need to upgrade, and I am not one to \\\"keep up with the Jones'\\\". The battery still lasts a day, sometimes two days, but it getting less reliable. An upgrade may be in my future. My iPhone is mostly used for travel applications, staying current on Slack and email, and occasionally an quick scan of Facebook and Twitter.\\n\\n[Apple Watch](http://www.apple.com/watch/)\\n\\nThis upgrade was afforded me by my employer, IBM, who subsidized the purchase. Since my iPhone does not support Apple Pay, the Apple Watch quickly found a permanent place on my wrist. Outside of payments, the quick glance access at notifications proves amazingly convenient. When I go running, the Watch proves a reliable sidekick to keep my pace.\\n\\nPrevious: [Garmin Tactix](https://buy.garmin.com/en-US/US/p/139389)\\n\\nAt 6'7\\\" I am a big guy, and as I travel frequently, I can be very hard on my watches. The Garmin Tactix is a tank of a watch. Great for when I go running, but useful for geocaching, and marking waypoints for interesting travels. Imagine my surprise to find [Peter Lubbers](https://twitter.com/peterlubbers) wears a Tactix as well - the only other person I have seen sporting this military-grade gear.\\n\\n[Timbuk2 Commute Messenger Bag](http://www.timbuk2.com/commute-messenger-bag/208.html)\\n\\nI have been a fan of the messenger style bags for years, and Timbuk2 has been my go-to brand for durability. I like the Commute Messenger because there is a back panel that safely holds my 15\\\" MacBook Pro, iPad, and Kindle, and can be unzipped to lay flat for airport security. No removing my laptop in the security line.\\n\\n\"}]],\"sections\":[[10,0]],\"ghostVersion\":\"3.0\"}","html":"<!--kg-card-begin: markdown--><p><a href=\"http://www.apple.com/macbook-pro/\">Apple 15&quot; MacBook Pro</a></p>\n<p>Despite sometimes leapfrogging the market, Apple makes a great development machine. The battery will get me through most of a days worth of meetings, while the metal body holds up to my regular commutes. My big hands prefer the 15&quot; model for the extra space.</p>\n<p><a href=\"http://www.apple.com/ipad-air-2/\">Apple iPad</a></p>\n<p>My first iPad, the first iPad, was left behind in an airplane seat pocket, and I was never able to recover it. I got the next version when it was released, and use it almost daily. The 31-pin connector is starting to fail these days, so I may have to upgrade before too long. I use my tablet mostly for reading, with a couple of go-to games.</p>\n<p><a href=\"https://kindle.amazon.com/\">Amazon Kindle</a></p>\n<p>While the iPad serves as my main reading device, there are ample times when the brightness of the screen is something I want to avoid. The Kindle fills that gap, letting me read in bed without the side-effects of blue light keeping me awake. I mostly read non-fiction. Content usually revolves around business, economics, and theology.</p>\n<p><a href=\"http://www.apple.com/iphone-se/\">Apple iPhone 5s</a></p>\n<p>Yup, still using an iPhone 5s. I just have not found the need to upgrade, and I am not one to &quot;keep up with the Jones'&quot;. The battery still lasts a day, sometimes two days, but it getting less reliable. An upgrade may be in my future. My iPhone is mostly used for travel applications, staying current on Slack and email, and occasionally an quick scan of Facebook and Twitter.</p>\n<p><a href=\"http://www.apple.com/watch/\">Apple Watch</a></p>\n<p>This upgrade was afforded me by my employer, IBM, who subsidized the purchase. Since my iPhone does not support Apple Pay, the Apple Watch quickly found a permanent place on my wrist. Outside of payments, the quick glance access at notifications proves amazingly convenient. When I go running, the Watch proves a reliable sidekick to keep my pace.</p>\n<p>Previous: <a href=\"https://buy.garmin.com/en-US/US/p/139389\">Garmin Tactix</a></p>\n<p>At 6'7&quot; I am a big guy, and as I travel frequently, I can be very hard on my watches. The Garmin Tactix is a tank of a watch. Great for when I go running, but useful for geocaching, and marking waypoints for interesting travels. Imagine my surprise to find <a href=\"https://twitter.com/peterlubbers\">Peter Lubbers</a> wears a Tactix as well - the only other person I have seen sporting this military-grade gear.</p>\n<p><a href=\"http://www.timbuk2.com/commute-messenger-bag/208.html\">Timbuk2 Commute Messenger Bag</a></p>\n<p>I have been a fan of the messenger style bags for years, and Timbuk2 has been my go-to brand for durability. I like the Commute Messenger because there is a back panel that safely holds my 15&quot; MacBook Pro, iPad, and Kindle, and can be unzipped to lay flat for airport security. No removing my laptop in the security line.</p>\n<!--kg-card-end: markdown-->","comment_id":"65","plaintext":"Apple 15\" MacBook Pro [http://www.apple.com/macbook-pro/]\n\nDespite sometimes leapfrogging the market, Apple makes a great development\nmachine. The battery will get me through most of a days worth of meetings, while\nthe metal body holds up to my regular commutes. My big hands prefer the 15\"\nmodel for the extra space.\n\nApple iPad [http://www.apple.com/ipad-air-2/]\n\nMy first iPad, the first iPad, was left behind in an airplane seat pocket, and I\nwas never able to recover it. I got the next version when it was released, and\nuse it almost daily. The 31-pin connector is starting to fail these days, so I\nmay have to upgrade before too long. I use my tablet mostly for reading, with a\ncouple of go-to games.\n\nAmazon Kindle [https://kindle.amazon.com/]\n\nWhile the iPad serves as my main reading device, there are ample times when the\nbrightness of the screen is something I want to avoid. The Kindle fills that\ngap, letting me read in bed without the side-effects of blue light keeping me\nawake. I mostly read non-fiction. Content usually revolves around business,\neconomics, and theology.\n\nApple iPhone 5s [http://www.apple.com/iphone-se/]\n\nYup, still using an iPhone 5s. I just have not found the need to upgrade, and I\nam not one to \"keep up with the Jones'\". The battery still lasts a day,\nsometimes two days, but it getting less reliable. An upgrade may be in my\nfuture. My iPhone is mostly used for travel applications, staying current on\nSlack and email, and occasionally an quick scan of Facebook and Twitter.\n\nApple Watch [http://www.apple.com/watch/]\n\nThis upgrade was afforded me by my employer, IBM, who subsidized the purchase.\nSince my iPhone does not support Apple Pay, the Apple Watch quickly found a\npermanent place on my wrist. Outside of payments, the quick glance access at\nnotifications proves amazingly convenient. When I go running, the Watch proves a\nreliable sidekick to keep my pace.\n\nPrevious: Garmin Tactix [https://buy.garmin.com/en-US/US/p/139389]\n\nAt 6'7\" I am a big guy, and as I travel frequently, I can be very hard on my\nwatches. The Garmin Tactix is a tank of a watch. Great for when I go running,\nbut useful for geocaching, and marking waypoints for interesting travels.\nImagine my surprise to find Peter Lubbers [https://twitter.com/peterlubbers] \nwears a Tactix as well - the only other person I have seen sporting this\nmilitary-grade gear.\n\nTimbuk2 Commute Messenger Bag\n[http://www.timbuk2.com/commute-messenger-bag/208.html]\n\nI have been a fan of the messenger style bags for years, and Timbuk2 has been my\ngo-to brand for durability. I like the Commute Messenger because there is a back\npanel that safely holds my 15\" MacBook Pro, iPad, and Kindle, and can be\nunzipped to lay flat for airport security. No removing my laptop in the security\nline.","feature_image":"http://images.kevinhoyt.com/sunset.plane.png","featured":0,"status":"published","locale":null,"visibility":"public","author_id":"1","created_at":"2017-01-26T03:15:58.000Z","updated_at":"2017-01-26T04:05:23.000Z","published_at":"2017-01-26T03:16:00.000Z","custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"type":"page","email_recipient_filter":"none"},{"id":"5adb8922351ffe0018a5774b","uuid":"8696ab65-224b-4cc3-8387-f21d06e693b3","title":"Web Bluetooth Bean","slug":"web-bluetooth-bean","mobiledoc":"{\"version\":\"0.3.1\",\"markups\":[],\"atoms\":[],\"cards\":[[\"markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\"Bluetooth has been around for a while now, but it has always been a feature for native applications. The Web Bluetooth specification has been brewing for a while, lived under a flag for some time, and is now starting to land in desktop browsers. I figured it was time to give this feature a trial run.\\n\\n###The Magical Fruit\\n\\nMy favorite ==Bluetooth device== is the [Light Blue Bean](https://punchthrough.com/bean) (and Bean+) from Punch Through. The original Bean runs off a coin cell, and has a breadboard directly integrated into the PCB. The Bean+ gets a bit larger, but has a rechargeable LiPo battery, and female headers for prototyping right out of the box.\\n\\n![The Light Blue Bean by Punch Through](http://images.kevinhoyt.com/punch.through.beans.png)\\n\\nBoth Beans feature not only a Bluetooth module, but also an ==ATmega328== - in other words, an Arduino. They are programmable ==over the air==, use (mostly) the ==Arduino workflow==, and can even run as ==iBeacon== devices for indoor location and other applications.\\n\\n###Sketchy\\n\\nThe Bean workflow adds to the Arduino tooling to provide Bean-specific functionality. You can leverage the ==built-in accelerometer, temperature sensor, fuel gauge, and RGB LED== from the Bean API. The Bean API also abstracts all the low-level complexities of working with Bluetooth devices. \\n\\nOne of the operating modes for BLE allows devices to ==pair without any of the wonkiness== of Bluetooth from earlier versions. To do this, the BLE stack arranges a pretty sophisticated contract. If you have seen my earlier post on BLE and iOS, you know there are a lot of steps. The Bean simplifies this down to Bean.setScratchData( slot, uint_array, length ).\\n\\n```\\n// Setup\\nvoid setup() {\\n  // Debug\\n  Serial.begin( 9600 );\\n\\n  // Deep sleep until needed\\n  Bean.enableWakeOnConnect( true );\\n}\\n\\n// Loop\\nvoid loop() {\\n  // Connected to a device\\n  if( Bean.getConnectionState() ) {\\n    // Set sensor values\\n    // Read LED color\\n    output();\\n    input();\\n    \\n    // Any desired delay (ms)\\n    // Bean.sleep( 1000 );    \\n  } else {\\n    // Nobody connected\\n    // Turn of LED\\n    // Deep sleep\\n    Bean.setLed( 0, 0, 0 );\\n    Bean.sleep( 0xFFFFFFFF );   \\n  }\\n}\\n```\\n\\nIn this sketch we use the ==Bean.enableWakeOnConnect( true )== method to tell the Bean to sit in a deep sleep if no device is connected. From there the sketch checks to see if anything is connected, and if so, will read sensor values, and make them available do BLE devices, and then check to see if a new RGB LED color is desired. If no device is connected, the Bean will go back to sleep.\\n\\n```\\n// Sensor values on characteristic\\nvoid output() {\\n  AccelerationReading acceleration;\\n  char content[20];\\n  uint8_t scratch[20];\\n\\n  // Read accelerometer\\n  acceleration = Bean.getAcceleration();\\n\\n  // Format values as string\\n  sprintf(\\n    content,\\n    \\\"%d,%d,%d,%d\\\",\\n    acceleration.xAxis,\\n    acceleration.yAxis,\\n    acceleration.zAxis,\\n    Bean.getTemperature()\\n  );\\n\\n  // Debug\\n  Serial.println( content );\\n\\n  // Put string into scratch format\\n  for( int i = 0; i < strlen( content ); i++ ) {\\n    scratch[i] = content[i];\\n  }\\n\\n  // Set scratch data for various sensors\\n  Bean.setScratchData( 1, scratch, strlen( content ) );\\n}\\n```\\n\\nScratch data is limited (by BLE, not the Bean) to ==20 bytes==. In this case, X, Y, and Z, axis will be a maximum of 4-bytes each, and two commas for a CSV format. That is 14-bytes so far. One more comma, and up to two bytes for the temperature reading (in Celcius). That adds up to 17-bytes. Any more data, and we would have to use additional scratch data slots.\\n\\n> I am formatting this data in CSV because accelerometer data is a signed integer, and the scratch data is unsigned. I could shift the bits, but dealing with the string (character array) is easier for me.\\n\\n```\\n// Desired LED color\\nvoid input() {\\n  ScratchData scratch;\\n  String content;\\n  int comma;\\n\\n  // Get scratch data for LED\\n  scratch = Bean.readScratchData( 2 );\\n\\n  // Set respective values\\n  // Easy inside uint range\\n  Bean.setLed(\\n    scratch.data[0],\\n    scratch.data[1],\\n    scratch.data[2]\\n  );\\n}\\n```\\n\\nI also ==read the scratch data== to see if there is a desired change to the color of the on-board RGB LED. Technically, I set the color with every loop iteration. If no new scratch data has been set, then we just read what was there and set the color again. Conveniently, the ==Bean.setLed( red, green, blue )== method uses unsigned integer values, so getting from the scratch data to the RGB values is as easy as reading elements from an array.\\n\\n###Web Bluetooth\\n\\nBecause there is such a large number of steps necessary to connect to a BLE device, trying to do this with callbacks would be a nightmare. To remedy this, ==the Web Bluetooth specification leans heavily on Promises==. If you are not used to Promises, then this will take some getting used to using.\\n\\n```\\n// Start the connection process\\n// Look for my specific Bean+\\nnavigator.bluetooth.requestDevice( { \\n  filters: [\\n    {name: Bean.NAME}\\n  ],\\n  optionalServices: [Bean.SERVICE] \\n} )\\n.then( device => {\\n  // Found device\\n  // User selected to pair with device\\n  // Browser now paired with device\\n  // Connect to attribute server\\n  this.bluetooth = device;\\n  this.bluetooth.addEventListener(\\n    'gattserverdisconnected', \\n    evt => this.doDisconnected( evt ) \\n  );\\n  return this.bluetooth.gatt.connect() \\n} )\\n.then( server => server.getPrimaryService( Bean.SERVICE ) )\\n.then( service => {\\n  this.service = service;\\n  return this.service.getCharacteristic( Bean.SENSORS );\\n} )\\n.then( characteristic => {\\n  // Connected to server\\n  // Connected to specific service\\n  // Retrieved a list of characteristics\\n\\n  // This is a good place to update UI\\n\\n  // Listen for characteristic changes\\n  return characteristic.startNotifications();\\n} )\\n.then( characteristic => {\\n  // Characteristic change listener\\n  characteristic.addEventListener(\\n    'characteristicvaluechanged',\\n    evt => this.doCharacteristicChanged( evt )\\n  );\\n} )\\n.then( service => {\\n  return this.service.getCharacteristic( Bean.RGB_LED );\\n} )\\n.then( characteristic => {\\n  this.rgbled = characteristic;\\n} )\\n.catch( error => { \\n  console.log( error ); \\n} ); \\n```\\n\\nBelieve it or not, that is almost entirely boilerplate code. At the very start, I tell the Web Bluetooth API to look for my specific Bean, by name, and having a specific service UUID. About two-thirds of the way through, once we have started listening for characteristic data changes (new data in the specified scratch slot), we register a listener to process the data. I also keep track of the GATT service itself because I want to write to it later.\\n\\n```\\n// Scratch characteristic changed\\n// Parse content\\n// Store relevant accelerometer values\\ndoCharacteristicChanged( evt ) {\\n  let scratch = null;\\n  let content = null;\\n  let parts = null;\\n\\n  // Parse to CSV\\n  // Then to array of strings\\n  scratch = new Uint8Array( \\n    evt.target.value.buffer \\n  );\\n  content = String.fromCharCode.apply( \\n    String, \\n    scratch \\n  );\\n  parts = content.split( ',' );\\n\\n  // Get integer value for x-axis\\n  // Map to 180-degree total range\\n  this.angleX = parseInt( parts[0] );\\n  this.angleX = this.map( \\n    this.angleX, \\n    -270, \\n    270, \\n    -90, \\n    90 \\n  );\\n\\n  // Get integer value for y-axis\\n  // Map to 180-degree total range\\n  this.angleY = parseInt( parts[1] );\\n  this.angleY = this.map( \\n    this.angleY, \\n    -270, \\n    270, \\n    -90, \\n    90 \\n  );    \\n\\n  // Temperature in centigrade\\n  this.temperature = parseInt( parts[3] );\\n}\\n```\\n\\nThe inverse of what happens in the sketch - turning a character array into an unsigned integer array - happens here, then I split the data and parse as needed. For my application, I run a ==linear transform== to take the data from an accelerometer-specific value, to a range I can use in my application.\\n\\n```\\ndoSwatch( evt ) {\\n  let color = null;\\n  let index = null;\\n\\n  // Get index from selected color\\n  index = evt.target.getAttribute( 'data-index' );\\n\\n  // Map index to color options\\n  color = new Uint8Array( 3 );\\n  color[0] = this.rainbow[index].red;\\n  color[1] = this.rainbow[index].green;\\n  color[2] = this.rainbow[index].blue;\\n\\n  // Write to characteristic\\n  this.rgbled.writeValue( color );\\n\\n  // Show selected color\\n  // Hide swatches\\n  this.current.children[0].style.backgroundColor = evt.target.style.backgroundColor;\\n}\\n```\\n\\nAgain, using unsigned integers for the RGB colors, and the scratch data makes things easy. I then tell that specific Bluetooth service to write the new scratch data. That is what gets picked up in the sketch, and then the Bean API is called to set the color of the RGB LED.\\n\\n<iframe width=\\\"560\\\" height=\\\"315\\\" src=\\\"https://www.youtube.com/embed/-1IVtuCmQ9I\\\" frameborder=\\\"0\\\" allowfullscreen></iframe>\\n\\n###What's Next\\n\\nOne problem I have repeatedly encountered is that my Web application will ==stop receiving notifications==. This seems to happen most often when I write new RGB characteristic data. The problem existed before I added that feature however. It also seems worse if I ==sleep less== (or not at all in this case) on the Arduino.\\n\\nBecause of all the ==sandboxing== that happens both at the Web Bluetooth level, and the Bean API level, I cannot tell which one is causing the problem. My guess is the Web Bluetooth API as this works without problems on a native iOS application. But I am honestly just guessing there, and would love to have the working group take a look. But would that involve shipping them a Bean? Weird problems in IoT land.\\n\"}]],\"sections\":[[10,0]],\"ghostVersion\":\"3.0\"}","html":"<!--kg-card-begin: markdown--><p>Bluetooth has been around for a while now, but it has always been a feature for native applications. The Web Bluetooth specification has been brewing for a while, lived under a flag for some time, and is now starting to land in desktop browsers. I figured it was time to give this feature a trial run.</p>\n<h3 id=\"themagicalfruit\">The Magical Fruit</h3>\n<p>My favorite <mark>Bluetooth device</mark> is the <a href=\"https://punchthrough.com/bean\">Light Blue Bean</a> (and Bean+) from Punch Through. The original Bean runs off a coin cell, and has a breadboard directly integrated into the PCB. The Bean+ gets a bit larger, but has a rechargeable LiPo battery, and female headers for prototyping right out of the box.</p>\n<p><img src=\"http://images.kevinhoyt.com/punch.through.beans.png\" alt=\"The Light Blue Bean by Punch Through\" loading=\"lazy\"></p>\n<p>Both Beans feature not only a Bluetooth module, but also an <mark>ATmega328</mark> - in other words, an Arduino. They are programmable <mark>over the air</mark>, use (mostly) the <mark>Arduino workflow</mark>, and can even run as <mark>iBeacon</mark> devices for indoor location and other applications.</p>\n<h3 id=\"sketchy\">Sketchy</h3>\n<p>The Bean workflow adds to the Arduino tooling to provide Bean-specific functionality. You can leverage the <mark>built-in accelerometer, temperature sensor, fuel gauge, and RGB LED</mark> from the Bean API. The Bean API also abstracts all the low-level complexities of working with Bluetooth devices.</p>\n<p>One of the operating modes for BLE allows devices to <mark>pair without any of the wonkiness</mark> of Bluetooth from earlier versions. To do this, the BLE stack arranges a pretty sophisticated contract. If you have seen my earlier post on BLE and iOS, you know there are a lot of steps. The Bean simplifies this down to Bean.setScratchData( slot, uint_array, length ).</p>\n<pre><code>// Setup\nvoid setup() {\n  // Debug\n  Serial.begin( 9600 );\n\n  // Deep sleep until needed\n  Bean.enableWakeOnConnect( true );\n}\n\n// Loop\nvoid loop() {\n  // Connected to a device\n  if( Bean.getConnectionState() ) {\n    // Set sensor values\n    // Read LED color\n    output();\n    input();\n    \n    // Any desired delay (ms)\n    // Bean.sleep( 1000 );    \n  } else {\n    // Nobody connected\n    // Turn of LED\n    // Deep sleep\n    Bean.setLed( 0, 0, 0 );\n    Bean.sleep( 0xFFFFFFFF );   \n  }\n}\n</code></pre>\n<p>In this sketch we use the <mark>Bean.enableWakeOnConnect( true )</mark> method to tell the Bean to sit in a deep sleep if no device is connected. From there the sketch checks to see if anything is connected, and if so, will read sensor values, and make them available do BLE devices, and then check to see if a new RGB LED color is desired. If no device is connected, the Bean will go back to sleep.</p>\n<pre><code>// Sensor values on characteristic\nvoid output() {\n  AccelerationReading acceleration;\n  char content[20];\n  uint8_t scratch[20];\n\n  // Read accelerometer\n  acceleration = Bean.getAcceleration();\n\n  // Format values as string\n  sprintf(\n    content,\n    &quot;%d,%d,%d,%d&quot;,\n    acceleration.xAxis,\n    acceleration.yAxis,\n    acceleration.zAxis,\n    Bean.getTemperature()\n  );\n\n  // Debug\n  Serial.println( content );\n\n  // Put string into scratch format\n  for( int i = 0; i &lt; strlen( content ); i++ ) {\n    scratch[i] = content[i];\n  }\n\n  // Set scratch data for various sensors\n  Bean.setScratchData( 1, scratch, strlen( content ) );\n}\n</code></pre>\n<p>Scratch data is limited (by BLE, not the Bean) to <mark>20 bytes</mark>. In this case, X, Y, and Z, axis will be a maximum of 4-bytes each, and two commas for a CSV format. That is 14-bytes so far. One more comma, and up to two bytes for the temperature reading (in Celcius). That adds up to 17-bytes. Any more data, and we would have to use additional scratch data slots.</p>\n<blockquote>\n<p>I am formatting this data in CSV because accelerometer data is a signed integer, and the scratch data is unsigned. I could shift the bits, but dealing with the string (character array) is easier for me.</p>\n</blockquote>\n<pre><code>// Desired LED color\nvoid input() {\n  ScratchData scratch;\n  String content;\n  int comma;\n\n  // Get scratch data for LED\n  scratch = Bean.readScratchData( 2 );\n\n  // Set respective values\n  // Easy inside uint range\n  Bean.setLed(\n    scratch.data[0],\n    scratch.data[1],\n    scratch.data[2]\n  );\n}\n</code></pre>\n<p>I also <mark>read the scratch data</mark> to see if there is a desired change to the color of the on-board RGB LED. Technically, I set the color with every loop iteration. If no new scratch data has been set, then we just read what was there and set the color again. Conveniently, the <mark>Bean.setLed( red, green, blue )</mark> method uses unsigned integer values, so getting from the scratch data to the RGB values is as easy as reading elements from an array.</p>\n<h3 id=\"webbluetooth\">Web Bluetooth</h3>\n<p>Because there is such a large number of steps necessary to connect to a BLE device, trying to do this with callbacks would be a nightmare. To remedy this, <mark>the Web Bluetooth specification leans heavily on Promises</mark>. If you are not used to Promises, then this will take some getting used to using.</p>\n<pre><code>// Start the connection process\n// Look for my specific Bean+\nnavigator.bluetooth.requestDevice( { \n  filters: [\n    {name: Bean.NAME}\n  ],\n  optionalServices: [Bean.SERVICE] \n} )\n.then( device =&gt; {\n  // Found device\n  // User selected to pair with device\n  // Browser now paired with device\n  // Connect to attribute server\n  this.bluetooth = device;\n  this.bluetooth.addEventListener(\n    'gattserverdisconnected', \n    evt =&gt; this.doDisconnected( evt ) \n  );\n  return this.bluetooth.gatt.connect() \n} )\n.then( server =&gt; server.getPrimaryService( Bean.SERVICE ) )\n.then( service =&gt; {\n  this.service = service;\n  return this.service.getCharacteristic( Bean.SENSORS );\n} )\n.then( characteristic =&gt; {\n  // Connected to server\n  // Connected to specific service\n  // Retrieved a list of characteristics\n\n  // This is a good place to update UI\n\n  // Listen for characteristic changes\n  return characteristic.startNotifications();\n} )\n.then( characteristic =&gt; {\n  // Characteristic change listener\n  characteristic.addEventListener(\n    'characteristicvaluechanged',\n    evt =&gt; this.doCharacteristicChanged( evt )\n  );\n} )\n.then( service =&gt; {\n  return this.service.getCharacteristic( Bean.RGB_LED );\n} )\n.then( characteristic =&gt; {\n  this.rgbled = characteristic;\n} )\n.catch( error =&gt; { \n  console.log( error ); \n} ); \n</code></pre>\n<p>Believe it or not, that is almost entirely boilerplate code. At the very start, I tell the Web Bluetooth API to look for my specific Bean, by name, and having a specific service UUID. About two-thirds of the way through, once we have started listening for characteristic data changes (new data in the specified scratch slot), we register a listener to process the data. I also keep track of the GATT service itself because I want to write to it later.</p>\n<pre><code>// Scratch characteristic changed\n// Parse content\n// Store relevant accelerometer values\ndoCharacteristicChanged( evt ) {\n  let scratch = null;\n  let content = null;\n  let parts = null;\n\n  // Parse to CSV\n  // Then to array of strings\n  scratch = new Uint8Array( \n    evt.target.value.buffer \n  );\n  content = String.fromCharCode.apply( \n    String, \n    scratch \n  );\n  parts = content.split( ',' );\n\n  // Get integer value for x-axis\n  // Map to 180-degree total range\n  this.angleX = parseInt( parts[0] );\n  this.angleX = this.map( \n    this.angleX, \n    -270, \n    270, \n    -90, \n    90 \n  );\n\n  // Get integer value for y-axis\n  // Map to 180-degree total range\n  this.angleY = parseInt( parts[1] );\n  this.angleY = this.map( \n    this.angleY, \n    -270, \n    270, \n    -90, \n    90 \n  );    \n\n  // Temperature in centigrade\n  this.temperature = parseInt( parts[3] );\n}\n</code></pre>\n<p>The inverse of what happens in the sketch - turning a character array into an unsigned integer array - happens here, then I split the data and parse as needed. For my application, I run a <mark>linear transform</mark> to take the data from an accelerometer-specific value, to a range I can use in my application.</p>\n<pre><code>doSwatch( evt ) {\n  let color = null;\n  let index = null;\n\n  // Get index from selected color\n  index = evt.target.getAttribute( 'data-index' );\n\n  // Map index to color options\n  color = new Uint8Array( 3 );\n  color[0] = this.rainbow[index].red;\n  color[1] = this.rainbow[index].green;\n  color[2] = this.rainbow[index].blue;\n\n  // Write to characteristic\n  this.rgbled.writeValue( color );\n\n  // Show selected color\n  // Hide swatches\n  this.current.children[0].style.backgroundColor = evt.target.style.backgroundColor;\n}\n</code></pre>\n<p>Again, using unsigned integers for the RGB colors, and the scratch data makes things easy. I then tell that specific Bluetooth service to write the new scratch data. That is what gets picked up in the sketch, and then the Bean API is called to set the color of the RGB LED.</p>\n<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/-1IVtuCmQ9I\" frameborder=\"0\" allowfullscreen></iframe>\n<h3 id=\"whatsnext\">What's Next</h3>\n<p>One problem I have repeatedly encountered is that my Web application will <mark>stop receiving notifications</mark>. This seems to happen most often when I write new RGB characteristic data. The problem existed before I added that feature however. It also seems worse if I <mark>sleep less</mark> (or not at all in this case) on the Arduino.</p>\n<p>Because of all the <mark>sandboxing</mark> that happens both at the Web Bluetooth level, and the Bean API level, I cannot tell which one is causing the problem. My guess is the Web Bluetooth API as this works without problems on a native iOS application. But I am honestly just guessing there, and would love to have the working group take a look. But would that involve shipping them a Bean? Weird problems in IoT land.</p>\n<!--kg-card-end: markdown-->","comment_id":"66","plaintext":"Bluetooth has been around for a while now, but it has always been a feature for\nnative applications. The Web Bluetooth specification has been brewing for a\nwhile, lived under a flag for some time, and is now starting to land in desktop\nbrowsers. I figured it was time to give this feature a trial run.\n\nThe Magical Fruit\nMy favorite Bluetooth device is the Light Blue Bean\n[https://punchthrough.com/bean] (and Bean+) from Punch Through. The original\nBean runs off a coin cell, and has a breadboard directly integrated into the\nPCB. The Bean+ gets a bit larger, but has a rechargeable LiPo battery, and\nfemale headers for prototyping right out of the box.\n\n\n\nBoth Beans feature not only a Bluetooth module, but also an ATmega328 - in other\nwords, an Arduino. They are programmable over the air, use (mostly) the Arduino\nworkflow, and can even run as iBeacon devices for indoor location and other\napplications.\n\nSketchy\nThe Bean workflow adds to the Arduino tooling to provide Bean-specific\nfunctionality. You can leverage the built-in accelerometer, temperature sensor,\nfuel gauge, and RGB LED from the Bean API. The Bean API also abstracts all the\nlow-level complexities of working with Bluetooth devices.\n\nOne of the operating modes for BLE allows devices to pair without any of the\nwonkiness of Bluetooth from earlier versions. To do this, the BLE stack arranges\na pretty sophisticated contract. If you have seen my earlier post on BLE and\niOS, you know there are a lot of steps. The Bean simplifies this down to\nBean.setScratchData( slot, uint_array, length ).\n\n// Setup\nvoid setup() {\n  // Debug\n  Serial.begin( 9600 );\n\n  // Deep sleep until needed\n  Bean.enableWakeOnConnect( true );\n}\n\n// Loop\nvoid loop() {\n  // Connected to a device\n  if( Bean.getConnectionState() ) {\n    // Set sensor values\n    // Read LED color\n    output();\n    input();\n    \n    // Any desired delay (ms)\n    // Bean.sleep( 1000 );    \n  } else {\n    // Nobody connected\n    // Turn of LED\n    // Deep sleep\n    Bean.setLed( 0, 0, 0 );\n    Bean.sleep( 0xFFFFFFFF );   \n  }\n}\n\n\nIn this sketch we use the Bean.enableWakeOnConnect( true ) method to tell the\nBean to sit in a deep sleep if no device is connected. From there the sketch\nchecks to see if anything is connected, and if so, will read sensor values, and\nmake them available do BLE devices, and then check to see if a new RGB LED color\nis desired. If no device is connected, the Bean will go back to sleep.\n\n// Sensor values on characteristic\nvoid output() {\n  AccelerationReading acceleration;\n  char content[20];\n  uint8_t scratch[20];\n\n  // Read accelerometer\n  acceleration = Bean.getAcceleration();\n\n  // Format values as string\n  sprintf(\n    content,\n    \"%d,%d,%d,%d\",\n    acceleration.xAxis,\n    acceleration.yAxis,\n    acceleration.zAxis,\n    Bean.getTemperature()\n  );\n\n  // Debug\n  Serial.println( content );\n\n  // Put string into scratch format\n  for( int i = 0; i < strlen( content ); i++ ) {\n    scratch[i] = content[i];\n  }\n\n  // Set scratch data for various sensors\n  Bean.setScratchData( 1, scratch, strlen( content ) );\n}\n\n\nScratch data is limited (by BLE, not the Bean) to 20 bytes. In this case, X, Y,\nand Z, axis will be a maximum of 4-bytes each, and two commas for a CSV format.\nThat is 14-bytes so far. One more comma, and up to two bytes for the temperature\nreading (in Celcius). That adds up to 17-bytes. Any more data, and we would have\nto use additional scratch data slots.\n\n> I am formatting this data in CSV because accelerometer data is a signed integer,\nand the scratch data is unsigned. I could shift the bits, but dealing with the\nstring (character array) is easier for me.\n\n\n// Desired LED color\nvoid input() {\n  ScratchData scratch;\n  String content;\n  int comma;\n\n  // Get scratch data for LED\n  scratch = Bean.readScratchData( 2 );\n\n  // Set respective values\n  // Easy inside uint range\n  Bean.setLed(\n    scratch.data[0],\n    scratch.data[1],\n    scratch.data[2]\n  );\n}\n\n\nI also read the scratch data to see if there is a desired change to the color of\nthe on-board RGB LED. Technically, I set the color with every loop iteration. If\nno new scratch data has been set, then we just read what was there and set the\ncolor again. Conveniently, the Bean.setLed( red, green, blue ) method uses\nunsigned integer values, so getting from the scratch data to the RGB values is\nas easy as reading elements from an array.\n\nWeb Bluetooth\nBecause there is such a large number of steps necessary to connect to a BLE\ndevice, trying to do this with callbacks would be a nightmare. To remedy this, \nthe Web Bluetooth specification leans heavily on Promises. If you are not used\nto Promises, then this will take some getting used to using.\n\n// Start the connection process\n// Look for my specific Bean+\nnavigator.bluetooth.requestDevice( { \n  filters: [\n    {name: Bean.NAME}\n  ],\n  optionalServices: [Bean.SERVICE] \n} )\n.then( device => {\n  // Found device\n  // User selected to pair with device\n  // Browser now paired with device\n  // Connect to attribute server\n  this.bluetooth = device;\n  this.bluetooth.addEventListener(\n    'gattserverdisconnected', \n    evt => this.doDisconnected( evt ) \n  );\n  return this.bluetooth.gatt.connect() \n} )\n.then( server => server.getPrimaryService( Bean.SERVICE ) )\n.then( service => {\n  this.service = service;\n  return this.service.getCharacteristic( Bean.SENSORS );\n} )\n.then( characteristic => {\n  // Connected to server\n  // Connected to specific service\n  // Retrieved a list of characteristics\n\n  // This is a good place to update UI\n\n  // Listen for characteristic changes\n  return characteristic.startNotifications();\n} )\n.then( characteristic => {\n  // Characteristic change listener\n  characteristic.addEventListener(\n    'characteristicvaluechanged',\n    evt => this.doCharacteristicChanged( evt )\n  );\n} )\n.then( service => {\n  return this.service.getCharacteristic( Bean.RGB_LED );\n} )\n.then( characteristic => {\n  this.rgbled = characteristic;\n} )\n.catch( error => { \n  console.log( error ); \n} ); \n\n\nBelieve it or not, that is almost entirely boilerplate code. At the very start,\nI tell the Web Bluetooth API to look for my specific Bean, by name, and having a\nspecific service UUID. About two-thirds of the way through, once we have started\nlistening for characteristic data changes (new data in the specified scratch\nslot), we register a listener to process the data. I also keep track of the GATT\nservice itself because I want to write to it later.\n\n// Scratch characteristic changed\n// Parse content\n// Store relevant accelerometer values\ndoCharacteristicChanged( evt ) {\n  let scratch = null;\n  let content = null;\n  let parts = null;\n\n  // Parse to CSV\n  // Then to array of strings\n  scratch = new Uint8Array( \n    evt.target.value.buffer \n  );\n  content = String.fromCharCode.apply( \n    String, \n    scratch \n  );\n  parts = content.split( ',' );\n\n  // Get integer value for x-axis\n  // Map to 180-degree total range\n  this.angleX = parseInt( parts[0] );\n  this.angleX = this.map( \n    this.angleX, \n    -270, \n    270, \n    -90, \n    90 \n  );\n\n  // Get integer value for y-axis\n  // Map to 180-degree total range\n  this.angleY = parseInt( parts[1] );\n  this.angleY = this.map( \n    this.angleY, \n    -270, \n    270, \n    -90, \n    90 \n  );    \n\n  // Temperature in centigrade\n  this.temperature = parseInt( parts[3] );\n}\n\n\nThe inverse of what happens in the sketch - turning a character array into an\nunsigned integer array - happens here, then I split the data and parse as\nneeded. For my application, I run a linear transform to take the data from an\naccelerometer-specific value, to a range I can use in my application.\n\ndoSwatch( evt ) {\n  let color = null;\n  let index = null;\n\n  // Get index from selected color\n  index = evt.target.getAttribute( 'data-index' );\n\n  // Map index to color options\n  color = new Uint8Array( 3 );\n  color[0] = this.rainbow[index].red;\n  color[1] = this.rainbow[index].green;\n  color[2] = this.rainbow[index].blue;\n\n  // Write to characteristic\n  this.rgbled.writeValue( color );\n\n  // Show selected color\n  // Hide swatches\n  this.current.children[0].style.backgroundColor = evt.target.style.backgroundColor;\n}\n\n\nAgain, using unsigned integers for the RGB colors, and the scratch data makes\nthings easy. I then tell that specific Bluetooth service to write the new\nscratch data. That is what gets picked up in the sketch, and then the Bean API\nis called to set the color of the RGB LED.\n\nWhat's Next\nOne problem I have repeatedly encountered is that my Web application will stop\nreceiving notifications. This seems to happen most often when I write new RGB\ncharacteristic data. The problem existed before I added that feature however. It\nalso seems worse if I sleep less (or not at all in this case) on the Arduino.\n\nBecause of all the sandboxing that happens both at the Web Bluetooth level, and\nthe Bean API level, I cannot tell which one is causing the problem. My guess is\nthe Web Bluetooth API as this works without problems on a native iOS\napplication. But I am honestly just guessing there, and would love to have the\nworking group take a look. But would that involve shipping them a Bean? Weird\nproblems in IoT land.","feature_image":"__GHOST_URL__/content/images/2019/01/black.beans.jpg","featured":0,"status":"published","locale":null,"visibility":"public","author_id":"1","created_at":"2017-02-22T21:22:03.000Z","updated_at":"2019-01-17T00:44:18.000Z","published_at":"2017-02-24T19:47:28.000Z","custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"type":"post","email_recipient_filter":"none"},{"id":"5adb8922351ffe0018a5774c","uuid":"e9eaf864-ab14-46c2-8b57-a295bb86fd2f","title":"Advocate: Getting Started","slug":"advocate-getting-started","mobiledoc":"{\"version\":\"0.3.1\",\"markups\":[],\"atoms\":[],\"cards\":[[\"markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\"The field of Developer Relations is still relatively young. I first got into the field in 2007, after seven years in Technical Sales. As a sales guy, I started attending conferences to work the show floor (drum up sales). Then I started presenting, and was humbled by the feeling of being able to help people. When the opportunity came up to move from Sales to Developer Relations, I made the jump and never looked back.\\n\\nThose first few years were very much figuring things out as I went. There were no developer relations books. Very few related blogs. You had to make it up as you went. Over time the strategies I formed became more and more tested. With that in mind, this is the first in a series about doing the work of a Developer Advocate.\\n\\n###Personal Brand\\n\\nYour employer has a brand. That brand may be new, as a startup, or more than 100 years old, such as the case with my current employer, IBM. A good brand stands the test of time. It gives people a single point of focus with which to identify.\\n\\nAs an advocate, if you intend to do this as your long-term career, you must have a personal brand. This gives people a single point of focus for you, regardless of your current employer, or technology emphasis. Without a personal brand, you risk becoming obsolete as the technology landscape changes.\\n\\nI have worked with countless technologies in my twenty (20) years of IT experience. My personal brand is what has allowed me to survive the shifts in my career from Java, to Flash, to Web Standards, to IoT. Without it, I would have had to start from scratch with each evolution.\\n\\n###The Long Tail\\n\\nAt the core of much of what follows is a business/statistics concept known as \\\"the long tail.\\\" This term refers to a distribution of numbers. When you publish content, it will get the most views right up front. Over time, the number of views will taper off significantly.\\n\\n![The long tail](http://images.kevinhoyt.com/the.long.tail.png)\\n\\nMany marketers will focus on getting that distribution as close to the front, and as steep as possible. I would argue that for an advocate, the long tail is what really matters. ==The release schedules and timelines of your employer are not necessarily the project timelines of your audience.== It may be a year or more before your audience finally gets a chance to implement new technologies. By minding the long tail, you are there when they need you.\\n\\n###Technical Writing\\n\\nI do not know any developer advocates that do not write technical articles. Writing helps you codify your thoughts. Writing helps you become a better communicator. Writing expresses your personality. Writing supports the long tail. Get the point? You need to write technical content. That value to your personal brand must not be underestimated.\\n\\n#####Blogging\\n\\nWhen I started in developer relations, blogging was the main way to accomplish this task - and for my money, it is still the best approach. If you are starting out, you should ==invest in a vanity domain and attach a blog to it==. Initially, you should ==aim to write something every week==, but do not apologize if you cannot make it - it is after all, your blog.\\n\\n1. **Speed** - There is no delay between when you have a thought and when you can publish that thought (other than the writing process itself). You do not have to get approval from any editorial source.\\n2. **Tone** - When you write for you, on your blog, the tone you establish in your head is yours. This helps surface who you are as a person. It puts your personality front-and-center.\\n3. **Content** - What you blog is yours. If you feel like writing an article about a movie you liked, you can do just that. And all the better - again, your personality is front-and-center.\\n4. **Consistency** - Let us be honest here, the days of working thirty (30) years for a single employer, and retiring with a pension, are pretty much over. If you change employers, you do not want to lose control over your content.\\n5. **Insight** - At the time of this writing, I can tell you exactly what the most popular content on my site is about. If I change employers, I still have that data.\\n6. **People** - I get to have a direct exchange with my readers in the comments. I get notified as soon as a comment arrives, can reply with how I feel, and learn about my regular readers.\\n\\n#####Content Outlets\\n\\nAs you establish your voice and presence in a community, it is likely that you will eventually be asked to write technical content for other outlets. Usually the first place is your employers developer center, or engineering blog. You should absolutely do this.\\n\\n- When you publish on an outlet other than your blog, you should also ask about being able to publish that same content on your own blog. \\n- You need to know who retains control over the content you write. Will you get regular reports of viewer statistics? Can you edit it to keep the content current?\\n- Seek out opportunities to have other outlets publish content from your blog. If you have content that is getting a lot of traction, other outlets will be interested in that content.\\n\\nYou might consider foregoing a blog and using an outlet such a Medium. Another popular outlet these days is LinkedIn articles. I am not against either of these, but they generally reflect more polished content. This means you may have to sacrifice on any number of those earlier points. These outlets should be considering an addition to your own blog.\\n\\n#####Books\\n\\nIf you get a chance to write a book, you should strongly consider it. Having written a book on Adobe AIR, I can tell you that the workload is immense. You need to be prepared for the impact of this on your schedule. \\n\\n![Adobe AIR for JavaScript Developers](http://images.kevinhoyt.com/adobe.air.book.jpg)\\n\\nAuthoring a book puts an exclamation point on your personal brand. It is one of those things that not everybody has done, or even can do. You can hand them out at presentations, and attend signing opportunities. They make you more desirable as a speaker.\\n\\n#####Every. Single. Day.\\n\\nI know some advocates who publish blog content nearly every single day, and attend conferences only a few times per year. This is a perfectly acceptable approach to developer advocacy. If you find that you have that much content to share, and enjoy the writing process, then the may be the best fit for your personality - and that is great!\\n\\n>I have never encountered a developer relations team that does not value the advocate with a voracious appetite for publishing technical content.\\n\\n###Video\\n\\nWhen I first started in developer relations, YouTube was just getting started (founded 2005). Since then, it has become a mainstream outlet for content of all kinds. You do not have to be a developer to know about YouTube. My daughter loves Minecraft videos. Cord cutters (those leaving cable providers behind) look to YouTube as a core alternative.\\n\\nThe power of video cannot be understated. ==As a Developer Advocate you should select a video content provider.== Remember that not all video has to be of something you have done. If you see a cool demo at a booth at a conference, record and publish that. Take a few minutes to interview other conference speakers - maybe even over a beer.\\n\\n#####Editing Video\\n\\nVideo takes considerably more time to produce. Generally speaking, you can expect to invest *at least* two minutes of editing time for every minute of run time. Then there is time to rehearse and record the material. A single five-minute video can easily take up an entire afternoon.\\n\\nAuthoring video content on a Mac is pretty easy these days. QuickTime is provided and can do screen recordings on the spot. If you are using an iOS device, you can even AirPlay into QuickTime to record screencasts of mobile applications you may develop or want to discuss.\\n\\nHaving spent fifteen (15) years at Adobe, I am partial to Adobe Premier Pro for editing. There are many alternatives to consider as well. Eventually, you should get comfortable with some video editing software. Putting fade-in/out transitions and titles in your videos help to make the content feel more professional.\\n\\n####Professional Editors\\n\\nSome larger companies will even dedicate professionals (or budget) for this task, and have a specific outlet for video. You should get to know these people, and schedule regular time to sit down and capture your latest content. Consider the long tail of that content when deciding what to say, and how to say it.\\n\\n####Bloopers\\n\\nRemember that your personal brand is important. I like to include outtakes/bloopers at the end of my videos. I do not always do this, but am always surprised by those that find, and comment, about them. These are your videos - let your personality shine.\\n\\n####Duration\\n\\nI personally prefer to publish videos that are in the ==five to ten minute== duration. I like this time because it keeps editing to a minimum. I also like this time because it is easily consumable. This timing also forces you to be very focused on what it is that you want to say. A rambling waste of time, or video of waiting for software to work (spinning up a server instance), is a sure way to discourage people from returning to your videos.\\n\\nThat being said, longer content is sometimes welcome as well. If you have permission to publish a video recording of you speaking, then your work is already done for you. \\n\\nI also like to record my own videos of the core of my presentations - just the key demos. Sometimes these tell a story that take longer than ten (10) minutes. The long tail is applicable here. As years go by, and the technology landscape changes, my code may no longer run, but a video allows me to show the work I did in a functioning state.\\n\\n####Backup\\n\\nI once knew an advocate, who went on to found and sell his own startup, that only ever presented videos of his demos. One time, his entire presentation was prerecorded - he just narrated each clip while on stage. Not everybody can pull this off, nor should they try, but there are some implications.\\n\\nEverybody knows that conference Internet access is sometimes difficult to manage. Spending weeks getting ready for a presentation, only to have your demo go bust because the Internet was slow or non-existent, is a let down as much for your audience as it is for you. Having videos on hand shows your professionalism, and allows you to still deliver on what you promised conference attendees and organizers.\\n\\nThis also allows you to point to the video URL in your presentation. While more conferences record sessions these days, you never know the timeline or quality. Giving attendees of your talks a means to get access to your content on the spot is immensely valuable.\\n\\n###Social Media\\n\\nAs advocates, we are often the public face of our employers. In this role we are inherently social. Conference organizers will want to know who we are before they accept our abstracts. Attendees will research you to decide if they want to attend your talk.\\n\\n####Twitter\\n\\nTwitter does not have the pull it once used to, but it is still a primary means for developers to connect. ==As a Developer Advocate, you should have a Twitter account.== You should place your Twitter handle prominently on your slides, your blog, your videos, and any other place or means you use to publish content.\\n\\nThe content on your Twitter feed should not be dedicated only to content relating to your employer - this diminishes your personal brand. Be sure to post content relevant to who you are as a person.\\n\\n> One of the advocates I trained, takes his family to Disney parks on a regular basis. He posts content with a twist during the visits, which he calls \\\"Dark Disney\\\". I look forward to this content almost as much as I look forward to his technical posts.\\n\\nYour employer may ask you to tweet certain content from time to time. They may even provide you with the blurb. Approach this with caution. If your stream is filled with nothing but content from or about your employer, than you will lose credibility with your audience. ==Trust is core to developer advocacy, and I cannot trust you to give me technical advice about technology if all you ever talk about is your employer.==\\n\\n\"}]],\"sections\":[[10,0]],\"ghostVersion\":\"3.0\"}","html":"<!--kg-card-begin: markdown--><p>The field of Developer Relations is still relatively young. I first got into the field in 2007, after seven years in Technical Sales. As a sales guy, I started attending conferences to work the show floor (drum up sales). Then I started presenting, and was humbled by the feeling of being able to help people. When the opportunity came up to move from Sales to Developer Relations, I made the jump and never looked back.</p>\n<p>Those first few years were very much figuring things out as I went. There were no developer relations books. Very few related blogs. You had to make it up as you went. Over time the strategies I formed became more and more tested. With that in mind, this is the first in a series about doing the work of a Developer Advocate.</p>\n<h3 id=\"personalbrand\">Personal Brand</h3>\n<p>Your employer has a brand. That brand may be new, as a startup, or more than 100 years old, such as the case with my current employer, IBM. A good brand stands the test of time. It gives people a single point of focus with which to identify.</p>\n<p>As an advocate, if you intend to do this as your long-term career, you must have a personal brand. This gives people a single point of focus for you, regardless of your current employer, or technology emphasis. Without a personal brand, you risk becoming obsolete as the technology landscape changes.</p>\n<p>I have worked with countless technologies in my twenty (20) years of IT experience. My personal brand is what has allowed me to survive the shifts in my career from Java, to Flash, to Web Standards, to IoT. Without it, I would have had to start from scratch with each evolution.</p>\n<h3 id=\"thelongtail\">The Long Tail</h3>\n<p>At the core of much of what follows is a business/statistics concept known as &quot;the long tail.&quot; This term refers to a distribution of numbers. When you publish content, it will get the most views right up front. Over time, the number of views will taper off significantly.</p>\n<p><img src=\"http://images.kevinhoyt.com/the.long.tail.png\" alt=\"The long tail\" loading=\"lazy\"></p>\n<p>Many marketers will focus on getting that distribution as close to the front, and as steep as possible. I would argue that for an advocate, the long tail is what really matters. <mark>The release schedules and timelines of your employer are not necessarily the project timelines of your audience.</mark> It may be a year or more before your audience finally gets a chance to implement new technologies. By minding the long tail, you are there when they need you.</p>\n<h3 id=\"technicalwriting\">Technical Writing</h3>\n<p>I do not know any developer advocates that do not write technical articles. Writing helps you codify your thoughts. Writing helps you become a better communicator. Writing expresses your personality. Writing supports the long tail. Get the point? You need to write technical content. That value to your personal brand must not be underestimated.</p>\n<h5 id=\"blogging\">Blogging</h5>\n<p>When I started in developer relations, blogging was the main way to accomplish this task - and for my money, it is still the best approach. If you are starting out, you should <mark>invest in a vanity domain and attach a blog to it</mark>. Initially, you should <mark>aim to write something every week</mark>, but do not apologize if you cannot make it - it is after all, your blog.</p>\n<ol>\n<li><strong>Speed</strong> - There is no delay between when you have a thought and when you can publish that thought (other than the writing process itself). You do not have to get approval from any editorial source.</li>\n<li><strong>Tone</strong> - When you write for you, on your blog, the tone you establish in your head is yours. This helps surface who you are as a person. It puts your personality front-and-center.</li>\n<li><strong>Content</strong> - What you blog is yours. If you feel like writing an article about a movie you liked, you can do just that. And all the better - again, your personality is front-and-center.</li>\n<li><strong>Consistency</strong> - Let us be honest here, the days of working thirty (30) years for a single employer, and retiring with a pension, are pretty much over. If you change employers, you do not want to lose control over your content.</li>\n<li><strong>Insight</strong> - At the time of this writing, I can tell you exactly what the most popular content on my site is about. If I change employers, I still have that data.</li>\n<li><strong>People</strong> - I get to have a direct exchange with my readers in the comments. I get notified as soon as a comment arrives, can reply with how I feel, and learn about my regular readers.</li>\n</ol>\n<h5 id=\"contentoutlets\">Content Outlets</h5>\n<p>As you establish your voice and presence in a community, it is likely that you will eventually be asked to write technical content for other outlets. Usually the first place is your employers developer center, or engineering blog. You should absolutely do this.</p>\n<ul>\n<li>When you publish on an outlet other than your blog, you should also ask about being able to publish that same content on your own blog.</li>\n<li>You need to know who retains control over the content you write. Will you get regular reports of viewer statistics? Can you edit it to keep the content current?</li>\n<li>Seek out opportunities to have other outlets publish content from your blog. If you have content that is getting a lot of traction, other outlets will be interested in that content.</li>\n</ul>\n<p>You might consider foregoing a blog and using an outlet such a Medium. Another popular outlet these days is LinkedIn articles. I am not against either of these, but they generally reflect more polished content. This means you may have to sacrifice on any number of those earlier points. These outlets should be considering an addition to your own blog.</p>\n<h5 id=\"books\">Books</h5>\n<p>If you get a chance to write a book, you should strongly consider it. Having written a book on Adobe AIR, I can tell you that the workload is immense. You need to be prepared for the impact of this on your schedule.</p>\n<p><img src=\"http://images.kevinhoyt.com/adobe.air.book.jpg\" alt=\"Adobe AIR for JavaScript Developers\" loading=\"lazy\"></p>\n<p>Authoring a book puts an exclamation point on your personal brand. It is one of those things that not everybody has done, or even can do. You can hand them out at presentations, and attend signing opportunities. They make you more desirable as a speaker.</p>\n<h5 id=\"everysingleday\">Every. Single. Day.</h5>\n<p>I know some advocates who publish blog content nearly every single day, and attend conferences only a few times per year. This is a perfectly acceptable approach to developer advocacy. If you find that you have that much content to share, and enjoy the writing process, then the may be the best fit for your personality - and that is great!</p>\n<blockquote>\n<p>I have never encountered a developer relations team that does not value the advocate with a voracious appetite for publishing technical content.</p>\n</blockquote>\n<h3 id=\"video\">Video</h3>\n<p>When I first started in developer relations, YouTube was just getting started (founded 2005). Since then, it has become a mainstream outlet for content of all kinds. You do not have to be a developer to know about YouTube. My daughter loves Minecraft videos. Cord cutters (those leaving cable providers behind) look to YouTube as a core alternative.</p>\n<p>The power of video cannot be understated. <mark>As a Developer Advocate you should select a video content provider.</mark> Remember that not all video has to be of something you have done. If you see a cool demo at a booth at a conference, record and publish that. Take a few minutes to interview other conference speakers - maybe even over a beer.</p>\n<h5 id=\"editingvideo\">Editing Video</h5>\n<p>Video takes considerably more time to produce. Generally speaking, you can expect to invest <em>at least</em> two minutes of editing time for every minute of run time. Then there is time to rehearse and record the material. A single five-minute video can easily take up an entire afternoon.</p>\n<p>Authoring video content on a Mac is pretty easy these days. QuickTime is provided and can do screen recordings on the spot. If you are using an iOS device, you can even AirPlay into QuickTime to record screencasts of mobile applications you may develop or want to discuss.</p>\n<p>Having spent fifteen (15) years at Adobe, I am partial to Adobe Premier Pro for editing. There are many alternatives to consider as well. Eventually, you should get comfortable with some video editing software. Putting fade-in/out transitions and titles in your videos help to make the content feel more professional.</p>\n<h4 id=\"professionaleditors\">Professional Editors</h4>\n<p>Some larger companies will even dedicate professionals (or budget) for this task, and have a specific outlet for video. You should get to know these people, and schedule regular time to sit down and capture your latest content. Consider the long tail of that content when deciding what to say, and how to say it.</p>\n<h4 id=\"bloopers\">Bloopers</h4>\n<p>Remember that your personal brand is important. I like to include outtakes/bloopers at the end of my videos. I do not always do this, but am always surprised by those that find, and comment, about them. These are your videos - let your personality shine.</p>\n<h4 id=\"duration\">Duration</h4>\n<p>I personally prefer to publish videos that are in the <mark>five to ten minute</mark> duration. I like this time because it keeps editing to a minimum. I also like this time because it is easily consumable. This timing also forces you to be very focused on what it is that you want to say. A rambling waste of time, or video of waiting for software to work (spinning up a server instance), is a sure way to discourage people from returning to your videos.</p>\n<p>That being said, longer content is sometimes welcome as well. If you have permission to publish a video recording of you speaking, then your work is already done for you.</p>\n<p>I also like to record my own videos of the core of my presentations - just the key demos. Sometimes these tell a story that take longer than ten (10) minutes. The long tail is applicable here. As years go by, and the technology landscape changes, my code may no longer run, but a video allows me to show the work I did in a functioning state.</p>\n<h4 id=\"backup\">Backup</h4>\n<p>I once knew an advocate, who went on to found and sell his own startup, that only ever presented videos of his demos. One time, his entire presentation was prerecorded - he just narrated each clip while on stage. Not everybody can pull this off, nor should they try, but there are some implications.</p>\n<p>Everybody knows that conference Internet access is sometimes difficult to manage. Spending weeks getting ready for a presentation, only to have your demo go bust because the Internet was slow or non-existent, is a let down as much for your audience as it is for you. Having videos on hand shows your professionalism, and allows you to still deliver on what you promised conference attendees and organizers.</p>\n<p>This also allows you to point to the video URL in your presentation. While more conferences record sessions these days, you never know the timeline or quality. Giving attendees of your talks a means to get access to your content on the spot is immensely valuable.</p>\n<h3 id=\"socialmedia\">Social Media</h3>\n<p>As advocates, we are often the public face of our employers. In this role we are inherently social. Conference organizers will want to know who we are before they accept our abstracts. Attendees will research you to decide if they want to attend your talk.</p>\n<h4 id=\"twitter\">Twitter</h4>\n<p>Twitter does not have the pull it once used to, but it is still a primary means for developers to connect. <mark>As a Developer Advocate, you should have a Twitter account.</mark> You should place your Twitter handle prominently on your slides, your blog, your videos, and any other place or means you use to publish content.</p>\n<p>The content on your Twitter feed should not be dedicated only to content relating to your employer - this diminishes your personal brand. Be sure to post content relevant to who you are as a person.</p>\n<blockquote>\n<p>One of the advocates I trained, takes his family to Disney parks on a regular basis. He posts content with a twist during the visits, which he calls &quot;Dark Disney&quot;. I look forward to this content almost as much as I look forward to his technical posts.</p>\n</blockquote>\n<p>Your employer may ask you to tweet certain content from time to time. They may even provide you with the blurb. Approach this with caution. If your stream is filled with nothing but content from or about your employer, than you will lose credibility with your audience. <mark>Trust is core to developer advocacy, and I cannot trust you to give me technical advice about technology if all you ever talk about is your employer.</mark></p>\n<!--kg-card-end: markdown-->","comment_id":"67","plaintext":"The field of Developer Relations is still relatively young. I first got into the\nfield in 2007, after seven years in Technical Sales. As a sales guy, I started\nattending conferences to work the show floor (drum up sales). Then I started\npresenting, and was humbled by the feeling of being able to help people. When\nthe opportunity came up to move from Sales to Developer Relations, I made the\njump and never looked back.\n\nThose first few years were very much figuring things out as I went. There were\nno developer relations books. Very few related blogs. You had to make it up as\nyou went. Over time the strategies I formed became more and more tested. With\nthat in mind, this is the first in a series about doing the work of a Developer\nAdvocate.\n\nPersonal Brand\nYour employer has a brand. That brand may be new, as a startup, or more than 100\nyears old, such as the case with my current employer, IBM. A good brand stands\nthe test of time. It gives people a single point of focus with which to\nidentify.\n\nAs an advocate, if you intend to do this as your long-term career, you must have\na personal brand. This gives people a single point of focus for you, regardless\nof your current employer, or technology emphasis. Without a personal brand, you\nrisk becoming obsolete as the technology landscape changes.\n\nI have worked with countless technologies in my twenty (20) years of IT\nexperience. My personal brand is what has allowed me to survive the shifts in my\ncareer from Java, to Flash, to Web Standards, to IoT. Without it, I would have\nhad to start from scratch with each evolution.\n\nThe Long Tail\nAt the core of much of what follows is a business/statistics concept known as\n\"the long tail.\" This term refers to a distribution of numbers. When you publish\ncontent, it will get the most views right up front. Over time, the number of\nviews will taper off significantly.\n\n\n\nMany marketers will focus on getting that distribution as close to the front,\nand as steep as possible. I would argue that for an advocate, the long tail is\nwhat really matters. The release schedules and timelines of your employer are\nnot necessarily the project timelines of your audience. It may be a year or more\nbefore your audience finally gets a chance to implement new technologies. By\nminding the long tail, you are there when they need you.\n\nTechnical Writing\nI do not know any developer advocates that do not write technical articles.\nWriting helps you codify your thoughts. Writing helps you become a better\ncommunicator. Writing expresses your personality. Writing supports the long\ntail. Get the point? You need to write technical content. That value to your\npersonal brand must not be underestimated.\n\nBlogging\nWhen I started in developer relations, blogging was the main way to accomplish\nthis task - and for my money, it is still the best approach. If you are starting\nout, you should invest in a vanity domain and attach a blog to it. Initially,\nyou should aim to write something every week, but do not apologize if you cannot\nmake it - it is after all, your blog.\n\n 1. Speed - There is no delay between when you have a thought and when you can\n    publish that thought (other than the writing process itself). You do not\n    have to get approval from any editorial source.\n 2. Tone - When you write for you, on your blog, the tone you establish in your\n    head is yours. This helps surface who you are as a person. It puts your\n    personality front-and-center.\n 3. Content - What you blog is yours. If you feel like writing an article about\n    a movie you liked, you can do just that. And all the better - again, your\n    personality is front-and-center.\n 4. Consistency - Let us be honest here, the days of working thirty (30) years\n    for a single employer, and retiring with a pension, are pretty much over. If\n    you change employers, you do not want to lose control over your content.\n 5. Insight - At the time of this writing, I can tell you exactly what the most\n    popular content on my site is about. If I change employers, I still have\n    that data.\n 6. People - I get to have a direct exchange with my readers in the comments. I\n    get notified as soon as a comment arrives, can reply with how I feel, and\n    learn about my regular readers.\n\nContent Outlets\nAs you establish your voice and presence in a community, it is likely that you\nwill eventually be asked to write technical content for other outlets. Usually\nthe first place is your employers developer center, or engineering blog. You\nshould absolutely do this.\n\n * When you publish on an outlet other than your blog, you should also ask about\n   being able to publish that same content on your own blog.\n * You need to know who retains control over the content you write. Will you get\n   regular reports of viewer statistics? Can you edit it to keep the content\n   current?\n * Seek out opportunities to have other outlets publish content from your blog.\n   If you have content that is getting a lot of traction, other outlets will be\n   interested in that content.\n\nYou might consider foregoing a blog and using an outlet such a Medium. Another\npopular outlet these days is LinkedIn articles. I am not against either of\nthese, but they generally reflect more polished content. This means you may have\nto sacrifice on any number of those earlier points. These outlets should be\nconsidering an addition to your own blog.\n\nBooks\nIf you get a chance to write a book, you should strongly consider it. Having\nwritten a book on Adobe AIR, I can tell you that the workload is immense. You\nneed to be prepared for the impact of this on your schedule.\n\n\n\nAuthoring a book puts an exclamation point on your personal brand. It is one of\nthose things that not everybody has done, or even can do. You can hand them out\nat presentations, and attend signing opportunities. They make you more desirable\nas a speaker.\n\nEvery. Single. Day.\nI know some advocates who publish blog content nearly every single day, and\nattend conferences only a few times per year. This is a perfectly acceptable\napproach to developer advocacy. If you find that you have that much content to\nshare, and enjoy the writing process, then the may be the best fit for your\npersonality - and that is great!\n\n> I have never encountered a developer relations team that does not value the\nadvocate with a voracious appetite for publishing technical content.\n\n\nVideo\nWhen I first started in developer relations, YouTube was just getting started\n(founded 2005). Since then, it has become a mainstream outlet for content of all\nkinds. You do not have to be a developer to know about YouTube. My daughter\nloves Minecraft videos. Cord cutters (those leaving cable providers behind) look\nto YouTube as a core alternative.\n\nThe power of video cannot be understated. As a Developer Advocate you should\nselect a video content provider. Remember that not all video has to be of\nsomething you have done. If you see a cool demo at a booth at a conference,\nrecord and publish that. Take a few minutes to interview other conference\nspeakers - maybe even over a beer.\n\nEditing Video\nVideo takes considerably more time to produce. Generally speaking, you can\nexpect to invest at least two minutes of editing time for every minute of run\ntime. Then there is time to rehearse and record the material. A single\nfive-minute video can easily take up an entire afternoon.\n\nAuthoring video content on a Mac is pretty easy these days. QuickTime is\nprovided and can do screen recordings on the spot. If you are using an iOS\ndevice, you can even AirPlay into QuickTime to record screencasts of mobile\napplications you may develop or want to discuss.\n\nHaving spent fifteen (15) years at Adobe, I am partial to Adobe Premier Pro for\nediting. There are many alternatives to consider as well. Eventually, you should\nget comfortable with some video editing software. Putting fade-in/out\ntransitions and titles in your videos help to make the content feel more\nprofessional.\n\nProfessional Editors\nSome larger companies will even dedicate professionals (or budget) for this\ntask, and have a specific outlet for video. You should get to know these people,\nand schedule regular time to sit down and capture your latest content. Consider\nthe long tail of that content when deciding what to say, and how to say it.\n\nBloopers\nRemember that your personal brand is important. I like to include\nouttakes/bloopers at the end of my videos. I do not always do this, but am\nalways surprised by those that find, and comment, about them. These are your\nvideos - let your personality shine.\n\nDuration\nI personally prefer to publish videos that are in the five to ten minute \nduration. I like this time because it keeps editing to a minimum. I also like\nthis time because it is easily consumable. This timing also forces you to be\nvery focused on what it is that you want to say. A rambling waste of time, or\nvideo of waiting for software to work (spinning up a server instance), is a sure\nway to discourage people from returning to your videos.\n\nThat being said, longer content is sometimes welcome as well. If you have\npermission to publish a video recording of you speaking, then your work is\nalready done for you.\n\nI also like to record my own videos of the core of my presentations - just the\nkey demos. Sometimes these tell a story that take longer than ten (10) minutes.\nThe long tail is applicable here. As years go by, and the technology landscape\nchanges, my code may no longer run, but a video allows me to show the work I did\nin a functioning state.\n\nBackup\nI once knew an advocate, who went on to found and sell his own startup, that\nonly ever presented videos of his demos. One time, his entire presentation was\nprerecorded - he just narrated each clip while on stage. Not everybody can pull\nthis off, nor should they try, but there are some implications.\n\nEverybody knows that conference Internet access is sometimes difficult to\nmanage. Spending weeks getting ready for a presentation, only to have your demo\ngo bust because the Internet was slow or non-existent, is a let down as much for\nyour audience as it is for you. Having videos on hand shows your\nprofessionalism, and allows you to still deliver on what you promised conference\nattendees and organizers.\n\nThis also allows you to point to the video URL in your presentation. While more\nconferences record sessions these days, you never know the timeline or quality.\nGiving attendees of your talks a means to get access to your content on the spot\nis immensely valuable.\n\nSocial Media\nAs advocates, we are often the public face of our employers. In this role we are\ninherently social. Conference organizers will want to know who we are before\nthey accept our abstracts. Attendees will research you to decide if they want to\nattend your talk.\n\nTwitter\nTwitter does not have the pull it once used to, but it is still a primary means\nfor developers to connect. As a Developer Advocate, you should have a Twitter\naccount. You should place your Twitter handle prominently on your slides, your\nblog, your videos, and any other place or means you use to publish content.\n\nThe content on your Twitter feed should not be dedicated only to content\nrelating to your employer - this diminishes your personal brand. Be sure to post\ncontent relevant to who you are as a person.\n\n> One of the advocates I trained, takes his family to Disney parks on a regular\nbasis. He posts content with a twist during the visits, which he calls \"Dark\nDisney\". I look forward to this content almost as much as I look forward to his\ntechnical posts.\n\n\nYour employer may ask you to tweet certain content from time to time. They may\neven provide you with the blurb. Approach this with caution. If your stream is\nfilled with nothing but content from or about your employer, than you will lose\ncredibility with your audience. Trust is core to developer advocacy, and I\ncannot trust you to give me technical advice about technology if all you ever\ntalk about is your employer.","feature_image":null,"featured":0,"status":"draft","locale":null,"visibility":"public","author_id":"1","created_at":"2017-04-01T16:14:15.000Z","updated_at":"2017-04-01T18:37:11.000Z","published_at":null,"custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"type":"post","email_recipient_filter":"none"},{"id":"5adb8922351ffe0018a5774d","uuid":"8ec19614-71e5-4758-afa8-1e10fb651045","title":"Blockchain Vocabulary","slug":"blockchain-vocabulary","mobiledoc":"{\"version\":\"0.3.1\",\"markups\":[],\"atoms\":[],\"cards\":[[\"markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\"Blockchain has gained a lot of attention this year. That is great! It is a fine solution for many problems. If you are a developer however, being asked to make since of what problems blockchain can solve for your business, then you may quickly find yourself in over your head. With that in mind, I thought I would put together a developer-centric vocabulary list.\\n\\n#####Disclaimer\\n\\nThe way in which I am about to approach defining blockchain terms may disagree with you if you are a developer already using the technology. Indeed, there seems to be an almost philosophical battle within the blockchain community as to how to represent the technology, and communicate it to others.\\n\\nI am not trying to start fires here, I am merely explaining blockchain in the way I put it together in my developer mind. It is the way that works for me, and gave me a foundation on which I was able to then deepen my understanding.\\n\\n#####Blockchain\\n\\nI will actually visit this definition three times in this post. Firstly, what the heck is \\\"blockchain\\\"?\\n\\nYou do not have to live in Silicon Valley to have heard of Bitcoin. The so-called cryptocurrency has had broad media coverage of both the positive and negative variety. Bitcoin was actually the first blockchain application.\\n\\n![Bitcoin was the first Blockchain application.](http://images.kevinhoyt.com/blockchain.vocab.2.png)\\n\\nThe way I see this, when we refer to Bitcoin, we are referring to the application layer in our architecture. This might be where your Node.js or Java EE lives in architectures you have today. In that architecture then, blockchain is effectively the data storage tier.\\n\\nTo be sure, calling blockchain just a \\\"database\\\" would be selling it short. There are several unique characteristics that make it stand out for certain types of applications. Another way to put this might be \\\"If blockchain is just a database, what kind of database is it? Relational? NoSQL?\\\"\\n\\n#####Ledger\\n\\nTechnically, blockchain is a ledger. The term \\\"ledger\\\" comes from general accounting. In accounting, every transaction gets recorded by the parties involved. If somebody changes a value, then the books do not add up, and a problem will surface relatively quickly (as soon as somebody goes reconcile the books). Ledgers are generally made up of a handful of columns - the date of the transaction, the category of the transaction, a column for income (credit), a column for expenses (debit), and the running balance.\\n\\n![An example ledger](http://images.kevinhoyt.com/blockchain.vocab.4.png)\\n\\nIn this example, if I remove the expense of \\\"Office Supplies\\\" for $125.36, then the rest of the lines will not add up. Something is wrong, and we know pretty much right away. Blockchain exhibits the same kind of behavior, but with chunks of data, not credits and debits.\\n\\n![Hash Chain](http://images.kevinhoyt.com/blockchain.vocab.5.png)\\n\\nLet us say that I have a record I want to put in my database (ledger). That value will be hashed. That hash is then attached to the record (usually the header) when the data is stored. When you go to change that record (create, update, delete), the original hash is included with the bytes that get hashed for this new record. The resulting hash is then attached to the record and stored. In this manner, a chain of hashes is formed. Altering a piece of data anywhere back in the chain would break the values after that point, invalidating the data, and failing any future transactions.\\n\\n> My favorite demonstration of this process, why it matters, and how it works, is [this video](https://www.youtube.com/watch?v=_160oMzblY8) from CirclePay Evangelist, [Anders Brownworth](https://twitter.com/anders94). It starts slow, but I personally guarantee a light bulb moment.\\n\\n#####Block. Chain.\\n\\nHere we are again. A **block** represents a group of transactions. You might think of this as your bank statement. Data in the block cannot be altered retroactively. A **chain** then refers to the list of ordered records, each with a link to the previous record. Blockchain!\\n\\n#####Distributed and Decentralized\\n\\nOf course, you would not put a database on a single server and call it done. You would want redundancy, load distribution, and backup, to name a few. The three main architectures to consider when setting up a database cluster (network) are centralized, decentralized, and distributed. Blockchain is a distributed ledger (database), with decentralized consensus. Let us break that down in a bit more detail.\\n\\n![Distributed and Decentralized](http://images.kevinhoyt.com/blockchain.vocab.9.png)\\n\\nAs a distributed ledger, blockchain networks are peer-to-peer, and can be difficult to maintain. Without a single point of failure however, one broken link in the network only makes more distributed networks, and does not bring down the entire system. The result is a highly scalable network. Since centralized systems follow a single framework, they do not have diversity, and evolve slowly. Decentralized and distributed systems have an up-front cost in establishing the architecture, but from there have tremendous evolution.\\n\\n> If you want a bit more depth on all these terms, and the breakdown of where they do/not work, I suggest [this blog post](https://medium.com/@bbc4468/centralized-vs-decentralized-vs-distributed-41d92d463868) by TinyOwl Founter, [Saurabh Goyal](https://twitter.com/bbc4468).\\n\\n#####Byzantine Fault Tolerance\\n\\nOne of the problems that emerges in a distributed system is how to coordinate communication over a potentially unreliable link. This is referred to as the \\\"Two Generals' Problem\\\" also called the \\\"Byzantine Generals' Problem\\\".\\n\\n![Two Generals' Problem](http://images.kevinhoyt.com/blockchain.vocab.10.png)\\n\\nTwo [Byzantine Empire] generals, A1 and A2, are preparing to attack fortified city, B. The armies are encamped near the city, each in its own valley. A third valley separates the two hills, and the only way for the two generals to communicate is by sending messengers through the valley. While the two generals have agreed to attack, they have not agreed upon a time for the attack.\\n\\nThis raises a whole variety of problems.\\n\\nIf A1 sends a message to A2, to say \\\"Attack at dawn\\\" A2 may never receive the message. Or A2 may get the message, but A1 has no way to confirm that. It could also happen that B intercepts the message, alters the attack time, and then sends it on to A2. The same problem happens when A2 sends a message to A1. All this, and we have not even introduced the problem that A1 may be traitorous!\\n\\nThe Two Generals' Problem was the first computer communication problem to be proven to be unsolvable.\\n\\nThis is a problem endemic to a distributed system such as blockchain. In order to fix it, you have to make some compromises on the original problem. A blockchain implementation such as [Hyperledger Fabric](https://www.hyperledger.org/), solves this using Practical Byzantine Fault Tolerance (PBFT). I will not elaborate on the compromises of PBFT here, mostly because it is an implementation detail that a typical developers will never need to think about.\\n\\n#####Consensus\\n\\nWith you typical database solution, there is an administrator. Or at the very least, various users, roles, and privileges, as to who can work on what data. At any point in time however, the administrator can yank to those privileges, and even modify the data stored in the database. As you might imagine, having that central authority can be problematic if we are talking about my bank statements.\\n\\n![Consensus](http://images.kevinhoyt.com/blockchain.vocab.11.png)\\n\\nOn a blockchain network, there is no centralized authority that determines the transaction order. Instead, many validating peers (nodes) implement the network consensus protocol. Consensus ensures that a quorum of nodes agree on the order in which the transactions are appended to the shared ledger. By resolving any discrepancies in the proposed transaction order, consensus guarantees that all blockchain network nodes are operating on an identical ledger.\\n\\n#####Proof-of-Work\\n\\nThe follow-on question then becomes who are the nodes on the network? What gives them the right to participate in the first place? \\n\\nWith Bitcoin, anybody can join the network. Consensus this is achieved through \\\"proof-of-work\\\". In order to commit a transaction, you must first solve a mathematical problem unique to that specific transaction. The first peer to solve the transaction wins consensus, and can in turn commit the transaction. This is a permissionless system.\\n\\nWhat, the what?\\n\\nConsider a form in a web page. The Internet is an open network. Anybody can join, visit your web form, and submit content that gets stored in your database. Even robots. Robots can send junk data into your system, and they can submit that web form really fast - fast enough to bring down the system entirely. A common technique to solving this problem is CAPTCHA.\\n\\nWith a CAPTCHA, you are asking the person submitting the data to **prove** that they are a person. They have to do some **work** to do that, such as reading skewed text, or adding two numbers presented in images. Proof-of-work is effectively CAPTCHA for computer.\\n\\n#####Permissions\\n\\nWhile Bitcoin leverages a permissionless system, there are also permissioned blockchain systems. These are often called private blockchains. In a private blockchain, peers must be authenticated, usually using certificates. This subtle change has several distinct impacts on blockchain - most notably the speed with which transactions can be processed.\\n\\nA private blockchain that provides for some requirement of authentication is where blockchain becomes especially valuable to businesses. \\n\\n![Blockchain for Business](http://images.kevinhoyt.com/blockchain.vocab.13.png)\\n\\nThink of your health records. You have a primary physician, but then you see a specialist for a specific medical condition. Likely you will see more than one specialist. If you have other conditions down the road, say a vision problem, or cancer, you will be seeing countless physicians, all with bits of your data, but without any one having the complete picture.\\n\\nNow think of all the health related data you probably store for yourself. Maybe you take your blood pressure. Track your run lately? Supplement with calcium? Track what you eat and the nutritional values? All this data about you, scattered everywhere.\\n\\nWhat if we had a database that all these different companies could use to work on your single, secure, health record? How much easier would it be to see that specialist for the first time? What kind of additional treatments might be uncovered by having the whole picture of your health? How much earlier could we detect cancer? The possibilities in this one industry alone represent a huge opportunity for the practical application of blockchain.\\n\\n#####Blockchain\\n\\nOkay, so now we have some vocabulary under our belts, let us revisit the definition of blockchain one last time.\\n\\n![The special characteristics of blockchain.](http://images.kevinhoyt.com/blockchain.vocab.14.png)\\n\\n![Business use-cases for blockchain.](http://images.kevinhoyt.com/blockchain.vocab.15.png)\\n\\nOne of the tricks with envisioning where blockchain technology can be useful is in separating it from physical assets such as money, or shipping. If you look at the features offered by blockchain, you will find many good applications. A wiki fits these characteristics. A shared calendaring system. Even the good old to-do list, where multiple parties (even companies) are organizing a new product launch, or a summer blockbuster movie release.\\n\\n#####Next Steps\\n\\nIndeed, industry analysts that are bullish on blockchain see it ultimately replacing the very fabric of how the Internet works today. I do not know if I am ready to go that far, but it is certainly a technology worth investigating for your next application.\\n\\n<iframe width=\\\"560\\\" height=\\\"315\\\" src=\\\"https://www.youtube.com/embed/SeBaUXn-Wnw\\\" frameborder=\\\"0\\\" allowfullscreen></iframe>\\n\\nOn those closing notes, if you want to see a to-do list type of application, understand how it is built, and see it in action, I have put together just such an example ([repository](https://github.com/IBM/todo-list-fabric)). Check it out, fork it and send a pull request if you want to contribute, or just leave a comment below with your thoughts. Happy blockchain!\"}]],\"sections\":[[10,0]],\"ghostVersion\":\"3.0\"}","html":"<!--kg-card-begin: markdown--><p>Blockchain has gained a lot of attention this year. That is great! It is a fine solution for many problems. If you are a developer however, being asked to make since of what problems blockchain can solve for your business, then you may quickly find yourself in over your head. With that in mind, I thought I would put together a developer-centric vocabulary list.</p>\n<h5 id=\"disclaimer\">Disclaimer</h5>\n<p>The way in which I am about to approach defining blockchain terms may disagree with you if you are a developer already using the technology. Indeed, there seems to be an almost philosophical battle within the blockchain community as to how to represent the technology, and communicate it to others.</p>\n<p>I am not trying to start fires here, I am merely explaining blockchain in the way I put it together in my developer mind. It is the way that works for me, and gave me a foundation on which I was able to then deepen my understanding.</p>\n<h5 id=\"blockchain\">Blockchain</h5>\n<p>I will actually visit this definition three times in this post. Firstly, what the heck is &quot;blockchain&quot;?</p>\n<p>You do not have to live in Silicon Valley to have heard of Bitcoin. The so-called cryptocurrency has had broad media coverage of both the positive and negative variety. Bitcoin was actually the first blockchain application.</p>\n<p><img src=\"http://images.kevinhoyt.com/blockchain.vocab.2.png\" alt=\"Bitcoin was the first Blockchain application.\" loading=\"lazy\"></p>\n<p>The way I see this, when we refer to Bitcoin, we are referring to the application layer in our architecture. This might be where your Node.js or Java EE lives in architectures you have today. In that architecture then, blockchain is effectively the data storage tier.</p>\n<p>To be sure, calling blockchain just a &quot;database&quot; would be selling it short. There are several unique characteristics that make it stand out for certain types of applications. Another way to put this might be &quot;If blockchain is just a database, what kind of database is it? Relational? NoSQL?&quot;</p>\n<h5 id=\"ledger\">Ledger</h5>\n<p>Technically, blockchain is a ledger. The term &quot;ledger&quot; comes from general accounting. In accounting, every transaction gets recorded by the parties involved. If somebody changes a value, then the books do not add up, and a problem will surface relatively quickly (as soon as somebody goes reconcile the books). Ledgers are generally made up of a handful of columns - the date of the transaction, the category of the transaction, a column for income (credit), a column for expenses (debit), and the running balance.</p>\n<p><img src=\"http://images.kevinhoyt.com/blockchain.vocab.4.png\" alt=\"An example ledger\" loading=\"lazy\"></p>\n<p>In this example, if I remove the expense of &quot;Office Supplies&quot; for $125.36, then the rest of the lines will not add up. Something is wrong, and we know pretty much right away. Blockchain exhibits the same kind of behavior, but with chunks of data, not credits and debits.</p>\n<p><img src=\"http://images.kevinhoyt.com/blockchain.vocab.5.png\" alt=\"Hash Chain\" loading=\"lazy\"></p>\n<p>Let us say that I have a record I want to put in my database (ledger). That value will be hashed. That hash is then attached to the record (usually the header) when the data is stored. When you go to change that record (create, update, delete), the original hash is included with the bytes that get hashed for this new record. The resulting hash is then attached to the record and stored. In this manner, a chain of hashes is formed. Altering a piece of data anywhere back in the chain would break the values after that point, invalidating the data, and failing any future transactions.</p>\n<blockquote>\n<p>My favorite demonstration of this process, why it matters, and how it works, is <a href=\"https://www.youtube.com/watch?v=_160oMzblY8\">this video</a> from CirclePay Evangelist, <a href=\"https://twitter.com/anders94\">Anders Brownworth</a>. It starts slow, but I personally guarantee a light bulb moment.</p>\n</blockquote>\n<h5 id=\"blockchain\">Block. Chain.</h5>\n<p>Here we are again. A <strong>block</strong> represents a group of transactions. You might think of this as your bank statement. Data in the block cannot be altered retroactively. A <strong>chain</strong> then refers to the list of ordered records, each with a link to the previous record. Blockchain!</p>\n<h5 id=\"distributedanddecentralized\">Distributed and Decentralized</h5>\n<p>Of course, you would not put a database on a single server and call it done. You would want redundancy, load distribution, and backup, to name a few. The three main architectures to consider when setting up a database cluster (network) are centralized, decentralized, and distributed. Blockchain is a distributed ledger (database), with decentralized consensus. Let us break that down in a bit more detail.</p>\n<p><img src=\"http://images.kevinhoyt.com/blockchain.vocab.9.png\" alt=\"Distributed and Decentralized\" loading=\"lazy\"></p>\n<p>As a distributed ledger, blockchain networks are peer-to-peer, and can be difficult to maintain. Without a single point of failure however, one broken link in the network only makes more distributed networks, and does not bring down the entire system. The result is a highly scalable network. Since centralized systems follow a single framework, they do not have diversity, and evolve slowly. Decentralized and distributed systems have an up-front cost in establishing the architecture, but from there have tremendous evolution.</p>\n<blockquote>\n<p>If you want a bit more depth on all these terms, and the breakdown of where they do/not work, I suggest <a href=\"https://medium.com/@bbc4468/centralized-vs-decentralized-vs-distributed-41d92d463868\">this blog post</a> by TinyOwl Founter, <a href=\"https://twitter.com/bbc4468\">Saurabh Goyal</a>.</p>\n</blockquote>\n<h5 id=\"byzantinefaulttolerance\">Byzantine Fault Tolerance</h5>\n<p>One of the problems that emerges in a distributed system is how to coordinate communication over a potentially unreliable link. This is referred to as the &quot;Two Generals' Problem&quot; also called the &quot;Byzantine Generals' Problem&quot;.</p>\n<p><img src=\"http://images.kevinhoyt.com/blockchain.vocab.10.png\" alt=\"Two Generals' Problem\" loading=\"lazy\"></p>\n<p>Two [Byzantine Empire] generals, A1 and A2, are preparing to attack fortified city, B. The armies are encamped near the city, each in its own valley. A third valley separates the two hills, and the only way for the two generals to communicate is by sending messengers through the valley. While the two generals have agreed to attack, they have not agreed upon a time for the attack.</p>\n<p>This raises a whole variety of problems.</p>\n<p>If A1 sends a message to A2, to say &quot;Attack at dawn&quot; A2 may never receive the message. Or A2 may get the message, but A1 has no way to confirm that. It could also happen that B intercepts the message, alters the attack time, and then sends it on to A2. The same problem happens when A2 sends a message to A1. All this, and we have not even introduced the problem that A1 may be traitorous!</p>\n<p>The Two Generals' Problem was the first computer communication problem to be proven to be unsolvable.</p>\n<p>This is a problem endemic to a distributed system such as blockchain. In order to fix it, you have to make some compromises on the original problem. A blockchain implementation such as <a href=\"https://www.hyperledger.org/\">Hyperledger Fabric</a>, solves this using Practical Byzantine Fault Tolerance (PBFT). I will not elaborate on the compromises of PBFT here, mostly because it is an implementation detail that a typical developers will never need to think about.</p>\n<h5 id=\"consensus\">Consensus</h5>\n<p>With you typical database solution, there is an administrator. Or at the very least, various users, roles, and privileges, as to who can work on what data. At any point in time however, the administrator can yank to those privileges, and even modify the data stored in the database. As you might imagine, having that central authority can be problematic if we are talking about my bank statements.</p>\n<p><img src=\"http://images.kevinhoyt.com/blockchain.vocab.11.png\" alt=\"Consensus\" loading=\"lazy\"></p>\n<p>On a blockchain network, there is no centralized authority that determines the transaction order. Instead, many validating peers (nodes) implement the network consensus protocol. Consensus ensures that a quorum of nodes agree on the order in which the transactions are appended to the shared ledger. By resolving any discrepancies in the proposed transaction order, consensus guarantees that all blockchain network nodes are operating on an identical ledger.</p>\n<h5 id=\"proofofwork\">Proof-of-Work</h5>\n<p>The follow-on question then becomes who are the nodes on the network? What gives them the right to participate in the first place?</p>\n<p>With Bitcoin, anybody can join the network. Consensus this is achieved through &quot;proof-of-work&quot;. In order to commit a transaction, you must first solve a mathematical problem unique to that specific transaction. The first peer to solve the transaction wins consensus, and can in turn commit the transaction. This is a permissionless system.</p>\n<p>What, the what?</p>\n<p>Consider a form in a web page. The Internet is an open network. Anybody can join, visit your web form, and submit content that gets stored in your database. Even robots. Robots can send junk data into your system, and they can submit that web form really fast - fast enough to bring down the system entirely. A common technique to solving this problem is CAPTCHA.</p>\n<p>With a CAPTCHA, you are asking the person submitting the data to <strong>prove</strong> that they are a person. They have to do some <strong>work</strong> to do that, such as reading skewed text, or adding two numbers presented in images. Proof-of-work is effectively CAPTCHA for computer.</p>\n<h5 id=\"permissions\">Permissions</h5>\n<p>While Bitcoin leverages a permissionless system, there are also permissioned blockchain systems. These are often called private blockchains. In a private blockchain, peers must be authenticated, usually using certificates. This subtle change has several distinct impacts on blockchain - most notably the speed with which transactions can be processed.</p>\n<p>A private blockchain that provides for some requirement of authentication is where blockchain becomes especially valuable to businesses.</p>\n<p><img src=\"http://images.kevinhoyt.com/blockchain.vocab.13.png\" alt=\"Blockchain for Business\" loading=\"lazy\"></p>\n<p>Think of your health records. You have a primary physician, but then you see a specialist for a specific medical condition. Likely you will see more than one specialist. If you have other conditions down the road, say a vision problem, or cancer, you will be seeing countless physicians, all with bits of your data, but without any one having the complete picture.</p>\n<p>Now think of all the health related data you probably store for yourself. Maybe you take your blood pressure. Track your run lately? Supplement with calcium? Track what you eat and the nutritional values? All this data about you, scattered everywhere.</p>\n<p>What if we had a database that all these different companies could use to work on your single, secure, health record? How much easier would it be to see that specialist for the first time? What kind of additional treatments might be uncovered by having the whole picture of your health? How much earlier could we detect cancer? The possibilities in this one industry alone represent a huge opportunity for the practical application of blockchain.</p>\n<h5 id=\"blockchain\">Blockchain</h5>\n<p>Okay, so now we have some vocabulary under our belts, let us revisit the definition of blockchain one last time.</p>\n<p><img src=\"http://images.kevinhoyt.com/blockchain.vocab.14.png\" alt=\"The special characteristics of blockchain.\" loading=\"lazy\"></p>\n<p><img src=\"http://images.kevinhoyt.com/blockchain.vocab.15.png\" alt=\"Business use-cases for blockchain.\" loading=\"lazy\"></p>\n<p>One of the tricks with envisioning where blockchain technology can be useful is in separating it from physical assets such as money, or shipping. If you look at the features offered by blockchain, you will find many good applications. A wiki fits these characteristics. A shared calendaring system. Even the good old to-do list, where multiple parties (even companies) are organizing a new product launch, or a summer blockbuster movie release.</p>\n<h5 id=\"nextsteps\">Next Steps</h5>\n<p>Indeed, industry analysts that are bullish on blockchain see it ultimately replacing the very fabric of how the Internet works today. I do not know if I am ready to go that far, but it is certainly a technology worth investigating for your next application.</p>\n<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/SeBaUXn-Wnw\" frameborder=\"0\" allowfullscreen></iframe>\n<p>On those closing notes, if you want to see a to-do list type of application, understand how it is built, and see it in action, I have put together just such an example (<a href=\"https://github.com/IBM/todo-list-fabric\">repository</a>). Check it out, fork it and send a pull request if you want to contribute, or just leave a comment below with your thoughts. Happy blockchain!</p>\n<!--kg-card-end: markdown-->","comment_id":"68","plaintext":"Blockchain has gained a lot of attention this year. That is great! It is a fine\nsolution for many problems. If you are a developer however, being asked to make\nsince of what problems blockchain can solve for your business, then you may\nquickly find yourself in over your head. With that in mind, I thought I would\nput together a developer-centric vocabulary list.\n\nDisclaimer\nThe way in which I am about to approach defining blockchain terms may disagree\nwith you if you are a developer already using the technology. Indeed, there\nseems to be an almost philosophical battle within the blockchain community as to\nhow to represent the technology, and communicate it to others.\n\nI am not trying to start fires here, I am merely explaining blockchain in the\nway I put it together in my developer mind. It is the way that works for me, and\ngave me a foundation on which I was able to then deepen my understanding.\n\nBlockchain\nI will actually visit this definition three times in this post. Firstly, what\nthe heck is \"blockchain\"?\n\nYou do not have to live in Silicon Valley to have heard of Bitcoin. The\nso-called cryptocurrency has had broad media coverage of both the positive and\nnegative variety. Bitcoin was actually the first blockchain application.\n\n\n\nThe way I see this, when we refer to Bitcoin, we are referring to the\napplication layer in our architecture. This might be where your Node.js or Java\nEE lives in architectures you have today. In that architecture then, blockchain\nis effectively the data storage tier.\n\nTo be sure, calling blockchain just a \"database\" would be selling it short.\nThere are several unique characteristics that make it stand out for certain\ntypes of applications. Another way to put this might be \"If blockchain is just a\ndatabase, what kind of database is it? Relational? NoSQL?\"\n\nLedger\nTechnically, blockchain is a ledger. The term \"ledger\" comes from general\naccounting. In accounting, every transaction gets recorded by the parties\ninvolved. If somebody changes a value, then the books do not add up, and a\nproblem will surface relatively quickly (as soon as somebody goes reconcile the\nbooks). Ledgers are generally made up of a handful of columns - the date of the\ntransaction, the category of the transaction, a column for income (credit), a\ncolumn for expenses (debit), and the running balance.\n\n\n\nIn this example, if I remove the expense of \"Office Supplies\" for $125.36, then\nthe rest of the lines will not add up. Something is wrong, and we know pretty\nmuch right away. Blockchain exhibits the same kind of behavior, but with chunks\nof data, not credits and debits.\n\n\n\nLet us say that I have a record I want to put in my database (ledger). That\nvalue will be hashed. That hash is then attached to the record (usually the\nheader) when the data is stored. When you go to change that record (create,\nupdate, delete), the original hash is included with the bytes that get hashed\nfor this new record. The resulting hash is then attached to the record and\nstored. In this manner, a chain of hashes is formed. Altering a piece of data\nanywhere back in the chain would break the values after that point, invalidating\nthe data, and failing any future transactions.\n\n> My favorite demonstration of this process, why it matters, and how it works, is \nthis video [https://www.youtube.com/watch?v=_160oMzblY8] from CirclePay\nEvangelist, Anders Brownworth [https://twitter.com/anders94]. It starts slow,\nbut I personally guarantee a light bulb moment.\n\n\nBlock. Chain.\nHere we are again. A block represents a group of transactions. You might think\nof this as your bank statement. Data in the block cannot be altered\nretroactively. A chain then refers to the list of ordered records, each with a\nlink to the previous record. Blockchain!\n\nDistributed and Decentralized\nOf course, you would not put a database on a single server and call it done. You\nwould want redundancy, load distribution, and backup, to name a few. The three\nmain architectures to consider when setting up a database cluster (network) are\ncentralized, decentralized, and distributed. Blockchain is a distributed ledger\n(database), with decentralized consensus. Let us break that down in a bit more\ndetail.\n\n\n\nAs a distributed ledger, blockchain networks are peer-to-peer, and can be\ndifficult to maintain. Without a single point of failure however, one broken\nlink in the network only makes more distributed networks, and does not bring\ndown the entire system. The result is a highly scalable network. Since\ncentralized systems follow a single framework, they do not have diversity, and\nevolve slowly. Decentralized and distributed systems have an up-front cost in\nestablishing the architecture, but from there have tremendous evolution.\n\n> If you want a bit more depth on all these terms, and the breakdown of where they\ndo/not work, I suggest this blog post\n[https://medium.com/@bbc4468/centralized-vs-decentralized-vs-distributed-41d92d463868] \nby TinyOwl Founter, Saurabh Goyal [https://twitter.com/bbc4468].\n\n\nByzantine Fault Tolerance\nOne of the problems that emerges in a distributed system is how to coordinate\ncommunication over a potentially unreliable link. This is referred to as the\n\"Two Generals' Problem\" also called the \"Byzantine Generals' Problem\".\n\n\n\nTwo [Byzantine Empire] generals, A1 and A2, are preparing to attack fortified\ncity, B. The armies are encamped near the city, each in its own valley. A third\nvalley separates the two hills, and the only way for the two generals to\ncommunicate is by sending messengers through the valley. While the two generals\nhave agreed to attack, they have not agreed upon a time for the attack.\n\nThis raises a whole variety of problems.\n\nIf A1 sends a message to A2, to say \"Attack at dawn\" A2 may never receive the\nmessage. Or A2 may get the message, but A1 has no way to confirm that. It could\nalso happen that B intercepts the message, alters the attack time, and then\nsends it on to A2. The same problem happens when A2 sends a message to A1. All\nthis, and we have not even introduced the problem that A1 may be traitorous!\n\nThe Two Generals' Problem was the first computer communication problem to be\nproven to be unsolvable.\n\nThis is a problem endemic to a distributed system such as blockchain. In order\nto fix it, you have to make some compromises on the original problem. A\nblockchain implementation such as Hyperledger Fabric\n[https://www.hyperledger.org/], solves this using Practical Byzantine Fault\nTolerance (PBFT). I will not elaborate on the compromises of PBFT here, mostly\nbecause it is an implementation detail that a typical developers will never need\nto think about.\n\nConsensus\nWith you typical database solution, there is an administrator. Or at the very\nleast, various users, roles, and privileges, as to who can work on what data. At\nany point in time however, the administrator can yank to those privileges, and\neven modify the data stored in the database. As you might imagine, having that\ncentral authority can be problematic if we are talking about my bank statements.\n\n\n\nOn a blockchain network, there is no centralized authority that determines the\ntransaction order. Instead, many validating peers (nodes) implement the network\nconsensus protocol. Consensus ensures that a quorum of nodes agree on the order\nin which the transactions are appended to the shared ledger. By resolving any\ndiscrepancies in the proposed transaction order, consensus guarantees that all\nblockchain network nodes are operating on an identical ledger.\n\nProof-of-Work\nThe follow-on question then becomes who are the nodes on the network? What gives\nthem the right to participate in the first place?\n\nWith Bitcoin, anybody can join the network. Consensus this is achieved through\n\"proof-of-work\". In order to commit a transaction, you must first solve a\nmathematical problem unique to that specific transaction. The first peer to\nsolve the transaction wins consensus, and can in turn commit the transaction.\nThis is a permissionless system.\n\nWhat, the what?\n\nConsider a form in a web page. The Internet is an open network. Anybody can\njoin, visit your web form, and submit content that gets stored in your database.\nEven robots. Robots can send junk data into your system, and they can submit\nthat web form really fast - fast enough to bring down the system entirely. A\ncommon technique to solving this problem is CAPTCHA.\n\nWith a CAPTCHA, you are asking the person submitting the data to prove that they\nare a person. They have to do some work to do that, such as reading skewed text,\nor adding two numbers presented in images. Proof-of-work is effectively CAPTCHA\nfor computer.\n\nPermissions\nWhile Bitcoin leverages a permissionless system, there are also permissioned\nblockchain systems. These are often called private blockchains. In a private\nblockchain, peers must be authenticated, usually using certificates. This subtle\nchange has several distinct impacts on blockchain - most notably the speed with\nwhich transactions can be processed.\n\nA private blockchain that provides for some requirement of authentication is\nwhere blockchain becomes especially valuable to businesses.\n\n\n\nThink of your health records. You have a primary physician, but then you see a\nspecialist for a specific medical condition. Likely you will see more than one\nspecialist. If you have other conditions down the road, say a vision problem, or\ncancer, you will be seeing countless physicians, all with bits of your data, but\nwithout any one having the complete picture.\n\nNow think of all the health related data you probably store for yourself. Maybe\nyou take your blood pressure. Track your run lately? Supplement with calcium?\nTrack what you eat and the nutritional values? All this data about you,\nscattered everywhere.\n\nWhat if we had a database that all these different companies could use to work\non your single, secure, health record? How much easier would it be to see that\nspecialist for the first time? What kind of additional treatments might be\nuncovered by having the whole picture of your health? How much earlier could we\ndetect cancer? The possibilities in this one industry alone represent a huge\nopportunity for the practical application of blockchain.\n\nBlockchain\nOkay, so now we have some vocabulary under our belts, let us revisit the\ndefinition of blockchain one last time.\n\n\n\n\n\nOne of the tricks with envisioning where blockchain technology can be useful is\nin separating it from physical assets such as money, or shipping. If you look at\nthe features offered by blockchain, you will find many good applications. A wiki\nfits these characteristics. A shared calendaring system. Even the good old to-do\nlist, where multiple parties (even companies) are organizing a new product\nlaunch, or a summer blockbuster movie release.\n\nNext Steps\nIndeed, industry analysts that are bullish on blockchain see it ultimately\nreplacing the very fabric of how the Internet works today. I do not know if I am\nready to go that far, but it is certainly a technology worth investigating for\nyour next application.\n\nOn those closing notes, if you want to see a to-do list type of application,\nunderstand how it is built, and see it in action, I have put together just such\nan example (repository [https://github.com/IBM/todo-list-fabric]). Check it out,\nfork it and send a pull request if you want to contribute, or just leave a\ncomment below with your thoughts. Happy blockchain!","feature_image":"__GHOST_URL__/content/images/2019/01/chain.close.up-1.jpg","featured":0,"status":"published","locale":null,"visibility":"public","author_id":"1","created_at":"2017-04-11T16:21:56.000Z","updated_at":"2019-01-17T00:43:23.000Z","published_at":"2017-04-11T18:46:46.000Z","custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"type":"post","email_recipient_filter":"none"},{"id":"5adb8922351ffe0018a5774e","uuid":"ec3bba7c-b5c5-4579-97c5-529b08f14ce8","title":"Content Keywords with Watson and Python","slug":"content-keywords-with-watson-and-python","mobiledoc":"{\"version\":\"0.3.1\",\"markups\":[],\"atoms\":[],\"cards\":[[\"markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\"I am always impressed by Python when I have to use it. The community always has great content, and the surface coverage of the language is immense. This allows you to do a whole lot in a very few lines of code. While I generally have to use Python on my Raspberry Pi projects, I recently needed it in aggregating content.\\n\\n####Natural Language Understanding\\n\\nWatson has a couple Natural Language Processing (NLP) APIs. One is a [rote classifying](https://www.ibm.com/watson/developercloud/nl-classifier.html) designed to help you figure out the ==meaning== of the content provided. This is useful in chatbot and other types of applications.\\n\\nThe other API, [Natural Language Understanding](https://www.ibm.com/watson/developercloud/natural-language-understanding.html) (NLU), gets more into the ==semantics== of the content, giving you deep insight into how the language of the content is used. One of the abilities of this API is ==retrieving key words== of the content - perfect for aggregating content.\\n\\n####Requests\\n\\nAs Watson exposes NLU as an ==HTTP-based API==, this makes it very approachable for Python. The key library in Python for making HTTP requests is, as you might expect, [Requests](http://docs.python-requests.org/en/master/).\\n\\n```\\n# Assemble API parameters\\n# Keywords only please\\nparams = {\\n  'version': '2017-02-27',\\n  'url': config['url'],\\n  'features': 'keywords',\\n  'keywords.emotion': 'false',\\n  'keywords.sentiment': 'false',\\n  'keywords.limit': config['limit']\\n}\\n\\n# Call API\\n# JSON results to Python dictionary\\nreq = requests.get( \\n  config['api'], \\n  auth = ( config['username'], config['password'] ), \\n  params = params \\n)\\nres = req.json()\\n```\\n\\nThe parameters can be provided to the API via a ==file upload, or a query string arguments==. I will be using query string arguments for this example. Most of the parameters I am providing here tell Watson to narrow down how it processes the content. I do not need all the semantics for my application, just the keywords.\\n\\n>The content for this API should live on the Web somewhere.\\n\\n==HTTP Basic Authentication== is used widely by the Watson APIs, and the Requests library provides credentials using the \\\"auth\\\" parameter. The Watson username and password will be provided for you when you register the service in your [IBM Bluemix](https://console.ng.bluemix.net/) account.\\n\\nThe results from interacting with the NLU API is a ==JSON response==. Now we have the keywords. Perfect!\\n\\n####Not So Fast\\n\\nPerfect, except, the Watson NLU API keywords may include ==more than one word==. I do not really want this for my application. I want a list of single words. A little Python parsing will make quick work of this for us.\\n\\n```\\n# Hold results\\nresults = []\\n\\n# Iterate found keywords\\nfor keyword in res['keywords']:\\n  # Over relevance threshold\\n  if keyword['relevance'] > config['threshold']:\\n    # Split keywords with more than one word\\n    # Single words into array of one element\\n    if keyword['text'].find( ' ' ) >= 0:\\n      words = keyword['text'].split( ' ' );\\n    else:\\n      words = [keyword['text']]\\n\\n    # Words in this keyword\\n    for word in words:\\n      found = False\\n\\n      # Discard known keywords\\n      for existing in results:\\n        if existing['text'] == word:\\n          found = True\\n          break\\n\\n      # Place into cleaned results array\\n      if found == False:\\n        results.append( {\\n          'relevance': keyword['relevance'],\\n          'text': word\\n        } )\\n```\\n\\nFirst, we need a place to store our new, optimized list, of keywords. \\n\\nNext we start looking through what Watson provided. Watson gives keywords a \\\"==relevance==\\\" score. After about the seventy percentile, it gets kind of broad, so I have opted to limit my optimized list to matches above that range.\\n\\nThen I split the Watson-provided keyword, if it has a space, into an array of ==isolated words==. If there is only one word, then I just shove that into an array with a single element.\\n\\n```\\nFrom this:\\n0.959446: blockchain\\n0.769433: data\\n0.735896: database\\n0.735514: blockchain network nodes\\n0.715231: problem\\n0.700369: data storage tier\\n\\nTo this:\\n0.959446: blockchain\\n0.769433: data\\n0.735896: database\\n0.735514: network\\n0.735514: nodes\\n0.715231: problem\\n0.700369: storage\\n0.700369: tier\\n```\\n\\nKeywords given by Watson may ==show up repeatedly==, and I do not want that - just unique entries, please. To account for that, I check the optimized list against the word in this specific iteration. If it is not found, then I add it to the optimized list.\\n\\n```\\n# Here you go\\nfor keyword in results:\\n  print '{0}: {1}'.format( keyword['relevance'], keyword['text'] )    \\n```\\n\\nFor demonstration purposes, we may want to see what our ==optimized list== looks like. We can use the Python \\\"format()\\\" function to take care of that.\\n\\n####Next Steps\\n\\nFor my aggregator, turn this array of Python dictionary instances into JSON, and store them in [IBM Cloudant](https://cloudant.com/) along with other information. With that, I am able to build a fairly rigorous news feed aggregator, based on the actual language, and not arbitrary string processing. Pretty cool!\\n\\nI have made a [GitHub Gist](https://gist.github.com/krhoyt/e3aa96650a7dfca4bdbc5ca776013c8a) available with the complete code if you find yourself in need of similar functionality. Happy cognitive processing!\\n\"}]],\"sections\":[[10,0]],\"ghostVersion\":\"3.0\"}","html":"<!--kg-card-begin: markdown--><p>I am always impressed by Python when I have to use it. The community always has great content, and the surface coverage of the language is immense. This allows you to do a whole lot in a very few lines of code. While I generally have to use Python on my Raspberry Pi projects, I recently needed it in aggregating content.</p>\n<h4 id=\"naturallanguageunderstanding\">Natural Language Understanding</h4>\n<p>Watson has a couple Natural Language Processing (NLP) APIs. One is a <a href=\"https://www.ibm.com/watson/developercloud/nl-classifier.html\">rote classifying</a> designed to help you figure out the <mark>meaning</mark> of the content provided. This is useful in chatbot and other types of applications.</p>\n<p>The other API, <a href=\"https://www.ibm.com/watson/developercloud/natural-language-understanding.html\">Natural Language Understanding</a> (NLU), gets more into the <mark>semantics</mark> of the content, giving you deep insight into how the language of the content is used. One of the abilities of this API is <mark>retrieving key words</mark> of the content - perfect for aggregating content.</p>\n<h4 id=\"requests\">Requests</h4>\n<p>As Watson exposes NLU as an <mark>HTTP-based API</mark>, this makes it very approachable for Python. The key library in Python for making HTTP requests is, as you might expect, <a href=\"http://docs.python-requests.org/en/master/\">Requests</a>.</p>\n<pre><code># Assemble API parameters\n# Keywords only please\nparams = {\n  'version': '2017-02-27',\n  'url': config['url'],\n  'features': 'keywords',\n  'keywords.emotion': 'false',\n  'keywords.sentiment': 'false',\n  'keywords.limit': config['limit']\n}\n\n# Call API\n# JSON results to Python dictionary\nreq = requests.get( \n  config['api'], \n  auth = ( config['username'], config['password'] ), \n  params = params \n)\nres = req.json()\n</code></pre>\n<p>The parameters can be provided to the API via a <mark>file upload, or a query string arguments</mark>. I will be using query string arguments for this example. Most of the parameters I am providing here tell Watson to narrow down how it processes the content. I do not need all the semantics for my application, just the keywords.</p>\n<blockquote>\n<p>The content for this API should live on the Web somewhere.</p>\n</blockquote>\n<p><mark>HTTP Basic Authentication</mark> is used widely by the Watson APIs, and the Requests library provides credentials using the &quot;auth&quot; parameter. The Watson username and password will be provided for you when you register the service in your <a href=\"https://console.ng.bluemix.net/\">IBM Bluemix</a> account.</p>\n<p>The results from interacting with the NLU API is a <mark>JSON response</mark>. Now we have the keywords. Perfect!</p>\n<h4 id=\"notsofast\">Not So Fast</h4>\n<p>Perfect, except, the Watson NLU API keywords may include <mark>more than one word</mark>. I do not really want this for my application. I want a list of single words. A little Python parsing will make quick work of this for us.</p>\n<pre><code># Hold results\nresults = []\n\n# Iterate found keywords\nfor keyword in res['keywords']:\n  # Over relevance threshold\n  if keyword['relevance'] &gt; config['threshold']:\n    # Split keywords with more than one word\n    # Single words into array of one element\n    if keyword['text'].find( ' ' ) &gt;= 0:\n      words = keyword['text'].split( ' ' );\n    else:\n      words = [keyword['text']]\n\n    # Words in this keyword\n    for word in words:\n      found = False\n\n      # Discard known keywords\n      for existing in results:\n        if existing['text'] == word:\n          found = True\n          break\n\n      # Place into cleaned results array\n      if found == False:\n        results.append( {\n          'relevance': keyword['relevance'],\n          'text': word\n        } )\n</code></pre>\n<p>First, we need a place to store our new, optimized list, of keywords.</p>\n<p>Next we start looking through what Watson provided. Watson gives keywords a &quot;<mark>relevance</mark>&quot; score. After about the seventy percentile, it gets kind of broad, so I have opted to limit my optimized list to matches above that range.</p>\n<p>Then I split the Watson-provided keyword, if it has a space, into an array of <mark>isolated words</mark>. If there is only one word, then I just shove that into an array with a single element.</p>\n<pre><code>From this:\n0.959446: blockchain\n0.769433: data\n0.735896: database\n0.735514: blockchain network nodes\n0.715231: problem\n0.700369: data storage tier\n\nTo this:\n0.959446: blockchain\n0.769433: data\n0.735896: database\n0.735514: network\n0.735514: nodes\n0.715231: problem\n0.700369: storage\n0.700369: tier\n</code></pre>\n<p>Keywords given by Watson may <mark>show up repeatedly</mark>, and I do not want that - just unique entries, please. To account for that, I check the optimized list against the word in this specific iteration. If it is not found, then I add it to the optimized list.</p>\n<pre><code># Here you go\nfor keyword in results:\n  print '{0}: {1}'.format( keyword['relevance'], keyword['text'] )    \n</code></pre>\n<p>For demonstration purposes, we may want to see what our <mark>optimized list</mark> looks like. We can use the Python &quot;format()&quot; function to take care of that.</p>\n<h4 id=\"nextsteps\">Next Steps</h4>\n<p>For my aggregator, turn this array of Python dictionary instances into JSON, and store them in <a href=\"https://cloudant.com/\">IBM Cloudant</a> along with other information. With that, I am able to build a fairly rigorous news feed aggregator, based on the actual language, and not arbitrary string processing. Pretty cool!</p>\n<p>I have made a <a href=\"https://gist.github.com/krhoyt/e3aa96650a7dfca4bdbc5ca776013c8a\">GitHub Gist</a> available with the complete code if you find yourself in need of similar functionality. Happy cognitive processing!</p>\n<!--kg-card-end: markdown-->","comment_id":"69","plaintext":"I am always impressed by Python when I have to use it. The community always has\ngreat content, and the surface coverage of the language is immense. This allows\nyou to do a whole lot in a very few lines of code. While I generally have to use\nPython on my Raspberry Pi projects, I recently needed it in aggregating content.\n\nNatural Language Understanding\nWatson has a couple Natural Language Processing (NLP) APIs. One is a rote\nclassifying [https://www.ibm.com/watson/developercloud/nl-classifier.html] \ndesigned to help you figure out the meaning of the content provided. This is\nuseful in chatbot and other types of applications.\n\nThe other API, Natural Language Understanding\n[https://www.ibm.com/watson/developercloud/natural-language-understanding.html] \n(NLU), gets more into the semantics of the content, giving you deep insight into\nhow the language of the content is used. One of the abilities of this API is \nretrieving key words of the content - perfect for aggregating content.\n\nRequests\nAs Watson exposes NLU as an HTTP-based API, this makes it very approachable for\nPython. The key library in Python for making HTTP requests is, as you might\nexpect, Requests [http://docs.python-requests.org/en/master/].\n\n# Assemble API parameters\n# Keywords only please\nparams = {\n  'version': '2017-02-27',\n  'url': config['url'],\n  'features': 'keywords',\n  'keywords.emotion': 'false',\n  'keywords.sentiment': 'false',\n  'keywords.limit': config['limit']\n}\n\n# Call API\n# JSON results to Python dictionary\nreq = requests.get( \n  config['api'], \n  auth = ( config['username'], config['password'] ), \n  params = params \n)\nres = req.json()\n\n\nThe parameters can be provided to the API via a file upload, or a query string\narguments. I will be using query string arguments for this example. Most of the\nparameters I am providing here tell Watson to narrow down how it processes the\ncontent. I do not need all the semantics for my application, just the keywords.\n\n> The content for this API should live on the Web somewhere.\n\n\nHTTP Basic Authentication is used widely by the Watson APIs, and the Requests\nlibrary provides credentials using the \"auth\" parameter. The Watson username and\npassword will be provided for you when you register the service in your IBM\nBluemix [https://console.ng.bluemix.net/] account.\n\nThe results from interacting with the NLU API is a JSON response. Now we have\nthe keywords. Perfect!\n\nNot So Fast\nPerfect, except, the Watson NLU API keywords may include more than one word. I\ndo not really want this for my application. I want a list of single words. A\nlittle Python parsing will make quick work of this for us.\n\n# Hold results\nresults = []\n\n# Iterate found keywords\nfor keyword in res['keywords']:\n  # Over relevance threshold\n  if keyword['relevance'] > config['threshold']:\n    # Split keywords with more than one word\n    # Single words into array of one element\n    if keyword['text'].find( ' ' ) >= 0:\n      words = keyword['text'].split( ' ' );\n    else:\n      words = [keyword['text']]\n\n    # Words in this keyword\n    for word in words:\n      found = False\n\n      # Discard known keywords\n      for existing in results:\n        if existing['text'] == word:\n          found = True\n          break\n\n      # Place into cleaned results array\n      if found == False:\n        results.append( {\n          'relevance': keyword['relevance'],\n          'text': word\n        } )\n\n\nFirst, we need a place to store our new, optimized list, of keywords.\n\nNext we start looking through what Watson provided. Watson gives keywords a \"\nrelevance\" score. After about the seventy percentile, it gets kind of broad, so\nI have opted to limit my optimized list to matches above that range.\n\nThen I split the Watson-provided keyword, if it has a space, into an array of \nisolated words. If there is only one word, then I just shove that into an array\nwith a single element.\n\nFrom this:\n0.959446: blockchain\n0.769433: data\n0.735896: database\n0.735514: blockchain network nodes\n0.715231: problem\n0.700369: data storage tier\n\nTo this:\n0.959446: blockchain\n0.769433: data\n0.735896: database\n0.735514: network\n0.735514: nodes\n0.715231: problem\n0.700369: storage\n0.700369: tier\n\n\nKeywords given by Watson may show up repeatedly, and I do not want that - just\nunique entries, please. To account for that, I check the optimized list against\nthe word in this specific iteration. If it is not found, then I add it to the\noptimized list.\n\n# Here you go\nfor keyword in results:\n  print '{0}: {1}'.format( keyword['relevance'], keyword['text'] )    \n\n\nFor demonstration purposes, we may want to see what our optimized list looks\nlike. We can use the Python \"format()\" function to take care of that.\n\nNext Steps\nFor my aggregator, turn this array of Python dictionary instances into JSON, and\nstore them in IBM Cloudant [https://cloudant.com/] along with other information.\nWith that, I am able to build a fairly rigorous news feed aggregator, based on\nthe actual language, and not arbitrary string processing. Pretty cool!\n\nI have made a GitHub Gist\n[https://gist.github.com/krhoyt/e3aa96650a7dfca4bdbc5ca776013c8a] available with\nthe complete code if you find yourself in need of similar functionality. Happy\ncognitive processing!","feature_image":"__GHOST_URL__/content/images/2019/01/watson-2.jpg","featured":0,"status":"published","locale":null,"visibility":"public","author_id":"1","created_at":"2017-04-21T20:56:55.000Z","updated_at":"2019-01-17T00:42:56.000Z","published_at":"2017-04-21T21:26:41.000Z","custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"type":"post","email_recipient_filter":"none"},{"id":"5adb8922351ffe0018a5774f","uuid":"5f37a384-0b7e-4d70-a63c-79f38a6359f9","title":"Animated File Upload Button","slug":"animated-file-upload-button","mobiledoc":"{\"version\":\"0.3.1\",\"markups\":[],\"atoms\":[],\"cards\":[[\"markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\"Among the countless ways to perform a file upload from an web page, this will likely be nothing new. Progress bars are also nothing new. However, when I ran across a [post](https://tympanus.net/codrops/2015/09/15/styling-customizing-file-inputs-smart-way/) from [Osvaldas Valutis](https://twitter.com/osvaldas) on customizing the appearance of the file input type, I thought it would be fun to make the file upload the actual progress indicator with a splash of animated SVG (kinda).\\n\\n####The Server\\n\\nEvery uploaded file needs a place to go, and for this I am using Node.js with [Multer](https://github.com/expressjs/multer). I will not cover this extensively here, but for the record, my Express route looks something like the following snippet.\\n\\n```\\nvar express = require( 'express' );\\nvar fs = require( 'fs' );\\nvar multer = require( 'multer' );\\nvar path = require( 'path' );\\nvar randomstring = require( 'randomstring' );\\n\\n// Router\\nvar router = express.Router();\\n\\n// Upload storage options\\n// Unique name with extension\\nvar storage = multer.diskStorage( {\\n  destination: 'uploads',\\n    filename: function( req, file, cb ) {\\n      cb( null, randomstring.generate() + '.jpg' );\\n    }\\n} );\\n\\n// Upload handler\\nvar upload = multer( {\\n  storage: storage\\n} );\\n\\n// Image upload\\nrouter.post( '/upload', upload.single( 'attachment' ), function( req, res ) {\\n  // Get name from path\\n  var parts = req.file.path.split( '/' );\\n  var file = parts[1].split( '.' )[0];\\n\\n  // Respond with name\\n  res.json( {\\n    name: file\\n  } );\\n} );\\n  \\n// Export\\nmodule.exports = router;\\n```\\n\\nOne thing to call out here is that I have a, not-so-thorough, unique naming function. Not so thorough because I ==should be checking== the file system for the randomly determined name - which I also ==lazily assume== to be a JPEG. I just needed somewhere to put the file to test the upload. ==YMMV.==\\n\\n####The HTML\\n\\nTo summarize the post by Osvaldas, you can ==associate the label element with the file input, and hide the file input==. When the label is clicked, the file input will present the selection dialog - no JavaScript needed. Since the label element can be a container for other HTML, ==I drop in some SVG==.\\n\\n```\\n<!-- File upload -->\\n<input id=\\\"upload\\\" type=\\\"file\\\">\\n<label for=\\\"upload\\\">\\n  <!-- SVG content in label -->\\n  <svg width=\\\"56\\\" height=\\\"56\\\">\\n    <!-- Background -->\\n    <circle cx=\\\"28\\\" cy=\\\"28\\\" r=\\\"28\\\" fill=\\\"red\\\"/>\\n\\n    <!-- Icon -->\\n    <!-- From Material Design -->\\n    <path \\n      class=\\\"icon\\\" \\n      d=\\\"M9 16h6v-6h4l-7-7-7 7h4zm-4 2h14v2H5z\\\" \\n      fill=\\\"white\\\" \\n      transform=\\\"translate( 15, 15 )\\\"/>\\n\\n    <!-- Animated completion indicator -->\\n    <!-- Pie slice -->\\n    <path \\n      class=\\\"pie\\\" \\n      d=\\\"M 28 28 L 28 0 A 28 28 1 0 1 28 0 z\\\" \\n      fill=\\\"red\\\" \\n      opacity=\\\"0\\\"/>\\n\\n    <!-- Numeric completion indicator -->\\n    <text \\n      x=\\\"28\\\" \\n      y=\\\"28\\\" \\n      text-anchor=\\\"middle\\\" \\n      fill=\\\"white\\\" \\n      font-size=\\\"14\\\" \\n      font-weight=\\\"700\\\" \\n      dominant-baseline=\\\"central\\\" \\n      opacity=\\\"0\\\">0%</text>\\n  </svg>\\n</label>\\n```\\n\\nThe SVG presents a colored circle with an upload icon inside of it. An element for drawing percentage upload complete is hidden, as is a numeric display for the same value. We will come around to those in a moment. The result looks something like the following image.\\n\\n![Custom file upload button](http://images.kevinhoyt.com/file.upload.circle.png)\\n\\n####The Selection\\n\\nThere are ==two events== we need to concern ourselves with initially - a click on the label surface, and a file selection. We can also go ahead and ==store references== to the elements we will use repeatedly, and ==initialize any values==. I do this in the constructor of an ES6 class I used to encapsulate all the upload functionality.\\n\\n```\\nconstructor( path ) {\\n  this.root = document.querySelector( path );\\n  this.root.addEventListener( 'click', evt => this.doClick( evt ) );\\n\\n  this.svg = this.root.querySelector( 'svg' );\\n\\n  // Hidden file form field\\n  this.file = document.querySelector( '#' + this.root.getAttribute( 'for' ) );\\n  this.file.addEventListener( 'change', evt => this.doFile( evt ) );\\n\\n  // Reference parts\\n  this.circle = this.svg.querySelector( 'circle' );\\n  this.complete = this.svg.querySelector( 'text' );\\n  this.icon = this.svg.querySelector( '.icon' );\\n  this.pie = this.svg.querySelector( '.pie' );\\n\\n  // Complete (0 - 100)\\n  this.percent = 0;\\n}\\n```\\n\\nI said earlier that clicking on the label would trigger file selection automatically without JavaScript. I listen for it anyways because I want to be able to ==prevent selecting another file, during the upload proces==s. If you have a separate form submit button, then you may have another way of approaching this requirement.\\n\\n```\\ndoClick( evt ) {\\n  // Hold up on file selection\\n  evt.preventDefault();\\n\\n  // Not currently uploading\\n  // Proceed\\n  if( this.icon.getAttributeNS( null, 'opacity' ) != '0' ) {\\n    this.file.click();          \\n  }\\n}\\n```\\n\\nHere I prevent the default behavior of the browser to present the file selection dialog. Then I check to see if one of the SVG elements used for indicating ==upload progress is visible==. If it is not, then an upload is not taking place, and we can go ahead and show the file selection dialog anyways.\\n\\n####The Upload\\n\\nI am performing the file upload as soon as the file is selected. No other UI is presented because the only thing I care about for my application in this instance is uploading of single image files in turn. This is for a facial recognition tool I am building that I will blog in the future.\\n\\n```\\n// File selected\\ndoFile( evt ) {\\n  // No file selected\\n  // Abort\\n  if( evt.target.files.length == 0 ) {\\n    return;\\n  }\\n\\n  // Set state for upload reporting\\n  this.icon.setAttributeNS( null, 'opacity', 0 );\\n  this.circle.setAttributeNS( null, 'opacity', 0.50 );\\n  this.pie.setAttributeNS( null, 'opacity', 1 );\\n  this.complete.innerHTML = '0%';\\n  this.complete.setAttributeNS( null, 'opacity', 1 );\\n\\n  // Instantiate\\n  // Hook events if needed\\n  if( this.xhr == null ) {\\n    this.xhr = new XMLHttpRequest();\\n    this.xhr.addEventListener( 'load', evt => this.doLoad( evt ) );\\n    this.xhr.upload.addEventListener( 'progress', evt => this.doProgress( evt ) );          \\n  }\\n\\n  // File to upload\\n  let data = new FormData();\\n  data.append( 'attachment', evt.target.files[0] );\\n\\n  // Send to API\\n  this.xhr.open( 'POST', '/api/image/upload', true );\\n  this.xhr.send( data );\\n}\\n```\\n\\nAs the user may choose not to select a file at all, and present the old \\\"Cancel\\\" button on me, the first thing I do is check to see that a ==file has actually been selected==. Hey! I am not always a lazy programmer!\\n\\nFrom there, I hide the icon path in the SVG, put some opacity on the background circle, show the element to indicate progress and the related numeric display. This effectively ==swaps out state== from waiting for selection to performing an upload.\\n\\nI use a ==XHR instance to perform the file upload==. I like this approach because it gives you a lot of control over the specifics.  To listen for the upload specifically, we will add a \\\"progress\\\" listener on the \\\"xhr.upload\\\" property.\\n\\n>If you put the progress listener on the XHR instance directly, you will only get one event fired, and it will not be the event you want, nor will it contain the correct data.\\n\\nA little splash of the FormData class allows us to ==associate the selected file with the XHR instance==. We then POST the file to the server. Here come the progress events! Time to get animating!\\n\\n####The Animation\\n\\nThe key properties in a progress event are \\\"loaded\\\" and \\\"total\\\" which contain the ==bytes uploaded== so far, and the ==number of bytes in the file==. A little splash of division will let us know the percentage completed.\\n\\n```\\n// Upload in progress\\n// Reflect values in visualization\\ndoProgress( evt ) {\\n  this.percent = ( evt.loaded / evt.total ) * 100;\\n  this.progress();\\n}\\n```\\n\\nIf the file is completely uploaded, then I reset the appearance of the label, ==swapping the state== back to a ready indicator.  \\n\\nI also ==reset the value of the file input==. This allows the user to select the same file repeatedly. If you do not reset the value, then selecting the same file does not indicate a change, and no event will be triggered.\\n\\n```\\n// Animate to current completion\\nprogress() {\\n  // Completed\\n  // Reset to ready\\n  if( this.percent == 100 ) {\\n    this.circle.setAttributeNS( null, 'opacity', 1 );\\n    this.pie.setAttributeNS( null, 'opacity', 0 );\\n    this.complete.setAttributeNS( null, 'opacity', 0 );\\n    this.icon.setAttributeNS( null, 'opacity', 1 );\\n\\n    this.percent = 0;\\n\\n    // Allow selection of same file\\n    // Set value to blank\\n    // Same file will look like a change\\n    this.file.value = '';\\n  }\\n\\n  // Completion arc calculator\\n  let angle = ( this.percent / 100 ) * 360;\\n  let radians = ( angle - 90 ) * Math.PI / 180;\\n  let arc = angle <= 180 ? '0' : '1';\\n  let slice = {\\n    x: 28 + ( 28 * Math.cos( radians ) ),\\n    y: 28 + ( 28 * Math.sin( radians ) )\\n  };\\n  let d = [\\n    'M', 28, 28, \\n    'L', 28, 0,\\n    'A', 28, 28, 1, arc, 1, slice.x, slice.y, \\n    'z'\\n  ].join( ' ' );\\n\\n  // Update completion pie\\n  // Update numeric indicator\\n  this.pie.setAttributeNS( null, 'd', d );\\n  this.complete.innerHTML = Math.round( this.percent ) + '%';\\n}\\n```\\n\\nTo indicate upload progress, I want to show a mini pie chart inside the button, which is already a circle. To correctly draw the pie takes some math. \\n\\nWe want the percent complete to reflect the ==part of the pie==. If a whole pie is 360 degrees, we want the percentage of that for the angle of our slice. We also want that value ==in radians== so we can bust out our mad trigonometry skills.\\n\\nArc sweep is one of the more obtuse, yet useful, parameters for drawing a path. It tells the rendering engine ==which way to draw the arc==. You can get some really wild results if you mess around with these values. In the case of our pie slice, we want the smaller angle drawn until the angle is more than 180 degrees (fifty percent complete). At that point we want the larger angle drawn.\\n\\nThe arc for this animation will start at the top of the circle. That gives us a ==known starting point. The end point is along the outer edge of the circle== reflecting the percentage complete. Great, so what is the X/Y coordinates for that? There is that splash of trigonometry.\\n\\n>Shout out to [this](http://stackoverflow.com/questions/5736398/how-to-calculate-the-svg-path-for-an-arc-of-a-circle) StackOverflow answer on piecing together circle coordinates and arc.\\n\\nOnce we have all those parameters, we can ==put them together to represent the path==, and populate the pie slice element. Not wanting to rely on just a pie, we can also put the ==numeric value== on display.\\n\\n####The Yummy\\n\\nWhen the file is completely uploaded, the \\\"load\\\" event fires on the XHR instance. I want to let any interested parties know that has happened, so I have the label element dispatch a ==custom event==.\\n\\n```\\n// Upload completed\\n// Response recieved\\ndoLoad( evt ) {\\n  let data = JSON.parse( this.xhr.responseText );\\n  console.log( data );\\n    \\n  let event = new CustomEvent( 'progress_complete', {detail: data} );\\n  this.root.dispatchEvent( event );\\n  }\\n```\\n\\n==You may have noticed that my server sends back the file name once the upload is complete.== I go ahead and parse that JSON and send the data along with the custom event.\\n\\nAnd the finished result is:\\n\\n![Animated file upload button.](http://images.kevinhoyt.com/file.upload.circle.gif)\\n\\n####Next Steps\\n\\nWell, first, do not be lazy, go back, and put additional validation on the file name at the server. Second would be to make sure that the image file is a JPEG (or other file type), if that sort of thing is important to your application. I actually intend to add that at the client with a little FileReader action.\\n\\nI have created a [gist](https://gist.github.com/krhoyt/3b6d76af63194052b98e07d970c53d5f) for the files in this post.\\n\"}]],\"sections\":[[10,0]],\"ghostVersion\":\"3.0\"}","html":"<!--kg-card-begin: markdown--><p>Among the countless ways to perform a file upload from an web page, this will likely be nothing new. Progress bars are also nothing new. However, when I ran across a <a href=\"https://tympanus.net/codrops/2015/09/15/styling-customizing-file-inputs-smart-way/\">post</a> from <a href=\"https://twitter.com/osvaldas\">Osvaldas Valutis</a> on customizing the appearance of the file input type, I thought it would be fun to make the file upload the actual progress indicator with a splash of animated SVG (kinda).</p>\n<h4 id=\"theserver\">The Server</h4>\n<p>Every uploaded file needs a place to go, and for this I am using Node.js with <a href=\"https://github.com/expressjs/multer\">Multer</a>. I will not cover this extensively here, but for the record, my Express route looks something like the following snippet.</p>\n<pre><code>var express = require( 'express' );\nvar fs = require( 'fs' );\nvar multer = require( 'multer' );\nvar path = require( 'path' );\nvar randomstring = require( 'randomstring' );\n\n// Router\nvar router = express.Router();\n\n// Upload storage options\n// Unique name with extension\nvar storage = multer.diskStorage( {\n  destination: 'uploads',\n    filename: function( req, file, cb ) {\n      cb( null, randomstring.generate() + '.jpg' );\n    }\n} );\n\n// Upload handler\nvar upload = multer( {\n  storage: storage\n} );\n\n// Image upload\nrouter.post( '/upload', upload.single( 'attachment' ), function( req, res ) {\n  // Get name from path\n  var parts = req.file.path.split( '/' );\n  var file = parts[1].split( '.' )[0];\n\n  // Respond with name\n  res.json( {\n    name: file\n  } );\n} );\n  \n// Export\nmodule.exports = router;\n</code></pre>\n<p>One thing to call out here is that I have a, not-so-thorough, unique naming function. Not so thorough because I <mark>should be checking</mark> the file system for the randomly determined name - which I also <mark>lazily assume</mark> to be a JPEG. I just needed somewhere to put the file to test the upload. <mark>YMMV.</mark></p>\n<h4 id=\"thehtml\">The HTML</h4>\n<p>To summarize the post by Osvaldas, you can <mark>associate the label element with the file input, and hide the file input</mark>. When the label is clicked, the file input will present the selection dialog - no JavaScript needed. Since the label element can be a container for other HTML, <mark>I drop in some SVG</mark>.</p>\n<pre><code>&lt;!-- File upload --&gt;\n&lt;input id=&quot;upload&quot; type=&quot;file&quot;&gt;\n&lt;label for=&quot;upload&quot;&gt;\n  &lt;!-- SVG content in label --&gt;\n  &lt;svg width=&quot;56&quot; height=&quot;56&quot;&gt;\n    &lt;!-- Background --&gt;\n    &lt;circle cx=&quot;28&quot; cy=&quot;28&quot; r=&quot;28&quot; fill=&quot;red&quot;/&gt;\n\n    &lt;!-- Icon --&gt;\n    &lt;!-- From Material Design --&gt;\n    &lt;path \n      class=&quot;icon&quot; \n      d=&quot;M9 16h6v-6h4l-7-7-7 7h4zm-4 2h14v2H5z&quot; \n      fill=&quot;white&quot; \n      transform=&quot;translate( 15, 15 )&quot;/&gt;\n\n    &lt;!-- Animated completion indicator --&gt;\n    &lt;!-- Pie slice --&gt;\n    &lt;path \n      class=&quot;pie&quot; \n      d=&quot;M 28 28 L 28 0 A 28 28 1 0 1 28 0 z&quot; \n      fill=&quot;red&quot; \n      opacity=&quot;0&quot;/&gt;\n\n    &lt;!-- Numeric completion indicator --&gt;\n    &lt;text \n      x=&quot;28&quot; \n      y=&quot;28&quot; \n      text-anchor=&quot;middle&quot; \n      fill=&quot;white&quot; \n      font-size=&quot;14&quot; \n      font-weight=&quot;700&quot; \n      dominant-baseline=&quot;central&quot; \n      opacity=&quot;0&quot;&gt;0%&lt;/text&gt;\n  &lt;/svg&gt;\n&lt;/label&gt;\n</code></pre>\n<p>The SVG presents a colored circle with an upload icon inside of it. An element for drawing percentage upload complete is hidden, as is a numeric display for the same value. We will come around to those in a moment. The result looks something like the following image.</p>\n<p><img src=\"http://images.kevinhoyt.com/file.upload.circle.png\" alt=\"Custom file upload button\" loading=\"lazy\"></p>\n<h4 id=\"theselection\">The Selection</h4>\n<p>There are <mark>two events</mark> we need to concern ourselves with initially - a click on the label surface, and a file selection. We can also go ahead and <mark>store references</mark> to the elements we will use repeatedly, and <mark>initialize any values</mark>. I do this in the constructor of an ES6 class I used to encapsulate all the upload functionality.</p>\n<pre><code>constructor( path ) {\n  this.root = document.querySelector( path );\n  this.root.addEventListener( 'click', evt =&gt; this.doClick( evt ) );\n\n  this.svg = this.root.querySelector( 'svg' );\n\n  // Hidden file form field\n  this.file = document.querySelector( '#' + this.root.getAttribute( 'for' ) );\n  this.file.addEventListener( 'change', evt =&gt; this.doFile( evt ) );\n\n  // Reference parts\n  this.circle = this.svg.querySelector( 'circle' );\n  this.complete = this.svg.querySelector( 'text' );\n  this.icon = this.svg.querySelector( '.icon' );\n  this.pie = this.svg.querySelector( '.pie' );\n\n  // Complete (0 - 100)\n  this.percent = 0;\n}\n</code></pre>\n<p>I said earlier that clicking on the label would trigger file selection automatically without JavaScript. I listen for it anyways because I want to be able to <mark>prevent selecting another file, during the upload proces</mark>s. If you have a separate form submit button, then you may have another way of approaching this requirement.</p>\n<pre><code>doClick( evt ) {\n  // Hold up on file selection\n  evt.preventDefault();\n\n  // Not currently uploading\n  // Proceed\n  if( this.icon.getAttributeNS( null, 'opacity' ) != '0' ) {\n    this.file.click();          \n  }\n}\n</code></pre>\n<p>Here I prevent the default behavior of the browser to present the file selection dialog. Then I check to see if one of the SVG elements used for indicating <mark>upload progress is visible</mark>. If it is not, then an upload is not taking place, and we can go ahead and show the file selection dialog anyways.</p>\n<h4 id=\"theupload\">The Upload</h4>\n<p>I am performing the file upload as soon as the file is selected. No other UI is presented because the only thing I care about for my application in this instance is uploading of single image files in turn. This is for a facial recognition tool I am building that I will blog in the future.</p>\n<pre><code>// File selected\ndoFile( evt ) {\n  // No file selected\n  // Abort\n  if( evt.target.files.length == 0 ) {\n    return;\n  }\n\n  // Set state for upload reporting\n  this.icon.setAttributeNS( null, 'opacity', 0 );\n  this.circle.setAttributeNS( null, 'opacity', 0.50 );\n  this.pie.setAttributeNS( null, 'opacity', 1 );\n  this.complete.innerHTML = '0%';\n  this.complete.setAttributeNS( null, 'opacity', 1 );\n\n  // Instantiate\n  // Hook events if needed\n  if( this.xhr == null ) {\n    this.xhr = new XMLHttpRequest();\n    this.xhr.addEventListener( 'load', evt =&gt; this.doLoad( evt ) );\n    this.xhr.upload.addEventListener( 'progress', evt =&gt; this.doProgress( evt ) );          \n  }\n\n  // File to upload\n  let data = new FormData();\n  data.append( 'attachment', evt.target.files[0] );\n\n  // Send to API\n  this.xhr.open( 'POST', '/api/image/upload', true );\n  this.xhr.send( data );\n}\n</code></pre>\n<p>As the user may choose not to select a file at all, and present the old &quot;Cancel&quot; button on me, the first thing I do is check to see that a <mark>file has actually been selected</mark>. Hey! I am not always a lazy programmer!</p>\n<p>From there, I hide the icon path in the SVG, put some opacity on the background circle, show the element to indicate progress and the related numeric display. This effectively <mark>swaps out state</mark> from waiting for selection to performing an upload.</p>\n<p>I use a <mark>XHR instance to perform the file upload</mark>. I like this approach because it gives you a lot of control over the specifics.  To listen for the upload specifically, we will add a &quot;progress&quot; listener on the &quot;xhr.upload&quot; property.</p>\n<blockquote>\n<p>If you put the progress listener on the XHR instance directly, you will only get one event fired, and it will not be the event you want, nor will it contain the correct data.</p>\n</blockquote>\n<p>A little splash of the FormData class allows us to <mark>associate the selected file with the XHR instance</mark>. We then POST the file to the server. Here come the progress events! Time to get animating!</p>\n<h4 id=\"theanimation\">The Animation</h4>\n<p>The key properties in a progress event are &quot;loaded&quot; and &quot;total&quot; which contain the <mark>bytes uploaded</mark> so far, and the <mark>number of bytes in the file</mark>. A little splash of division will let us know the percentage completed.</p>\n<pre><code>// Upload in progress\n// Reflect values in visualization\ndoProgress( evt ) {\n  this.percent = ( evt.loaded / evt.total ) * 100;\n  this.progress();\n}\n</code></pre>\n<p>If the file is completely uploaded, then I reset the appearance of the label, <mark>swapping the state</mark> back to a ready indicator.</p>\n<p>I also <mark>reset the value of the file input</mark>. This allows the user to select the same file repeatedly. If you do not reset the value, then selecting the same file does not indicate a change, and no event will be triggered.</p>\n<pre><code>// Animate to current completion\nprogress() {\n  // Completed\n  // Reset to ready\n  if( this.percent == 100 ) {\n    this.circle.setAttributeNS( null, 'opacity', 1 );\n    this.pie.setAttributeNS( null, 'opacity', 0 );\n    this.complete.setAttributeNS( null, 'opacity', 0 );\n    this.icon.setAttributeNS( null, 'opacity', 1 );\n\n    this.percent = 0;\n\n    // Allow selection of same file\n    // Set value to blank\n    // Same file will look like a change\n    this.file.value = '';\n  }\n\n  // Completion arc calculator\n  let angle = ( this.percent / 100 ) * 360;\n  let radians = ( angle - 90 ) * Math.PI / 180;\n  let arc = angle &lt;= 180 ? '0' : '1';\n  let slice = {\n    x: 28 + ( 28 * Math.cos( radians ) ),\n    y: 28 + ( 28 * Math.sin( radians ) )\n  };\n  let d = [\n    'M', 28, 28, \n    'L', 28, 0,\n    'A', 28, 28, 1, arc, 1, slice.x, slice.y, \n    'z'\n  ].join( ' ' );\n\n  // Update completion pie\n  // Update numeric indicator\n  this.pie.setAttributeNS( null, 'd', d );\n  this.complete.innerHTML = Math.round( this.percent ) + '%';\n}\n</code></pre>\n<p>To indicate upload progress, I want to show a mini pie chart inside the button, which is already a circle. To correctly draw the pie takes some math.</p>\n<p>We want the percent complete to reflect the <mark>part of the pie</mark>. If a whole pie is 360 degrees, we want the percentage of that for the angle of our slice. We also want that value <mark>in radians</mark> so we can bust out our mad trigonometry skills.</p>\n<p>Arc sweep is one of the more obtuse, yet useful, parameters for drawing a path. It tells the rendering engine <mark>which way to draw the arc</mark>. You can get some really wild results if you mess around with these values. In the case of our pie slice, we want the smaller angle drawn until the angle is more than 180 degrees (fifty percent complete). At that point we want the larger angle drawn.</p>\n<p>The arc for this animation will start at the top of the circle. That gives us a <mark>known starting point. The end point is along the outer edge of the circle</mark> reflecting the percentage complete. Great, so what is the X/Y coordinates for that? There is that splash of trigonometry.</p>\n<blockquote>\n<p>Shout out to <a href=\"http://stackoverflow.com/questions/5736398/how-to-calculate-the-svg-path-for-an-arc-of-a-circle\">this</a> StackOverflow answer on piecing together circle coordinates and arc.</p>\n</blockquote>\n<p>Once we have all those parameters, we can <mark>put them together to represent the path</mark>, and populate the pie slice element. Not wanting to rely on just a pie, we can also put the <mark>numeric value</mark> on display.</p>\n<h4 id=\"theyummy\">The Yummy</h4>\n<p>When the file is completely uploaded, the &quot;load&quot; event fires on the XHR instance. I want to let any interested parties know that has happened, so I have the label element dispatch a <mark>custom event</mark>.</p>\n<pre><code>// Upload completed\n// Response recieved\ndoLoad( evt ) {\n  let data = JSON.parse( this.xhr.responseText );\n  console.log( data );\n    \n  let event = new CustomEvent( 'progress_complete', {detail: data} );\n  this.root.dispatchEvent( event );\n  }\n</code></pre>\n<p><mark>You may have noticed that my server sends back the file name once the upload is complete.</mark> I go ahead and parse that JSON and send the data along with the custom event.</p>\n<p>And the finished result is:</p>\n<p><img src=\"http://images.kevinhoyt.com/file.upload.circle.gif\" alt=\"Animated file upload button.\" loading=\"lazy\"></p>\n<h4 id=\"nextsteps\">Next Steps</h4>\n<p>Well, first, do not be lazy, go back, and put additional validation on the file name at the server. Second would be to make sure that the image file is a JPEG (or other file type), if that sort of thing is important to your application. I actually intend to add that at the client with a little FileReader action.</p>\n<p>I have created a <a href=\"https://gist.github.com/krhoyt/3b6d76af63194052b98e07d970c53d5f\">gist</a> for the files in this post.</p>\n<!--kg-card-end: markdown-->","comment_id":"70","plaintext":"Among the countless ways to perform a file upload from an web page, this will\nlikely be nothing new. Progress bars are also nothing new. However, when I ran\nacross a post\n[https://tympanus.net/codrops/2015/09/15/styling-customizing-file-inputs-smart-way/] \nfrom Osvaldas Valutis [https://twitter.com/osvaldas] on customizing the\nappearance of the file input type, I thought it would be fun to make the file\nupload the actual progress indicator with a splash of animated SVG (kinda).\n\nThe Server\nEvery uploaded file needs a place to go, and for this I am using Node.js with \nMulter [https://github.com/expressjs/multer]. I will not cover this extensively\nhere, but for the record, my Express route looks something like the following\nsnippet.\n\nvar express = require( 'express' );\nvar fs = require( 'fs' );\nvar multer = require( 'multer' );\nvar path = require( 'path' );\nvar randomstring = require( 'randomstring' );\n\n// Router\nvar router = express.Router();\n\n// Upload storage options\n// Unique name with extension\nvar storage = multer.diskStorage( {\n  destination: 'uploads',\n    filename: function( req, file, cb ) {\n      cb( null, randomstring.generate() + '.jpg' );\n    }\n} );\n\n// Upload handler\nvar upload = multer( {\n  storage: storage\n} );\n\n// Image upload\nrouter.post( '/upload', upload.single( 'attachment' ), function( req, res ) {\n  // Get name from path\n  var parts = req.file.path.split( '/' );\n  var file = parts[1].split( '.' )[0];\n\n  // Respond with name\n  res.json( {\n    name: file\n  } );\n} );\n  \n// Export\nmodule.exports = router;\n\n\nOne thing to call out here is that I have a, not-so-thorough, unique naming\nfunction. Not so thorough because I should be checking the file system for the\nrandomly determined name - which I also lazily assume to be a JPEG. I just\nneeded somewhere to put the file to test the upload. YMMV.\n\nThe HTML\nTo summarize the post by Osvaldas, you can associate the label element with the\nfile input, and hide the file input. When the label is clicked, the file input\nwill present the selection dialog - no JavaScript needed. Since the label\nelement can be a container for other HTML, I drop in some SVG.\n\n<!-- File upload -->\n<input id=\"upload\" type=\"file\">\n<label for=\"upload\">\n  <!-- SVG content in label -->\n  <svg width=\"56\" height=\"56\">\n    <!-- Background -->\n    <circle cx=\"28\" cy=\"28\" r=\"28\" fill=\"red\"/>\n\n    <!-- Icon -->\n    <!-- From Material Design -->\n    <path \n      class=\"icon\" \n      d=\"M9 16h6v-6h4l-7-7-7 7h4zm-4 2h14v2H5z\" \n      fill=\"white\" \n      transform=\"translate( 15, 15 )\"/>\n\n    <!-- Animated completion indicator -->\n    <!-- Pie slice -->\n    <path \n      class=\"pie\" \n      d=\"M 28 28 L 28 0 A 28 28 1 0 1 28 0 z\" \n      fill=\"red\" \n      opacity=\"0\"/>\n\n    <!-- Numeric completion indicator -->\n    <text \n      x=\"28\" \n      y=\"28\" \n      text-anchor=\"middle\" \n      fill=\"white\" \n      font-size=\"14\" \n      font-weight=\"700\" \n      dominant-baseline=\"central\" \n      opacity=\"0\">0%</text>\n  </svg>\n</label>\n\n\nThe SVG presents a colored circle with an upload icon inside of it. An element\nfor drawing percentage upload complete is hidden, as is a numeric display for\nthe same value. We will come around to those in a moment. The result looks\nsomething like the following image.\n\n\n\nThe Selection\nThere are two events we need to concern ourselves with initially - a click on\nthe label surface, and a file selection. We can also go ahead and store\nreferences to the elements we will use repeatedly, and initialize any values. I\ndo this in the constructor of an ES6 class I used to encapsulate all the upload\nfunctionality.\n\nconstructor( path ) {\n  this.root = document.querySelector( path );\n  this.root.addEventListener( 'click', evt => this.doClick( evt ) );\n\n  this.svg = this.root.querySelector( 'svg' );\n\n  // Hidden file form field\n  this.file = document.querySelector( '#' + this.root.getAttribute( 'for' ) );\n  this.file.addEventListener( 'change', evt => this.doFile( evt ) );\n\n  // Reference parts\n  this.circle = this.svg.querySelector( 'circle' );\n  this.complete = this.svg.querySelector( 'text' );\n  this.icon = this.svg.querySelector( '.icon' );\n  this.pie = this.svg.querySelector( '.pie' );\n\n  // Complete (0 - 100)\n  this.percent = 0;\n}\n\n\nI said earlier that clicking on the label would trigger file selection\nautomatically without JavaScript. I listen for it anyways because I want to be\nable to prevent selecting another file, during the upload process. If you have a\nseparate form submit button, then you may have another way of approaching this\nrequirement.\n\ndoClick( evt ) {\n  // Hold up on file selection\n  evt.preventDefault();\n\n  // Not currently uploading\n  // Proceed\n  if( this.icon.getAttributeNS( null, 'opacity' ) != '0' ) {\n    this.file.click();          \n  }\n}\n\n\nHere I prevent the default behavior of the browser to present the file selection\ndialog. Then I check to see if one of the SVG elements used for indicating \nupload progress is visible. If it is not, then an upload is not taking place,\nand we can go ahead and show the file selection dialog anyways.\n\nThe Upload\nI am performing the file upload as soon as the file is selected. No other UI is\npresented because the only thing I care about for my application in this\ninstance is uploading of single image files in turn. This is for a facial\nrecognition tool I am building that I will blog in the future.\n\n// File selected\ndoFile( evt ) {\n  // No file selected\n  // Abort\n  if( evt.target.files.length == 0 ) {\n    return;\n  }\n\n  // Set state for upload reporting\n  this.icon.setAttributeNS( null, 'opacity', 0 );\n  this.circle.setAttributeNS( null, 'opacity', 0.50 );\n  this.pie.setAttributeNS( null, 'opacity', 1 );\n  this.complete.innerHTML = '0%';\n  this.complete.setAttributeNS( null, 'opacity', 1 );\n\n  // Instantiate\n  // Hook events if needed\n  if( this.xhr == null ) {\n    this.xhr = new XMLHttpRequest();\n    this.xhr.addEventListener( 'load', evt => this.doLoad( evt ) );\n    this.xhr.upload.addEventListener( 'progress', evt => this.doProgress( evt ) );          \n  }\n\n  // File to upload\n  let data = new FormData();\n  data.append( 'attachment', evt.target.files[0] );\n\n  // Send to API\n  this.xhr.open( 'POST', '/api/image/upload', true );\n  this.xhr.send( data );\n}\n\n\nAs the user may choose not to select a file at all, and present the old \"Cancel\"\nbutton on me, the first thing I do is check to see that a file has actually been\nselected. Hey! I am not always a lazy programmer!\n\nFrom there, I hide the icon path in the SVG, put some opacity on the background\ncircle, show the element to indicate progress and the related numeric display.\nThis effectively swaps out state from waiting for selection to performing an\nupload.\n\nI use a XHR instance to perform the file upload. I like this approach because it\ngives you a lot of control over the specifics. To listen for the upload\nspecifically, we will add a \"progress\" listener on the \"xhr.upload\" property.\n\n> If you put the progress listener on the XHR instance directly, you will only get\none event fired, and it will not be the event you want, nor will it contain the\ncorrect data.\n\n\nA little splash of the FormData class allows us to associate the selected file\nwith the XHR instance. We then POST the file to the server. Here come the\nprogress events! Time to get animating!\n\nThe Animation\nThe key properties in a progress event are \"loaded\" and \"total\" which contain\nthe bytes uploaded so far, and the number of bytes in the file. A little splash\nof division will let us know the percentage completed.\n\n// Upload in progress\n// Reflect values in visualization\ndoProgress( evt ) {\n  this.percent = ( evt.loaded / evt.total ) * 100;\n  this.progress();\n}\n\n\nIf the file is completely uploaded, then I reset the appearance of the label, \nswapping the state back to a ready indicator.\n\nI also reset the value of the file input. This allows the user to select the\nsame file repeatedly. If you do not reset the value, then selecting the same\nfile does not indicate a change, and no event will be triggered.\n\n// Animate to current completion\nprogress() {\n  // Completed\n  // Reset to ready\n  if( this.percent == 100 ) {\n    this.circle.setAttributeNS( null, 'opacity', 1 );\n    this.pie.setAttributeNS( null, 'opacity', 0 );\n    this.complete.setAttributeNS( null, 'opacity', 0 );\n    this.icon.setAttributeNS( null, 'opacity', 1 );\n\n    this.percent = 0;\n\n    // Allow selection of same file\n    // Set value to blank\n    // Same file will look like a change\n    this.file.value = '';\n  }\n\n  // Completion arc calculator\n  let angle = ( this.percent / 100 ) * 360;\n  let radians = ( angle - 90 ) * Math.PI / 180;\n  let arc = angle <= 180 ? '0' : '1';\n  let slice = {\n    x: 28 + ( 28 * Math.cos( radians ) ),\n    y: 28 + ( 28 * Math.sin( radians ) )\n  };\n  let d = [\n    'M', 28, 28, \n    'L', 28, 0,\n    'A', 28, 28, 1, arc, 1, slice.x, slice.y, \n    'z'\n  ].join( ' ' );\n\n  // Update completion pie\n  // Update numeric indicator\n  this.pie.setAttributeNS( null, 'd', d );\n  this.complete.innerHTML = Math.round( this.percent ) + '%';\n}\n\n\nTo indicate upload progress, I want to show a mini pie chart inside the button,\nwhich is already a circle. To correctly draw the pie takes some math.\n\nWe want the percent complete to reflect the part of the pie. If a whole pie is\n360 degrees, we want the percentage of that for the angle of our slice. We also\nwant that value in radians so we can bust out our mad trigonometry skills.\n\nArc sweep is one of the more obtuse, yet useful, parameters for drawing a path.\nIt tells the rendering engine which way to draw the arc. You can get some really\nwild results if you mess around with these values. In the case of our pie slice,\nwe want the smaller angle drawn until the angle is more than 180 degrees (fifty\npercent complete). At that point we want the larger angle drawn.\n\nThe arc for this animation will start at the top of the circle. That gives us a \nknown starting point. The end point is along the outer edge of the circle \nreflecting the percentage complete. Great, so what is the X/Y coordinates for\nthat? There is that splash of trigonometry.\n\n> Shout out to this\n[http://stackoverflow.com/questions/5736398/how-to-calculate-the-svg-path-for-an-arc-of-a-circle] \nStackOverflow answer on piecing together circle coordinates and arc.\n\n\nOnce we have all those parameters, we can put them together to represent the\npath, and populate the pie slice element. Not wanting to rely on just a pie, we\ncan also put the numeric value on display.\n\nThe Yummy\nWhen the file is completely uploaded, the \"load\" event fires on the XHR\ninstance. I want to let any interested parties know that has happened, so I have\nthe label element dispatch a custom event.\n\n// Upload completed\n// Response recieved\ndoLoad( evt ) {\n  let data = JSON.parse( this.xhr.responseText );\n  console.log( data );\n    \n  let event = new CustomEvent( 'progress_complete', {detail: data} );\n  this.root.dispatchEvent( event );\n  }\n\n\nYou may have noticed that my server sends back the file name once the upload is\ncomplete. I go ahead and parse that JSON and send the data along with the custom\nevent.\n\nAnd the finished result is:\n\n\n\nNext Steps\nWell, first, do not be lazy, go back, and put additional validation on the file\nname at the server. Second would be to make sure that the image file is a JPEG\n(or other file type), if that sort of thing is important to your application. I\nactually intend to add that at the client with a little FileReader action.\n\nI have created a gist\n[https://gist.github.com/krhoyt/3b6d76af63194052b98e07d970c53d5f] for the files\nin this post.","feature_image":"__GHOST_URL__/content/images/2019/01/webby.dew-1.jpg","featured":0,"status":"published","locale":null,"visibility":"public","author_id":"1","created_at":"2017-05-18T07:52:07.000Z","updated_at":"2019-01-17T00:42:12.000Z","published_at":"2017-05-18T09:30:47.000Z","custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"type":"post","email_recipient_filter":"none"},{"id":"5adb8922351ffe0018a57750","uuid":"c2bcabc0-00e0-4d95-bcf2-d279d501e612","title":"Face Detection with Tracking.JS","slug":"face-detection-with-tracking-js","mobiledoc":"{\"version\":\"0.3.1\",\"markups\":[],\"atoms\":[],\"cards\":[[\"markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\"Tracking.JS is a JavaScript library that brings a few machine vision algorithms, and a splash of related utilities, to the browser. The [web site](https://trackingjs.com) has live examples with links to the source code for each, but I wanted to test it using my own images. Here is how I went about it, and how you can test using your own images without figuring out any code at all.\\n\\n####The Goal\\n\\n![Test application in action.](http://images.kevinhoyt.com/trackingjs.local.png)\\n\\nRather than work with the project-provided, sized, image, I want to work with my own images - whatever their size. In order to allow others to do the same, I wanted to make this use local features where possible so there was no barrier to entry of forking, configuring, running, etc.\\n\\nWhere I ended was a small application that uses FileReader, drag/drop, and canvas, in conjunction with Tracking.JS to let anybody test face detection. Because it is all local, there is no upload/privacy concern. Just head to the web page, and [check it out](http://temp.kevinhoyt.com/ibm/trackingjs/people/).\\n\\n####The File\\n\\nThe first step is in getting the local file. We can use a standard ==file input== to allow selection through the browser-provided dialog. Once a selection is made, we can use the ==FileReader== class to read the file contents. Then we assign the resulting data URL to an image element for rendering.\\n\\n```\\n// In the ES6 class constructor\\nthis.uploader = document.querySelector( '#uploader' );\\nthis.uploader.addEventListener( 'change', evt => this.doUpload( evt ) );\\n\\nthis.reader = new FileReader();\\nthis.reader.addEventListener( 'load', evt => this.doRead( evt ) );\\n\\nthis.holder = document.querySelector( '#holder' );\\nthis.holder.addEventListener( 'load', evt => this.doImage( evt ) );\\n\\n...\\n\\n// Called when a file is selected\\n// Start analyzing\\ndoUpload( evt ) {\\n  this.process( evt.target.files[0] );\\n}\\n\\n// Read file locally\\nprocess( file ) {\\n  this.reader.readAsDataURL( file );\\n}\\n\\n// Finished reading local file\\n// Populate image element\\ndoRead( evt ) {\\n  this.holder.src = evt.target.result;\\n}\\n\\n// Image element loaded\\n// Scale respective canvas surface\\n// Paint content\\ndoImage( evt ) {\\n  ...\\n}\\n```\\n\\nYou might notice the \\\"process()\\\" method here which is separated from the event handler because it will also be used by drag-and-drop functionality. \\n\\nTo enable ==local drag/drop==, you first need to ignore the default browser behavior and substitute your own when the file is dragged over the browser. Then you also need to ignore the default behavior on the drop event. At that point you will have a reference to the file, which is passed onto the \\\"process()\\\" method for local reading, and populating of the image element.\\n\\n```\\n// In the ES6 class constructor\\nthis.layout = document.querySelector( '#layout' );\\nthis.layout.addEventListener( 'dragover', evt => this.doDragOver( evt ) );\\nthis.layout.addEventListener( 'drop', evt => this.doDragDrop( evt ) );\\n\\n...\\n\\n// File is dragged over viewport\\n// Prevent default behavior (view)\\n// Enable drop\\ndoDragOver( evt ) {\\n  evt.stopPropagation();\\n  evt.preventDefault();\\n  evt.dataTransfer.dropEffect = 'copy';    \\n}\\n\\n// File dropped on viewport\\n// Prevent default behavior (view)\\n// Start analyzing\\ndoDragDrop( evt ) {\\n  evt.stopPropagation();\\n  evt.preventDefault();\\n  this.process( evt.dataTransfer.files[0] );\\n}\\n```\\n\\n####The Render\\n\\nBecause the selected file may be very large, larger than the browser viewport, we will want to ==render it to fit==. Canvas is a good way to do this, and also gives us an easy way to draw highlights on the detected faces.\\n\\n```\\n// Image element loaded\\n// Scale respective canvas surface\\n// Paint content\\ndoImage( evt ) {\\n  // Original image ratio\\n  // Used to keep dimensions consistent\\n  let ratio = this.holder.clientWidth / this.holder.clientHeight;\\n\\n  // Landscape or portrait\\n  // Size canvas respectively\\n  if( this.holder.clientWidth > this.holder.clientHeight ) {\\n    this.surface.width = Math.round( window.innerWidth * People.LANDSCAPE_SCALE );\\n    this.surface.height = this.surface.width / ratio;\\n  } else {\\n    this.surface.height = Math.round( window.innerHeight * People.PORTRAIT_SCALE );\\n    this.surface.width = this.surface.height * ratio;      \\n  }\\n\\n  // Get context\\n  // Draw scaled image on to canvas\\n  this.context = this.surface.getContext( '2d' );\\n  this.context.drawImage( this.holder, 0, 0, this.surface.width, this.surface.height );    \\n\\n  // Find faces\\n  tracking.track( '#surface', this.tracker );    \\n}\\n```\\n\\nWe want to make sure we keep the original dimensions of the file as we scale it down, and then account for landscape and portrait orientations. From there we can use the ==canvas context== \\\"drawImage()\\\" method to draw from the image element, to the canvas, and scale the image to fit along the way.\\n\\n####The Analysis\\n\\nWith the image on the canvas, the next step is to call Tracking.JS to analyze the image. The \\\"tracking.track()\\\" ==static method== takes an image or canvas element, and a reference to the type of tracking to perform. You can look for faces, mouths, and eyes, or any combination therein.\\n\\n```\\n// In the ES6 class constructor\\nthis.tracker = new tracking.ObjectTracker( 'face' );\\nthis.tracker.setStepSize( People.STEP_SIZE );\\nthis.tracker.addListener( 'track', evt => this.doTrack( evt ) );\\n\\n\\n// Facial tracking completed\\n// Highlight faces\\ndoTrack( evt ) {\\n  // Style\\n  this.context.beginPath();\\n  this.context.lineWidth = 6;\\n  this.context.strokeStyle = 'yellow';\\n\\n  // Faces\\n  for( let face of evt.data ) {\\n    this.context.rect( face.x, face.y, face.width, face.height );\\n  }\\n\\n  // Draw\\n  this.context.stroke();\\n\\n  // Show\\n  this.surface.style.opacity = 1.0;\\n\\n  // Reset for same file selection\\n  this.uploader.value = '';\\n}\\n```\\n\\nThe result from the tracking is an ==array of found features== - in this case, the faces that were detected. We can iterate through the results, and then use the canvas context to draw rectangles to highlight the faces.\\n\\n####Next Steps\\n\\nYou will notice that never does the file actually get uploaded to a server. There is ==no server needed== beyond serving the web page and associated assets. This means there are also no privacy concerns. You can now see if Tracking.JS will work for you by testing your images locally.\\n\\nI answered a few GitHub issues for the Tracking.JS project, and found that it seems to be ==largely abandoned==. That does not make it any less useful. It would be great to see the library updated to ES6 or TypeScript (or the likes). Maybe some big company with loads of resources like IBM could pick up that work.\\n\\nProject files are in a [gist](https://gist.github.com/krhoyt/5b19e64fc58276a7c2f55ded7fd93a99) if you want the whole source.\\n\"}]],\"sections\":[[10,0]],\"ghostVersion\":\"3.0\"}","html":"<!--kg-card-begin: markdown--><p>Tracking.JS is a JavaScript library that brings a few machine vision algorithms, and a splash of related utilities, to the browser. The <a href=\"https://trackingjs.com\">web site</a> has live examples with links to the source code for each, but I wanted to test it using my own images. Here is how I went about it, and how you can test using your own images without figuring out any code at all.</p>\n<h4 id=\"thegoal\">The Goal</h4>\n<p><img src=\"http://images.kevinhoyt.com/trackingjs.local.png\" alt=\"Test application in action.\" loading=\"lazy\"></p>\n<p>Rather than work with the project-provided, sized, image, I want to work with my own images - whatever their size. In order to allow others to do the same, I wanted to make this use local features where possible so there was no barrier to entry of forking, configuring, running, etc.</p>\n<p>Where I ended was a small application that uses FileReader, drag/drop, and canvas, in conjunction with Tracking.JS to let anybody test face detection. Because it is all local, there is no upload/privacy concern. Just head to the web page, and <a href=\"http://temp.kevinhoyt.com/ibm/trackingjs/people/\">check it out</a>.</p>\n<h4 id=\"thefile\">The File</h4>\n<p>The first step is in getting the local file. We can use a standard <mark>file input</mark> to allow selection through the browser-provided dialog. Once a selection is made, we can use the <mark>FileReader</mark> class to read the file contents. Then we assign the resulting data URL to an image element for rendering.</p>\n<pre><code>// In the ES6 class constructor\nthis.uploader = document.querySelector( '#uploader' );\nthis.uploader.addEventListener( 'change', evt =&gt; this.doUpload( evt ) );\n\nthis.reader = new FileReader();\nthis.reader.addEventListener( 'load', evt =&gt; this.doRead( evt ) );\n\nthis.holder = document.querySelector( '#holder' );\nthis.holder.addEventListener( 'load', evt =&gt; this.doImage( evt ) );\n\n...\n\n// Called when a file is selected\n// Start analyzing\ndoUpload( evt ) {\n  this.process( evt.target.files[0] );\n}\n\n// Read file locally\nprocess( file ) {\n  this.reader.readAsDataURL( file );\n}\n\n// Finished reading local file\n// Populate image element\ndoRead( evt ) {\n  this.holder.src = evt.target.result;\n}\n\n// Image element loaded\n// Scale respective canvas surface\n// Paint content\ndoImage( evt ) {\n  ...\n}\n</code></pre>\n<p>You might notice the &quot;process()&quot; method here which is separated from the event handler because it will also be used by drag-and-drop functionality.</p>\n<p>To enable <mark>local drag/drop</mark>, you first need to ignore the default browser behavior and substitute your own when the file is dragged over the browser. Then you also need to ignore the default behavior on the drop event. At that point you will have a reference to the file, which is passed onto the &quot;process()&quot; method for local reading, and populating of the image element.</p>\n<pre><code>// In the ES6 class constructor\nthis.layout = document.querySelector( '#layout' );\nthis.layout.addEventListener( 'dragover', evt =&gt; this.doDragOver( evt ) );\nthis.layout.addEventListener( 'drop', evt =&gt; this.doDragDrop( evt ) );\n\n...\n\n// File is dragged over viewport\n// Prevent default behavior (view)\n// Enable drop\ndoDragOver( evt ) {\n  evt.stopPropagation();\n  evt.preventDefault();\n  evt.dataTransfer.dropEffect = 'copy';    \n}\n\n// File dropped on viewport\n// Prevent default behavior (view)\n// Start analyzing\ndoDragDrop( evt ) {\n  evt.stopPropagation();\n  evt.preventDefault();\n  this.process( evt.dataTransfer.files[0] );\n}\n</code></pre>\n<h4 id=\"therender\">The Render</h4>\n<p>Because the selected file may be very large, larger than the browser viewport, we will want to <mark>render it to fit</mark>. Canvas is a good way to do this, and also gives us an easy way to draw highlights on the detected faces.</p>\n<pre><code>// Image element loaded\n// Scale respective canvas surface\n// Paint content\ndoImage( evt ) {\n  // Original image ratio\n  // Used to keep dimensions consistent\n  let ratio = this.holder.clientWidth / this.holder.clientHeight;\n\n  // Landscape or portrait\n  // Size canvas respectively\n  if( this.holder.clientWidth &gt; this.holder.clientHeight ) {\n    this.surface.width = Math.round( window.innerWidth * People.LANDSCAPE_SCALE );\n    this.surface.height = this.surface.width / ratio;\n  } else {\n    this.surface.height = Math.round( window.innerHeight * People.PORTRAIT_SCALE );\n    this.surface.width = this.surface.height * ratio;      \n  }\n\n  // Get context\n  // Draw scaled image on to canvas\n  this.context = this.surface.getContext( '2d' );\n  this.context.drawImage( this.holder, 0, 0, this.surface.width, this.surface.height );    \n\n  // Find faces\n  tracking.track( '#surface', this.tracker );    \n}\n</code></pre>\n<p>We want to make sure we keep the original dimensions of the file as we scale it down, and then account for landscape and portrait orientations. From there we can use the <mark>canvas context</mark> &quot;drawImage()&quot; method to draw from the image element, to the canvas, and scale the image to fit along the way.</p>\n<h4 id=\"theanalysis\">The Analysis</h4>\n<p>With the image on the canvas, the next step is to call Tracking.JS to analyze the image. The &quot;tracking.track()&quot; <mark>static method</mark> takes an image or canvas element, and a reference to the type of tracking to perform. You can look for faces, mouths, and eyes, or any combination therein.</p>\n<pre><code>// In the ES6 class constructor\nthis.tracker = new tracking.ObjectTracker( 'face' );\nthis.tracker.setStepSize( People.STEP_SIZE );\nthis.tracker.addListener( 'track', evt =&gt; this.doTrack( evt ) );\n\n\n// Facial tracking completed\n// Highlight faces\ndoTrack( evt ) {\n  // Style\n  this.context.beginPath();\n  this.context.lineWidth = 6;\n  this.context.strokeStyle = 'yellow';\n\n  // Faces\n  for( let face of evt.data ) {\n    this.context.rect( face.x, face.y, face.width, face.height );\n  }\n\n  // Draw\n  this.context.stroke();\n\n  // Show\n  this.surface.style.opacity = 1.0;\n\n  // Reset for same file selection\n  this.uploader.value = '';\n}\n</code></pre>\n<p>The result from the tracking is an <mark>array of found features</mark> - in this case, the faces that were detected. We can iterate through the results, and then use the canvas context to draw rectangles to highlight the faces.</p>\n<h4 id=\"nextsteps\">Next Steps</h4>\n<p>You will notice that never does the file actually get uploaded to a server. There is <mark>no server needed</mark> beyond serving the web page and associated assets. This means there are also no privacy concerns. You can now see if Tracking.JS will work for you by testing your images locally.</p>\n<p>I answered a few GitHub issues for the Tracking.JS project, and found that it seems to be <mark>largely abandoned</mark>. That does not make it any less useful. It would be great to see the library updated to ES6 or TypeScript (or the likes). Maybe some big company with loads of resources like IBM could pick up that work.</p>\n<p>Project files are in a <a href=\"https://gist.github.com/krhoyt/5b19e64fc58276a7c2f55ded7fd93a99\">gist</a> if you want the whole source.</p>\n<!--kg-card-end: markdown-->","comment_id":"71","plaintext":"Tracking.JS is a JavaScript library that brings a few machine vision algorithms,\nand a splash of related utilities, to the browser. The web site\n[https://trackingjs.com] has live examples with links to the source code for\neach, but I wanted to test it using my own images. Here is how I went about it,\nand how you can test using your own images without figuring out any code at all.\n\nThe Goal\n\n\nRather than work with the project-provided, sized, image, I want to work with my\nown images - whatever their size. In order to allow others to do the same, I\nwanted to make this use local features where possible so there was no barrier to\nentry of forking, configuring, running, etc.\n\nWhere I ended was a small application that uses FileReader, drag/drop, and\ncanvas, in conjunction with Tracking.JS to let anybody test face detection.\nBecause it is all local, there is no upload/privacy concern. Just head to the\nweb page, and check it out [http://temp.kevinhoyt.com/ibm/trackingjs/people/].\n\nThe File\nThe first step is in getting the local file. We can use a standard file input to\nallow selection through the browser-provided dialog. Once a selection is made,\nwe can use the FileReader class to read the file contents. Then we assign the\nresulting data URL to an image element for rendering.\n\n// In the ES6 class constructor\nthis.uploader = document.querySelector( '#uploader' );\nthis.uploader.addEventListener( 'change', evt => this.doUpload( evt ) );\n\nthis.reader = new FileReader();\nthis.reader.addEventListener( 'load', evt => this.doRead( evt ) );\n\nthis.holder = document.querySelector( '#holder' );\nthis.holder.addEventListener( 'load', evt => this.doImage( evt ) );\n\n...\n\n// Called when a file is selected\n// Start analyzing\ndoUpload( evt ) {\n  this.process( evt.target.files[0] );\n}\n\n// Read file locally\nprocess( file ) {\n  this.reader.readAsDataURL( file );\n}\n\n// Finished reading local file\n// Populate image element\ndoRead( evt ) {\n  this.holder.src = evt.target.result;\n}\n\n// Image element loaded\n// Scale respective canvas surface\n// Paint content\ndoImage( evt ) {\n  ...\n}\n\n\nYou might notice the \"process()\" method here which is separated from the event\nhandler because it will also be used by drag-and-drop functionality.\n\nTo enable local drag/drop, you first need to ignore the default browser behavior\nand substitute your own when the file is dragged over the browser. Then you also\nneed to ignore the default behavior on the drop event. At that point you will\nhave a reference to the file, which is passed onto the \"process()\" method for\nlocal reading, and populating of the image element.\n\n// In the ES6 class constructor\nthis.layout = document.querySelector( '#layout' );\nthis.layout.addEventListener( 'dragover', evt => this.doDragOver( evt ) );\nthis.layout.addEventListener( 'drop', evt => this.doDragDrop( evt ) );\n\n...\n\n// File is dragged over viewport\n// Prevent default behavior (view)\n// Enable drop\ndoDragOver( evt ) {\n  evt.stopPropagation();\n  evt.preventDefault();\n  evt.dataTransfer.dropEffect = 'copy';    \n}\n\n// File dropped on viewport\n// Prevent default behavior (view)\n// Start analyzing\ndoDragDrop( evt ) {\n  evt.stopPropagation();\n  evt.preventDefault();\n  this.process( evt.dataTransfer.files[0] );\n}\n\n\nThe Render\nBecause the selected file may be very large, larger than the browser viewport,\nwe will want to render it to fit. Canvas is a good way to do this, and also\ngives us an easy way to draw highlights on the detected faces.\n\n// Image element loaded\n// Scale respective canvas surface\n// Paint content\ndoImage( evt ) {\n  // Original image ratio\n  // Used to keep dimensions consistent\n  let ratio = this.holder.clientWidth / this.holder.clientHeight;\n\n  // Landscape or portrait\n  // Size canvas respectively\n  if( this.holder.clientWidth > this.holder.clientHeight ) {\n    this.surface.width = Math.round( window.innerWidth * People.LANDSCAPE_SCALE );\n    this.surface.height = this.surface.width / ratio;\n  } else {\n    this.surface.height = Math.round( window.innerHeight * People.PORTRAIT_SCALE );\n    this.surface.width = this.surface.height * ratio;      \n  }\n\n  // Get context\n  // Draw scaled image on to canvas\n  this.context = this.surface.getContext( '2d' );\n  this.context.drawImage( this.holder, 0, 0, this.surface.width, this.surface.height );    \n\n  // Find faces\n  tracking.track( '#surface', this.tracker );    \n}\n\n\nWe want to make sure we keep the original dimensions of the file as we scale it\ndown, and then account for landscape and portrait orientations. From there we\ncan use the canvas context \"drawImage()\" method to draw from the image element,\nto the canvas, and scale the image to fit along the way.\n\nThe Analysis\nWith the image on the canvas, the next step is to call Tracking.JS to analyze\nthe image. The \"tracking.track()\" static method takes an image or canvas\nelement, and a reference to the type of tracking to perform. You can look for\nfaces, mouths, and eyes, or any combination therein.\n\n// In the ES6 class constructor\nthis.tracker = new tracking.ObjectTracker( 'face' );\nthis.tracker.setStepSize( People.STEP_SIZE );\nthis.tracker.addListener( 'track', evt => this.doTrack( evt ) );\n\n\n// Facial tracking completed\n// Highlight faces\ndoTrack( evt ) {\n  // Style\n  this.context.beginPath();\n  this.context.lineWidth = 6;\n  this.context.strokeStyle = 'yellow';\n\n  // Faces\n  for( let face of evt.data ) {\n    this.context.rect( face.x, face.y, face.width, face.height );\n  }\n\n  // Draw\n  this.context.stroke();\n\n  // Show\n  this.surface.style.opacity = 1.0;\n\n  // Reset for same file selection\n  this.uploader.value = '';\n}\n\n\nThe result from the tracking is an array of found features - in this case, the\nfaces that were detected. We can iterate through the results, and then use the\ncanvas context to draw rectangles to highlight the faces.\n\nNext Steps\nYou will notice that never does the file actually get uploaded to a server.\nThere is no server needed beyond serving the web page and associated assets.\nThis means there are also no privacy concerns. You can now see if Tracking.JS\nwill work for you by testing your images locally.\n\nI answered a few GitHub issues for the Tracking.JS project, and found that it\nseems to be largely abandoned. That does not make it any less useful. It would\nbe great to see the library updated to ES6 or TypeScript (or the likes). Maybe\nsome big company with loads of resources like IBM could pick up that work.\n\nProject files are in a gist\n[https://gist.github.com/krhoyt/5b19e64fc58276a7c2f55ded7fd93a99] if you want\nthe whole source.","feature_image":"__GHOST_URL__/content/images/2019/01/webby.dew.jpg","featured":0,"status":"published","locale":null,"visibility":"public","author_id":"1","created_at":"2017-05-18T22:39:24.000Z","updated_at":"2019-01-17T00:41:18.000Z","published_at":"2017-05-22T20:25:49.000Z","custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"type":"post","email_recipient_filter":"none"},{"id":"5adb8922351ffe0018a57751","uuid":"f1e924e1-8af5-4b1d-9c8c-fd39ed5dbd05","title":"Particle and Google Maps Integration","slug":"particle-and-google-maps-integration","mobiledoc":"{\"version\":\"0.3.1\",\"markups\":[],\"atoms\":[],\"cards\":[[\"markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\"At Google IO 2017 just a few weeks ago, Particle announced an integration with Google Maps. This integration allowed you to get the location of a Photon without the need for a GPS. Instead location is achieved by referencing the wireless access points in the vicinity (a less scary way of saying all your IPs are belong to us). I thought this was pretty slick and wanted to give it a test drive.\\n\\n####It Works\\n\\nIt took me all of ==fifteen minutes== to get the integration working. While Particle has [documentation](https://docs.particle.io/tutorials/integrations/google-maps/) on the feature, I actually landed on [this tutorial](https://particle.hackster.io/gusgonnet/map-your-particles-e34878?ref=channel&ref_id=286_trending___&offset=0) by [Gustavo Gonnet](https://github.com/gusgonnet), first. Accuracy for my Photon, tested on different wireless networks in different parts of town, might as well have been using GPS - it was that accurate. Bravo!\\n\\n####Ch-Ch-Ch-Ch-Changes\\n\\nIn Gustavo's tutorial, you eventually get to a point where you clone a Google repository with an example ==Node.js application== for mapping your Photons in the the browser. You start the Node.js application, head to the browser, ==log into the application== using your Particle account, and then you get to see the location of your Photons.\\n\\nI felt the whole logging into the web page to be kind of out of place. I am guessing that they were going for broad use. So the first thing I did was to ==migrate the login to the server using a configuration file== that original application was already using.\\n\\n```\\nvar Particle = require( 'particle-api-js' );\\n\\n// Particle\\nvar particle = new Particle();\\n\\n// Login\\nparticle.login( {\\n  username: config.particle_username, \\n  password: config.particle_password \\n} ).then(\\n  function( data ) {\\n    // Listen to event stream\\n    // Specific to my devices\\n    // Can use device ID if known\\n    particle.getEventStream( { \\n      auth: data.body.access_token,\\n      deviceId: 'mine',\\n    } ).then( function( stream ) {\\n      // Stream event arrived\\n      stream.on( 'event', function( evt ) {\\n        // Look for location-specific event\\n        if( evt.name.startsWith( 'hook-response/' + config.event_name ) ) {\\n          // Parse out location details\\n          var parts = evt.data.split( ',' );\\n          \\n          // Assemble message\\n          var msg = JSON.stringify( {\\n            id: evt.name.split( '/' )[2],\\n            published: evt.published_at,\\n            position: {\\n              lat: parseFloat( parts[0] ),\\n              lng: parseFloat( parts[1] ),\\n            },\\n            accuracy: parseInt( parts[2] )\\n          } );          \\n\\n          // Send to clients\\n          io.emit( 'location', msg );\\n        }       \\n      } );\\n    } );    \\n  },\\n  function( err ) {\\n    console.log( err );\\n  }\\n);\\n```\\n\\nThat original application was also using Express templates for the client map view. There is only one page, and the use of templating is very limited, so next I ==separated out the client and server== completely. Given that the original application used WebSockets to push the latest location information, this transition was pretty easy.\\n\\n```\\n// Socket\\nvar io = require( 'socket.io' )( server );\\n\\n...\\n\\n// Send to clients\\nio.emit( 'location', msg );\\n```\\n\\nMoving away from the views, separating out client and server, and pulling the Particle login to the server, let me ==strip out a lot of overhead around session management==, etc. from the original application. I also moved to ==Socket IO== which has robust client connection fallback, with cleaner integration at the server.\\n\\n```\\nclass Location {\\n  constructor() {\\n    // Create the map instance\\n    // Set default location\\n    // Set default zoom level\\n    this._map = new google.maps.Map( document.querySelector( '#map' ), {\\n      center: {lat: 41.1106266, lng: -73.7248718},\\n      zoom: 14\\n    } );\\n    \\n    // Marker showing location\\n    // Wait for first location event\\n    this._marker = null;\\n    \\n    // Socket\\n    // Listen for location events\\n    this._socket = io();\\n    this._socket.on( 'location', evt => this.doLocation( evt ) );\\n  }\\n    \\n  // Location event\\n  // Position marker and map\\n  doLocation( evt ) {\\n    // Parse JSON\\n    var data = JSON.parse( evt );\\n    console.log( data );\\n    \\n    // First location event\\n    // Instantiate marker\\n    if( this._marker == null ) {\\n      this._marker = new google.maps.Marker( {\\n        map: this._map\\n      } );                \\n    }\\n    \\n    // Position marker\\n    // Center map\\n    this._marker.setPosition( data.position );\\n    this._map.setCenter( data.position );         \\n  }\\n};\\n\\n// Here we go!\\nlet app = new Location();\\n```\\n\\n####Next Steps\\n\\nWhen I was originally testing the Particle stream monitoring at the server (Node.js), I was shocked at how many stream events are published across the ==entire Particle Cloud==. If you leave out the \\\"mine\\\" parameter, you will see everything not otherwise locked down.\\n\\nThe sheer scale of the data makes me wonder if I could feed it all into ==Watson==, and see what machine learning could make of it all. The full stream is totally \\\"dark data\\\" varying widely in structure. That is a perfect Watson application. Maybe some aspect of Watson in general would make for a good next Particle integration.\\n\\nI also posted my code into a [GitHub Gist](https://gist.github.com/krhoyt/b1ef78a6cacbd79ea103351bb4aa979c) if you want to check it out for yourself.\\n\"}]],\"sections\":[[10,0]],\"ghostVersion\":\"3.0\"}","html":"<!--kg-card-begin: markdown--><p>At Google IO 2017 just a few weeks ago, Particle announced an integration with Google Maps. This integration allowed you to get the location of a Photon without the need for a GPS. Instead location is achieved by referencing the wireless access points in the vicinity (a less scary way of saying all your IPs are belong to us). I thought this was pretty slick and wanted to give it a test drive.</p>\n<h4 id=\"itworks\">It Works</h4>\n<p>It took me all of <mark>fifteen minutes</mark> to get the integration working. While Particle has <a href=\"https://docs.particle.io/tutorials/integrations/google-maps/\">documentation</a> on the feature, I actually landed on <a href=\"https://particle.hackster.io/gusgonnet/map-your-particles-e34878?ref=channel&amp;ref_id=286_trending___&amp;offset=0\">this tutorial</a> by <a href=\"https://github.com/gusgonnet\">Gustavo Gonnet</a>, first. Accuracy for my Photon, tested on different wireless networks in different parts of town, might as well have been using GPS - it was that accurate. Bravo!</p>\n<h4 id=\"chchchchchanges\">Ch-Ch-Ch-Ch-Changes</h4>\n<p>In Gustavo's tutorial, you eventually get to a point where you clone a Google repository with an example <mark>Node.js application</mark> for mapping your Photons in the the browser. You start the Node.js application, head to the browser, <mark>log into the application</mark> using your Particle account, and then you get to see the location of your Photons.</p>\n<p>I felt the whole logging into the web page to be kind of out of place. I am guessing that they were going for broad use. So the first thing I did was to <mark>migrate the login to the server using a configuration file</mark> that original application was already using.</p>\n<pre><code>var Particle = require( 'particle-api-js' );\n\n// Particle\nvar particle = new Particle();\n\n// Login\nparticle.login( {\n  username: config.particle_username, \n  password: config.particle_password \n} ).then(\n  function( data ) {\n    // Listen to event stream\n    // Specific to my devices\n    // Can use device ID if known\n    particle.getEventStream( { \n      auth: data.body.access_token,\n      deviceId: 'mine',\n    } ).then( function( stream ) {\n      // Stream event arrived\n      stream.on( 'event', function( evt ) {\n        // Look for location-specific event\n        if( evt.name.startsWith( 'hook-response/' + config.event_name ) ) {\n          // Parse out location details\n          var parts = evt.data.split( ',' );\n          \n          // Assemble message\n          var msg = JSON.stringify( {\n            id: evt.name.split( '/' )[2],\n            published: evt.published_at,\n            position: {\n              lat: parseFloat( parts[0] ),\n              lng: parseFloat( parts[1] ),\n            },\n            accuracy: parseInt( parts[2] )\n          } );          \n\n          // Send to clients\n          io.emit( 'location', msg );\n        }       \n      } );\n    } );    \n  },\n  function( err ) {\n    console.log( err );\n  }\n);\n</code></pre>\n<p>That original application was also using Express templates for the client map view. There is only one page, and the use of templating is very limited, so next I <mark>separated out the client and server</mark> completely. Given that the original application used WebSockets to push the latest location information, this transition was pretty easy.</p>\n<pre><code>// Socket\nvar io = require( 'socket.io' )( server );\n\n...\n\n// Send to clients\nio.emit( 'location', msg );\n</code></pre>\n<p>Moving away from the views, separating out client and server, and pulling the Particle login to the server, let me <mark>strip out a lot of overhead around session management</mark>, etc. from the original application. I also moved to <mark>Socket IO</mark> which has robust client connection fallback, with cleaner integration at the server.</p>\n<pre><code>class Location {\n  constructor() {\n    // Create the map instance\n    // Set default location\n    // Set default zoom level\n    this._map = new google.maps.Map( document.querySelector( '#map' ), {\n      center: {lat: 41.1106266, lng: -73.7248718},\n      zoom: 14\n    } );\n    \n    // Marker showing location\n    // Wait for first location event\n    this._marker = null;\n    \n    // Socket\n    // Listen for location events\n    this._socket = io();\n    this._socket.on( 'location', evt =&gt; this.doLocation( evt ) );\n  }\n    \n  // Location event\n  // Position marker and map\n  doLocation( evt ) {\n    // Parse JSON\n    var data = JSON.parse( evt );\n    console.log( data );\n    \n    // First location event\n    // Instantiate marker\n    if( this._marker == null ) {\n      this._marker = new google.maps.Marker( {\n        map: this._map\n      } );                \n    }\n    \n    // Position marker\n    // Center map\n    this._marker.setPosition( data.position );\n    this._map.setCenter( data.position );         \n  }\n};\n\n// Here we go!\nlet app = new Location();\n</code></pre>\n<h4 id=\"nextsteps\">Next Steps</h4>\n<p>When I was originally testing the Particle stream monitoring at the server (Node.js), I was shocked at how many stream events are published across the <mark>entire Particle Cloud</mark>. If you leave out the &quot;mine&quot; parameter, you will see everything not otherwise locked down.</p>\n<p>The sheer scale of the data makes me wonder if I could feed it all into <mark>Watson</mark>, and see what machine learning could make of it all. The full stream is totally &quot;dark data&quot; varying widely in structure. That is a perfect Watson application. Maybe some aspect of Watson in general would make for a good next Particle integration.</p>\n<p>I also posted my code into a <a href=\"https://gist.github.com/krhoyt/b1ef78a6cacbd79ea103351bb4aa979c\">GitHub Gist</a> if you want to check it out for yourself.</p>\n<!--kg-card-end: markdown-->","comment_id":"72","plaintext":"At Google IO 2017 just a few weeks ago, Particle announced an integration with\nGoogle Maps. This integration allowed you to get the location of a Photon\nwithout the need for a GPS. Instead location is achieved by referencing the\nwireless access points in the vicinity (a less scary way of saying all your IPs\nare belong to us). I thought this was pretty slick and wanted to give it a test\ndrive.\n\nIt Works\nIt took me all of fifteen minutes to get the integration working. While Particle\nhas documentation [https://docs.particle.io/tutorials/integrations/google-maps/] \non the feature, I actually landed on this tutorial\n[https://particle.hackster.io/gusgonnet/map-your-particles-e34878?ref=channel&ref_id=286_trending___&offset=0] \nby Gustavo Gonnet [https://github.com/gusgonnet], first. Accuracy for my Photon,\ntested on different wireless networks in different parts of town, might as well\nhave been using GPS - it was that accurate. Bravo!\n\nCh-Ch-Ch-Ch-Changes\nIn Gustavo's tutorial, you eventually get to a point where you clone a Google\nrepository with an example Node.js application for mapping your Photons in the\nthe browser. You start the Node.js application, head to the browser, log into\nthe application using your Particle account, and then you get to see the\nlocation of your Photons.\n\nI felt the whole logging into the web page to be kind of out of place. I am\nguessing that they were going for broad use. So the first thing I did was to \nmigrate the login to the server using a configuration file that original\napplication was already using.\n\nvar Particle = require( 'particle-api-js' );\n\n// Particle\nvar particle = new Particle();\n\n// Login\nparticle.login( {\n  username: config.particle_username, \n  password: config.particle_password \n} ).then(\n  function( data ) {\n    // Listen to event stream\n    // Specific to my devices\n    // Can use device ID if known\n    particle.getEventStream( { \n      auth: data.body.access_token,\n      deviceId: 'mine',\n    } ).then( function( stream ) {\n      // Stream event arrived\n      stream.on( 'event', function( evt ) {\n        // Look for location-specific event\n        if( evt.name.startsWith( 'hook-response/' + config.event_name ) ) {\n          // Parse out location details\n          var parts = evt.data.split( ',' );\n          \n          // Assemble message\n          var msg = JSON.stringify( {\n            id: evt.name.split( '/' )[2],\n            published: evt.published_at,\n            position: {\n              lat: parseFloat( parts[0] ),\n              lng: parseFloat( parts[1] ),\n            },\n            accuracy: parseInt( parts[2] )\n          } );          \n\n          // Send to clients\n          io.emit( 'location', msg );\n        }       \n      } );\n    } );    \n  },\n  function( err ) {\n    console.log( err );\n  }\n);\n\n\nThat original application was also using Express templates for the client map\nview. There is only one page, and the use of templating is very limited, so next\nI separated out the client and server completely. Given that the original\napplication used WebSockets to push the latest location information, this\ntransition was pretty easy.\n\n// Socket\nvar io = require( 'socket.io' )( server );\n\n...\n\n// Send to clients\nio.emit( 'location', msg );\n\n\nMoving away from the views, separating out client and server, and pulling the\nParticle login to the server, let me strip out a lot of overhead around session\nmanagement, etc. from the original application. I also moved to Socket IO which\nhas robust client connection fallback, with cleaner integration at the server.\n\nclass Location {\n  constructor() {\n    // Create the map instance\n    // Set default location\n    // Set default zoom level\n    this._map = new google.maps.Map( document.querySelector( '#map' ), {\n      center: {lat: 41.1106266, lng: -73.7248718},\n      zoom: 14\n    } );\n    \n    // Marker showing location\n    // Wait for first location event\n    this._marker = null;\n    \n    // Socket\n    // Listen for location events\n    this._socket = io();\n    this._socket.on( 'location', evt => this.doLocation( evt ) );\n  }\n    \n  // Location event\n  // Position marker and map\n  doLocation( evt ) {\n    // Parse JSON\n    var data = JSON.parse( evt );\n    console.log( data );\n    \n    // First location event\n    // Instantiate marker\n    if( this._marker == null ) {\n      this._marker = new google.maps.Marker( {\n        map: this._map\n      } );                \n    }\n    \n    // Position marker\n    // Center map\n    this._marker.setPosition( data.position );\n    this._map.setCenter( data.position );         \n  }\n};\n\n// Here we go!\nlet app = new Location();\n\n\nNext Steps\nWhen I was originally testing the Particle stream monitoring at the server\n(Node.js), I was shocked at how many stream events are published across the \nentire Particle Cloud. If you leave out the \"mine\" parameter, you will see\neverything not otherwise locked down.\n\nThe sheer scale of the data makes me wonder if I could feed it all into Watson,\nand see what machine learning could make of it all. The full stream is totally\n\"dark data\" varying widely in structure. That is a perfect Watson application.\nMaybe some aspect of Watson in general would make for a good next Particle\nintegration.\n\nI also posted my code into a GitHub Gist\n[https://gist.github.com/krhoyt/b1ef78a6cacbd79ea103351bb4aa979c] if you want to\ncheck it out for yourself.","feature_image":"__GHOST_URL__/content/images/2019/01/pony.express.jpg","featured":0,"status":"published","locale":null,"visibility":"public","author_id":"1","created_at":"2017-05-28T18:47:48.000Z","updated_at":"2019-01-17T00:40:22.000Z","published_at":"2017-05-28T18:47:00.000Z","custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"type":"post","email_recipient_filter":"none"},{"id":"5adb8922351ffe0018a57752","uuid":"27d17702-573b-4ae3-b663-cde9da2fe1ec","title":"Preflight Image Hashing","slug":"preflight-image-hashing","mobiledoc":"{\"version\":\"0.3.1\",\"markups\":[],\"atoms\":[],\"cards\":[[\"markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\"Sometime earlier this year, I became enamored with image hashing. The idea is to read an image, perform various maths on the RGB color model, and output a unique string. Effectively, an image fingerprint. It turns out there are a lot of ways to do approach this task, usually trading off speed and accuracy - images can be rotated, resized, recolored, etc. and potentially get the same fingerprint.\\n\\n####==Update==\\n\\nShortly after writing this post, it occurred to me that I could speed up the perceived performance by leaning on ==Web Workers== - using a ==background thread== for the math-heavy image analysis. The following example does not use that approach, however if that is something you are interested in, please let me know and I will do a more complete updated post.\\n\\n####The Problem\\n\\nI was recently working on a project that allowed people to ==upload images== to be analyzed in various ways. Along the way, that person may ==forget what images== they have already uploaded. In those cases, the application ==did not need to do the upload== at all. This seemed like a perfect fit for image hashing.\\n\\nWith image hashing, I could get the fingerprint of the file ==locally before uploading it==, check the server to see if that fingerprint had already been uploaded, and then upload the image if necessary, or retrieve the previous analysis results. [FileReader](https://developer.mozilla.org/en-US/docs/Web/API/FileReader) would allow me to access the file locally, and I could get the RGB color model from putting the image on a [canvas element](https://developer.mozilla.org/en-US/docs/Web/API/CanvasRenderingContext2D/getImageData).\\n\\n####Blockhash.IO\\n\\nI originally started off performing this hash check on the server using the \\\"[imghash](https://github.com/pwlmaciejewski/imghash)\\\" package. The \\\"imghash\\\" package uses a module called \\\"[blockhash-js](https://github.com/commonsmachinery/blockhash-js)\\\" which is run from [Blockhash.IO](http://blockhash.io/). Blockhash IO can supposedly work in the browser as well, but I encountered two problems. \\n\\nThe first problem, like so many other JavaScript libraries, is that ==nobody had updated== the code in over three years, and there were ==bugs== in getting it running in the browser.  The second, is that the library used ==XHR to load the images== to be hashed from the server. Avoiding moving the image over the wire was the whole point of the effort.\\n\\nThere were issues filed, and even pull requests that would have made substantial improvements, but the project organizers disagreed on the changes, which ended in no forward progress. In the end, I decided to ==combine a number of the snippets== provided in the issues, with other snippets of improvements from the pull requests, and throw in a dash of my own ==local file access== knowledge. I also ported it, loosely, over to ==ES6== along the way.\\n\\n####Behold the Hash\\n\\nThe first step to image hashing in the browser is to get a ==file reference==. There are a few ways to manage this, but I used ==drag and drop== from outside the browser. \\n\\n```\\n// File dropped\\n// Perform hash\\ndoDragDrop( evt ) {\\n  evt.preventDefault();\\n  console.log( 'File drop.' );\\n\\n  this.file = evt.dataTransfer.files[0];\\n  this.blockhash.blockhash( \\n    this.file, \\n    8, \\n    2, \\n    hash => this.doHash( hash ) \\n  );\\n}\\n```\\n\\nSince the image may need to be uploaded if no matching hash/fingerprint is found, we will store a ==reference to the file==. The main method in the Blockhash library is \\\"blockhash( file, bits, method, callback )\\\". The \\\"bits\\\" and \\\"method\\\" parameters allows for ==tuning for speed== depending on your needs. When it is hashing the image, the provided callback will be called with the resulting hash value.\\n\\n>It is worth noting that on fairly sizable images hashing will take some time. In my case 4,300 x 2,900 pixel images, weighing in at 1.5 Mb took about one second to hash. Smaller images were demonstrably faster.\\n\\nAfter all the processing that goes on behind the scenes, the final fingerprint looks something like \\\"==033e1c3c34b4343c==\\\". That is the ==unique fingerprint== for this picture of my daughter in the cockpit of a helicopter. It does not matter the size or coloring of the image, that fingerprint will always be the result.\\n\\n![Paige in a helicopter.](http://images.kevinhoyt.com/helicopter.paige.jpg)\\n\\n####Ground Control to Major Tom\\n\\nNext up is to check at the server for the existence of this hash. You can do this in whatever language you want. I exposed a simple API endpoint in Node.js that checked the file system for an image with a ==name matching the hash==.\\n\\n```\\n// Image hashed\\n// See if it exists on the server\\ndoHash( hash ) {\\n  console.log( 'Hash: ' + hash );\\n\\n  this.xhr.addEventListener( 'load', this.doPreflightLoad );\\n  this.xhr.open( 'POST', '/api/image/preflight', true );\\n  this.xhr.setRequestHeader( 'Content-Type', 'application/json' );    \\n  this.xhr.send( JSON.stringify( {\\n    hash: hash\\n  } ) );\\n}\\n```\\n\\nIn this case, since image hashes are unique fingerprints, I chose to store the uploaded image using the ==hash at the name==. Another way to go about this would be to use a dynamically generated name, and then store the mapping to the ==hash in a data store==.\\n\\n####Commencing Countdown\\n\\nWhen the server responds, you will know if this file has been uploaded before or not. If it has been uploaded, and you want to display the image, you can either read the local file again, or load the image from the server. In my case, I chose to ==load the image from the server==.\\n\\n```\\n// File check completed\\n// Populate if exists\\n// Upload file if it does not\\ndoPreflightLoad( evt ) {\\n  let data = JSON.parse( this.xhr.responseText );\\n  console.log( data );\\n\\n  // Clean up\\n  this.xhr.removeEventListener( 'load', this.doPreflightLoad );\\n\\n  // Exists\\n  // Populate image element\\n  if( data.exists ) {\\n    console.log( 'Found.' );\\n    this.show( data.hash );\\n  } else {\\n    // Does not exist\\n    // Upload file to server\\n    console.log( 'Not found.' );      \\n\\n    // Form\\n    let data = new FormData();\\n    data.append( 'image', this.file );\\n\\n    // Upload\\n    this.xhr.addEventListener( 'load', this.doUpload );\\n    this.xhr.open( 'POST', '/api/image/original', true );\\n    this.xhr.send( data );\\n  }\\n}\\n```\\n\\nIf the file ==does not exist==, then it will need to be ==uploaded==. For this I use a [FormData](https://developer.mozilla.org/en-US/docs/Web/API/FormData) instance, the file reference we stored at the beginning of the process, and a POST to another Node.js API endpoint. When the file is uploaded, the response contains the same hash, which I then specify as a path in an image element, and load it from the server.\\n\\n####Engines On\\n\\nThere are some ==trade-offs== I am making by taking this approach. Most notably is that there is a considerable amount of processing that happens on the client, and that may ==slow down the user interaction==. Would it have been faster to simply uploaded the file and manage the hashing at the server? In my tests, doing the work on the client, and uploading only when necessary, was ==at least a little faster== than the \\\"always upload route\\\" - demonstrably faster for smaller images.\\n\\nIf ==reducing duplicate image uploads== is a concern for your project, then image hashing on the client is certainly worth consideration. Given that the fingerprint comes in handy for other uses, certainly makes it worth using liberally on the server, and in the data store.\\n\\nI have placed the pertinent parts of the application, including my port of Blockhash.IO to the browser using FileReader, in a [GitHub Gist](https://gist.github.com/krhoyt/cf9de4eed5f406de044fd1851fb0f5dc) should you find it useful for your project.\"}]],\"sections\":[[10,0]],\"ghostVersion\":\"3.0\"}","html":"<!--kg-card-begin: markdown--><p>Sometime earlier this year, I became enamored with image hashing. The idea is to read an image, perform various maths on the RGB color model, and output a unique string. Effectively, an image fingerprint. It turns out there are a lot of ways to do approach this task, usually trading off speed and accuracy - images can be rotated, resized, recolored, etc. and potentially get the same fingerprint.</p>\n<h4 id=\"update\"><mark>Update</mark></h4>\n<p>Shortly after writing this post, it occurred to me that I could speed up the perceived performance by leaning on <mark>Web Workers</mark> - using a <mark>background thread</mark> for the math-heavy image analysis. The following example does not use that approach, however if that is something you are interested in, please let me know and I will do a more complete updated post.</p>\n<h4 id=\"theproblem\">The Problem</h4>\n<p>I was recently working on a project that allowed people to <mark>upload images</mark> to be analyzed in various ways. Along the way, that person may <mark>forget what images</mark> they have already uploaded. In those cases, the application <mark>did not need to do the upload</mark> at all. This seemed like a perfect fit for image hashing.</p>\n<p>With image hashing, I could get the fingerprint of the file <mark>locally before uploading it</mark>, check the server to see if that fingerprint had already been uploaded, and then upload the image if necessary, or retrieve the previous analysis results. <a href=\"https://developer.mozilla.org/en-US/docs/Web/API/FileReader\">FileReader</a> would allow me to access the file locally, and I could get the RGB color model from putting the image on a <a href=\"https://developer.mozilla.org/en-US/docs/Web/API/CanvasRenderingContext2D/getImageData\">canvas element</a>.</p>\n<h4 id=\"blockhashio\">Blockhash.IO</h4>\n<p>I originally started off performing this hash check on the server using the &quot;<a href=\"https://github.com/pwlmaciejewski/imghash\">imghash</a>&quot; package. The &quot;imghash&quot; package uses a module called &quot;<a href=\"https://github.com/commonsmachinery/blockhash-js\">blockhash-js</a>&quot; which is run from <a href=\"http://blockhash.io/\">Blockhash.IO</a>. Blockhash IO can supposedly work in the browser as well, but I encountered two problems.</p>\n<p>The first problem, like so many other JavaScript libraries, is that <mark>nobody had updated</mark> the code in over three years, and there were <mark>bugs</mark> in getting it running in the browser.  The second, is that the library used <mark>XHR to load the images</mark> to be hashed from the server. Avoiding moving the image over the wire was the whole point of the effort.</p>\n<p>There were issues filed, and even pull requests that would have made substantial improvements, but the project organizers disagreed on the changes, which ended in no forward progress. In the end, I decided to <mark>combine a number of the snippets</mark> provided in the issues, with other snippets of improvements from the pull requests, and throw in a dash of my own <mark>local file access</mark> knowledge. I also ported it, loosely, over to <mark>ES6</mark> along the way.</p>\n<h4 id=\"beholdthehash\">Behold the Hash</h4>\n<p>The first step to image hashing in the browser is to get a <mark>file reference</mark>. There are a few ways to manage this, but I used <mark>drag and drop</mark> from outside the browser.</p>\n<pre><code>// File dropped\n// Perform hash\ndoDragDrop( evt ) {\n  evt.preventDefault();\n  console.log( 'File drop.' );\n\n  this.file = evt.dataTransfer.files[0];\n  this.blockhash.blockhash( \n    this.file, \n    8, \n    2, \n    hash =&gt; this.doHash( hash ) \n  );\n}\n</code></pre>\n<p>Since the image may need to be uploaded if no matching hash/fingerprint is found, we will store a <mark>reference to the file</mark>. The main method in the Blockhash library is &quot;blockhash( file, bits, method, callback )&quot;. The &quot;bits&quot; and &quot;method&quot; parameters allows for <mark>tuning for speed</mark> depending on your needs. When it is hashing the image, the provided callback will be called with the resulting hash value.</p>\n<blockquote>\n<p>It is worth noting that on fairly sizable images hashing will take some time. In my case 4,300 x 2,900 pixel images, weighing in at 1.5 Mb took about one second to hash. Smaller images were demonstrably faster.</p>\n</blockquote>\n<p>After all the processing that goes on behind the scenes, the final fingerprint looks something like &quot;<mark>033e1c3c34b4343c</mark>&quot;. That is the <mark>unique fingerprint</mark> for this picture of my daughter in the cockpit of a helicopter. It does not matter the size or coloring of the image, that fingerprint will always be the result.</p>\n<p><img src=\"http://images.kevinhoyt.com/helicopter.paige.jpg\" alt=\"Paige in a helicopter.\" loading=\"lazy\"></p>\n<h4 id=\"groundcontroltomajortom\">Ground Control to Major Tom</h4>\n<p>Next up is to check at the server for the existence of this hash. You can do this in whatever language you want. I exposed a simple API endpoint in Node.js that checked the file system for an image with a <mark>name matching the hash</mark>.</p>\n<pre><code>// Image hashed\n// See if it exists on the server\ndoHash( hash ) {\n  console.log( 'Hash: ' + hash );\n\n  this.xhr.addEventListener( 'load', this.doPreflightLoad );\n  this.xhr.open( 'POST', '/api/image/preflight', true );\n  this.xhr.setRequestHeader( 'Content-Type', 'application/json' );    \n  this.xhr.send( JSON.stringify( {\n    hash: hash\n  } ) );\n}\n</code></pre>\n<p>In this case, since image hashes are unique fingerprints, I chose to store the uploaded image using the <mark>hash at the name</mark>. Another way to go about this would be to use a dynamically generated name, and then store the mapping to the <mark>hash in a data store</mark>.</p>\n<h4 id=\"commencingcountdown\">Commencing Countdown</h4>\n<p>When the server responds, you will know if this file has been uploaded before or not. If it has been uploaded, and you want to display the image, you can either read the local file again, or load the image from the server. In my case, I chose to <mark>load the image from the server</mark>.</p>\n<pre><code>// File check completed\n// Populate if exists\n// Upload file if it does not\ndoPreflightLoad( evt ) {\n  let data = JSON.parse( this.xhr.responseText );\n  console.log( data );\n\n  // Clean up\n  this.xhr.removeEventListener( 'load', this.doPreflightLoad );\n\n  // Exists\n  // Populate image element\n  if( data.exists ) {\n    console.log( 'Found.' );\n    this.show( data.hash );\n  } else {\n    // Does not exist\n    // Upload file to server\n    console.log( 'Not found.' );      \n\n    // Form\n    let data = new FormData();\n    data.append( 'image', this.file );\n\n    // Upload\n    this.xhr.addEventListener( 'load', this.doUpload );\n    this.xhr.open( 'POST', '/api/image/original', true );\n    this.xhr.send( data );\n  }\n}\n</code></pre>\n<p>If the file <mark>does not exist</mark>, then it will need to be <mark>uploaded</mark>. For this I use a <a href=\"https://developer.mozilla.org/en-US/docs/Web/API/FormData\">FormData</a> instance, the file reference we stored at the beginning of the process, and a POST to another Node.js API endpoint. When the file is uploaded, the response contains the same hash, which I then specify as a path in an image element, and load it from the server.</p>\n<h4 id=\"engineson\">Engines On</h4>\n<p>There are some <mark>trade-offs</mark> I am making by taking this approach. Most notably is that there is a considerable amount of processing that happens on the client, and that may <mark>slow down the user interaction</mark>. Would it have been faster to simply uploaded the file and manage the hashing at the server? In my tests, doing the work on the client, and uploading only when necessary, was <mark>at least a little faster</mark> than the &quot;always upload route&quot; - demonstrably faster for smaller images.</p>\n<p>If <mark>reducing duplicate image uploads</mark> is a concern for your project, then image hashing on the client is certainly worth consideration. Given that the fingerprint comes in handy for other uses, certainly makes it worth using liberally on the server, and in the data store.</p>\n<p>I have placed the pertinent parts of the application, including my port of Blockhash.IO to the browser using FileReader, in a <a href=\"https://gist.github.com/krhoyt/cf9de4eed5f406de044fd1851fb0f5dc\">GitHub Gist</a> should you find it useful for your project.</p>\n<!--kg-card-end: markdown-->","comment_id":"74","plaintext":"Sometime earlier this year, I became enamored with image hashing. The idea is to\nread an image, perform various maths on the RGB color model, and output a unique\nstring. Effectively, an image fingerprint. It turns out there are a lot of ways\nto do approach this task, usually trading off speed and accuracy - images can be\nrotated, resized, recolored, etc. and potentially get the same fingerprint.\n\nUpdate\nShortly after writing this post, it occurred to me that I could speed up the\nperceived performance by leaning on Web Workers - using a background thread for\nthe math-heavy image analysis. The following example does not use that approach,\nhowever if that is something you are interested in, please let me know and I\nwill do a more complete updated post.\n\nThe Problem\nI was recently working on a project that allowed people to upload images to be\nanalyzed in various ways. Along the way, that person may forget what images they\nhave already uploaded. In those cases, the application did not need to do the\nupload at all. This seemed like a perfect fit for image hashing.\n\nWith image hashing, I could get the fingerprint of the file locally before\nuploading it, check the server to see if that fingerprint had already been\nuploaded, and then upload the image if necessary, or retrieve the previous\nanalysis results. FileReader\n[https://developer.mozilla.org/en-US/docs/Web/API/FileReader] would allow me to\naccess the file locally, and I could get the RGB color model from putting the\nimage on a canvas element\n[https://developer.mozilla.org/en-US/docs/Web/API/CanvasRenderingContext2D/getImageData]\n.\n\nBlockhash.IO\nI originally started off performing this hash check on the server using the \"\nimghash [https://github.com/pwlmaciejewski/imghash]\" package. The \"imghash\"\npackage uses a module called \"blockhash-js\n[https://github.com/commonsmachinery/blockhash-js]\" which is run from \nBlockhash.IO [http://blockhash.io/]. Blockhash IO can supposedly work in the\nbrowser as well, but I encountered two problems.\n\nThe first problem, like so many other JavaScript libraries, is that nobody had\nupdated the code in over three years, and there were bugs in getting it running\nin the browser. The second, is that the library used XHR to load the images to\nbe hashed from the server. Avoiding moving the image over the wire was the whole\npoint of the effort.\n\nThere were issues filed, and even pull requests that would have made substantial\nimprovements, but the project organizers disagreed on the changes, which ended\nin no forward progress. In the end, I decided to combine a number of the\nsnippets provided in the issues, with other snippets of improvements from the\npull requests, and throw in a dash of my own local file access knowledge. I also\nported it, loosely, over to ES6 along the way.\n\nBehold the Hash\nThe first step to image hashing in the browser is to get a file reference. There\nare a few ways to manage this, but I used drag and drop from outside the\nbrowser.\n\n// File dropped\n// Perform hash\ndoDragDrop( evt ) {\n  evt.preventDefault();\n  console.log( 'File drop.' );\n\n  this.file = evt.dataTransfer.files[0];\n  this.blockhash.blockhash( \n    this.file, \n    8, \n    2, \n    hash => this.doHash( hash ) \n  );\n}\n\n\nSince the image may need to be uploaded if no matching hash/fingerprint is\nfound, we will store a reference to the file. The main method in the Blockhash\nlibrary is \"blockhash( file, bits, method, callback )\". The \"bits\" and \"method\"\nparameters allows for tuning for speed depending on your needs. When it is\nhashing the image, the provided callback will be called with the resulting hash\nvalue.\n\n> It is worth noting that on fairly sizable images hashing will take some time. In\nmy case 4,300 x 2,900 pixel images, weighing in at 1.5 Mb took about one second\nto hash. Smaller images were demonstrably faster.\n\n\nAfter all the processing that goes on behind the scenes, the final fingerprint\nlooks something like \"033e1c3c34b4343c\". That is the unique fingerprint for this\npicture of my daughter in the cockpit of a helicopter. It does not matter the\nsize or coloring of the image, that fingerprint will always be the result.\n\n\n\nGround Control to Major Tom\nNext up is to check at the server for the existence of this hash. You can do\nthis in whatever language you want. I exposed a simple API endpoint in Node.js\nthat checked the file system for an image with a name matching the hash.\n\n// Image hashed\n// See if it exists on the server\ndoHash( hash ) {\n  console.log( 'Hash: ' + hash );\n\n  this.xhr.addEventListener( 'load', this.doPreflightLoad );\n  this.xhr.open( 'POST', '/api/image/preflight', true );\n  this.xhr.setRequestHeader( 'Content-Type', 'application/json' );    \n  this.xhr.send( JSON.stringify( {\n    hash: hash\n  } ) );\n}\n\n\nIn this case, since image hashes are unique fingerprints, I chose to store the\nuploaded image using the hash at the name. Another way to go about this would be\nto use a dynamically generated name, and then store the mapping to the hash in a\ndata store.\n\nCommencing Countdown\nWhen the server responds, you will know if this file has been uploaded before or\nnot. If it has been uploaded, and you want to display the image, you can either\nread the local file again, or load the image from the server. In my case, I\nchose to load the image from the server.\n\n// File check completed\n// Populate if exists\n// Upload file if it does not\ndoPreflightLoad( evt ) {\n  let data = JSON.parse( this.xhr.responseText );\n  console.log( data );\n\n  // Clean up\n  this.xhr.removeEventListener( 'load', this.doPreflightLoad );\n\n  // Exists\n  // Populate image element\n  if( data.exists ) {\n    console.log( 'Found.' );\n    this.show( data.hash );\n  } else {\n    // Does not exist\n    // Upload file to server\n    console.log( 'Not found.' );      \n\n    // Form\n    let data = new FormData();\n    data.append( 'image', this.file );\n\n    // Upload\n    this.xhr.addEventListener( 'load', this.doUpload );\n    this.xhr.open( 'POST', '/api/image/original', true );\n    this.xhr.send( data );\n  }\n}\n\n\nIf the file does not exist, then it will need to be uploaded. For this I use a \nFormData [https://developer.mozilla.org/en-US/docs/Web/API/FormData] instance,\nthe file reference we stored at the beginning of the process, and a POST to\nanother Node.js API endpoint. When the file is uploaded, the response contains\nthe same hash, which I then specify as a path in an image element, and load it\nfrom the server.\n\nEngines On\nThere are some trade-offs I am making by taking this approach. Most notably is\nthat there is a considerable amount of processing that happens on the client,\nand that may slow down the user interaction. Would it have been faster to simply\nuploaded the file and manage the hashing at the server? In my tests, doing the\nwork on the client, and uploading only when necessary, was at least a little\nfaster than the \"always upload route\" - demonstrably faster for smaller images.\n\nIf reducing duplicate image uploads is a concern for your project, then image\nhashing on the client is certainly worth consideration. Given that the\nfingerprint comes in handy for other uses, certainly makes it worth using\nliberally on the server, and in the data store.\n\nI have placed the pertinent parts of the application, including my port of\nBlockhash.IO to the browser using FileReader, in a GitHub Gist\n[https://gist.github.com/krhoyt/cf9de4eed5f406de044fd1851fb0f5dc] should you\nfind it useful for your project.","feature_image":"__GHOST_URL__/content/images/2019/01/hashbrowns.jpg","featured":0,"status":"published","locale":null,"visibility":"public","author_id":"1","created_at":"2017-05-31T18:13:06.000Z","updated_at":"2019-01-17T00:39:21.000Z","published_at":"2017-06-06T10:13:00.000Z","custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"type":"post","email_recipient_filter":"none"},{"id":"5adb8922351ffe0018a57753","uuid":"4c02c768-05e1-48dc-9847-637951f82f67","title":"Watson Conversation System Entities","slug":"watson-conversation-system-entities","mobiledoc":"{\"version\":\"0.3.1\",\"markups\":[],\"atoms\":[],\"cards\":[[\"markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\"If you are building a chatbot, then Watson offers a product called Conversation. If you are wanting to get keywords from a body of text, Watson offers Natural Language Understanding. Sometimes however you are looking for something in-between. This is a post about that in-between.\\n\\n####Natural Language Understanding\\n\\nThe [Watson NLU](https://www.ibm.com/watson/developercloud/natural-language-understanding.html) API is great for getting a deep understanding (hence the \\\"U\\\") about a larger piece of content. The ==taxonomy== of a blog post for example. The more content you throw at it, the better the results. So what happens when all you have is \\\"==I will be home tomorrow.==\\\"?\\n\\nThere is ==not enough meat== there for NLU to be very helpful - in fact, it may not be enough for it to figure out anything. It is a simple conversational phrase. Depending on your application however, the words \\\"home\\\" and \\\"tomorrow\\\" have real meaning and real impact to what actions come next.\\n\\n####Watson Conversation\\n\\n[Watson Conversation](https://www.ibm.com/watson/developercloud/conversation.html) is a fantastic tool for building a ==chatbot== solution. The web-based ==visual tooling==, lets you design how the chat should take place. For every action in the tooling, there is a ==REST API== to support it. Your application will generally integrate via this API.\\n\\nOne of the APIs that is exposed, and will mostly likely be the center point of most application integration, is ==posting a message==. This is effectively the user telling the chatbot some piece of information. The content gets evaluated by Watson Conversation, run against the workflows you have described with the tooling, and returns some information for your application to pass along to the user.\\n\\nIn this flow, Watson Conversation certainly needs to understand \\\"I will be home tomorrow.\\\" This is where ==system entities== come into play.\\n\\n####System Entities\\n\\nAs humans, we know that \\\"==home==\\\" is a ==location==, and that \\\"==tomorrow==\\\" refers to the ==next day==. If we know the current day is \\\"June 5\\\" then we know that \\\"tomorrow\\\" refers to \\\"June 6.\\\" Or specifically, for an application \\\"06-06-2017\\\".\\n\\nAnother example might be \\\"==I have three dollars.==\\\" The word \\\"==three==\\\" there might be something specific that your application wants to know means \\\"==3==\\\" as a ==number==. This is what system entities are all about. They are exposed in the Watson Conversation tooling as \\\"@\\\" directives, but we can view them, and work with them directly from our application code as well.\\n\\n####Post a Message\\n\\nTo post a message, and get system entities, it is expected that you have created a Watson Conversation instance in your [Bluemix](https://console.ng.bluemix.net/) console, and that you have create a workspace. ==The workspace does not even have to have a workflow designed== - it just needs to exist. With our ==Watson Conversation username, password, and workspace ID== in hand, we can then POST a message to the API.\\n\\n```\\ncurl -X \\\"POST\\\" \\\"https://gateway.watsonplatform.net/conversation/api/v1/workspaces/f9731df0-ff4b-4179-a7dd-57ffcffdd102/message?version=2017-05-26\\\" \\\\\\n     -H \\\"Content-Type: application/json; charset=utf-8\\\" \\\\\\n     -u 2afd321f-e2bd-44e6-9d4b-e7c161b62d39:4finK0sGgiVc \\\\\\n     -d $'{\\n  \\\"input\\\": {\\n    \\\"text\\\": \\\"I will be home tomorrow.\\\"\\n  }\\n}'\\n```\\n\\nThe response will be a ==JSON-formatted== string. Among the details of the response will be an array of \\\"==entities==\\\". Each entity will have some ==type== specified in the \\\"entity\\\" property. The \\\"==location==\\\" tells you where in the provided string Watson thinks that entity applies. The \\\"==value==\\\" will vary depending on the type of entity. For this example, one entity is a \\\"sys-date\\\" so the value is the respective date. Finally Watson will give you a \\\"==confidence==\\\" level between zero (0) and one (1).\\n\\n```\\n{\\n  \\\"intents\\\":[],\\n  \\\"entities\\\":[{\\n    \\\"entity\\\": \\\"amenity\\\",\\n    \\\"location\\\": [10,14],\\n    \\\"value\\\": \\\"place\\\",\\n    \\\"confidence\\\": 1\\n  },{\\n    \\\"entity\\\": \\\"sys-date\\\",\\n    \\\"location\\\": [15,23],\\n    \\\"value\\\": \\\"2017-06-06\\\",\\n    \\\"confidence\\\": 1,\\n    \\\"metadata\\\": {\\n      \\\"calendar_type\\\": \\\"GREGORIAN\\\",\\n      \\\"timezone\\\": \\\"GMT\\\"\\n    }\\n  }],\\n  \\\"input\\\":{\\n    \\\"text\\\": \\\"I will be home tomorrow.\\\"\\n  },\\n  ...\\n}\\n```\\n\\n==This turns out to be super helpful, even if our application is not involved in a conversation.== Say for example, in a ==date picker== that lets the user specify the timeline ==using natural language==. Rather than have a date picker proper, they could simply type in \\\"tomorrow\\\" and Watson Conversation could then take care of the rest. What is the rest of the conversational output? For a date picker? Who cares! ==In the case of a date picker, we can use the service solely for that natural language feature.==\\n\\n####Bonus Round\\n\\nTo help visualize Watson Conversation system entities, I have put together a [simple application](http://intense.mybluemix.net/). You enter the phrase you are interested in testing, and the application will list out the resulting system entities. I wrote the application using Node.js on a Cloud Foundry instance in Bluemix.\\n\\n![Watson Conversation System Entities](http://images.kevinhoyt.com/watson.conversation.entities.png)\\n\\nSince I have just one function, I later decided to go ==serverless== with [OpenWhisk](https://developer.ibm.com/openwhisk/). You can find the ==Node.js== code, the ==OpenWhisk function== (in JavaScript), and even a ==Python== version you can test from the CLI, in a [GitHub Gist](https://gist.github.com/krhoyt/242cf8f3e483365c30edc4c711c93ce0) I created. I will write more about the OpenWhisk implementation in a future post.\\n\\n####Next Steps\\n\\nTo be clear, system entities is still a ==beta== feature. The [documentation](https://www.ibm.com/watson/developercloud/doc/conversation/system-entities.html#sys-datetime) calls out being able to find ==locations== as well, but in my testing, that is not yet ready for production. For example \\\"==I will be in Boston tomorrow.==\\\" will eventually be able to pick out not only the date, but \\\"==Boston==\\\" as a ==geographic location==. Would this be useful to your application? Let me know in the comments below, and I will pass your specific ==feedback== and use-case on to the product team.\\n\"}]],\"sections\":[[10,0]],\"ghostVersion\":\"3.0\"}","html":"<!--kg-card-begin: markdown--><p>If you are building a chatbot, then Watson offers a product called Conversation. If you are wanting to get keywords from a body of text, Watson offers Natural Language Understanding. Sometimes however you are looking for something in-between. This is a post about that in-between.</p>\n<h4 id=\"naturallanguageunderstanding\">Natural Language Understanding</h4>\n<p>The <a href=\"https://www.ibm.com/watson/developercloud/natural-language-understanding.html\">Watson NLU</a> API is great for getting a deep understanding (hence the &quot;U&quot;) about a larger piece of content. The <mark>taxonomy</mark> of a blog post for example. The more content you throw at it, the better the results. So what happens when all you have is &quot;<mark>I will be home tomorrow.</mark>&quot;?</p>\n<p>There is <mark>not enough meat</mark> there for NLU to be very helpful - in fact, it may not be enough for it to figure out anything. It is a simple conversational phrase. Depending on your application however, the words &quot;home&quot; and &quot;tomorrow&quot; have real meaning and real impact to what actions come next.</p>\n<h4 id=\"watsonconversation\">Watson Conversation</h4>\n<p><a href=\"https://www.ibm.com/watson/developercloud/conversation.html\">Watson Conversation</a> is a fantastic tool for building a <mark>chatbot</mark> solution. The web-based <mark>visual tooling</mark>, lets you design how the chat should take place. For every action in the tooling, there is a <mark>REST API</mark> to support it. Your application will generally integrate via this API.</p>\n<p>One of the APIs that is exposed, and will mostly likely be the center point of most application integration, is <mark>posting a message</mark>. This is effectively the user telling the chatbot some piece of information. The content gets evaluated by Watson Conversation, run against the workflows you have described with the tooling, and returns some information for your application to pass along to the user.</p>\n<p>In this flow, Watson Conversation certainly needs to understand &quot;I will be home tomorrow.&quot; This is where <mark>system entities</mark> come into play.</p>\n<h4 id=\"systementities\">System Entities</h4>\n<p>As humans, we know that &quot;<mark>home</mark>&quot; is a <mark>location</mark>, and that &quot;<mark>tomorrow</mark>&quot; refers to the <mark>next day</mark>. If we know the current day is &quot;June 5&quot; then we know that &quot;tomorrow&quot; refers to &quot;June 6.&quot; Or specifically, for an application &quot;06-06-2017&quot;.</p>\n<p>Another example might be &quot;<mark>I have three dollars.</mark>&quot; The word &quot;<mark>three</mark>&quot; there might be something specific that your application wants to know means &quot;<mark>3</mark>&quot; as a <mark>number</mark>. This is what system entities are all about. They are exposed in the Watson Conversation tooling as &quot;@&quot; directives, but we can view them, and work with them directly from our application code as well.</p>\n<h4 id=\"postamessage\">Post a Message</h4>\n<p>To post a message, and get system entities, it is expected that you have created a Watson Conversation instance in your <a href=\"https://console.ng.bluemix.net/\">Bluemix</a> console, and that you have create a workspace. <mark>The workspace does not even have to have a workflow designed</mark> - it just needs to exist. With our <mark>Watson Conversation username, password, and workspace ID</mark> in hand, we can then POST a message to the API.</p>\n<pre><code>curl -X &quot;POST&quot; &quot;https://gateway.watsonplatform.net/conversation/api/v1/workspaces/f9731df0-ff4b-4179-a7dd-57ffcffdd102/message?version=2017-05-26&quot; \\\n     -H &quot;Content-Type: application/json; charset=utf-8&quot; \\\n     -u 2afd321f-e2bd-44e6-9d4b-e7c161b62d39:4finK0sGgiVc \\\n     -d $'{\n  &quot;input&quot;: {\n    &quot;text&quot;: &quot;I will be home tomorrow.&quot;\n  }\n}'\n</code></pre>\n<p>The response will be a <mark>JSON-formatted</mark> string. Among the details of the response will be an array of &quot;<mark>entities</mark>&quot;. Each entity will have some <mark>type</mark> specified in the &quot;entity&quot; property. The &quot;<mark>location</mark>&quot; tells you where in the provided string Watson thinks that entity applies. The &quot;<mark>value</mark>&quot; will vary depending on the type of entity. For this example, one entity is a &quot;sys-date&quot; so the value is the respective date. Finally Watson will give you a &quot;<mark>confidence</mark>&quot; level between zero (0) and one (1).</p>\n<pre><code>{\n  &quot;intents&quot;:[],\n  &quot;entities&quot;:[{\n    &quot;entity&quot;: &quot;amenity&quot;,\n    &quot;location&quot;: [10,14],\n    &quot;value&quot;: &quot;place&quot;,\n    &quot;confidence&quot;: 1\n  },{\n    &quot;entity&quot;: &quot;sys-date&quot;,\n    &quot;location&quot;: [15,23],\n    &quot;value&quot;: &quot;2017-06-06&quot;,\n    &quot;confidence&quot;: 1,\n    &quot;metadata&quot;: {\n      &quot;calendar_type&quot;: &quot;GREGORIAN&quot;,\n      &quot;timezone&quot;: &quot;GMT&quot;\n    }\n  }],\n  &quot;input&quot;:{\n    &quot;text&quot;: &quot;I will be home tomorrow.&quot;\n  },\n  ...\n}\n</code></pre>\n<p><mark>This turns out to be super helpful, even if our application is not involved in a conversation.</mark> Say for example, in a <mark>date picker</mark> that lets the user specify the timeline <mark>using natural language</mark>. Rather than have a date picker proper, they could simply type in &quot;tomorrow&quot; and Watson Conversation could then take care of the rest. What is the rest of the conversational output? For a date picker? Who cares! <mark>In the case of a date picker, we can use the service solely for that natural language feature.</mark></p>\n<h4 id=\"bonusround\">Bonus Round</h4>\n<p>To help visualize Watson Conversation system entities, I have put together a <a href=\"http://intense.mybluemix.net/\">simple application</a>. You enter the phrase you are interested in testing, and the application will list out the resulting system entities. I wrote the application using Node.js on a Cloud Foundry instance in Bluemix.</p>\n<p><img src=\"http://images.kevinhoyt.com/watson.conversation.entities.png\" alt=\"Watson Conversation System Entities\" loading=\"lazy\"></p>\n<p>Since I have just one function, I later decided to go <mark>serverless</mark> with <a href=\"https://developer.ibm.com/openwhisk/\">OpenWhisk</a>. You can find the <mark>Node.js</mark> code, the <mark>OpenWhisk function</mark> (in JavaScript), and even a <mark>Python</mark> version you can test from the CLI, in a <a href=\"https://gist.github.com/krhoyt/242cf8f3e483365c30edc4c711c93ce0\">GitHub Gist</a> I created. I will write more about the OpenWhisk implementation in a future post.</p>\n<h4 id=\"nextsteps\">Next Steps</h4>\n<p>To be clear, system entities is still a <mark>beta</mark> feature. The <a href=\"https://www.ibm.com/watson/developercloud/doc/conversation/system-entities.html#sys-datetime\">documentation</a> calls out being able to find <mark>locations</mark> as well, but in my testing, that is not yet ready for production. For example &quot;<mark>I will be in Boston tomorrow.</mark>&quot; will eventually be able to pick out not only the date, but &quot;<mark>Boston</mark>&quot; as a <mark>geographic location</mark>. Would this be useful to your application? Let me know in the comments below, and I will pass your specific <mark>feedback</mark> and use-case on to the product team.</p>\n<!--kg-card-end: markdown-->","comment_id":"75","plaintext":"If you are building a chatbot, then Watson offers a product called Conversation.\nIf you are wanting to get keywords from a body of text, Watson offers Natural\nLanguage Understanding. Sometimes however you are looking for something\nin-between. This is a post about that in-between.\n\nNatural Language Understanding\nThe Watson NLU\n[https://www.ibm.com/watson/developercloud/natural-language-understanding.html] \nAPI is great for getting a deep understanding (hence the \"U\") about a larger\npiece of content. The taxonomy of a blog post for example. The more content you\nthrow at it, the better the results. So what happens when all you have is \"I\nwill be home tomorrow.\"?\n\nThere is not enough meat there for NLU to be very helpful - in fact, it may not\nbe enough for it to figure out anything. It is a simple conversational phrase.\nDepending on your application however, the words \"home\" and \"tomorrow\" have real\nmeaning and real impact to what actions come next.\n\nWatson Conversation\nWatson Conversation\n[https://www.ibm.com/watson/developercloud/conversation.html] is a fantastic\ntool for building a chatbot solution. The web-based visual tooling, lets you\ndesign how the chat should take place. For every action in the tooling, there is\na REST API to support it. Your application will generally integrate via this\nAPI.\n\nOne of the APIs that is exposed, and will mostly likely be the center point of\nmost application integration, is posting a message. This is effectively the user\ntelling the chatbot some piece of information. The content gets evaluated by\nWatson Conversation, run against the workflows you have described with the\ntooling, and returns some information for your application to pass along to the\nuser.\n\nIn this flow, Watson Conversation certainly needs to understand \"I will be home\ntomorrow.\" This is where system entities come into play.\n\nSystem Entities\nAs humans, we know that \"home\" is a location, and that \"tomorrow\" refers to the \nnext day. If we know the current day is \"June 5\" then we know that \"tomorrow\"\nrefers to \"June 6.\" Or specifically, for an application \"06-06-2017\".\n\nAnother example might be \"I have three dollars.\" The word \"three\" there might be\nsomething specific that your application wants to know means \"3\" as a number.\nThis is what system entities are all about. They are exposed in the Watson\nConversation tooling as \"@\" directives, but we can view them, and work with them\ndirectly from our application code as well.\n\nPost a Message\nTo post a message, and get system entities, it is expected that you have created\na Watson Conversation instance in your Bluemix [https://console.ng.bluemix.net/] \nconsole, and that you have create a workspace. The workspace does not even have\nto have a workflow designed - it just needs to exist. With our Watson\nConversation username, password, and workspace ID in hand, we can then POST a\nmessage to the API.\n\ncurl -X \"POST\" \"https://gateway.watsonplatform.net/conversation/api/v1/workspaces/f9731df0-ff4b-4179-a7dd-57ffcffdd102/message?version=2017-05-26\" \\\n     -H \"Content-Type: application/json; charset=utf-8\" \\\n     -u 2afd321f-e2bd-44e6-9d4b-e7c161b62d39:4finK0sGgiVc \\\n     -d $'{\n  \"input\": {\n    \"text\": \"I will be home tomorrow.\"\n  }\n}'\n\n\nThe response will be a JSON-formatted string. Among the details of the response\nwill be an array of \"entities\". Each entity will have some type specified in the\n\"entity\" property. The \"location\" tells you where in the provided string Watson\nthinks that entity applies. The \"value\" will vary depending on the type of\nentity. For this example, one entity is a \"sys-date\" so the value is the\nrespective date. Finally Watson will give you a \"confidence\" level between zero\n(0) and one (1).\n\n{\n  \"intents\":[],\n  \"entities\":[{\n    \"entity\": \"amenity\",\n    \"location\": [10,14],\n    \"value\": \"place\",\n    \"confidence\": 1\n  },{\n    \"entity\": \"sys-date\",\n    \"location\": [15,23],\n    \"value\": \"2017-06-06\",\n    \"confidence\": 1,\n    \"metadata\": {\n      \"calendar_type\": \"GREGORIAN\",\n      \"timezone\": \"GMT\"\n    }\n  }],\n  \"input\":{\n    \"text\": \"I will be home tomorrow.\"\n  },\n  ...\n}\n\n\nThis turns out to be super helpful, even if our application is not involved in a\nconversation. Say for example, in a date picker that lets the user specify the\ntimeline using natural language. Rather than have a date picker proper, they\ncould simply type in \"tomorrow\" and Watson Conversation could then take care of\nthe rest. What is the rest of the conversational output? For a date picker? Who\ncares! In the case of a date picker, we can use the service solely for that\nnatural language feature.\n\nBonus Round\nTo help visualize Watson Conversation system entities, I have put together a \nsimple application [http://intense.mybluemix.net/]. You enter the phrase you are\ninterested in testing, and the application will list out the resulting system\nentities. I wrote the application using Node.js on a Cloud Foundry instance in\nBluemix.\n\n\n\nSince I have just one function, I later decided to go serverless with OpenWhisk\n[https://developer.ibm.com/openwhisk/]. You can find the Node.js code, the \nOpenWhisk function (in JavaScript), and even a Python version you can test from\nthe CLI, in a GitHub Gist\n[https://gist.github.com/krhoyt/242cf8f3e483365c30edc4c711c93ce0] I created. I\nwill write more about the OpenWhisk implementation in a future post.\n\nNext Steps\nTo be clear, system entities is still a beta feature. The documentation\n[https://www.ibm.com/watson/developercloud/doc/conversation/system-entities.html#sys-datetime] \ncalls out being able to find locations as well, but in my testing, that is not\nyet ready for production. For example \"I will be in Boston tomorrow.\" will\neventually be able to pick out not only the date, but \"Boston\" as a geographic\nlocation. Would this be useful to your application? Let me know in the comments\nbelow, and I will pass your specific feedback and use-case on to the product\nteam.","feature_image":"__GHOST_URL__/content/images/2019/01/watson-1.jpg","featured":0,"status":"published","locale":null,"visibility":"public","author_id":"1","created_at":"2017-06-05T19:45:05.000Z","updated_at":"2019-01-17T00:38:21.000Z","published_at":"2017-06-08T10:45:00.000Z","custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"type":"post","email_recipient_filter":"none"},{"id":"5adb8922351ffe0018a57754","uuid":"fccd46f5-91d6-40d6-bb95-b8e1f761861c","title":"Developer Advocates and Ideas","slug":"conference-abstract-ideas","mobiledoc":"{\"version\":\"0.3.1\",\"markups\":[],\"atoms\":[],\"cards\":[[\"markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\"Over the past decade, I have submitted abstracts to countless conferences, and been accepted at hundreds of events around a good portion of the planet. I do not say that out of ego, and in fact, I never even thought about the sheer number until I was recently asked \\\"Where do you get abstract ideas?\\\" Where do ideas come from? A good question indeed.\\n\\n####Market Analysis\\n\\nI spend a lot of time thinking about the software market. Probably too much. In that thought though, I often think less about the product, and more about ==the problems that the product is trying to solve==, and how people will use it. A recent example is AR (augmented reality).\\n\\nI first saw AR done with Flash years ago. It was a cool gimmick. A few years later, I saw my daughter with an [AR coloring book](http://www.crayola.com/splash/products/ColorAlive). She would color the picture, point her phone's camera at the picture, and the scene would pop off the page and come to life. It was there that AR stopped being a gimmick to me, and ==started being a feature==.\\n\\nAR on the client side is still nascent. It requires a heavy mix of 3D modeling, computer vision, hardware, and much more. There is ==fertile ground for ideas==. \\n\\nAn abstract on using a specific 3D framework such as [Three.js](https://threejs.org/) is a great topic. Further then would be a comparison of the 3D frameworks on a single platform. Or potentially, across various platforms. These types of topics help developers understand what is best for their application, and that, to me, is exactly what advocacy is about - ==making developers heroes for the task that **they** are trying to accomplish==.\\n\\n####Meetups\\n\\nMeetups are an awesome source of inspiration for ideas. You get to see what people are working on, talk with others about their thoughts on the topic, and hear questions. The questions help understand the things developers are attempting to accomplish with the technology.\\n\\nOne of the terms you might hear in a traditional IT environment is ==10% time==. That is the time that developers get to work with technologies that they find significant in some fashion or other. As an advocate, you get ==90% time== (not quite, but you get the idea). You get to spend 90% of your time working with technologies that are significant to you. ==The technologies that are significant to you as an advocate, are the ones that will make heroes of the developers that spend the other 90% of their time implementing product for their employers.==\\n\\n####Booth Duty\\n\\nBooth duty is often dreaded by advocates. Long days, in an often unforgiving booth configuration, knowing full well that you have a pile of work to do when you get back to the hotel. For me though, ==booth duty means more questions== - more clarity on what it is that developers are trying to accomplish with your technology, or technologies related to it.\\n\\nAt a booth you will likely start to hear a trend in the questions. As example might be how your product works with other products that were announced at the event. Or how an attendee was in a session where the presenter was talking about a specific technology, and thinks that ==your product might help== implement what they heard.\\n\\nAs an advocate, these are diamonds in the rough. ==Do not dismiss the question, or simply point to some obscure documentation, rather, think about the questions.== Explore the things attendees mentioned. See if you can integrate them with your product(s). When you find a synergy, you have the foundation of an abstract relevant to a broad audience that you can submit to conferences.\\n\\n####Exhibit Floor\\n\\nAnother conference-related area often neglected by advocates is the rest of the exhibit floor. Get your badge scanned and take in some demonstrations from other vendors. Getting your badge scanned means more email spam to deal with if you are a corporate developer, but as an advocate it is the ==beginning of a professional network== - one that is a window into how other vendors are thinking about a similar space.\\n\\nMuch as making developers heroes by spending your 90% time on their problems, ==spending 90% time on how to integrate with other vendors== makes the vendor (the developers who work for that vendor) heroes. In some situations, these explorations can lead to fruitful partnerships. This will make you a hero at your place of business.\\n\\n####Attending Sessions\\n\\nSo long as we are talking about events, ==going to the sessions, listening to them, and taking thoughtful notes== will help you start to formulate ideas for your own sessions. Further, as a reflection of the meetup scenario, the questions that other attendees ask reveals the problems they are trying to solve. If you can help them solve them, then you are making them heroes.\\n\\n####Presenting Sessions\\n\\nIt never fails that at the end of your presentation, despite a lengthy question and answer portion, there will always be somebody that approaches you after your session ends. They will have questions. ==Genuinely listen to those questions.== Think about where those questions come from. Did you leave something out in your presentation? Did you open an unexpected door to an area you had not previously envisioned? \\n\\nA common approach by advocates is to refer to a blog post, or some documentation somewhere. This is fine, but you can ==go further==. If the question is something you think you might be able to solve directly, get their contact information. ==Try a naive implementation== to answer their questions. ==Share the code== with them. These types of work often create the foundation for future conference abstracts.\\n\\n####Event Organizers\\n\\nNobody understands an event, whether it be a conference or a meetup, better than the people that organize it. As an advocate, ==events are your bread and butter==, so it behooves you, if only from a networking perspective, to know who they are, and build a relationship with them. Event organizers often have their own ideas of what they want(ed) to have presented. Often, conversing with the organizers will reveal topics that are submitted too frequently. This helps you refine your ideas, and submit better abstracts to their events, and others.\\n\\n####You\\n\\nThis then brings the source of inspiration back to you. You can be your own source of inspiration. I personally keep an [[Evernote](https://evernote.com/)] notebook dedicated to inspiration. Sometimes I put silly pictures in the notebook. Transcripts from TED talks. Efforts by cities to solve municipal problems. User interfaces or interactions I find compelling will go in there as well. When I need an idea, I look back through that notebook.\\n\\n==The other aspect of inspiration is in solving problems that you have personally. ==\\n\\nI once gave an IoT talk on storing and analyzing data. The backdrop of my demonstration was a device I built to track how often my cats used the litter box. Gross? Sure! But I was curious, and IoT seemed like it could help me answer that question. While ML (machine learning) was not prevalent at the time, I now wonder if I could use it to identify which cat used which litter box. Moar ideas!\\n\\nThis exploration of litter box usage lead to all fashion of ==interesting problems to solve== such as designing an enclosure - cat urine and electronics do not mix, but a metal enclosure would create a [Faraday cage](https://en.wikipedia.org/wiki/Faraday_cage). I did not even know what a Faraday cage was until I had to start solving the problem on my own. Conference abstracts in their own right.\\n\\nThe key takeaway is to ==be your own hero==, and do not be afraid to ==incorporate your own problems and personality== into the solution. It has been said many times that the best products are ones created to solve an individual problem. [Flickr](https://www.flickr.com/) as an example, was not the company's original goal - it was a [side project](http://time.com/6855/flickr-turns-10-the-rise-fall-and-revival-of-a-photo-sharing-community/) to solve a specific pain point.\\n\\n####Next Steps\\n\\nHow about you? How do you come up with ideas for conference abstracts? Did I miss something? With some practice, I think you will find that you will have ==more ideas than you can possibly complete==, and that you will have to start prioritizing, and ==sharing==, them to continue to ==help developers be heroes==.\\n\"}]],\"sections\":[[10,0]],\"ghostVersion\":\"3.0\"}","html":"<!--kg-card-begin: markdown--><p>Over the past decade, I have submitted abstracts to countless conferences, and been accepted at hundreds of events around a good portion of the planet. I do not say that out of ego, and in fact, I never even thought about the sheer number until I was recently asked &quot;Where do you get abstract ideas?&quot; Where do ideas come from? A good question indeed.</p>\n<h4 id=\"marketanalysis\">Market Analysis</h4>\n<p>I spend a lot of time thinking about the software market. Probably too much. In that thought though, I often think less about the product, and more about <mark>the problems that the product is trying to solve</mark>, and how people will use it. A recent example is AR (augmented reality).</p>\n<p>I first saw AR done with Flash years ago. It was a cool gimmick. A few years later, I saw my daughter with an <a href=\"http://www.crayola.com/splash/products/ColorAlive\">AR coloring book</a>. She would color the picture, point her phone's camera at the picture, and the scene would pop off the page and come to life. It was there that AR stopped being a gimmick to me, and <mark>started being a feature</mark>.</p>\n<p>AR on the client side is still nascent. It requires a heavy mix of 3D modeling, computer vision, hardware, and much more. There is <mark>fertile ground for ideas</mark>.</p>\n<p>An abstract on using a specific 3D framework such as <a href=\"https://threejs.org/\">Three.js</a> is a great topic. Further then would be a comparison of the 3D frameworks on a single platform. Or potentially, across various platforms. These types of topics help developers understand what is best for their application, and that, to me, is exactly what advocacy is about - <mark>making developers heroes for the task that <strong>they</strong> are trying to accomplish</mark>.</p>\n<h4 id=\"meetups\">Meetups</h4>\n<p>Meetups are an awesome source of inspiration for ideas. You get to see what people are working on, talk with others about their thoughts on the topic, and hear questions. The questions help understand the things developers are attempting to accomplish with the technology.</p>\n<p>One of the terms you might hear in a traditional IT environment is <mark>10% time</mark>. That is the time that developers get to work with technologies that they find significant in some fashion or other. As an advocate, you get <mark>90% time</mark> (not quite, but you get the idea). You get to spend 90% of your time working with technologies that are significant to you. <mark>The technologies that are significant to you as an advocate, are the ones that will make heroes of the developers that spend the other 90% of their time implementing product for their employers.</mark></p>\n<h4 id=\"boothduty\">Booth Duty</h4>\n<p>Booth duty is often dreaded by advocates. Long days, in an often unforgiving booth configuration, knowing full well that you have a pile of work to do when you get back to the hotel. For me though, <mark>booth duty means more questions</mark> - more clarity on what it is that developers are trying to accomplish with your technology, or technologies related to it.</p>\n<p>At a booth you will likely start to hear a trend in the questions. As example might be how your product works with other products that were announced at the event. Or how an attendee was in a session where the presenter was talking about a specific technology, and thinks that <mark>your product might help</mark> implement what they heard.</p>\n<p>As an advocate, these are diamonds in the rough. <mark>Do not dismiss the question, or simply point to some obscure documentation, rather, think about the questions.</mark> Explore the things attendees mentioned. See if you can integrate them with your product(s). When you find a synergy, you have the foundation of an abstract relevant to a broad audience that you can submit to conferences.</p>\n<h4 id=\"exhibitfloor\">Exhibit Floor</h4>\n<p>Another conference-related area often neglected by advocates is the rest of the exhibit floor. Get your badge scanned and take in some demonstrations from other vendors. Getting your badge scanned means more email spam to deal with if you are a corporate developer, but as an advocate it is the <mark>beginning of a professional network</mark> - one that is a window into how other vendors are thinking about a similar space.</p>\n<p>Much as making developers heroes by spending your 90% time on their problems, <mark>spending 90% time on how to integrate with other vendors</mark> makes the vendor (the developers who work for that vendor) heroes. In some situations, these explorations can lead to fruitful partnerships. This will make you a hero at your place of business.</p>\n<h4 id=\"attendingsessions\">Attending Sessions</h4>\n<p>So long as we are talking about events, <mark>going to the sessions, listening to them, and taking thoughtful notes</mark> will help you start to formulate ideas for your own sessions. Further, as a reflection of the meetup scenario, the questions that other attendees ask reveals the problems they are trying to solve. If you can help them solve them, then you are making them heroes.</p>\n<h4 id=\"presentingsessions\">Presenting Sessions</h4>\n<p>It never fails that at the end of your presentation, despite a lengthy question and answer portion, there will always be somebody that approaches you after your session ends. They will have questions. <mark>Genuinely listen to those questions.</mark> Think about where those questions come from. Did you leave something out in your presentation? Did you open an unexpected door to an area you had not previously envisioned?</p>\n<p>A common approach by advocates is to refer to a blog post, or some documentation somewhere. This is fine, but you can <mark>go further</mark>. If the question is something you think you might be able to solve directly, get their contact information. <mark>Try a naive implementation</mark> to answer their questions. <mark>Share the code</mark> with them. These types of work often create the foundation for future conference abstracts.</p>\n<h4 id=\"eventorganizers\">Event Organizers</h4>\n<p>Nobody understands an event, whether it be a conference or a meetup, better than the people that organize it. As an advocate, <mark>events are your bread and butter</mark>, so it behooves you, if only from a networking perspective, to know who they are, and build a relationship with them. Event organizers often have their own ideas of what they want(ed) to have presented. Often, conversing with the organizers will reveal topics that are submitted too frequently. This helps you refine your ideas, and submit better abstracts to their events, and others.</p>\n<h4 id=\"you\">You</h4>\n<p>This then brings the source of inspiration back to you. You can be your own source of inspiration. I personally keep an [<a href=\"https://evernote.com/\">Evernote</a>] notebook dedicated to inspiration. Sometimes I put silly pictures in the notebook. Transcripts from TED talks. Efforts by cities to solve municipal problems. User interfaces or interactions I find compelling will go in there as well. When I need an idea, I look back through that notebook.</p>\n<p>==The other aspect of inspiration is in solving problems that you have personally. ==</p>\n<p>I once gave an IoT talk on storing and analyzing data. The backdrop of my demonstration was a device I built to track how often my cats used the litter box. Gross? Sure! But I was curious, and IoT seemed like it could help me answer that question. While ML (machine learning) was not prevalent at the time, I now wonder if I could use it to identify which cat used which litter box. Moar ideas!</p>\n<p>This exploration of litter box usage lead to all fashion of <mark>interesting problems to solve</mark> such as designing an enclosure - cat urine and electronics do not mix, but a metal enclosure would create a <a href=\"https://en.wikipedia.org/wiki/Faraday_cage\">Faraday cage</a>. I did not even know what a Faraday cage was until I had to start solving the problem on my own. Conference abstracts in their own right.</p>\n<p>The key takeaway is to <mark>be your own hero</mark>, and do not be afraid to <mark>incorporate your own problems and personality</mark> into the solution. It has been said many times that the best products are ones created to solve an individual problem. <a href=\"https://www.flickr.com/\">Flickr</a> as an example, was not the company's original goal - it was a <a href=\"http://time.com/6855/flickr-turns-10-the-rise-fall-and-revival-of-a-photo-sharing-community/\">side project</a> to solve a specific pain point.</p>\n<h4 id=\"nextsteps\">Next Steps</h4>\n<p>How about you? How do you come up with ideas for conference abstracts? Did I miss something? With some practice, I think you will find that you will have <mark>more ideas than you can possibly complete</mark>, and that you will have to start prioritizing, and <mark>sharing</mark>, them to continue to <mark>help developers be heroes</mark>.</p>\n<!--kg-card-end: markdown-->","comment_id":"77","plaintext":"Over the past decade, I have submitted abstracts to countless conferences, and\nbeen accepted at hundreds of events around a good portion of the planet. I do\nnot say that out of ego, and in fact, I never even thought about the sheer\nnumber until I was recently asked \"Where do you get abstract ideas?\" Where do\nideas come from? A good question indeed.\n\nMarket Analysis\nI spend a lot of time thinking about the software market. Probably too much. In\nthat thought though, I often think less about the product, and more about the\nproblems that the product is trying to solve, and how people will use it. A\nrecent example is AR (augmented reality).\n\nI first saw AR done with Flash years ago. It was a cool gimmick. A few years\nlater, I saw my daughter with an AR coloring book\n[http://www.crayola.com/splash/products/ColorAlive]. She would color the\npicture, point her phone's camera at the picture, and the scene would pop off\nthe page and come to life. It was there that AR stopped being a gimmick to me,\nand started being a feature.\n\nAR on the client side is still nascent. It requires a heavy mix of 3D modeling,\ncomputer vision, hardware, and much more. There is fertile ground for ideas.\n\nAn abstract on using a specific 3D framework such as Three.js\n[https://threejs.org/] is a great topic. Further then would be a comparison of\nthe 3D frameworks on a single platform. Or potentially, across various\nplatforms. These types of topics help developers understand what is best for\ntheir application, and that, to me, is exactly what advocacy is about - making\ndevelopers heroes for the task that they are trying to accomplish.\n\nMeetups\nMeetups are an awesome source of inspiration for ideas. You get to see what\npeople are working on, talk with others about their thoughts on the topic, and\nhear questions. The questions help understand the things developers are\nattempting to accomplish with the technology.\n\nOne of the terms you might hear in a traditional IT environment is 10% time.\nThat is the time that developers get to work with technologies that they find\nsignificant in some fashion or other. As an advocate, you get 90% time (not\nquite, but you get the idea). You get to spend 90% of your time working with\ntechnologies that are significant to you. The technologies that are significant\nto you as an advocate, are the ones that will make heroes of the developers that\nspend the other 90% of their time implementing product for their employers.\n\nBooth Duty\nBooth duty is often dreaded by advocates. Long days, in an often unforgiving\nbooth configuration, knowing full well that you have a pile of work to do when\nyou get back to the hotel. For me though, booth duty means more questions - more\nclarity on what it is that developers are trying to accomplish with your\ntechnology, or technologies related to it.\n\nAt a booth you will likely start to hear a trend in the questions. As example\nmight be how your product works with other products that were announced at the\nevent. Or how an attendee was in a session where the presenter was talking about\na specific technology, and thinks that your product might help implement what\nthey heard.\n\nAs an advocate, these are diamonds in the rough. Do not dismiss the question, or\nsimply point to some obscure documentation, rather, think about the questions. \nExplore the things attendees mentioned. See if you can integrate them with your\nproduct(s). When you find a synergy, you have the foundation of an abstract\nrelevant to a broad audience that you can submit to conferences.\n\nExhibit Floor\nAnother conference-related area often neglected by advocates is the rest of the\nexhibit floor. Get your badge scanned and take in some demonstrations from other\nvendors. Getting your badge scanned means more email spam to deal with if you\nare a corporate developer, but as an advocate it is the beginning of a\nprofessional network - one that is a window into how other vendors are thinking\nabout a similar space.\n\nMuch as making developers heroes by spending your 90% time on their problems, \nspending 90% time on how to integrate with other vendors makes the vendor (the\ndevelopers who work for that vendor) heroes. In some situations, these\nexplorations can lead to fruitful partnerships. This will make you a hero at\nyour place of business.\n\nAttending Sessions\nSo long as we are talking about events, going to the sessions, listening to\nthem, and taking thoughtful notes will help you start to formulate ideas for\nyour own sessions. Further, as a reflection of the meetup scenario, the\nquestions that other attendees ask reveals the problems they are trying to\nsolve. If you can help them solve them, then you are making them heroes.\n\nPresenting Sessions\nIt never fails that at the end of your presentation, despite a lengthy question\nand answer portion, there will always be somebody that approaches you after your\nsession ends. They will have questions. Genuinely listen to those questions. \nThink about where those questions come from. Did you leave something out in your\npresentation? Did you open an unexpected door to an area you had not previously\nenvisioned?\n\nA common approach by advocates is to refer to a blog post, or some documentation\nsomewhere. This is fine, but you can go further. If the question is something\nyou think you might be able to solve directly, get their contact information. \nTry a naive implementation to answer their questions. Share the code with them.\nThese types of work often create the foundation for future conference abstracts.\n\nEvent Organizers\nNobody understands an event, whether it be a conference or a meetup, better than\nthe people that organize it. As an advocate, events are your bread and butter,\nso it behooves you, if only from a networking perspective, to know who they are,\nand build a relationship with them. Event organizers often have their own ideas\nof what they want(ed) to have presented. Often, conversing with the organizers\nwill reveal topics that are submitted too frequently. This helps you refine your\nideas, and submit better abstracts to their events, and others.\n\nYou\nThis then brings the source of inspiration back to you. You can be your own\nsource of inspiration. I personally keep an [Evernote [https://evernote.com/]]\nnotebook dedicated to inspiration. Sometimes I put silly pictures in the\nnotebook. Transcripts from TED talks. Efforts by cities to solve municipal\nproblems. User interfaces or interactions I find compelling will go in there as\nwell. When I need an idea, I look back through that notebook.\n\n==The other aspect of inspiration is in solving problems that you have\npersonally. ==\n\nI once gave an IoT talk on storing and analyzing data. The backdrop of my\ndemonstration was a device I built to track how often my cats used the litter\nbox. Gross? Sure! But I was curious, and IoT seemed like it could help me answer\nthat question. While ML (machine learning) was not prevalent at the time, I now\nwonder if I could use it to identify which cat used which litter box. Moar\nideas!\n\nThis exploration of litter box usage lead to all fashion of interesting problems\nto solve such as designing an enclosure - cat urine and electronics do not mix,\nbut a metal enclosure would create a Faraday cage\n[https://en.wikipedia.org/wiki/Faraday_cage]. I did not even know what a Faraday\ncage was until I had to start solving the problem on my own. Conference\nabstracts in their own right.\n\nThe key takeaway is to be your own hero, and do not be afraid to incorporate\nyour own problems and personality into the solution. It has been said many times\nthat the best products are ones created to solve an individual problem. Flickr\n[https://www.flickr.com/] as an example, was not the company's original goal -\nit was a side project\n[http://time.com/6855/flickr-turns-10-the-rise-fall-and-revival-of-a-photo-sharing-community/] \nto solve a specific pain point.\n\nNext Steps\nHow about you? How do you come up with ideas for conference abstracts? Did I\nmiss something? With some practice, I think you will find that you will have \nmore ideas than you can possibly complete, and that you will have to start\nprioritizing, and sharing, them to continue to help developers be heroes.","feature_image":"__GHOST_URL__/content/images/2019/01/webvisions.audience-1.jpg","featured":0,"status":"published","locale":null,"visibility":"public","author_id":"1","created_at":"2017-06-13T19:15:26.000Z","updated_at":"2019-01-17T00:37:52.000Z","published_at":"2017-06-13T21:53:44.000Z","custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"type":"post","email_recipient_filter":"none"},{"id":"5adb8922351ffe0018a57755","uuid":"18543814-ed71-49d6-bf94-8de20f6c7107","title":"Async OpenWhisk Web Action with CORS","slug":"async-openwhisk-web-action-with-cors","mobiledoc":"{\"version\":\"0.3.1\",\"markups\":[],\"atoms\":[],\"cards\":[[\"markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\"[Apache OpenWhisk](http://openwhisk.org/) is a serverless (functions as a service) cloud platform, originated at IBM. It has broad language support, robust tooling, and fine-grained consumption (saves money). As a champion of the Web, here are a few patterns that I use repeatedly.\\n\\n####OpenWhisk\\n\\nThere is a growing community, and great documentation for OpenWhisk, so I will not cover getting started here. There are however a couple of important concepts it will be helpful to cover. An OpenWhisk function entry point is a function named \\\"main\\\".\\n\\n```\\nfunction main( args ) {\\n  return {payload: 'Hello ' + args.name};\\n}\\n```\\n\\nTo deploy the function, you can use the CLI tooling (my preference), or the web-based IDE.\\n\\n```\\nwsk action create hello hello.js\\n```\\n\\nAt this point, you can invoke your function using a REST API provided by OpenWhisk (or the CLI, or the web-based IDE).\\n\\n####Arguments\\n\\nYou can pass arguments into your function using a couple different approaches - along with the request, and/or during deployment. You can even use both. Often times, my OpenWhisk functions will invoke another API, say Watson Language Translation. ==That API however has its own credentials. I do not want to have to pass those in the open with every invocation.==\\n\\n```\\nWATSON_USERNAME=\\\"2af6421f-adcd-43e6-9a4b-e7c161f92c39\\\"\\nWATSON_PASSWORD=\\\"41inTtGsgiYc\\\"\\n\\n```\\n\\nTo solve this problem, a good approach is to use a \\\"==local.env==\\\" file containing name/value pairs with the credentials for any APIs you want to invoke. You can then run \\\"==source local.env==\\\" at the command line to add these variables to your terminal session. Now we can pull them into the deployment of our OpenWhisk function.\\n\\n```\\n~/wsk action create \\n  translation \\n  translation.js \\n  --param WATSON_USERNAME \\\"$WATSON_USERNAME\\\" \\n  --param WATSON_PASSWORD \\\"$WATSON_PASSWORD\\\"\\n```\\n\\nNow these parameters will be ==merged== with any other arguments being provided, and made available to your OpenWhisk function.\\n\\n####Web Actions\\n\\nWhile our OpenWhisk function is exposed via a REST API, that API is for OpenWhisk itself, telling it which function to invoke with what parameters. ==The function itself is not exposed to REST calls==. To expose an OpenWhisk function directly to REST calls, we can tap a feature called \\\"[Web Actions](https://console.ng.bluemix.net/docs/openwhisk/openwhisk_webactions.html#openwhisk_webactions)\\\".\\n\\nWeb Actions are enabled by adding a parameter to our deployment. The function will then be given a public URL you can call directly (say via [Paw](https://paw.cloud/), or just the browser).\\n\\n```\\n~/wsk action create \\n  translation\\n  translation.js \\n  --web true \\n  --param WATSON_USERNAME \\\"$WATSON_USERNAME\\\" \\n  --param WATSON_PASSWORD \\\"$WATSON_PASSWORD\\\"\\n```\\n\\n####CORS\\n\\nWhile we now have a URL endpoint we could point our browser-based JavaScript at, we will run into security problems - namely ==no CORS support==. The URL for OpenWhisk is likely going to be a different domain, so the browser will block the call. We can enable CORS within our OpenWhisk function, by telling it to ==return the necessary header==.\\n\\n```\\nfunction main( args ) {\\n  return {\\n    headers: {\\n      'Access-Control-Allow-Origin': '*'\\n    },\\n    statusCode: 200,\\n    body: new Buffer( JSON.stringify( {\\n      greeting: 'Hello ' + arg.name\\n    } ) ).toString( 'base64' )\\n  };\\n}\\n```\\n\\nNote that enabling CORS, and dealing with a JSON return in this fashion, changes how OpenWhisk expects functions to interact. If you are making OpenWhisk sequences, you will want to wrap the call in another OpenWhisk function.\\n\\n####Asynchronous Calls\\n\\nAt this point we can include third-party API credentials in our OpenWhisk function, make it available to the Web, and enable CORS for access in the browser. The last step is to make that third-party API call.\\n\\nThe problem we will run into here is that OpenWhisk is itself an asynchronous call. So to call a third-party API, you have an asynchronous call inside an asynchronous call. ==We need to orchestrate these two.==\\n\\n```\\nvar request = require( 'request-promise' );\\n\\nfunction main( args ) {\\n  // Make API call\\n  // Return promise\\n  return request( {\\n    url: url,\\n    method: 'POST',\\n    auth: {\\n      // Deploy arguments\\n      user: args.WATSON_USERNAME,\\n      pass: args.WATSON_PASSWORD\\n    },\\n    json: {\\n      input: {\\n        // Arguments in call\\n        text: 'Hello ' + args.name\\n      }\\n    }\\n  } ).then( function( result ) {\\n    // CORS\\n    // JSON\\n    return {\\n      headers: {\\n        'Access-Control-Allow-Origin': '*',        \\n        'Content-Type': 'application/json'\\n      },\\n      statusCode: 200,\\n      body: new Buffer( JSON.stringify( \\n        result \\n      ) ).toString( 'base64' )\\n    };\\n  } );\\n}\\n```\\n\\nWhere you might use the NPM \\\"[request](https://github.com/request/request)\\\" package in a Node.js application, we will use \\\"[request-promise](https://github.com/request/request-promise)\\\". This pulls together everything I have talked about up to this point. We are now ready to invoke this OpenWhisk function ==directly from a web page==.\\n\\n```\\nlet xhr = new XMLHttpRequest();\\nxhr.addEventListener( \\n  'load', \\n  (evt) => {\\n    console.log( xhr.responseText );\\n  }\\n);\\nxhr.open( 'POST', url, true );\\nxhr.send( JSON.stringify( {\\n  name: 'Kevin'\\n} );\\n```\\n\\n####Quick Recap\\n\\n- Keep a \\\"local.env\\\" file to hold API credentials\\n- Add the environment variables using \\\"source local.env\\\"\\n- Use \\\"--PARAM\\\" when deploying the function\\n- Enable Web Actions using \\\"--web true\\\" when deploying the function\\n- Support CORS in the return of your function\\n- Promisify third-party API requests using \\\"request-promise\\\"\\n\\n####Next Steps\\n\\nWeb Actions have a huge variety of options. You can use an extension such as \\\"json\\\" on the URL to specify the return content for example, and even drill down into the return from the request itself. ==If the Web is your thing, and Web Actions appeal to you, it is worth spending an afternoon playing with all the various options.==\\n\\nTo be sure, this example is a little contrived. It turns out that if you are using OpenWhisk on Bluemix, Watson integration is provided for you. The patterns hold true though regardless of the endpoints you want to use. Are there other API integrations that should ship with OpenWhisk? Let us know in the comments below!\\n\"}]],\"sections\":[[10,0]],\"ghostVersion\":\"3.0\"}","html":"<!--kg-card-begin: markdown--><p><a href=\"http://openwhisk.org/\">Apache OpenWhisk</a> is a serverless (functions as a service) cloud platform, originated at IBM. It has broad language support, robust tooling, and fine-grained consumption (saves money). As a champion of the Web, here are a few patterns that I use repeatedly.</p>\n<h4 id=\"openwhisk\">OpenWhisk</h4>\n<p>There is a growing community, and great documentation for OpenWhisk, so I will not cover getting started here. There are however a couple of important concepts it will be helpful to cover. An OpenWhisk function entry point is a function named &quot;main&quot;.</p>\n<pre><code>function main( args ) {\n  return {payload: 'Hello ' + args.name};\n}\n</code></pre>\n<p>To deploy the function, you can use the CLI tooling (my preference), or the web-based IDE.</p>\n<pre><code>wsk action create hello hello.js\n</code></pre>\n<p>At this point, you can invoke your function using a REST API provided by OpenWhisk (or the CLI, or the web-based IDE).</p>\n<h4 id=\"arguments\">Arguments</h4>\n<p>You can pass arguments into your function using a couple different approaches - along with the request, and/or during deployment. You can even use both. Often times, my OpenWhisk functions will invoke another API, say Watson Language Translation. <mark>That API however has its own credentials. I do not want to have to pass those in the open with every invocation.</mark></p>\n<pre><code>WATSON_USERNAME=&quot;2af6421f-adcd-43e6-9a4b-e7c161f92c39&quot;\nWATSON_PASSWORD=&quot;41inTtGsgiYc&quot;\n\n</code></pre>\n<p>To solve this problem, a good approach is to use a &quot;<mark>local.env</mark>&quot; file containing name/value pairs with the credentials for any APIs you want to invoke. You can then run &quot;<mark>source local.env</mark>&quot; at the command line to add these variables to your terminal session. Now we can pull them into the deployment of our OpenWhisk function.</p>\n<pre><code>~/wsk action create \n  translation \n  translation.js \n  --param WATSON_USERNAME &quot;$WATSON_USERNAME&quot; \n  --param WATSON_PASSWORD &quot;$WATSON_PASSWORD&quot;\n</code></pre>\n<p>Now these parameters will be <mark>merged</mark> with any other arguments being provided, and made available to your OpenWhisk function.</p>\n<h4 id=\"webactions\">Web Actions</h4>\n<p>While our OpenWhisk function is exposed via a REST API, that API is for OpenWhisk itself, telling it which function to invoke with what parameters. <mark>The function itself is not exposed to REST calls</mark>. To expose an OpenWhisk function directly to REST calls, we can tap a feature called &quot;<a href=\"https://console.ng.bluemix.net/docs/openwhisk/openwhisk_webactions.html#openwhisk_webactions\">Web Actions</a>&quot;.</p>\n<p>Web Actions are enabled by adding a parameter to our deployment. The function will then be given a public URL you can call directly (say via <a href=\"https://paw.cloud/\">Paw</a>, or just the browser).</p>\n<pre><code>~/wsk action create \n  translation\n  translation.js \n  --web true \n  --param WATSON_USERNAME &quot;$WATSON_USERNAME&quot; \n  --param WATSON_PASSWORD &quot;$WATSON_PASSWORD&quot;\n</code></pre>\n<h4 id=\"cors\">CORS</h4>\n<p>While we now have a URL endpoint we could point our browser-based JavaScript at, we will run into security problems - namely <mark>no CORS support</mark>. The URL for OpenWhisk is likely going to be a different domain, so the browser will block the call. We can enable CORS within our OpenWhisk function, by telling it to <mark>return the necessary header</mark>.</p>\n<pre><code>function main( args ) {\n  return {\n    headers: {\n      'Access-Control-Allow-Origin': '*'\n    },\n    statusCode: 200,\n    body: new Buffer( JSON.stringify( {\n      greeting: 'Hello ' + arg.name\n    } ) ).toString( 'base64' )\n  };\n}\n</code></pre>\n<p>Note that enabling CORS, and dealing with a JSON return in this fashion, changes how OpenWhisk expects functions to interact. If you are making OpenWhisk sequences, you will want to wrap the call in another OpenWhisk function.</p>\n<h4 id=\"asynchronouscalls\">Asynchronous Calls</h4>\n<p>At this point we can include third-party API credentials in our OpenWhisk function, make it available to the Web, and enable CORS for access in the browser. The last step is to make that third-party API call.</p>\n<p>The problem we will run into here is that OpenWhisk is itself an asynchronous call. So to call a third-party API, you have an asynchronous call inside an asynchronous call. <mark>We need to orchestrate these two.</mark></p>\n<pre><code>var request = require( 'request-promise' );\n\nfunction main( args ) {\n  // Make API call\n  // Return promise\n  return request( {\n    url: url,\n    method: 'POST',\n    auth: {\n      // Deploy arguments\n      user: args.WATSON_USERNAME,\n      pass: args.WATSON_PASSWORD\n    },\n    json: {\n      input: {\n        // Arguments in call\n        text: 'Hello ' + args.name\n      }\n    }\n  } ).then( function( result ) {\n    // CORS\n    // JSON\n    return {\n      headers: {\n        'Access-Control-Allow-Origin': '*',        \n        'Content-Type': 'application/json'\n      },\n      statusCode: 200,\n      body: new Buffer( JSON.stringify( \n        result \n      ) ).toString( 'base64' )\n    };\n  } );\n}\n</code></pre>\n<p>Where you might use the NPM &quot;<a href=\"https://github.com/request/request\">request</a>&quot; package in a Node.js application, we will use &quot;<a href=\"https://github.com/request/request-promise\">request-promise</a>&quot;. This pulls together everything I have talked about up to this point. We are now ready to invoke this OpenWhisk function <mark>directly from a web page</mark>.</p>\n<pre><code>let xhr = new XMLHttpRequest();\nxhr.addEventListener( \n  'load', \n  (evt) =&gt; {\n    console.log( xhr.responseText );\n  }\n);\nxhr.open( 'POST', url, true );\nxhr.send( JSON.stringify( {\n  name: 'Kevin'\n} );\n</code></pre>\n<h4 id=\"quickrecap\">Quick Recap</h4>\n<ul>\n<li>Keep a &quot;local.env&quot; file to hold API credentials</li>\n<li>Add the environment variables using &quot;source local.env&quot;</li>\n<li>Use &quot;--PARAM&quot; when deploying the function</li>\n<li>Enable Web Actions using &quot;--web true&quot; when deploying the function</li>\n<li>Support CORS in the return of your function</li>\n<li>Promisify third-party API requests using &quot;request-promise&quot;</li>\n</ul>\n<h4 id=\"nextsteps\">Next Steps</h4>\n<p>Web Actions have a huge variety of options. You can use an extension such as &quot;json&quot; on the URL to specify the return content for example, and even drill down into the return from the request itself. <mark>If the Web is your thing, and Web Actions appeal to you, it is worth spending an afternoon playing with all the various options.</mark></p>\n<p>To be sure, this example is a little contrived. It turns out that if you are using OpenWhisk on Bluemix, Watson integration is provided for you. The patterns hold true though regardless of the endpoints you want to use. Are there other API integrations that should ship with OpenWhisk? Let us know in the comments below!</p>\n<!--kg-card-end: markdown-->","comment_id":"78","plaintext":"Apache OpenWhisk [http://openwhisk.org/] is a serverless (functions as a\nservice) cloud platform, originated at IBM. It has broad language support,\nrobust tooling, and fine-grained consumption (saves money). As a champion of the\nWeb, here are a few patterns that I use repeatedly.\n\nOpenWhisk\nThere is a growing community, and great documentation for OpenWhisk, so I will\nnot cover getting started here. There are however a couple of important concepts\nit will be helpful to cover. An OpenWhisk function entry point is a function\nnamed \"main\".\n\nfunction main( args ) {\n  return {payload: 'Hello ' + args.name};\n}\n\n\nTo deploy the function, you can use the CLI tooling (my preference), or the\nweb-based IDE.\n\nwsk action create hello hello.js\n\n\nAt this point, you can invoke your function using a REST API provided by\nOpenWhisk (or the CLI, or the web-based IDE).\n\nArguments\nYou can pass arguments into your function using a couple different approaches -\nalong with the request, and/or during deployment. You can even use both. Often\ntimes, my OpenWhisk functions will invoke another API, say Watson Language\nTranslation. That API however has its own credentials. I do not want to have to\npass those in the open with every invocation.\n\nWATSON_USERNAME=\"2af6421f-adcd-43e6-9a4b-e7c161f92c39\"\nWATSON_PASSWORD=\"41inTtGsgiYc\"\n\n\n\nTo solve this problem, a good approach is to use a \"local.env\" file containing\nname/value pairs with the credentials for any APIs you want to invoke. You can\nthen run \"source local.env\" at the command line to add these variables to your\nterminal session. Now we can pull them into the deployment of our OpenWhisk\nfunction.\n\n~/wsk action create \n  translation \n  translation.js \n  --param WATSON_USERNAME \"$WATSON_USERNAME\" \n  --param WATSON_PASSWORD \"$WATSON_PASSWORD\"\n\n\nNow these parameters will be merged with any other arguments being provided, and\nmade available to your OpenWhisk function.\n\nWeb Actions\nWhile our OpenWhisk function is exposed via a REST API, that API is for\nOpenWhisk itself, telling it which function to invoke with what parameters. The\nfunction itself is not exposed to REST calls. To expose an OpenWhisk function\ndirectly to REST calls, we can tap a feature called \"Web Actions\n[https://console.ng.bluemix.net/docs/openwhisk/openwhisk_webactions.html#openwhisk_webactions]\n\".\n\nWeb Actions are enabled by adding a parameter to our deployment. The function\nwill then be given a public URL you can call directly (say via Paw\n[https://paw.cloud/], or just the browser).\n\n~/wsk action create \n  translation\n  translation.js \n  --web true \n  --param WATSON_USERNAME \"$WATSON_USERNAME\" \n  --param WATSON_PASSWORD \"$WATSON_PASSWORD\"\n\n\nCORS\nWhile we now have a URL endpoint we could point our browser-based JavaScript at,\nwe will run into security problems - namely no CORS support. The URL for\nOpenWhisk is likely going to be a different domain, so the browser will block\nthe call. We can enable CORS within our OpenWhisk function, by telling it to \nreturn the necessary header.\n\nfunction main( args ) {\n  return {\n    headers: {\n      'Access-Control-Allow-Origin': '*'\n    },\n    statusCode: 200,\n    body: new Buffer( JSON.stringify( {\n      greeting: 'Hello ' + arg.name\n    } ) ).toString( 'base64' )\n  };\n}\n\n\nNote that enabling CORS, and dealing with a JSON return in this fashion, changes\nhow OpenWhisk expects functions to interact. If you are making OpenWhisk\nsequences, you will want to wrap the call in another OpenWhisk function.\n\nAsynchronous Calls\nAt this point we can include third-party API credentials in our OpenWhisk\nfunction, make it available to the Web, and enable CORS for access in the\nbrowser. The last step is to make that third-party API call.\n\nThe problem we will run into here is that OpenWhisk is itself an asynchronous\ncall. So to call a third-party API, you have an asynchronous call inside an\nasynchronous call. We need to orchestrate these two.\n\nvar request = require( 'request-promise' );\n\nfunction main( args ) {\n  // Make API call\n  // Return promise\n  return request( {\n    url: url,\n    method: 'POST',\n    auth: {\n      // Deploy arguments\n      user: args.WATSON_USERNAME,\n      pass: args.WATSON_PASSWORD\n    },\n    json: {\n      input: {\n        // Arguments in call\n        text: 'Hello ' + args.name\n      }\n    }\n  } ).then( function( result ) {\n    // CORS\n    // JSON\n    return {\n      headers: {\n        'Access-Control-Allow-Origin': '*',        \n        'Content-Type': 'application/json'\n      },\n      statusCode: 200,\n      body: new Buffer( JSON.stringify( \n        result \n      ) ).toString( 'base64' )\n    };\n  } );\n}\n\n\nWhere you might use the NPM \"request [https://github.com/request/request]\"\npackage in a Node.js application, we will use \"request-promise\n[https://github.com/request/request-promise]\". This pulls together everything I\nhave talked about up to this point. We are now ready to invoke this OpenWhisk\nfunction directly from a web page.\n\nlet xhr = new XMLHttpRequest();\nxhr.addEventListener( \n  'load', \n  (evt) => {\n    console.log( xhr.responseText );\n  }\n);\nxhr.open( 'POST', url, true );\nxhr.send( JSON.stringify( {\n  name: 'Kevin'\n} );\n\n\nQuick Recap\n * Keep a \"local.env\" file to hold API credentials\n * Add the environment variables using \"source local.env\"\n * Use \"--PARAM\" when deploying the function\n * Enable Web Actions using \"--web true\" when deploying the function\n * Support CORS in the return of your function\n * Promisify third-party API requests using \"request-promise\"\n\nNext Steps\nWeb Actions have a huge variety of options. You can use an extension such as\n\"json\" on the URL to specify the return content for example, and even drill down\ninto the return from the request itself. If the Web is your thing, and Web\nActions appeal to you, it is worth spending an afternoon playing with all the\nvarious options.\n\nTo be sure, this example is a little contrived. It turns out that if you are\nusing OpenWhisk on Bluemix, Watson integration is provided for you. The patterns\nhold true though regardless of the endpoints you want to use. Are there other\nAPI integrations that should ship with OpenWhisk? Let us know in the comments\nbelow!","feature_image":"__GHOST_URL__/content/images/2019/01/whisk.eggs-1.jpg","featured":0,"status":"published","locale":null,"visibility":"public","author_id":"1","created_at":"2017-06-15T19:18:36.000Z","updated_at":"2019-01-17T00:29:54.000Z","published_at":"2017-06-15T20:39:41.000Z","custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"type":"post","email_recipient_filter":"none"},{"id":"5adb8922351ffe0018a57756","uuid":"ebd09967-eb3d-4c86-b960-90be59025415","title":"HTTP Requests from Server-Side Swift","slug":"http-requests-from-server-side-swift","mobiledoc":"{\"version\":\"0.3.1\",\"markups\":[],\"atoms\":[],\"cards\":[[\"markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\"As with any server environment, there are numerous (countless?) ways to make an HTTP request from a server-side Swift project. Most frameworks have their preferred approach. When it comes to Kitura, an approach that mirrors many of the requests libraries from other server environments, is KituraNet.\\n\\n####Swift on the Server?\\n\\nWhen Apple ==open sourced Swift==, among the first places it landed was on the server. This makes sense. Java is showing its age. Other mainstream options tend to lack features desired by many enterprise developers (strong data types, much). And iOS developers can build full-stack applications - an important consideration for a mobile-first world where iOS owns a large portion of the market. The IBM approach to server-side Swift is an open source project called, [Kitura](http://www.kitura.io/) - also open source.\\n\\n####KituraNet\\n\\n[KituraNet](http://ibm-swift.github.io/Kitura-net/) is designed to make ==HTTP requests== from Kitura. It is ==included== with the Kitura package, so it makes for an easy place to start as well. The following example provides a route to translate content against [Watson Language Translator](https://www.ibm.com/watson/developercloud/language-translator.html).\\n\\n####Input\\n\\nIn order to translate content, Watson needs to know what language the ==source== material is in currently, the desired ==target== destination of the translation, and the ==content== (text) to translate. As a bare minimum then, the input to the Kitura route will be a JSON string in the following format.\\n\\n```\\n{\\n  \\\"source\\\": \\\"en\\\",\\n  \\\"target\\\": \\\"es\\\",\\n  \\\"text\\\": \\\"Hello\\\"\\n}\\n```\\n\\nThis is exactly what [Watson Translator API](https://www.ibm.com/watson/developercloud/language-translator/api/v2/#translate) expects, and can be simply passed along as the body of the HTTP call. Watson credentials are needed to access the Language Translator API. These can be safely stored in a server resource of your choosing. I will be putting them into the code directly for the purposes of this demonstration (changed from their original values).\\n\\n####Request\\n\\nTo get the body of an HTTP request in Kitura, we use call \\\"==BodyParser.readBodyData(with:)==\\\". The result of the call is a [Data](https://developer.apple.com/documentation/foundation/data) object, which we pass along with the request in a moment. You can also choose to parse the string into JSON if you need to further manipulate the content.\\n\\n```\\nlet body = try BodyParser.readBodyData(with: req)\\n```\\n\\nNext we will set any headers that we need for the Watson Translator API call. In this case, we need to let Watson know that we are working with a JSON string.\\n\\n```\\nvar headers = [String:String]()\\nheaders[\\\"Accept\\\"] = \\\"application/json\\\"\\nheaders[\\\"Content-Type\\\"] = \\\"application/json\\\"\\n```\\n\\nThe [ClientRequest.Options](http://ibm-swift.github.io/Kitura-net/Classes/ClientRequest/Options.html) class gives us a place to store all the relevant information for the request. This includes not only the URL endpoint, but also a ==username and password for HTTP Basic authentication==, which is what Watson uses for almost all of its API.\\n\\n```\\nvar options: [ClientRequest.Options] = []\\noptions.append(.username(\\\"b45dad35-e93c-4737-8604-edc0d8b2bfdf\\\"))\\noptions.append(.password(\\\"4fk38dYMCUeL\\\"))\\noptions.append(.schema(\\\"https://\\\"))\\noptions.append(.method(\\\"POST\\\"))\\noptions.append(.port(443))\\noptions.append(.hostname(\\\"gateway.watsonplatform.net\\\"))\\noptions.append(.path(\\\"/language-translator/api/v2/translate\\\"))\\noptions.append(.headers(headers))\\n```\\n\\nKituraNet exposes an \\\"[HTTP.request(_:callback:)](http://ibm-swift.github.io/Kitura-net/Classes/HTTP.html)\\\" method with a callback for when the request has completed. We will use the callback notation of the Swift language here to handle the response.\\n\\n```\\n_ = HTTP.request(options) { response in\\n  do {\\n    var data = Data()\\n    try response?.readAllData(into: &data)\\n    let body = String.init(data: data, encoding: .utf8)\\n            \\n    res.send(body!)\\n  } catch {\\n    // Error\\n  }\\n}.end(body)\\n```\\n\\nSimilar to the handling of the incoming request from the client, we use the [ClientResponse](http://ibm-swift.github.io/Kitura-net/Classes/ClientResponse.html) object to read all the response data from the HTTP call. In this case, the return from Watson Translator is a JSON string. We do not care about that JSON here, just the string. Once we have that, we can use the [router response object](http://ibm-swift.github.io/Kitura/Classes/RouterResponse.html) of the initial call to return the string.\\n\\nTo actually send the client-provided content, a JSON string, you provide that data in the \\\"==ClientRequest.end(_ data: Data, close: Bool = false)==\\\" call as a Data object instance.\\n\\n####Next Steps\\n\\nIt may seem silly to write an explanation of this process. It looks pretty similar to \\\"[request](https://github.com/request/request)\\\" package one might use from a Node.js project. And it is. However it took me the better part of a day to track down all the parts and pieces of how this process works. Hopefully it will save time on your next Kitura project.\\n\\nIf you have not already taken a look at Kitura, and you are a Swift developer looking to go full stack with your skills, or a Java developer looking for your next server environment, but are not sure JavaScript is the right direction, then check out Kitura. You can even run Kitura on the IBM Bluemix cloud.\\n\"}]],\"sections\":[[10,0]],\"ghostVersion\":\"3.0\"}","html":"<!--kg-card-begin: markdown--><p>As with any server environment, there are numerous (countless?) ways to make an HTTP request from a server-side Swift project. Most frameworks have their preferred approach. When it comes to Kitura, an approach that mirrors many of the requests libraries from other server environments, is KituraNet.</p>\n<h4 id=\"swiftontheserver\">Swift on the Server?</h4>\n<p>When Apple <mark>open sourced Swift</mark>, among the first places it landed was on the server. This makes sense. Java is showing its age. Other mainstream options tend to lack features desired by many enterprise developers (strong data types, much). And iOS developers can build full-stack applications - an important consideration for a mobile-first world where iOS owns a large portion of the market. The IBM approach to server-side Swift is an open source project called, <a href=\"http://www.kitura.io/\">Kitura</a> - also open source.</p>\n<h4 id=\"kituranet\">KituraNet</h4>\n<p><a href=\"http://ibm-swift.github.io/Kitura-net/\">KituraNet</a> is designed to make <mark>HTTP requests</mark> from Kitura. It is <mark>included</mark> with the Kitura package, so it makes for an easy place to start as well. The following example provides a route to translate content against <a href=\"https://www.ibm.com/watson/developercloud/language-translator.html\">Watson Language Translator</a>.</p>\n<h4 id=\"input\">Input</h4>\n<p>In order to translate content, Watson needs to know what language the <mark>source</mark> material is in currently, the desired <mark>target</mark> destination of the translation, and the <mark>content</mark> (text) to translate. As a bare minimum then, the input to the Kitura route will be a JSON string in the following format.</p>\n<pre><code>{\n  &quot;source&quot;: &quot;en&quot;,\n  &quot;target&quot;: &quot;es&quot;,\n  &quot;text&quot;: &quot;Hello&quot;\n}\n</code></pre>\n<p>This is exactly what <a href=\"https://www.ibm.com/watson/developercloud/language-translator/api/v2/#translate\">Watson Translator API</a> expects, and can be simply passed along as the body of the HTTP call. Watson credentials are needed to access the Language Translator API. These can be safely stored in a server resource of your choosing. I will be putting them into the code directly for the purposes of this demonstration (changed from their original values).</p>\n<h4 id=\"request\">Request</h4>\n<p>To get the body of an HTTP request in Kitura, we use call &quot;<mark>BodyParser.readBodyData(with:)</mark>&quot;. The result of the call is a <a href=\"https://developer.apple.com/documentation/foundation/data\">Data</a> object, which we pass along with the request in a moment. You can also choose to parse the string into JSON if you need to further manipulate the content.</p>\n<pre><code>let body = try BodyParser.readBodyData(with: req)\n</code></pre>\n<p>Next we will set any headers that we need for the Watson Translator API call. In this case, we need to let Watson know that we are working with a JSON string.</p>\n<pre><code>var headers = [String:String]()\nheaders[&quot;Accept&quot;] = &quot;application/json&quot;\nheaders[&quot;Content-Type&quot;] = &quot;application/json&quot;\n</code></pre>\n<p>The <a href=\"http://ibm-swift.github.io/Kitura-net/Classes/ClientRequest/Options.html\">ClientRequest.Options</a> class gives us a place to store all the relevant information for the request. This includes not only the URL endpoint, but also a <mark>username and password for HTTP Basic authentication</mark>, which is what Watson uses for almost all of its API.</p>\n<pre><code>var options: [ClientRequest.Options] = []\noptions.append(.username(&quot;b45dad35-e93c-4737-8604-edc0d8b2bfdf&quot;))\noptions.append(.password(&quot;4fk38dYMCUeL&quot;))\noptions.append(.schema(&quot;https://&quot;))\noptions.append(.method(&quot;POST&quot;))\noptions.append(.port(443))\noptions.append(.hostname(&quot;gateway.watsonplatform.net&quot;))\noptions.append(.path(&quot;/language-translator/api/v2/translate&quot;))\noptions.append(.headers(headers))\n</code></pre>\n<p>KituraNet exposes an &quot;<a href=\"http://ibm-swift.github.io/Kitura-net/Classes/HTTP.html\">HTTP.request(_:callback:)</a>&quot; method with a callback for when the request has completed. We will use the callback notation of the Swift language here to handle the response.</p>\n<pre><code>_ = HTTP.request(options) { response in\n  do {\n    var data = Data()\n    try response?.readAllData(into: &amp;data)\n    let body = String.init(data: data, encoding: .utf8)\n            \n    res.send(body!)\n  } catch {\n    // Error\n  }\n}.end(body)\n</code></pre>\n<p>Similar to the handling of the incoming request from the client, we use the <a href=\"http://ibm-swift.github.io/Kitura-net/Classes/ClientResponse.html\">ClientResponse</a> object to read all the response data from the HTTP call. In this case, the return from Watson Translator is a JSON string. We do not care about that JSON here, just the string. Once we have that, we can use the <a href=\"http://ibm-swift.github.io/Kitura/Classes/RouterResponse.html\">router response object</a> of the initial call to return the string.</p>\n<p>To actually send the client-provided content, a JSON string, you provide that data in the &quot;<mark>ClientRequest.end(_ data: Data, close: Bool = false)</mark>&quot; call as a Data object instance.</p>\n<h4 id=\"nextsteps\">Next Steps</h4>\n<p>It may seem silly to write an explanation of this process. It looks pretty similar to &quot;<a href=\"https://github.com/request/request\">request</a>&quot; package one might use from a Node.js project. And it is. However it took me the better part of a day to track down all the parts and pieces of how this process works. Hopefully it will save time on your next Kitura project.</p>\n<p>If you have not already taken a look at Kitura, and you are a Swift developer looking to go full stack with your skills, or a Java developer looking for your next server environment, but are not sure JavaScript is the right direction, then check out Kitura. You can even run Kitura on the IBM Bluemix cloud.</p>\n<!--kg-card-end: markdown-->","comment_id":"79","plaintext":"As with any server environment, there are numerous (countless?) ways to make an\nHTTP request from a server-side Swift project. Most frameworks have their\npreferred approach. When it comes to Kitura, an approach that mirrors many of\nthe requests libraries from other server environments, is KituraNet.\n\nSwift on the Server?\nWhen Apple open sourced Swift, among the first places it landed was on the\nserver. This makes sense. Java is showing its age. Other mainstream options tend\nto lack features desired by many enterprise developers (strong data types,\nmuch). And iOS developers can build full-stack applications - an important\nconsideration for a mobile-first world where iOS owns a large portion of the\nmarket. The IBM approach to server-side Swift is an open source project called, \nKitura [http://www.kitura.io/] - also open source.\n\nKituraNet\nKituraNet [http://ibm-swift.github.io/Kitura-net/] is designed to make HTTP\nrequests from Kitura. It is included with the Kitura package, so it makes for an\neasy place to start as well. The following example provides a route to translate\ncontent against Watson Language Translator\n[https://www.ibm.com/watson/developercloud/language-translator.html].\n\nInput\nIn order to translate content, Watson needs to know what language the source \nmaterial is in currently, the desired target destination of the translation, and\nthe content (text) to translate. As a bare minimum then, the input to the Kitura\nroute will be a JSON string in the following format.\n\n{\n  \"source\": \"en\",\n  \"target\": \"es\",\n  \"text\": \"Hello\"\n}\n\n\nThis is exactly what Watson Translator API\n[https://www.ibm.com/watson/developercloud/language-translator/api/v2/#translate] \nexpects, and can be simply passed along as the body of the HTTP call. Watson\ncredentials are needed to access the Language Translator API. These can be\nsafely stored in a server resource of your choosing. I will be putting them into\nthe code directly for the purposes of this demonstration (changed from their\noriginal values).\n\nRequest\nTo get the body of an HTTP request in Kitura, we use call \"\nBodyParser.readBodyData(with:)\". The result of the call is a Data\n[https://developer.apple.com/documentation/foundation/data] object, which we\npass along with the request in a moment. You can also choose to parse the string\ninto JSON if you need to further manipulate the content.\n\nlet body = try BodyParser.readBodyData(with: req)\n\n\nNext we will set any headers that we need for the Watson Translator API call. In\nthis case, we need to let Watson know that we are working with a JSON string.\n\nvar headers = [String:String]()\nheaders[\"Accept\"] = \"application/json\"\nheaders[\"Content-Type\"] = \"application/json\"\n\n\nThe ClientRequest.Options\n[http://ibm-swift.github.io/Kitura-net/Classes/ClientRequest/Options.html] class\ngives us a place to store all the relevant information for the request. This\nincludes not only the URL endpoint, but also a username and password for HTTP\nBasic authentication, which is what Watson uses for almost all of its API.\n\nvar options: [ClientRequest.Options] = []\noptions.append(.username(\"b45dad35-e93c-4737-8604-edc0d8b2bfdf\"))\noptions.append(.password(\"4fk38dYMCUeL\"))\noptions.append(.schema(\"https://\"))\noptions.append(.method(\"POST\"))\noptions.append(.port(443))\noptions.append(.hostname(\"gateway.watsonplatform.net\"))\noptions.append(.path(\"/language-translator/api/v2/translate\"))\noptions.append(.headers(headers))\n\n\nKituraNet exposes an \"HTTP.request(_:callback:)\n[http://ibm-swift.github.io/Kitura-net/Classes/HTTP.html]\" method with a\ncallback for when the request has completed. We will use the callback notation\nof the Swift language here to handle the response.\n\n_ = HTTP.request(options) { response in\n  do {\n    var data = Data()\n    try response?.readAllData(into: &data)\n    let body = String.init(data: data, encoding: .utf8)\n            \n    res.send(body!)\n  } catch {\n    // Error\n  }\n}.end(body)\n\n\nSimilar to the handling of the incoming request from the client, we use the \nClientResponse\n[http://ibm-swift.github.io/Kitura-net/Classes/ClientResponse.html] object to\nread all the response data from the HTTP call. In this case, the return from\nWatson Translator is a JSON string. We do not care about that JSON here, just\nthe string. Once we have that, we can use the router response object\n[http://ibm-swift.github.io/Kitura/Classes/RouterResponse.html] of the initial\ncall to return the string.\n\nTo actually send the client-provided content, a JSON string, you provide that\ndata in the \"ClientRequest.end(_ data: Data, close: Bool = false)\" call as a\nData object instance.\n\nNext Steps\nIt may seem silly to write an explanation of this process. It looks pretty\nsimilar to \"request [https://github.com/request/request]\" package one might use\nfrom a Node.js project. And it is. However it took me the better part of a day\nto track down all the parts and pieces of how this process works. Hopefully it\nwill save time on your next Kitura project.\n\nIf you have not already taken a look at Kitura, and you are a Swift developer\nlooking to go full stack with your skills, or a Java developer looking for your\nnext server environment, but are not sure JavaScript is the right direction,\nthen check out Kitura. You can even run Kitura on the IBM Bluemix cloud.","feature_image":"__GHOST_URL__/content/images/2019/01/swift.jpg","featured":0,"status":"published","locale":null,"visibility":"public","author_id":"1","created_at":"2017-06-19T17:19:21.000Z","updated_at":"2019-01-17T00:37:06.000Z","published_at":"2017-06-20T20:52:00.000Z","custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"type":"post","email_recipient_filter":"none"},{"id":"5adb8922351ffe0018a57757","uuid":"3a5d1abe-22d9-4b1d-b739-4ca1766dac2d","title":"Slap Robin with the ZX Gesture Sensor","slug":"slap-robin-with-the-zx-gesture-sensor","mobiledoc":"{\"version\":\"0.3.1\",\"markups\":[],\"atoms\":[],\"cards\":[[\"markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\"Every now and again, I run across a sensor that just sounds like fun. Such is the case when I read about the ZX Gesture Sensor from Sparkfun and XYZ Interactive. Put it together with the Internet, and I just may have gotten a little slap-happy.\\n\\n####The Sensor\\n\\nThe [ZX Gesture Sensor]() measures about 2.5\\\" long and about 1\\\" deep. It consists of ==two IR emitters== - one on either end, and an ==IR sensor== in the middle. When you move your hand in front of the sensor, IR light bounces off of it, which is in turn picked up by the sensor.\\n\\n![Image courtesy of Sparkfun.](http://images.kevinhoyt.com/zx.gesture.sensor.jpg)\\n\\nYou can get detailed data over the ==I2C interface==, or just ask the sensor to give you data when it detects a gesture. There is ==Arduino== code provided, but I wanted to connect this bad boy to the Internet. I reached into my kit, grabbed a [Particle Photon](https://www.particle.io/), and migrated the Arduino code to a Photon library.\\n\\n####The Photon\\n\\nCopy and paste from the Arduino library into the Particle IDE, met with successful compilation on the first pass. I did eventually go back and pull out parts I knew were not needed, such as the \\\"Arduino.h\\\" header reference, but ==nothing else== was really required of me.\\n\\n```\\n#include \\\"ZX_Sensor.h\\\"\\n\\nGestureType gesture;\\nuint8_t     speed;\\nZX_Sensor   sensor = ZX_Sensor( 0x10 );\\n\\nvoid setup() {\\n  // Initialize sensor\\n  sensor.init();\\n}\\n\\nvoid loop() {\\n  char content[255];\\n\\n  // Data available\\n  if( sensor.gestureAvailable() ) {\\n\\n    // Read\\n    gesture = sensor.readGesture();\\n    speed = sensor.readGestureSpeed();\\n\\n    // What just happened\\n    switch( gesture ) {\\n      case NO_GESTURE:\\n        sprintf( content, \\\"-1,%u\\\", speed );\\n        break;\\n\\n      case RIGHT_SWIPE:\\n        sprintf( content, \\\"0,%u\\\", speed );\\n        break;\\n\\n      case LEFT_SWIPE:\\n        sprintf( content, \\\"1,%u\\\", speed );\\n        break;\\n\\n      case UP_SWIPE:\\n        sprintf( content, \\\"3,%u\\\", speed );\\n        break;\\n\\n    }\\n\\n    // Publish event\\n    Particle.publish( \\\"slap\\\", content, PRIVATE );\\n  }\\n}\\n\\n```\\n\\nWhile you can connect a Photon directly to an MQTT broker, or even make HTTP requests, I find both of these to be particularly problematic from a ==security== perspective. I prefer to use the [Particle WebHooks](https://docs.particle.io/guide/tools-and-features/webhooks/) because data from the Photon, to the Particle servers, is secured with ==TLS==. This also allows me to ==keep my endpoint credentials tucked away from the client.==\\n\\n```\\ncurl -X \\\"POST\\\" \\n  \\\"https://YOUR_ORG.messaging.internetofthings.ibmcloud.com:8883/api/v0002/application/types/YOUR_TYPE/devices/YOUR_DEVICE/events/swipe\\\" \\\\\\n     -H \\\"Content-Type: application/json; charset=utf-8\\\" \\\\\\n     -u a-YOUR_ORG-YOUR_USERNAME:YOUR_PASSWORD \\\\\\n     -d $'{\\n  \\\"speed\\\": 10,\\n  \\\"direction\\\": \\\"left\\\"\\n}'\\n```\\n\\nThe WebHook integration then makes an ==HTTPS== request to the [Watson IoT](https://www.ibm.com/internet-of-things/) MQTT broker. Yup, you can ==POST messages to the broker using HTTPS==! Messages are in turn broadcast out over the specified MQTT topics. To get this to the Web, I had a Node.js server running on IBM Bluemix.\\n\\n####The Server\\n\\nThe Node.js server has two main parts. The first part is that it subscribes to Watson IoT message via an MQTT client library. Again, keeping my connectivity credentials on the server, away from any clients, helps me to sleep well at night.\\n\\n```\\n// Connect to Watson IoT\\nvar client  = mqtt.connect( \\n  config.iot_host, \\n  {\\n    clientId: config.iot_client + '_' + Math.round( ( Math.random() * 10000 ) ),\\n    username: config.iot_username,\\n    password: config.iot_password,\\n    port: 1883\\n  }\\n);\\n\\n...\\n\\n// Connected to Watson\\n// Subscribe for sensor data\\nclient.on( 'connect', function() {\\n  console.log( 'Broker connected.' );\\n\\n  client.subscribe( config.iot_topic, function( error, granted ) {\\n    console.log( 'Subscribed.' );\\n  } );\\n} );\\n\\n// New message arrived\\nclient.on( 'message', function( topic, message ) {\\n  console.log( message.toString() );\\n\\n  // Send to client screen\\n  io.emit( 'slap', message.toString() );\\n} );\\n\\n```\\n\\nWhen the gesture message arrives, it is pushed to the connected clients via [Socket IO](https://socket.io/). Socket IO is a nice fit here because it adds some smarts to the connection to the browser. The ability to reconnect is one of my favorite features, but it also simplifies managing destinations over straight WebSocket.\\n\\n>I tend to be a bit of a purist when it comes to WebSocket, so giving an endorsement to Socket IO is a big step for me.\\n\\n```\\n// Application\\nvar app = express();\\n\\n// Static for main files\\napp.use( '/', express.static( 'public' ) );\\n\\n// Bluemix\\nvar env = cfenv.getAppEnv();\\n\\n// Listen\\nvar server = app.listen( env.port, env.bind, function() {\\n  // Debug\\n  console.log( 'Started on: ' + env.port );\\n} );\\n\\n...\\n\\n// Socket\\nvar io = require( 'socket.io' )( server );\\n\\n// New socket connection\\nio.on( 'connection', function( socket ) {\\n  console.log( 'Client connected.' );\\n} );\\n```\\n\\n####The Meme\\n\\nAs I was gesturing away, I kept thinking to myself \\\"Now what?\\\" It was around this time when the original Batman, [Adam West](https://en.wikipedia.org/wiki/Adam_West), passed away. I was reminded of the meme of Batman slapping Robin in the comic books, and a demo was born.\\n\\n![The original reason that Batman slaps Robin.](http://images.kevinhoyt.com/batman.slap.robin.jpg)\\n\\n####The Browser\\n\\nI figured that if I was making a gesture of swiping left, that Batman should be slapping Robin using his left hand. If I was making a gesture of swiping right, then Batman should use his right hand. That would be as easy as displaying the image flipped horizontally based on the data.\\n\\n```\\n// Connect to server\\n// Listen for slap event\\nthis.socket = io();\\nthis.socket.on( 'slap', evt => this.doMessage( evt ) );\\n\\n...\\n\\ndoMessage( evt ) {\\n  // Parse\\n  let message = JSON.parse( evt );\\n  let parts = message.data.split( ',' );\\n  let direction = parseInt( parts[0] );\\n\\n  // Show image for direction of gesture\\n  switch( direction ) {\\n    case 0:\\n      this.slap.style.backgroundImage = 'url( img/slap.right.png )';\\n      break;\\n\\n    case 1:\\n      this.slap.style.backgroundImage = 'url( img/slap.left.png )';\\n      break;      \\n  }\\n\\n  // Display for a limited time\\n  this.interval = setInterval( () => {\\n    this.slap.style.backgroundImage = '';\\n    clearInterval( this.interval );\\n    this.interval = null;\\n  }, 2000 );\\n\\n  // Debug\\n  console.log( message );\\n}\\n```\\n\\nThe image of Batman slapping Robin stays around for two seconds, and then disappears.\\n\\n<iframe width=\\\"560\\\" height=\\\"315\\\" src=\\\"https://www.youtube.com/embed/dxYOkCFIuRI\\\" frameborder=\\\"0\\\" allowfullscreen></iframe>\\n\\n*Note: While I love the Particle Photon, the single biggest complaint I have with it, surfaced during the recording of this video - no 5GHz support. In a noisy environment like my home, this means that the Photon is regularly disconnecting.*\\n\\n####Next Steps\\n\\nWhile this may seem like a trivial application, it is actually a ==solid boilerplate== for connecting any type of sensor data, ==securely==, to any infrastructure. Credentials are kept on servers, away from prying eyes, while the sockets cleanly deliver the data from the sensor to any connected client. Security is critical with IoT, and this is a great way to start off your next application.\\n\"}]],\"sections\":[[10,0]],\"ghostVersion\":\"3.0\"}","html":"<!--kg-card-begin: markdown--><p>Every now and again, I run across a sensor that just sounds like fun. Such is the case when I read about the ZX Gesture Sensor from Sparkfun and XYZ Interactive. Put it together with the Internet, and I just may have gotten a little slap-happy.</p>\n<h4 id=\"thesensor\">The Sensor</h4>\n<p>The <a href=\"\">ZX Gesture Sensor</a> measures about 2.5&quot; long and about 1&quot; deep. It consists of <mark>two IR emitters</mark> - one on either end, and an <mark>IR sensor</mark> in the middle. When you move your hand in front of the sensor, IR light bounces off of it, which is in turn picked up by the sensor.</p>\n<p><img src=\"http://images.kevinhoyt.com/zx.gesture.sensor.jpg\" alt=\"Image courtesy of Sparkfun.\" loading=\"lazy\"></p>\n<p>You can get detailed data over the <mark>I2C interface</mark>, or just ask the sensor to give you data when it detects a gesture. There is <mark>Arduino</mark> code provided, but I wanted to connect this bad boy to the Internet. I reached into my kit, grabbed a <a href=\"https://www.particle.io/\">Particle Photon</a>, and migrated the Arduino code to a Photon library.</p>\n<h4 id=\"thephoton\">The Photon</h4>\n<p>Copy and paste from the Arduino library into the Particle IDE, met with successful compilation on the first pass. I did eventually go back and pull out parts I knew were not needed, such as the &quot;Arduino.h&quot; header reference, but <mark>nothing else</mark> was really required of me.</p>\n<pre><code>#include &quot;ZX_Sensor.h&quot;\n\nGestureType gesture;\nuint8_t     speed;\nZX_Sensor   sensor = ZX_Sensor( 0x10 );\n\nvoid setup() {\n  // Initialize sensor\n  sensor.init();\n}\n\nvoid loop() {\n  char content[255];\n\n  // Data available\n  if( sensor.gestureAvailable() ) {\n\n    // Read\n    gesture = sensor.readGesture();\n    speed = sensor.readGestureSpeed();\n\n    // What just happened\n    switch( gesture ) {\n      case NO_GESTURE:\n        sprintf( content, &quot;-1,%u&quot;, speed );\n        break;\n\n      case RIGHT_SWIPE:\n        sprintf( content, &quot;0,%u&quot;, speed );\n        break;\n\n      case LEFT_SWIPE:\n        sprintf( content, &quot;1,%u&quot;, speed );\n        break;\n\n      case UP_SWIPE:\n        sprintf( content, &quot;3,%u&quot;, speed );\n        break;\n\n    }\n\n    // Publish event\n    Particle.publish( &quot;slap&quot;, content, PRIVATE );\n  }\n}\n\n</code></pre>\n<p>While you can connect a Photon directly to an MQTT broker, or even make HTTP requests, I find both of these to be particularly problematic from a <mark>security</mark> perspective. I prefer to use the <a href=\"https://docs.particle.io/guide/tools-and-features/webhooks/\">Particle WebHooks</a> because data from the Photon, to the Particle servers, is secured with <mark>TLS</mark>. This also allows me to <mark>keep my endpoint credentials tucked away from the client.</mark></p>\n<pre><code>curl -X &quot;POST&quot; \n  &quot;https://YOUR_ORG.messaging.internetofthings.ibmcloud.com:8883/api/v0002/application/types/YOUR_TYPE/devices/YOUR_DEVICE/events/swipe&quot; \\\n     -H &quot;Content-Type: application/json; charset=utf-8&quot; \\\n     -u a-YOUR_ORG-YOUR_USERNAME:YOUR_PASSWORD \\\n     -d $'{\n  &quot;speed&quot;: 10,\n  &quot;direction&quot;: &quot;left&quot;\n}'\n</code></pre>\n<p>The WebHook integration then makes an <mark>HTTPS</mark> request to the <a href=\"https://www.ibm.com/internet-of-things/\">Watson IoT</a> MQTT broker. Yup, you can <mark>POST messages to the broker using HTTPS</mark>! Messages are in turn broadcast out over the specified MQTT topics. To get this to the Web, I had a Node.js server running on IBM Bluemix.</p>\n<h4 id=\"theserver\">The Server</h4>\n<p>The Node.js server has two main parts. The first part is that it subscribes to Watson IoT message via an MQTT client library. Again, keeping my connectivity credentials on the server, away from any clients, helps me to sleep well at night.</p>\n<pre><code>// Connect to Watson IoT\nvar client  = mqtt.connect( \n  config.iot_host, \n  {\n    clientId: config.iot_client + '_' + Math.round( ( Math.random() * 10000 ) ),\n    username: config.iot_username,\n    password: config.iot_password,\n    port: 1883\n  }\n);\n\n...\n\n// Connected to Watson\n// Subscribe for sensor data\nclient.on( 'connect', function() {\n  console.log( 'Broker connected.' );\n\n  client.subscribe( config.iot_topic, function( error, granted ) {\n    console.log( 'Subscribed.' );\n  } );\n} );\n\n// New message arrived\nclient.on( 'message', function( topic, message ) {\n  console.log( message.toString() );\n\n  // Send to client screen\n  io.emit( 'slap', message.toString() );\n} );\n\n</code></pre>\n<p>When the gesture message arrives, it is pushed to the connected clients via <a href=\"https://socket.io/\">Socket IO</a>. Socket IO is a nice fit here because it adds some smarts to the connection to the browser. The ability to reconnect is one of my favorite features, but it also simplifies managing destinations over straight WebSocket.</p>\n<blockquote>\n<p>I tend to be a bit of a purist when it comes to WebSocket, so giving an endorsement to Socket IO is a big step for me.</p>\n</blockquote>\n<pre><code>// Application\nvar app = express();\n\n// Static for main files\napp.use( '/', express.static( 'public' ) );\n\n// Bluemix\nvar env = cfenv.getAppEnv();\n\n// Listen\nvar server = app.listen( env.port, env.bind, function() {\n  // Debug\n  console.log( 'Started on: ' + env.port );\n} );\n\n...\n\n// Socket\nvar io = require( 'socket.io' )( server );\n\n// New socket connection\nio.on( 'connection', function( socket ) {\n  console.log( 'Client connected.' );\n} );\n</code></pre>\n<h4 id=\"thememe\">The Meme</h4>\n<p>As I was gesturing away, I kept thinking to myself &quot;Now what?&quot; It was around this time when the original Batman, <a href=\"https://en.wikipedia.org/wiki/Adam_West\">Adam West</a>, passed away. I was reminded of the meme of Batman slapping Robin in the comic books, and a demo was born.</p>\n<p><img src=\"http://images.kevinhoyt.com/batman.slap.robin.jpg\" alt=\"The original reason that Batman slaps Robin.\" loading=\"lazy\"></p>\n<h4 id=\"thebrowser\">The Browser</h4>\n<p>I figured that if I was making a gesture of swiping left, that Batman should be slapping Robin using his left hand. If I was making a gesture of swiping right, then Batman should use his right hand. That would be as easy as displaying the image flipped horizontally based on the data.</p>\n<pre><code>// Connect to server\n// Listen for slap event\nthis.socket = io();\nthis.socket.on( 'slap', evt =&gt; this.doMessage( evt ) );\n\n...\n\ndoMessage( evt ) {\n  // Parse\n  let message = JSON.parse( evt );\n  let parts = message.data.split( ',' );\n  let direction = parseInt( parts[0] );\n\n  // Show image for direction of gesture\n  switch( direction ) {\n    case 0:\n      this.slap.style.backgroundImage = 'url( img/slap.right.png )';\n      break;\n\n    case 1:\n      this.slap.style.backgroundImage = 'url( img/slap.left.png )';\n      break;      \n  }\n\n  // Display for a limited time\n  this.interval = setInterval( () =&gt; {\n    this.slap.style.backgroundImage = '';\n    clearInterval( this.interval );\n    this.interval = null;\n  }, 2000 );\n\n  // Debug\n  console.log( message );\n}\n</code></pre>\n<p>The image of Batman slapping Robin stays around for two seconds, and then disappears.</p>\n<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/dxYOkCFIuRI\" frameborder=\"0\" allowfullscreen></iframe>\n<p><em>Note: While I love the Particle Photon, the single biggest complaint I have with it, surfaced during the recording of this video - no 5GHz support. In a noisy environment like my home, this means that the Photon is regularly disconnecting.</em></p>\n<h4 id=\"nextsteps\">Next Steps</h4>\n<p>While this may seem like a trivial application, it is actually a <mark>solid boilerplate</mark> for connecting any type of sensor data, <mark>securely</mark>, to any infrastructure. Credentials are kept on servers, away from prying eyes, while the sockets cleanly deliver the data from the sensor to any connected client. Security is critical with IoT, and this is a great way to start off your next application.</p>\n<!--kg-card-end: markdown-->","comment_id":"80","plaintext":"Every now and again, I run across a sensor that just sounds like fun. Such is\nthe case when I read about the ZX Gesture Sensor from Sparkfun and XYZ\nInteractive. Put it together with the Internet, and I just may have gotten a\nlittle slap-happy.\n\nThe Sensor\nThe ZX Gesture Sensor measures about 2.5\" long and about 1\" deep. It consists of \ntwo IR emitters - one on either end, and an IR sensor in the middle. When you\nmove your hand in front of the sensor, IR light bounces off of it, which is in\nturn picked up by the sensor.\n\n\n\nYou can get detailed data over the I2C interface, or just ask the sensor to give\nyou data when it detects a gesture. There is Arduino code provided, but I wanted\nto connect this bad boy to the Internet. I reached into my kit, grabbed a \nParticle Photon [https://www.particle.io/], and migrated the Arduino code to a\nPhoton library.\n\nThe Photon\nCopy and paste from the Arduino library into the Particle IDE, met with\nsuccessful compilation on the first pass. I did eventually go back and pull out\nparts I knew were not needed, such as the \"Arduino.h\" header reference, but \nnothing else was really required of me.\n\n#include \"ZX_Sensor.h\"\n\nGestureType gesture;\nuint8_t     speed;\nZX_Sensor   sensor = ZX_Sensor( 0x10 );\n\nvoid setup() {\n  // Initialize sensor\n  sensor.init();\n}\n\nvoid loop() {\n  char content[255];\n\n  // Data available\n  if( sensor.gestureAvailable() ) {\n\n    // Read\n    gesture = sensor.readGesture();\n    speed = sensor.readGestureSpeed();\n\n    // What just happened\n    switch( gesture ) {\n      case NO_GESTURE:\n        sprintf( content, \"-1,%u\", speed );\n        break;\n\n      case RIGHT_SWIPE:\n        sprintf( content, \"0,%u\", speed );\n        break;\n\n      case LEFT_SWIPE:\n        sprintf( content, \"1,%u\", speed );\n        break;\n\n      case UP_SWIPE:\n        sprintf( content, \"3,%u\", speed );\n        break;\n\n    }\n\n    // Publish event\n    Particle.publish( \"slap\", content, PRIVATE );\n  }\n}\n\n\n\nWhile you can connect a Photon directly to an MQTT broker, or even make HTTP\nrequests, I find both of these to be particularly problematic from a security \nperspective. I prefer to use the Particle WebHooks\n[https://docs.particle.io/guide/tools-and-features/webhooks/] because data from\nthe Photon, to the Particle servers, is secured with TLS. This also allows me to \nkeep my endpoint credentials tucked away from the client.\n\ncurl -X \"POST\" \n  \"https://YOUR_ORG.messaging.internetofthings.ibmcloud.com:8883/api/v0002/application/types/YOUR_TYPE/devices/YOUR_DEVICE/events/swipe\" \\\n     -H \"Content-Type: application/json; charset=utf-8\" \\\n     -u a-YOUR_ORG-YOUR_USERNAME:YOUR_PASSWORD \\\n     -d $'{\n  \"speed\": 10,\n  \"direction\": \"left\"\n}'\n\n\nThe WebHook integration then makes an HTTPS request to the Watson IoT\n[https://www.ibm.com/internet-of-things/] MQTT broker. Yup, you can POST\nmessages to the broker using HTTPS! Messages are in turn broadcast out over the\nspecified MQTT topics. To get this to the Web, I had a Node.js server running on\nIBM Bluemix.\n\nThe Server\nThe Node.js server has two main parts. The first part is that it subscribes to\nWatson IoT message via an MQTT client library. Again, keeping my connectivity\ncredentials on the server, away from any clients, helps me to sleep well at\nnight.\n\n// Connect to Watson IoT\nvar client  = mqtt.connect( \n  config.iot_host, \n  {\n    clientId: config.iot_client + '_' + Math.round( ( Math.random() * 10000 ) ),\n    username: config.iot_username,\n    password: config.iot_password,\n    port: 1883\n  }\n);\n\n...\n\n// Connected to Watson\n// Subscribe for sensor data\nclient.on( 'connect', function() {\n  console.log( 'Broker connected.' );\n\n  client.subscribe( config.iot_topic, function( error, granted ) {\n    console.log( 'Subscribed.' );\n  } );\n} );\n\n// New message arrived\nclient.on( 'message', function( topic, message ) {\n  console.log( message.toString() );\n\n  // Send to client screen\n  io.emit( 'slap', message.toString() );\n} );\n\n\n\nWhen the gesture message arrives, it is pushed to the connected clients via \nSocket IO [https://socket.io/]. Socket IO is a nice fit here because it adds\nsome smarts to the connection to the browser. The ability to reconnect is one of\nmy favorite features, but it also simplifies managing destinations over straight\nWebSocket.\n\n> I tend to be a bit of a purist when it comes to WebSocket, so giving an\nendorsement to Socket IO is a big step for me.\n\n\n// Application\nvar app = express();\n\n// Static for main files\napp.use( '/', express.static( 'public' ) );\n\n// Bluemix\nvar env = cfenv.getAppEnv();\n\n// Listen\nvar server = app.listen( env.port, env.bind, function() {\n  // Debug\n  console.log( 'Started on: ' + env.port );\n} );\n\n...\n\n// Socket\nvar io = require( 'socket.io' )( server );\n\n// New socket connection\nio.on( 'connection', function( socket ) {\n  console.log( 'Client connected.' );\n} );\n\n\nThe Meme\nAs I was gesturing away, I kept thinking to myself \"Now what?\" It was around\nthis time when the original Batman, Adam West\n[https://en.wikipedia.org/wiki/Adam_West], passed away. I was reminded of the\nmeme of Batman slapping Robin in the comic books, and a demo was born.\n\n\n\nThe Browser\nI figured that if I was making a gesture of swiping left, that Batman should be\nslapping Robin using his left hand. If I was making a gesture of swiping right,\nthen Batman should use his right hand. That would be as easy as displaying the\nimage flipped horizontally based on the data.\n\n// Connect to server\n// Listen for slap event\nthis.socket = io();\nthis.socket.on( 'slap', evt => this.doMessage( evt ) );\n\n...\n\ndoMessage( evt ) {\n  // Parse\n  let message = JSON.parse( evt );\n  let parts = message.data.split( ',' );\n  let direction = parseInt( parts[0] );\n\n  // Show image for direction of gesture\n  switch( direction ) {\n    case 0:\n      this.slap.style.backgroundImage = 'url( img/slap.right.png )';\n      break;\n\n    case 1:\n      this.slap.style.backgroundImage = 'url( img/slap.left.png )';\n      break;      \n  }\n\n  // Display for a limited time\n  this.interval = setInterval( () => {\n    this.slap.style.backgroundImage = '';\n    clearInterval( this.interval );\n    this.interval = null;\n  }, 2000 );\n\n  // Debug\n  console.log( message );\n}\n\n\nThe image of Batman slapping Robin stays around for two seconds, and then\ndisappears.\n\nNote: While I love the Particle Photon, the single biggest complaint I have with\nit, surfaced during the recording of this video - no 5GHz support. In a noisy\nenvironment like my home, this means that the Photon is regularly disconnecting.\n\nNext Steps\nWhile this may seem like a trivial application, it is actually a solid\nboilerplate for connecting any type of sensor data, securely, to any\ninfrastructure. Credentials are kept on servers, away from prying eyes, while\nthe sockets cleanly deliver the data from the sensor to any connected client.\nSecurity is critical with IoT, and this is a great way to start off your next\napplication.","feature_image":"__GHOST_URL__/content/images/2019/01/adam.west.batman.jpg","featured":0,"status":"published","locale":null,"visibility":"public","author_id":"1","created_at":"2017-06-23T21:14:58.000Z","updated_at":"2019-01-17T00:36:12.000Z","published_at":"2017-07-11T12:25:00.000Z","custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"type":"post","email_recipient_filter":"none"},{"id":"5adb8922351ffe0018a57758","uuid":"f5d0f133-8422-4a44-beb5-6a7eb2535848","title":"Developer Advocates and Content","slug":"developer-advocates-and-content","mobiledoc":"{\"version\":\"0.3.1\",\"markups\":[],\"atoms\":[],\"cards\":[[\"markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\"Congratulations! Your first presentation as a Developer Advocate lay just ahead. You have put in the hours to build a really cool demonstration. It is a masterpiece of elegant code. Now comes the rest of the job.\\n\\nBeing a developer advocate means a lot more than showing up with a shiny demo. Of course we want to educate. Yes, you want to share the problems you encountered. Always be learning from others. There is however an aspect of the role that is often overlooked - everything (all the work) that goes with it.\\n\\n####Presentation\\n\\nThere are books on building and giving presentations. You should read them. In the interest of brevity however, here is how I generally go about it.\\n\\nFirst, I build an ==outline==. It does not have to have every little detail in it, but it should be decently thorough. Do not forget a slide about ==who you are==, and why we should listen. Also consider the \\\"==tell them what you are going to tell them, tell them, then tell them what you told them==\\\" adage.\\n\\nMoving to the slides, I like to put one bullet on every slide to start. Then I head to the speaker notes and ==start writing the narrative of the story I want to tell==. Three or four sentences will work fine here, but if the speaker notes scroll, then I am getting too wordy.\\n\\nAlong the way I might consolidate slides. I might change some of my original assumptions. I might cut pieces that distract from my narrative. The result is that you could effectively ==read the speaker notes and give the presentation== (not that you should do that - **reading your slides is bad**).\\n\\nNow I will go back and actually ==build the slides==. This might lead to more consolidation, or more expansion. I tend to try for a presentation wherein the slides can ==speak for themselves== without me presenting them, but at the same time, ==not loaded with bullets== to read from. Getting this balance takes time and experience.\\n\\nThink you have got it? Well, remember, you are a developer, so it is time to ==refactor==. Divide the minutes in your presentation (leaving time for Q&A), by the number of slides in your deck. If you have 40 minutes, and 20 slides, that is 2 minutes per slide. Is that ==time-to-slides ratio== realistic? Depends on your content. Change the narrative and slide content respectively to fit.\\n\\n> I strongly suggest presenting in the [Ignite](https://en.wikipedia.org/wiki/Ignite_(event)) style at least once in your presentation career. There is nothing quite as effective to make you appreciate just how tight a presentation can be - or how sloppy yours is.\\n\\nIt is worth noting that you probably will talk for too long anyways. Or that the demo will take more time than you thought. Or that guy in the back keeps asking questions you were not ready to answer. Or the event organizer has \\\"a few announcements\\\" that take up a chunk of your presentation. Or there is an AV problem. Or ... ==rarely does a presentation go to plan==.\\n\\n####You Did It!\\n\\nWhew! You made it through! If you are an introvert like me, it is time to crawl into your quiet space and recharge your batteries. Alas, the work of a developer advocate is never done. In fact, the real work has not even begun yet. Let us get busy with the ==follow-through==.\\n\\n####Share Your Slides\\n\\nChances are, somebody who attended your presentation will want your slides. If they do not, there is a good chance that somewhere down the line, somebody that did not see your presentation will want your slides. ==Neither will see anything if you do not publish them.==\\n\\n[Slideshare](https://www.slideshare.net/) (part of LinkedIn, part of Microsoft) is among the more common ways to accomplish this task. The ==more details== you can give Slideshare when you publish your slides, the better. You are looking for cross-site search engine optimization (SEO), where ==discovery of a piece of content on one outlet, leads the consumer to your other pieces of related content on other outlets==.\\n\\nI use Google Slides almost exclusively, and if I have filled the speaker notes with the core narrative of my presentation, and built the slides to speak for themselves, then I will also likely just [share](https://github.com/krhoyt/Jfokus2017/blob/master/iot-chasm.pdf) the presentation openly. I also like to download the [PDF version](https://github.com/krhoyt/Jfokus2017/blob/master/iot-chasm.pdf) of the slides and put them in the GitHub repository with the source code for my demonstration. We will get to your code in a moment.\\n\\n####Technical Write-Up\\n\\nYou have been ==blogging all the problems== you encountered when building your shiny demo, right? (Passive aggressive, much?) If not, you should have been. When you run into a problem, that is a blog post. Blog posts do not need to be enlightened masterpieces. I like to think of them as a personal diary and record, that I happen to share with the world. And I look back to [my own blog posts](http://www.kevinhoyt.com/2016/05/20/the-12-steps-of-bluetooth-swift/) frequently to remember how I solved a problem.\\n\\nAt this point however, you have a story to tell. You wrote a narrative for your presentation. You have developed an opinion. ==Now it is time for the blog post that pulls all those other posts about the problems you encountered along the way, into something more cohesive.== It is time to let your opinion loose.\\n\\nWhen you are writing this post, do not forget to include links to the [other](http://www.kevinhoyt.com/2017/06/13/conference-abstract-ideas/) [relative](http://www.kevinhoyt.com/2016/08/26/measuring-developer-relations/) [posts](http://www.kevinhoyt.com/2016/06/08/evangelist-advocate-community/), and a link to the GitHub repository. Perhaps embed the YouTube video. Wait. What? What YouTube video?\\n\\n####Video\\n\\nMany conferences will record the video of your presentation. That is good. You should share that on Twitter. It is not uncommon for me however, to create a [YouTube](https://www.youtube.com/watch?v=-1IVtuCmQ9I&t=42s) video of the shiny demonstration as well. ==Just the shiny parts.== Something ==under ten minutes== - ideally less than five minutes. This is what I will embed in my [related blog post](http://www.kevinhoyt.com/2017/02/24/web-bluetooth-bean/).\\n\\nYou might choose [Vimeo](https://vimeo.com/), or some other host. What is important here, as with Slideshare, is that the ==more details== you can provide about the video, when submitting it, the better. For example, the video description might have a link to the blog post. ==Make sure that your profile is complete==, and that it includes your social contact points (Twitter, GitHub, etc.).\\n\\n==It is worth it to put some energy into the production of your demonstration video.== An opening title slide is nice. Something that includes your social contact points, the name of your employer, your name and email, and maybe even your photo.\\n\\nWhen you record the video, be sure to ==say something==. I cannot count the number of useful projects I have seen with videos of somebody pushing a button and something happening, without any narration. The frustration overwhelms! What looked useful, just became annoying. ==Talk about what it is that you are doing, and why you are doing it.== Yes, you sound funny on video - just like everybody else. What is important is that you sound like you know what you are talking about. That is what will keep me listening.\\n\\nSometimes you will experience technical problems at conferences. Some problems are really common, such as getting reliable Internet access. This can of course cause havoc with your shiny demo. ==Having a recording of your hard work, before you ever set foot into the venue, can pay high dividends on the effort.== No Internet? No problem.\\n\\n####Source Code\\n\\nIf you are a developer advocate, I am going to assume that you wrote some code for your shiny demo. You put a lot of \\\"==sweat equity==\\\" in getting the demo working. Do not let that go to waste. ==Sharing your code== on a platform such as [GitHub](https://github.com/krhoyt) is critical.\\n\\nWhen it comes to team projects, there are various schools of thoughts on comments in your code. To this I would say that your demo is not a team project - it is ==educational in nature==. You may come back to the code in a year. Somebody else may not need the code for a year. Can you remember what you did and why you did it after a year of projects and presentations?\\n\\n==I prefer heavy commenting== in the code I publish that goes along with conference presentations. Not every single line, but I do take the time to comment code that might otherwise seem redundant. \\n\\nEven if you did comment everything, you still would not have the complete picture. That is where the ==README== file comes into play. Be sure to flesh out the README to reflect the ==overall architecture of the demonstration==. I like to link to the products I use, and describe what it is I used them to accomplish.\\n\\n> For a workshop, I will actually make the README file the [courseware](https://github.com/krhoyt/WatsonWorkshop).\\n\\nIt is worth considering links to the venue web page, your overview blog post, and video, as well. Again, this is about ==maximizing your SEO footprint==. Cross-linking is key, but you want to do it in a manner that is appropriate, not trashy.\\n\\nI will also often put references to the presentation venue, the dates, the description, etc. ==It is always nice to give a bump to the people that gave you the stage==, but it is also nice to look back when it comes time for your annual review.\\n\\nI personally have a ==dedicated repository for many of the events at which I present==. That repository will have all the code, PDF slides from the presentation(s), and the README will contain session titles, descriptions, and links to pertinent content (blog posts, video, slides, etc). Using this approach, you can put ==a single short link== in all your materials for that given conference, and even get some ==view measurements== on the side.\\n\\nWhile I do not like repeating content across events, I sometimes get ==repeated requests== for specific content. In those cases, I build a repository specifically for that content. I will strip off the conference specific hooks, and start to ==treat it as a proper project==. The related assets will reflect that.\\n\\n####Social Channels\\n\\nLast, but not least, is the use of your social channels. I have already mentioned tweeting the video recording of your presentation (if there was one) done by the venue organizers. You should also be sure to ==tweet your blog post== overview. Something along the lines of \\\"Video and write-up of my @venue presentation. Thanks for having me! http://my.blog.com/presentation\\\" will do nicely.\\n\\n==Twitter is currently the most dominant social channel for developers==, so I have used it as an example throughout this post. It is not however, the only social network. If you feel that your Facebook community will be interested in your content, then you should certainly publish it there as well.\\n\\nI actually consider [Medium](https://medium.com/@krhoyt) (at the time of this writing) to be another ==social channel==. While your blog should be the center of the technical articles you write, it is not uncommon to ==re-post on Medium, or even a corporate blog==. While these platforms come and go, and your employer may change, you want to be able to keep the legacy of your hard work intact. ==This means presenting your brand, in its entirety, on your own domain.==\\n\\n####Next Steps\\n\\nWait a second. You expect ...\\n\\n- Build the shiny demo\\n- Present the shiny demo\\n- Share the shiny demo code\\n- Share the presentation\\n- Video the shiny demo\\n- Write about the problems\\n- Write about the shiny demo\\n- Tweet the shiny demo\\n- Cross-link it all\\n\\nAll while I am ...\\n\\n- Attending corporate meetings\\n- Submitting call for papers\\n- Booking travel\\n- Actually traveling\\n- Filing expenses from the travel\\n- Keeping email under control\\n- Attending meetups\\n- Running my own meetup\\n- Contributing to open source\\n- Monitoring the Twittersphere\\n- Helping people on Stack Overflow\\n- Helping organize corporate events\\n- Learn new technologies ... squirrel!\\n- And any other fire drills\\n\\nYes! I do! That really is the whole point of this post - ==being a developer advocate is a full-time job==. \\n\\nPeople (and by \\\"people\\\" I mean ==senior management==) often see the jet-set, beer drinking, party attending, side of advocacy, and nothing else. And sure, ==advocacy is a cost center==, and they approve the expenses every time. They are going to see that side of it.\\n\\n==The reality is that developer advocacy done right takes an immense amount of effort==. The landscape is constantly shifting. There are deadlines looming everywhere. In my experience, if you cannot learn to manage all of this in three years, you will burn out, and leave developer advocacy as a career. It is that kind of a workload - ==people burn out in under three years==.\\n\\n> And by burnout, I do not mean take a holiday and feel better. I know advocates that burned out and now raise sheep on a farm (not kidding).\\n\\nIf you are ==senior management== reading this, I would implore you to ==respect the load put upon developer advocacy== (developer relations on the whole). The last thing advocates need is for you to start demanding justification for their existence - telling them about KPIs they need to hit. Some magical out-of-thin-air number of activations goal. Or to show up suddenly with new demands. \\n\\n**Advocates have enough to do already, just to do the job properly for you.**\\n\\nRather than view developer advocates as a marketing/sales hybrid that you can screw tighter for better results, respect advocates as professionals in their own right. Advocates want to help their employers. They are going to operate in the best interests of the company. All you have to do is ==remove the occasional barrier, and stay out of the way==, in order to reap the rewards.\\n\"}]],\"sections\":[[10,0]],\"ghostVersion\":\"3.0\"}","html":"<!--kg-card-begin: markdown--><p>Congratulations! Your first presentation as a Developer Advocate lay just ahead. You have put in the hours to build a really cool demonstration. It is a masterpiece of elegant code. Now comes the rest of the job.</p>\n<p>Being a developer advocate means a lot more than showing up with a shiny demo. Of course we want to educate. Yes, you want to share the problems you encountered. Always be learning from others. There is however an aspect of the role that is often overlooked - everything (all the work) that goes with it.</p>\n<h4 id=\"presentation\">Presentation</h4>\n<p>There are books on building and giving presentations. You should read them. In the interest of brevity however, here is how I generally go about it.</p>\n<p>First, I build an <mark>outline</mark>. It does not have to have every little detail in it, but it should be decently thorough. Do not forget a slide about <mark>who you are</mark>, and why we should listen. Also consider the &quot;<mark>tell them what you are going to tell them, tell them, then tell them what you told them</mark>&quot; adage.</p>\n<p>Moving to the slides, I like to put one bullet on every slide to start. Then I head to the speaker notes and <mark>start writing the narrative of the story I want to tell</mark>. Three or four sentences will work fine here, but if the speaker notes scroll, then I am getting too wordy.</p>\n<p>Along the way I might consolidate slides. I might change some of my original assumptions. I might cut pieces that distract from my narrative. The result is that you could effectively <mark>read the speaker notes and give the presentation</mark> (not that you should do that - <strong>reading your slides is bad</strong>).</p>\n<p>Now I will go back and actually <mark>build the slides</mark>. This might lead to more consolidation, or more expansion. I tend to try for a presentation wherein the slides can <mark>speak for themselves</mark> without me presenting them, but at the same time, <mark>not loaded with bullets</mark> to read from. Getting this balance takes time and experience.</p>\n<p>Think you have got it? Well, remember, you are a developer, so it is time to <mark>refactor</mark>. Divide the minutes in your presentation (leaving time for Q&amp;A), by the number of slides in your deck. If you have 40 minutes, and 20 slides, that is 2 minutes per slide. Is that <mark>time-to-slides ratio</mark> realistic? Depends on your content. Change the narrative and slide content respectively to fit.</p>\n<blockquote>\n<p>I strongly suggest presenting in the <a href=\"https://en.wikipedia.org/wiki/Ignite_(event)\">Ignite</a> style at least once in your presentation career. There is nothing quite as effective to make you appreciate just how tight a presentation can be - or how sloppy yours is.</p>\n</blockquote>\n<p>It is worth noting that you probably will talk for too long anyways. Or that the demo will take more time than you thought. Or that guy in the back keeps asking questions you were not ready to answer. Or the event organizer has &quot;a few announcements&quot; that take up a chunk of your presentation. Or there is an AV problem. Or ... <mark>rarely does a presentation go to plan</mark>.</p>\n<h4 id=\"youdidit\">You Did It!</h4>\n<p>Whew! You made it through! If you are an introvert like me, it is time to crawl into your quiet space and recharge your batteries. Alas, the work of a developer advocate is never done. In fact, the real work has not even begun yet. Let us get busy with the <mark>follow-through</mark>.</p>\n<h4 id=\"shareyourslides\">Share Your Slides</h4>\n<p>Chances are, somebody who attended your presentation will want your slides. If they do not, there is a good chance that somewhere down the line, somebody that did not see your presentation will want your slides. <mark>Neither will see anything if you do not publish them.</mark></p>\n<p><a href=\"https://www.slideshare.net/\">Slideshare</a> (part of LinkedIn, part of Microsoft) is among the more common ways to accomplish this task. The <mark>more details</mark> you can give Slideshare when you publish your slides, the better. You are looking for cross-site search engine optimization (SEO), where <mark>discovery of a piece of content on one outlet, leads the consumer to your other pieces of related content on other outlets</mark>.</p>\n<p>I use Google Slides almost exclusively, and if I have filled the speaker notes with the core narrative of my presentation, and built the slides to speak for themselves, then I will also likely just <a href=\"https://github.com/krhoyt/Jfokus2017/blob/master/iot-chasm.pdf\">share</a> the presentation openly. I also like to download the <a href=\"https://github.com/krhoyt/Jfokus2017/blob/master/iot-chasm.pdf\">PDF version</a> of the slides and put them in the GitHub repository with the source code for my demonstration. We will get to your code in a moment.</p>\n<h4 id=\"technicalwriteup\">Technical Write-Up</h4>\n<p>You have been <mark>blogging all the problems</mark> you encountered when building your shiny demo, right? (Passive aggressive, much?) If not, you should have been. When you run into a problem, that is a blog post. Blog posts do not need to be enlightened masterpieces. I like to think of them as a personal diary and record, that I happen to share with the world. And I look back to <a href=\"http://www.kevinhoyt.com/2016/05/20/the-12-steps-of-bluetooth-swift/\">my own blog posts</a> frequently to remember how I solved a problem.</p>\n<p>At this point however, you have a story to tell. You wrote a narrative for your presentation. You have developed an opinion. <mark>Now it is time for the blog post that pulls all those other posts about the problems you encountered along the way, into something more cohesive.</mark> It is time to let your opinion loose.</p>\n<p>When you are writing this post, do not forget to include links to the <a href=\"http://www.kevinhoyt.com/2017/06/13/conference-abstract-ideas/\">other</a> <a href=\"http://www.kevinhoyt.com/2016/08/26/measuring-developer-relations/\">relative</a> <a href=\"http://www.kevinhoyt.com/2016/06/08/evangelist-advocate-community/\">posts</a>, and a link to the GitHub repository. Perhaps embed the YouTube video. Wait. What? What YouTube video?</p>\n<h4 id=\"video\">Video</h4>\n<p>Many conferences will record the video of your presentation. That is good. You should share that on Twitter. It is not uncommon for me however, to create a <a href=\"https://www.youtube.com/watch?v=-1IVtuCmQ9I&amp;t=42s\">YouTube</a> video of the shiny demonstration as well. <mark>Just the shiny parts.</mark> Something <mark>under ten minutes</mark> - ideally less than five minutes. This is what I will embed in my <a href=\"http://www.kevinhoyt.com/2017/02/24/web-bluetooth-bean/\">related blog post</a>.</p>\n<p>You might choose <a href=\"https://vimeo.com/\">Vimeo</a>, or some other host. What is important here, as with Slideshare, is that the <mark>more details</mark> you can provide about the video, when submitting it, the better. For example, the video description might have a link to the blog post. <mark>Make sure that your profile is complete</mark>, and that it includes your social contact points (Twitter, GitHub, etc.).</p>\n<p><mark>It is worth it to put some energy into the production of your demonstration video.</mark> An opening title slide is nice. Something that includes your social contact points, the name of your employer, your name and email, and maybe even your photo.</p>\n<p>When you record the video, be sure to <mark>say something</mark>. I cannot count the number of useful projects I have seen with videos of somebody pushing a button and something happening, without any narration. The frustration overwhelms! What looked useful, just became annoying. <mark>Talk about what it is that you are doing, and why you are doing it.</mark> Yes, you sound funny on video - just like everybody else. What is important is that you sound like you know what you are talking about. That is what will keep me listening.</p>\n<p>Sometimes you will experience technical problems at conferences. Some problems are really common, such as getting reliable Internet access. This can of course cause havoc with your shiny demo. <mark>Having a recording of your hard work, before you ever set foot into the venue, can pay high dividends on the effort.</mark> No Internet? No problem.</p>\n<h4 id=\"sourcecode\">Source Code</h4>\n<p>If you are a developer advocate, I am going to assume that you wrote some code for your shiny demo. You put a lot of &quot;<mark>sweat equity</mark>&quot; in getting the demo working. Do not let that go to waste. <mark>Sharing your code</mark> on a platform such as <a href=\"https://github.com/krhoyt\">GitHub</a> is critical.</p>\n<p>When it comes to team projects, there are various schools of thoughts on comments in your code. To this I would say that your demo is not a team project - it is <mark>educational in nature</mark>. You may come back to the code in a year. Somebody else may not need the code for a year. Can you remember what you did and why you did it after a year of projects and presentations?</p>\n<p><mark>I prefer heavy commenting</mark> in the code I publish that goes along with conference presentations. Not every single line, but I do take the time to comment code that might otherwise seem redundant.</p>\n<p>Even if you did comment everything, you still would not have the complete picture. That is where the <mark>README</mark> file comes into play. Be sure to flesh out the README to reflect the <mark>overall architecture of the demonstration</mark>. I like to link to the products I use, and describe what it is I used them to accomplish.</p>\n<blockquote>\n<p>For a workshop, I will actually make the README file the <a href=\"https://github.com/krhoyt/WatsonWorkshop\">courseware</a>.</p>\n</blockquote>\n<p>It is worth considering links to the venue web page, your overview blog post, and video, as well. Again, this is about <mark>maximizing your SEO footprint</mark>. Cross-linking is key, but you want to do it in a manner that is appropriate, not trashy.</p>\n<p>I will also often put references to the presentation venue, the dates, the description, etc. <mark>It is always nice to give a bump to the people that gave you the stage</mark>, but it is also nice to look back when it comes time for your annual review.</p>\n<p>I personally have a <mark>dedicated repository for many of the events at which I present</mark>. That repository will have all the code, PDF slides from the presentation(s), and the README will contain session titles, descriptions, and links to pertinent content (blog posts, video, slides, etc). Using this approach, you can put <mark>a single short link</mark> in all your materials for that given conference, and even get some <mark>view measurements</mark> on the side.</p>\n<p>While I do not like repeating content across events, I sometimes get <mark>repeated requests</mark> for specific content. In those cases, I build a repository specifically for that content. I will strip off the conference specific hooks, and start to <mark>treat it as a proper project</mark>. The related assets will reflect that.</p>\n<h4 id=\"socialchannels\">Social Channels</h4>\n<p>Last, but not least, is the use of your social channels. I have already mentioned tweeting the video recording of your presentation (if there was one) done by the venue organizers. You should also be sure to <mark>tweet your blog post</mark> overview. Something along the lines of &quot;Video and write-up of my @venue presentation. Thanks for having me! <a href=\"http://my.blog.com/presentation\">http://my.blog.com/presentation</a>&quot; will do nicely.</p>\n<p><mark>Twitter is currently the most dominant social channel for developers</mark>, so I have used it as an example throughout this post. It is not however, the only social network. If you feel that your Facebook community will be interested in your content, then you should certainly publish it there as well.</p>\n<p>I actually consider <a href=\"https://medium.com/@krhoyt\">Medium</a> (at the time of this writing) to be another <mark>social channel</mark>. While your blog should be the center of the technical articles you write, it is not uncommon to <mark>re-post on Medium, or even a corporate blog</mark>. While these platforms come and go, and your employer may change, you want to be able to keep the legacy of your hard work intact. <mark>This means presenting your brand, in its entirety, on your own domain.</mark></p>\n<h4 id=\"nextsteps\">Next Steps</h4>\n<p>Wait a second. You expect ...</p>\n<ul>\n<li>Build the shiny demo</li>\n<li>Present the shiny demo</li>\n<li>Share the shiny demo code</li>\n<li>Share the presentation</li>\n<li>Video the shiny demo</li>\n<li>Write about the problems</li>\n<li>Write about the shiny demo</li>\n<li>Tweet the shiny demo</li>\n<li>Cross-link it all</li>\n</ul>\n<p>All while I am ...</p>\n<ul>\n<li>Attending corporate meetings</li>\n<li>Submitting call for papers</li>\n<li>Booking travel</li>\n<li>Actually traveling</li>\n<li>Filing expenses from the travel</li>\n<li>Keeping email under control</li>\n<li>Attending meetups</li>\n<li>Running my own meetup</li>\n<li>Contributing to open source</li>\n<li>Monitoring the Twittersphere</li>\n<li>Helping people on Stack Overflow</li>\n<li>Helping organize corporate events</li>\n<li>Learn new technologies ... squirrel!</li>\n<li>And any other fire drills</li>\n</ul>\n<p>Yes! I do! That really is the whole point of this post - <mark>being a developer advocate is a full-time job</mark>.</p>\n<p>People (and by &quot;people&quot; I mean <mark>senior management</mark>) often see the jet-set, beer drinking, party attending, side of advocacy, and nothing else. And sure, <mark>advocacy is a cost center</mark>, and they approve the expenses every time. They are going to see that side of it.</p>\n<p><mark>The reality is that developer advocacy done right takes an immense amount of effort</mark>. The landscape is constantly shifting. There are deadlines looming everywhere. In my experience, if you cannot learn to manage all of this in three years, you will burn out, and leave developer advocacy as a career. It is that kind of a workload - <mark>people burn out in under three years</mark>.</p>\n<blockquote>\n<p>And by burnout, I do not mean take a holiday and feel better. I know advocates that burned out and now raise sheep on a farm (not kidding).</p>\n</blockquote>\n<p>If you are <mark>senior management</mark> reading this, I would implore you to <mark>respect the load put upon developer advocacy</mark> (developer relations on the whole). The last thing advocates need is for you to start demanding justification for their existence - telling them about KPIs they need to hit. Some magical out-of-thin-air number of activations goal. Or to show up suddenly with new demands.</p>\n<p><strong>Advocates have enough to do already, just to do the job properly for you.</strong></p>\n<p>Rather than view developer advocates as a marketing/sales hybrid that you can screw tighter for better results, respect advocates as professionals in their own right. Advocates want to help their employers. They are going to operate in the best interests of the company. All you have to do is <mark>remove the occasional barrier, and stay out of the way</mark>, in order to reap the rewards.</p>\n<!--kg-card-end: markdown-->","comment_id":"81","plaintext":"Congratulations! Your first presentation as a Developer Advocate lay just ahead.\nYou have put in the hours to build a really cool demonstration. It is a\nmasterpiece of elegant code. Now comes the rest of the job.\n\nBeing a developer advocate means a lot more than showing up with a shiny demo.\nOf course we want to educate. Yes, you want to share the problems you\nencountered. Always be learning from others. There is however an aspect of the\nrole that is often overlooked - everything (all the work) that goes with it.\n\nPresentation\nThere are books on building and giving presentations. You should read them. In\nthe interest of brevity however, here is how I generally go about it.\n\nFirst, I build an outline. It does not have to have every little detail in it,\nbut it should be decently thorough. Do not forget a slide about who you are, and\nwhy we should listen. Also consider the \"tell them what you are going to tell\nthem, tell them, then tell them what you told them\" adage.\n\nMoving to the slides, I like to put one bullet on every slide to start. Then I\nhead to the speaker notes and start writing the narrative of the story I want to\ntell. Three or four sentences will work fine here, but if the speaker notes\nscroll, then I am getting too wordy.\n\nAlong the way I might consolidate slides. I might change some of my original\nassumptions. I might cut pieces that distract from my narrative. The result is\nthat you could effectively read the speaker notes and give the presentation (not\nthat you should do that - reading your slides is bad).\n\nNow I will go back and actually build the slides. This might lead to more\nconsolidation, or more expansion. I tend to try for a presentation wherein the\nslides can speak for themselves without me presenting them, but at the same\ntime, not loaded with bullets to read from. Getting this balance takes time and\nexperience.\n\nThink you have got it? Well, remember, you are a developer, so it is time to \nrefactor. Divide the minutes in your presentation (leaving time for Q&A), by the\nnumber of slides in your deck. If you have 40 minutes, and 20 slides, that is 2\nminutes per slide. Is that time-to-slides ratio realistic? Depends on your\ncontent. Change the narrative and slide content respectively to fit.\n\n> I strongly suggest presenting in the Ignite\n[https://en.wikipedia.org/wiki/Ignite_(event)] style at least once in your\npresentation career. There is nothing quite as effective to make you appreciate\njust how tight a presentation can be - or how sloppy yours is.\n\n\nIt is worth noting that you probably will talk for too long anyways. Or that the\ndemo will take more time than you thought. Or that guy in the back keeps asking\nquestions you were not ready to answer. Or the event organizer has \"a few\nannouncements\" that take up a chunk of your presentation. Or there is an AV\nproblem. Or ... rarely does a presentation go to plan.\n\nYou Did It!\nWhew! You made it through! If you are an introvert like me, it is time to crawl\ninto your quiet space and recharge your batteries. Alas, the work of a developer\nadvocate is never done. In fact, the real work has not even begun yet. Let us\nget busy with the follow-through.\n\nShare Your Slides\nChances are, somebody who attended your presentation will want your slides. If\nthey do not, there is a good chance that somewhere down the line, somebody that\ndid not see your presentation will want your slides. Neither will see anything\nif you do not publish them.\n\nSlideshare [https://www.slideshare.net/] (part of LinkedIn, part of Microsoft)\nis among the more common ways to accomplish this task. The more details you can\ngive Slideshare when you publish your slides, the better. You are looking for\ncross-site search engine optimization (SEO), where discovery of a piece of\ncontent on one outlet, leads the consumer to your other pieces of related\ncontent on other outlets.\n\nI use Google Slides almost exclusively, and if I have filled the speaker notes\nwith the core narrative of my presentation, and built the slides to speak for\nthemselves, then I will also likely just share\n[https://github.com/krhoyt/Jfokus2017/blob/master/iot-chasm.pdf] the\npresentation openly. I also like to download the PDF version\n[https://github.com/krhoyt/Jfokus2017/blob/master/iot-chasm.pdf] of the slides\nand put them in the GitHub repository with the source code for my demonstration.\nWe will get to your code in a moment.\n\nTechnical Write-Up\nYou have been blogging all the problems you encountered when building your shiny\ndemo, right? (Passive aggressive, much?) If not, you should have been. When you\nrun into a problem, that is a blog post. Blog posts do not need to be\nenlightened masterpieces. I like to think of them as a personal diary and\nrecord, that I happen to share with the world. And I look back to my own blog\nposts [http://www.kevinhoyt.com/2016/05/20/the-12-steps-of-bluetooth-swift/] \nfrequently to remember how I solved a problem.\n\nAt this point however, you have a story to tell. You wrote a narrative for your\npresentation. You have developed an opinion. Now it is time for the blog post\nthat pulls all those other posts about the problems you encountered along the\nway, into something more cohesive. It is time to let your opinion loose.\n\nWhen you are writing this post, do not forget to include links to the other\n[http://www.kevinhoyt.com/2017/06/13/conference-abstract-ideas/] relative\n[http://www.kevinhoyt.com/2016/08/26/measuring-developer-relations/] posts\n[http://www.kevinhoyt.com/2016/06/08/evangelist-advocate-community/], and a link\nto the GitHub repository. Perhaps embed the YouTube video. Wait. What? What\nYouTube video?\n\nVideo\nMany conferences will record the video of your presentation. That is good. You\nshould share that on Twitter. It is not uncommon for me however, to create a \nYouTube [https://www.youtube.com/watch?v=-1IVtuCmQ9I&t=42s] video of the shiny\ndemonstration as well. Just the shiny parts. Something under ten minutes -\nideally less than five minutes. This is what I will embed in my related blog\npost [http://www.kevinhoyt.com/2017/02/24/web-bluetooth-bean/].\n\nYou might choose Vimeo [https://vimeo.com/], or some other host. What is\nimportant here, as with Slideshare, is that the more details you can provide\nabout the video, when submitting it, the better. For example, the video\ndescription might have a link to the blog post. Make sure that your profile is\ncomplete, and that it includes your social contact points (Twitter, GitHub,\netc.).\n\nIt is worth it to put some energy into the production of your demonstration\nvideo. An opening title slide is nice. Something that includes your social\ncontact points, the name of your employer, your name and email, and maybe even\nyour photo.\n\nWhen you record the video, be sure to say something. I cannot count the number\nof useful projects I have seen with videos of somebody pushing a button and\nsomething happening, without any narration. The frustration overwhelms! What\nlooked useful, just became annoying. Talk about what it is that you are doing,\nand why you are doing it. Yes, you sound funny on video - just like everybody\nelse. What is important is that you sound like you know what you are talking\nabout. That is what will keep me listening.\n\nSometimes you will experience technical problems at conferences. Some problems\nare really common, such as getting reliable Internet access. This can of course\ncause havoc with your shiny demo. Having a recording of your hard work, before\nyou ever set foot into the venue, can pay high dividends on the effort. No\nInternet? No problem.\n\nSource Code\nIf you are a developer advocate, I am going to assume that you wrote some code\nfor your shiny demo. You put a lot of \"sweat equity\" in getting the demo\nworking. Do not let that go to waste. Sharing your code on a platform such as \nGitHub [https://github.com/krhoyt] is critical.\n\nWhen it comes to team projects, there are various schools of thoughts on\ncomments in your code. To this I would say that your demo is not a team project\n- it is educational in nature. You may come back to the code in a year. Somebody\nelse may not need the code for a year. Can you remember what you did and why you\ndid it after a year of projects and presentations?\n\nI prefer heavy commenting in the code I publish that goes along with conference\npresentations. Not every single line, but I do take the time to comment code\nthat might otherwise seem redundant.\n\nEven if you did comment everything, you still would not have the complete\npicture. That is where the README file comes into play. Be sure to flesh out the\nREADME to reflect the overall architecture of the demonstration. I like to link\nto the products I use, and describe what it is I used them to accomplish.\n\n> For a workshop, I will actually make the README file the courseware\n[https://github.com/krhoyt/WatsonWorkshop].\n\n\nIt is worth considering links to the venue web page, your overview blog post,\nand video, as well. Again, this is about maximizing your SEO footprint.\nCross-linking is key, but you want to do it in a manner that is appropriate, not\ntrashy.\n\nI will also often put references to the presentation venue, the dates, the\ndescription, etc. It is always nice to give a bump to the people that gave you\nthe stage, but it is also nice to look back when it comes time for your annual\nreview.\n\nI personally have a dedicated repository for many of the events at which I\npresent. That repository will have all the code, PDF slides from the\npresentation(s), and the README will contain session titles, descriptions, and\nlinks to pertinent content (blog posts, video, slides, etc). Using this\napproach, you can put a single short link in all your materials for that given\nconference, and even get some view measurements on the side.\n\nWhile I do not like repeating content across events, I sometimes get repeated\nrequests for specific content. In those cases, I build a repository specifically\nfor that content. I will strip off the conference specific hooks, and start to \ntreat it as a proper project. The related assets will reflect that.\n\nSocial Channels\nLast, but not least, is the use of your social channels. I have already\nmentioned tweeting the video recording of your presentation (if there was one)\ndone by the venue organizers. You should also be sure to tweet your blog post \noverview. Something along the lines of \"Video and write-up of my @venue\npresentation. Thanks for having me! http://my.blog.com/presentation\" will do\nnicely.\n\nTwitter is currently the most dominant social channel for developers, so I have\nused it as an example throughout this post. It is not however, the only social\nnetwork. If you feel that your Facebook community will be interested in your\ncontent, then you should certainly publish it there as well.\n\nI actually consider Medium [https://medium.com/@krhoyt] (at the time of this\nwriting) to be another social channel. While your blog should be the center of\nthe technical articles you write, it is not uncommon to re-post on Medium, or\neven a corporate blog. While these platforms come and go, and your employer may\nchange, you want to be able to keep the legacy of your hard work intact. This\nmeans presenting your brand, in its entirety, on your own domain.\n\nNext Steps\nWait a second. You expect ...\n\n * Build the shiny demo\n * Present the shiny demo\n * Share the shiny demo code\n * Share the presentation\n * Video the shiny demo\n * Write about the problems\n * Write about the shiny demo\n * Tweet the shiny demo\n * Cross-link it all\n\nAll while I am ...\n\n * Attending corporate meetings\n * Submitting call for papers\n * Booking travel\n * Actually traveling\n * Filing expenses from the travel\n * Keeping email under control\n * Attending meetups\n * Running my own meetup\n * Contributing to open source\n * Monitoring the Twittersphere\n * Helping people on Stack Overflow\n * Helping organize corporate events\n * Learn new technologies ... squirrel!\n * And any other fire drills\n\nYes! I do! That really is the whole point of this post - being a developer\nadvocate is a full-time job.\n\nPeople (and by \"people\" I mean senior management) often see the jet-set, beer\ndrinking, party attending, side of advocacy, and nothing else. And sure, \nadvocacy is a cost center, and they approve the expenses every time. They are\ngoing to see that side of it.\n\nThe reality is that developer advocacy done right takes an immense amount of\neffort. The landscape is constantly shifting. There are deadlines looming\neverywhere. In my experience, if you cannot learn to manage all of this in three\nyears, you will burn out, and leave developer advocacy as a career. It is that\nkind of a workload - people burn out in under three years.\n\n> And by burnout, I do not mean take a holiday and feel better. I know advocates\nthat burned out and now raise sheep on a farm (not kidding).\n\n\nIf you are senior management reading this, I would implore you to respect the\nload put upon developer advocacy (developer relations on the whole). The last\nthing advocates need is for you to start demanding justification for their\nexistence - telling them about KPIs they need to hit. Some magical\nout-of-thin-air number of activations goal. Or to show up suddenly with new\ndemands.\n\nAdvocates have enough to do already, just to do the job properly for you.\n\nRather than view developer advocates as a marketing/sales hybrid that you can\nscrew tighter for better results, respect advocates as professionals in their\nown right. Advocates want to help their employers. They are going to operate in\nthe best interests of the company. All you have to do is remove the occasional\nbarrier, and stay out of the way, in order to reap the rewards.","feature_image":"__GHOST_URL__/content/images/2019/01/webvisions.audience.jpg","featured":0,"status":"published","locale":null,"visibility":"public","author_id":"1","created_at":"2017-08-16T07:09:55.000Z","updated_at":"2019-01-17T00:35:29.000Z","published_at":"2017-08-16T10:38:54.000Z","custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"type":"post","email_recipient_filter":"none"},{"id":"5adb8922351ffe0018a57759","uuid":"f0f066fe-2ec0-4fef-9d88-0d1c326c459b","title":"Event Notifications from Blockchain","slug":"event-notifications-from-blockchain","mobiledoc":"{\"version\":\"0.3.1\",\"markups\":[],\"atoms\":[],\"cards\":[[\"markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\"As a ledger, among the core functionality of blockchain is to store data. Having that data encrypted, with an audit trail, distributed across all peers in the network, with consensus among nodes, is what really makes a blockchain solution compelling. A blockchain network is not always the only part in a system. Sometimes other systems need to know when something happens to the ledger. Getting those notifications is the topic of this post.\\n\\n###Hyperledger Fabric\\n\\n[Blockchain](https://en.wikipedia.org/wiki/Blockchain) is a computing concept, and there are many implementations. Each implementation does things slightly different from the other. [Hyperledger Fabric](https://www.hyperledger.org/projects/fabric) is an open source blockchain implementation run by [The Linux Foundation](https://www.linuxfoundation.org/), of which [IBM](https://www.ibm.com/blockchain/) is a member. Hyperledger Fabric differs from other implementations in that it is a ==permissioned== blockchain (as opposed to permissionless). In short, nodes on the network have been verified through a certificate to be allowed to join and participate. This brings with it several benefits that make it ideal for myriad business applications - namely, removal of the \\\"proof of work\\\" overhead, yielding substantial ==performance improvements==.\\n\\n[![Hyperledger Fabric](http://images.kevinhoyt.com/hyperledger.fabric.png)](https://www.hyperledger.org/projects/fabric)\\n\\nI will be using an instance of Hyperledger Fabric for this post. Per the documentation, I have Fabric running in a Docker container on my local machine (MacBook Pro). [Setting up and configuring Fabric](https://hyperledger.github.io/composer/installing/development-tools.html) is beyond the scope of this post.\\n\\n###Hyperledger Composer\\n\\nUnless you are a [Go](https://golang.org/) developer, you are going to find developing an application on most blockchain implementations to be particularly cumbersome at first (foreign toolchain). The Hyperledger team figured this might be the case, and created [Hyperledger Composer](https://hyperledger.github.io/composer/), which makes developing applications far more approachable. In fact, if you know a splash of JavaScript, you know enough to build an application, using Composer, on Hyperledger Fabric.\\n\\nComposer consists of various parts and pieces - the primary of which is called a ==Business Network Application==, or ==BNA== for short, or \\\"==banana==\\\" for fun. A business network application is made up of ==four main parts==.\\n\\n- ==Model==: This defines the different parts of your application. For example, if you are auctioning vehicles, you might have an asset named \\\"Car\\\" with properties like \\\"make\\\" and \\\"year\\\". Models can include ==four different type of descriptions==:\\n\\n - ==Assets==: Effectively, the \\\"things\\\" in your system. Car, house, account.\\n\\n - ==Participants==: The types of roles that will be involved in your application. In the case of a vehicle auction, this might be a buyer, seller, auction house, financial institution, and more.\\n\\n - ==Transactions==: Most of the time transactions take place when participants exchange an asset. This does not always have to be the case however, as we will see later.\\n\\n - ==Events==: Events that transactions can emit when conditions within the system match your defined criteria. We will revisit these later as well.\\n\\n- ==Logic==: For Composer, this is a ==JavaScript== function that surfaces the logic of how those model pieces fit together. For example, when somebody bids on the car, you may want to validate that they have enough cash on hand for the transaction.\\n\\n- ==Query==: This is a fairly recent addition to Composer, and allows you to write SQL (something that looks like SQL anyways), to find assets in the ledger. This bring better performance to finding out ==what cars were sold at auction for more than $10,000 but less than $15,000==.\\n\\n- ==Access==: Last but not least is an access control list (==ACL==). This is not particularly different from any other ACL you may have encountered with other systems, and defines who has access to what data and operations.\\n\\n[![Composer Playground](http://images.kevinhoyt.com/composer.playground.png)](https://composer-playground.mybluemix.net/editor)\\n\\nWant to take Hyperledger Composer for a spin without installing it locally? The Composer team hosts a \\\"[playground](https://composer-playground.mybluemix.net/editor)\\\" which allows you to develop and test business networks in the browser (against local storage). Once you have finished testing, you can download the \\\"banana file\\\" from the playground, and deploy it onto Hyperledger Fabric.\\n\\n###Going Bananas\\n\\nHyperledger Composer provides a ==command-line interface (CLI)==, and ==SDK==, to continue to make the process of deploying blockchain applications as easy as possible. With Fabric running, and your BNA file downloaded, the following Composer CLI command will deploy the file.\\n\\n```\\ncomposer network deploy -a real-time-stocks.bna -p hlfv1 -i PeerAdmin -s randomString\\n```\\n\\nFrom there, you still need to interact with your application on Hyperledger Fabric. You have two ways to go about this:\\n\\n- ==Node.js SDK==: Ideal for integrating Fabric directly with your existing Node.js infrastructure.\\n\\n- ==REST server==: Get up and going, quickly! This exposes all your assets and transactions (defined in the model and logic files) to any application that can speak REST.\\n\\nYou can invoke the REST server using the Composer CLI. It looks at your deployment, and generates the API, and even provides a [Swagger](https://swagger.io/) interface for you to further test out your application.\\n\\n```\\ncomposer-rest-server -p hlfv1 -n real-time-stocks -i admin -s adminpw -N never -w\\n```\\n\\nThe thing about a REST interface is that it is a ==request/response== operation. Given the earlier vehicle auction, if I use the REST interface to add a car, or change the property on a car, that is in the ledger, that is all that happens. Notice that last little \\\"==-w==\\\" in the CLI? That actually adds a ==WebSocket== implementation to the REST server. Anything that can speak WebSocket can then listen for events that happen on the blockchain.\\n\\n###Just One Catch\\n\\nIf you will recall from when I broke down the pieces earlier in this post, events can only be triggered from transactions. ==If you use the REST API to change the price of a car that is up for auction, that call has no associated logic, and will not emit any events - changing the value of a property does not require a transaction to be implemented.==\\n\\nWhile transactions are generally for when assets change hands, there is nothing stopping you from writing transactions that ==emulate CRUD== operations on your assets when you need to emit events. Instead of calling the REST interface for assets, you ==call the REST interface to invoke a transaction==. The transaction invokes your logic file, and it is there that you can emit events.\\n\\n###Why Do This\\n\\nComing full circle, this functionality to emit events is particularly useful to ==send events, via WebSocket, to other systems== when ledger values have changed (or meet some criteria). For example, you may have to print a bill of sale when a car is sold. A printer could listen for a \\\"PrintBillOfSale\\\" event, and automate that task.\\n\\nAnother common use of notifications when things change on the ledger is a dashboard.\\n\\n###Stock Trader Dashboard \\n\\nTaking our newly auctioned car for a spin, take a sharp left at Completely Unrelated Avenue. When I was exploring event notifications from blockchain transactions (specifically Composer transactions on Fabric), the concept of a stock trading dashboard came to mind. The car auction is just easier to communicate the parts and pieces of a Composer/Fabric system 😁.\\n\\n```\\nnamespace org.acme.market\\n\\nasset Stock identified by symbol {\\n  o String symbol\\n  o String name\\n  o Double low\\n  o Double high\\n  o Double open\\n  o Double last\\n  o Double change\\n}\\n\\nparticipant Trader identified by id {\\n  o String id\\n  --> Stock[] portfolio\\n}\\n\\ntransaction Trade {\\n  --> Stock stock\\n  o Double price\\n}\\n\\nevent TradeComplete {\\n  o String symbol\\n  o Double low\\n  o Double high\\n  o Double open\\n  o Double last\\n  o Double change\\n}\\n```\\n\\nIn this Composer model file, we see a \\\"Stock\\\" ==asset== with various properties. There is a \\\"Trader\\\" ==participant== that goes unused in this application outside of the ==ACL==. Then there is the ==transaction== \\\"Trade\\\". This is what we will call to change the price of a stock asset. That transaction can emit a \\\"TradeComplete\\\" ==event==, which has various properties describing the latest valuation.\\n\\n```\\n/**\\n * Change stock values as transaction to get event notification.\\n * @param {org.acme.market.Trade} tx Transaction instance.\\n * @transaction\\n */\\nfunction trade( tx ) {\\n  tx.stock.low = Math.min( tx.price, tx.stock.low );\\n  tx.stock.high = Math.max( tx.price, tx.stock.high );\\n  tx.stock.change = Math.round( ( tx.price - tx.stock.last ) * 100 ) / 100;\\n  tx.stock.last = tx.price;\\n  \\n  // Get the asset registry\\n  return getAssetRegistry( 'org.acme.market.Stock' )\\n    .then( function( registry ) {\\n      // Update the asset\\n      return registry.update( tx.stock );\\n    } )\\n\\t.then( function() {\\n      // Generate event\\n      var event = getFactory().newEvent( \\n        'org.acme.market', \\n        'TradeComplete' \\n      );\\n    \\n      // Set properties\\n      event.symbol = tx.stock.symbol;\\n      event.low = tx.stock.low;\\n      event.high = tx.stock.high;\\n      event.open = tx.stock.open;\\n      event.last = tx.stock.last;\\n      event.change = tx.stock.change;\\n    \\n      // Emit\\n      emit( event );\\n    } );\\n}  \\n```\\n\\nIn the ==logic file== (JavaScript), the \\\"Trade\\\" transaction is further defined. It makes changes to the stock asset, and then saves it to the ledger. After that it emits the \\\"TradeComplete\\\" event with the latest valuation. \\n\\n> Yes, this is really a blockchain application, and yes, this is really JavaScript. Composer is pretty cool!\\n\\nUsing the CLI commands from earlier, I can deploy this business network, and stand up a REST server (including WebSocket support for the events), with just two commands. \\n\\nTo submit changes to the stock assets, I have a Python script that picks a random stock, randomly changes the price to be within the high/low values (just keeping things easy here), and then submits that stock, along with the new price, to the \\\"Trade\\\" ==transaction==. This happens once per second to demonstrate emitting and handle events from blockchain.\\n\\n```\\nimport csv\\nimport requests\\nimport time\\n\\nfrom random import randint\\nfrom random import random\\n\\nwith open( 'short.list.csv', 'rb' ) as financials:\\n  reader = csv.reader( financials )\\n  portfolio = list( reader )\\n\\nwhile True:\\n  stock = portfolio[randint( 0, len( portfolio ) - 1 )]\\n\\n  if len( stock[8] ) == 0:\\n    continue\\n\\n  last = randint( \\n    int( float( stock[8] ) * 100 ), \\n    int( float( stock[9] ) * 100 ) \\n  )\\n\\n  trade = {\\n    '$class': 'org.acme.market.Trade',\\n    'stock': stock[0],\\n    'price': float( last ) / 100,\\n  }\\n  requests.post( 'http://localhost:3000/api/Trade', json = trade )\\n  time.sleep( 1 )  \\n```\\n\\nOver on the web-based dashboard, a WebSocket listens for events. When it gets the \\\"TradeComplete\\\" event, it updates the respective parts of the user interface (data table, and chart). And with that we have built an application on blockchain, and handles event notifications in secondary systems.\\n\\n<iframe width=\\\"560\\\" height=\\\"315\\\" src=\\\"https://www.youtube.com/embed/DUvfR4Wi1Ck\\\" frameborder=\\\"0\\\" allowfullscreen></iframe>\\n\\n###What's Next\\n\\nIf you find any of this compelling, I would strongly encourage you to spend some time kicking around Composer in the browser-based \\\"playground\\\". There is ==nothing to install=, and only knowledge of building blockchain applications (with JavaScript nonetheless) to gain. \\n\\nWhen you are testing your application, you will see an option in the left sidebar to see all the changes to the ledger. If you have submitted a transaction that emits an event, you can even see the event and its associated details in the list (so you know you are getting what you want before deployment).\\n\\nThe source code for this project is in the IBM repository of my [GitHub](https://github.com/krhoyt/IBM/tree/master/blockchain/events) account.\"}]],\"sections\":[[10,0]],\"ghostVersion\":\"3.0\"}","html":"<!--kg-card-begin: markdown--><p>As a ledger, among the core functionality of blockchain is to store data. Having that data encrypted, with an audit trail, distributed across all peers in the network, with consensus among nodes, is what really makes a blockchain solution compelling. A blockchain network is not always the only part in a system. Sometimes other systems need to know when something happens to the ledger. Getting those notifications is the topic of this post.</p>\n<h3 id=\"hyperledgerfabric\">Hyperledger Fabric</h3>\n<p><a href=\"https://en.wikipedia.org/wiki/Blockchain\">Blockchain</a> is a computing concept, and there are many implementations. Each implementation does things slightly different from the other. <a href=\"https://www.hyperledger.org/projects/fabric\">Hyperledger Fabric</a> is an open source blockchain implementation run by <a href=\"https://www.linuxfoundation.org/\">The Linux Foundation</a>, of which <a href=\"https://www.ibm.com/blockchain/\">IBM</a> is a member. Hyperledger Fabric differs from other implementations in that it is a <mark>permissioned</mark> blockchain (as opposed to permissionless). In short, nodes on the network have been verified through a certificate to be allowed to join and participate. This brings with it several benefits that make it ideal for myriad business applications - namely, removal of the &quot;proof of work&quot; overhead, yielding substantial <mark>performance improvements</mark>.</p>\n<p><a href=\"https://www.hyperledger.org/projects/fabric\"><img src=\"http://images.kevinhoyt.com/hyperledger.fabric.png\" alt=\"Hyperledger Fabric\" loading=\"lazy\"></a></p>\n<p>I will be using an instance of Hyperledger Fabric for this post. Per the documentation, I have Fabric running in a Docker container on my local machine (MacBook Pro). <a href=\"https://hyperledger.github.io/composer/installing/development-tools.html\">Setting up and configuring Fabric</a> is beyond the scope of this post.</p>\n<h3 id=\"hyperledgercomposer\">Hyperledger Composer</h3>\n<p>Unless you are a <a href=\"https://golang.org/\">Go</a> developer, you are going to find developing an application on most blockchain implementations to be particularly cumbersome at first (foreign toolchain). The Hyperledger team figured this might be the case, and created <a href=\"https://hyperledger.github.io/composer/\">Hyperledger Composer</a>, which makes developing applications far more approachable. In fact, if you know a splash of JavaScript, you know enough to build an application, using Composer, on Hyperledger Fabric.</p>\n<p>Composer consists of various parts and pieces - the primary of which is called a <mark>Business Network Application</mark>, or <mark>BNA</mark> for short, or &quot;<mark>banana</mark>&quot; for fun. A business network application is made up of <mark>four main parts</mark>.</p>\n<ul>\n<li>\n<p><mark>Model</mark>: This defines the different parts of your application. For example, if you are auctioning vehicles, you might have an asset named &quot;Car&quot; with properties like &quot;make&quot; and &quot;year&quot;. Models can include <mark>four different type of descriptions</mark>:</p>\n</li>\n<li>\n<p><mark>Assets</mark>: Effectively, the &quot;things&quot; in your system. Car, house, account.</p>\n</li>\n<li>\n<p><mark>Participants</mark>: The types of roles that will be involved in your application. In the case of a vehicle auction, this might be a buyer, seller, auction house, financial institution, and more.</p>\n</li>\n<li>\n<p><mark>Transactions</mark>: Most of the time transactions take place when participants exchange an asset. This does not always have to be the case however, as we will see later.</p>\n</li>\n<li>\n<p><mark>Events</mark>: Events that transactions can emit when conditions within the system match your defined criteria. We will revisit these later as well.</p>\n</li>\n<li>\n<p><mark>Logic</mark>: For Composer, this is a <mark>JavaScript</mark> function that surfaces the logic of how those model pieces fit together. For example, when somebody bids on the car, you may want to validate that they have enough cash on hand for the transaction.</p>\n</li>\n<li>\n<p><mark>Query</mark>: This is a fairly recent addition to Composer, and allows you to write SQL (something that looks like SQL anyways), to find assets in the ledger. This bring better performance to finding out <mark>what cars were sold at auction for more than $10,000 but less than $15,000</mark>.</p>\n</li>\n<li>\n<p><mark>Access</mark>: Last but not least is an access control list (<mark>ACL</mark>). This is not particularly different from any other ACL you may have encountered with other systems, and defines who has access to what data and operations.</p>\n</li>\n</ul>\n<p><a href=\"https://composer-playground.mybluemix.net/editor\"><img src=\"http://images.kevinhoyt.com/composer.playground.png\" alt=\"Composer Playground\" loading=\"lazy\"></a></p>\n<p>Want to take Hyperledger Composer for a spin without installing it locally? The Composer team hosts a &quot;<a href=\"https://composer-playground.mybluemix.net/editor\">playground</a>&quot; which allows you to develop and test business networks in the browser (against local storage). Once you have finished testing, you can download the &quot;banana file&quot; from the playground, and deploy it onto Hyperledger Fabric.</p>\n<h3 id=\"goingbananas\">Going Bananas</h3>\n<p>Hyperledger Composer provides a <mark>command-line interface (CLI)</mark>, and <mark>SDK</mark>, to continue to make the process of deploying blockchain applications as easy as possible. With Fabric running, and your BNA file downloaded, the following Composer CLI command will deploy the file.</p>\n<pre><code>composer network deploy -a real-time-stocks.bna -p hlfv1 -i PeerAdmin -s randomString\n</code></pre>\n<p>From there, you still need to interact with your application on Hyperledger Fabric. You have two ways to go about this:</p>\n<ul>\n<li>\n<p><mark>Node.js SDK</mark>: Ideal for integrating Fabric directly with your existing Node.js infrastructure.</p>\n</li>\n<li>\n<p><mark>REST server</mark>: Get up and going, quickly! This exposes all your assets and transactions (defined in the model and logic files) to any application that can speak REST.</p>\n</li>\n</ul>\n<p>You can invoke the REST server using the Composer CLI. It looks at your deployment, and generates the API, and even provides a <a href=\"https://swagger.io/\">Swagger</a> interface for you to further test out your application.</p>\n<pre><code>composer-rest-server -p hlfv1 -n real-time-stocks -i admin -s adminpw -N never -w\n</code></pre>\n<p>The thing about a REST interface is that it is a <mark>request/response</mark> operation. Given the earlier vehicle auction, if I use the REST interface to add a car, or change the property on a car, that is in the ledger, that is all that happens. Notice that last little &quot;<mark>-w</mark>&quot; in the CLI? That actually adds a <mark>WebSocket</mark> implementation to the REST server. Anything that can speak WebSocket can then listen for events that happen on the blockchain.</p>\n<h3 id=\"justonecatch\">Just One Catch</h3>\n<p>If you will recall from when I broke down the pieces earlier in this post, events can only be triggered from transactions. <mark>If you use the REST API to change the price of a car that is up for auction, that call has no associated logic, and will not emit any events - changing the value of a property does not require a transaction to be implemented.</mark></p>\n<p>While transactions are generally for when assets change hands, there is nothing stopping you from writing transactions that <mark>emulate CRUD</mark> operations on your assets when you need to emit events. Instead of calling the REST interface for assets, you <mark>call the REST interface to invoke a transaction</mark>. The transaction invokes your logic file, and it is there that you can emit events.</p>\n<h3 id=\"whydothis\">Why Do This</h3>\n<p>Coming full circle, this functionality to emit events is particularly useful to <mark>send events, via WebSocket, to other systems</mark> when ledger values have changed (or meet some criteria). For example, you may have to print a bill of sale when a car is sold. A printer could listen for a &quot;PrintBillOfSale&quot; event, and automate that task.</p>\n<p>Another common use of notifications when things change on the ledger is a dashboard.</p>\n<h3 id=\"stocktraderdashboard\">Stock Trader Dashboard</h3>\n<p>Taking our newly auctioned car for a spin, take a sharp left at Completely Unrelated Avenue. When I was exploring event notifications from blockchain transactions (specifically Composer transactions on Fabric), the concept of a stock trading dashboard came to mind. The car auction is just easier to communicate the parts and pieces of a Composer/Fabric system 😁.</p>\n<pre><code>namespace org.acme.market\n\nasset Stock identified by symbol {\n  o String symbol\n  o String name\n  o Double low\n  o Double high\n  o Double open\n  o Double last\n  o Double change\n}\n\nparticipant Trader identified by id {\n  o String id\n  --&gt; Stock[] portfolio\n}\n\ntransaction Trade {\n  --&gt; Stock stock\n  o Double price\n}\n\nevent TradeComplete {\n  o String symbol\n  o Double low\n  o Double high\n  o Double open\n  o Double last\n  o Double change\n}\n</code></pre>\n<p>In this Composer model file, we see a &quot;Stock&quot; <mark>asset</mark> with various properties. There is a &quot;Trader&quot; <mark>participant</mark> that goes unused in this application outside of the <mark>ACL</mark>. Then there is the <mark>transaction</mark> &quot;Trade&quot;. This is what we will call to change the price of a stock asset. That transaction can emit a &quot;TradeComplete&quot; <mark>event</mark>, which has various properties describing the latest valuation.</p>\n<pre><code>/**\n * Change stock values as transaction to get event notification.\n * @param {org.acme.market.Trade} tx Transaction instance.\n * @transaction\n */\nfunction trade( tx ) {\n  tx.stock.low = Math.min( tx.price, tx.stock.low );\n  tx.stock.high = Math.max( tx.price, tx.stock.high );\n  tx.stock.change = Math.round( ( tx.price - tx.stock.last ) * 100 ) / 100;\n  tx.stock.last = tx.price;\n  \n  // Get the asset registry\n  return getAssetRegistry( 'org.acme.market.Stock' )\n    .then( function( registry ) {\n      // Update the asset\n      return registry.update( tx.stock );\n    } )\n\t.then( function() {\n      // Generate event\n      var event = getFactory().newEvent( \n        'org.acme.market', \n        'TradeComplete' \n      );\n    \n      // Set properties\n      event.symbol = tx.stock.symbol;\n      event.low = tx.stock.low;\n      event.high = tx.stock.high;\n      event.open = tx.stock.open;\n      event.last = tx.stock.last;\n      event.change = tx.stock.change;\n    \n      // Emit\n      emit( event );\n    } );\n}  \n</code></pre>\n<p>In the <mark>logic file</mark> (JavaScript), the &quot;Trade&quot; transaction is further defined. It makes changes to the stock asset, and then saves it to the ledger. After that it emits the &quot;TradeComplete&quot; event with the latest valuation.</p>\n<blockquote>\n<p>Yes, this is really a blockchain application, and yes, this is really JavaScript. Composer is pretty cool!</p>\n</blockquote>\n<p>Using the CLI commands from earlier, I can deploy this business network, and stand up a REST server (including WebSocket support for the events), with just two commands.</p>\n<p>To submit changes to the stock assets, I have a Python script that picks a random stock, randomly changes the price to be within the high/low values (just keeping things easy here), and then submits that stock, along with the new price, to the &quot;Trade&quot; <mark>transaction</mark>. This happens once per second to demonstrate emitting and handle events from blockchain.</p>\n<pre><code>import csv\nimport requests\nimport time\n\nfrom random import randint\nfrom random import random\n\nwith open( 'short.list.csv', 'rb' ) as financials:\n  reader = csv.reader( financials )\n  portfolio = list( reader )\n\nwhile True:\n  stock = portfolio[randint( 0, len( portfolio ) - 1 )]\n\n  if len( stock[8] ) == 0:\n    continue\n\n  last = randint( \n    int( float( stock[8] ) * 100 ), \n    int( float( stock[9] ) * 100 ) \n  )\n\n  trade = {\n    '$class': 'org.acme.market.Trade',\n    'stock': stock[0],\n    'price': float( last ) / 100,\n  }\n  requests.post( 'http://localhost:3000/api/Trade', json = trade )\n  time.sleep( 1 )  \n</code></pre>\n<p>Over on the web-based dashboard, a WebSocket listens for events. When it gets the &quot;TradeComplete&quot; event, it updates the respective parts of the user interface (data table, and chart). And with that we have built an application on blockchain, and handles event notifications in secondary systems.</p>\n<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/DUvfR4Wi1Ck\" frameborder=\"0\" allowfullscreen></iframe>\n<h3 id=\"whatsnext\">What's Next</h3>\n<p>If you find any of this compelling, I would strongly encourage you to spend some time kicking around Composer in the browser-based &quot;playground&quot;. There is ==nothing to install=, and only knowledge of building blockchain applications (with JavaScript nonetheless) to gain.</p>\n<p>When you are testing your application, you will see an option in the left sidebar to see all the changes to the ledger. If you have submitted a transaction that emits an event, you can even see the event and its associated details in the list (so you know you are getting what you want before deployment).</p>\n<p>The source code for this project is in the IBM repository of my <a href=\"https://github.com/krhoyt/IBM/tree/master/blockchain/events\">GitHub</a> account.</p>\n<!--kg-card-end: markdown-->","comment_id":"82","plaintext":"As a ledger, among the core functionality of blockchain is to store data. Having\nthat data encrypted, with an audit trail, distributed across all peers in the\nnetwork, with consensus among nodes, is what really makes a blockchain solution\ncompelling. A blockchain network is not always the only part in a system.\nSometimes other systems need to know when something happens to the ledger.\nGetting those notifications is the topic of this post.\n\nHyperledger Fabric\nBlockchain [https://en.wikipedia.org/wiki/Blockchain] is a computing concept,\nand there are many implementations. Each implementation does things slightly\ndifferent from the other. Hyperledger Fabric\n[https://www.hyperledger.org/projects/fabric] is an open source blockchain\nimplementation run by The Linux Foundation [https://www.linuxfoundation.org/],\nof which IBM [https://www.ibm.com/blockchain/] is a member. Hyperledger Fabric\ndiffers from other implementations in that it is a permissioned blockchain (as\nopposed to permissionless). In short, nodes on the network have been verified\nthrough a certificate to be allowed to join and participate. This brings with it\nseveral benefits that make it ideal for myriad business applications - namely,\nremoval of the \"proof of work\" overhead, yielding substantial performance\nimprovements.\n\n [https://www.hyperledger.org/projects/fabric]\n\nI will be using an instance of Hyperledger Fabric for this post. Per the\ndocumentation, I have Fabric running in a Docker container on my local machine\n(MacBook Pro). Setting up and configuring Fabric\n[https://hyperledger.github.io/composer/installing/development-tools.html] is\nbeyond the scope of this post.\n\nHyperledger Composer\nUnless you are a Go [https://golang.org/] developer, you are going to find\ndeveloping an application on most blockchain implementations to be particularly\ncumbersome at first (foreign toolchain). The Hyperledger team figured this might\nbe the case, and created Hyperledger Composer\n[https://hyperledger.github.io/composer/], which makes developing applications\nfar more approachable. In fact, if you know a splash of JavaScript, you know\nenough to build an application, using Composer, on Hyperledger Fabric.\n\nComposer consists of various parts and pieces - the primary of which is called a \nBusiness Network Application, or BNA for short, or \"banana\" for fun. A business\nnetwork application is made up of four main parts.\n\n * Model: This defines the different parts of your application. For example, if\n   you are auctioning vehicles, you might have an asset named \"Car\" with\n   properties like \"make\" and \"year\". Models can include four different type of\n   descriptions:\n   \n   \n * Assets: Effectively, the \"things\" in your system. Car, house, account.\n   \n   \n * Participants: The types of roles that will be involved in your application.\n   In the case of a vehicle auction, this might be a buyer, seller, auction\n   house, financial institution, and more.\n   \n   \n * Transactions: Most of the time transactions take place when participants\n   exchange an asset. This does not always have to be the case however, as we\n   will see later.\n   \n   \n * Events: Events that transactions can emit when conditions within the system\n   match your defined criteria. We will revisit these later as well.\n   \n   \n * Logic: For Composer, this is a JavaScript function that surfaces the logic of\n   how those model pieces fit together. For example, when somebody bids on the\n   car, you may want to validate that they have enough cash on hand for the\n   transaction.\n   \n   \n * Query: This is a fairly recent addition to Composer, and allows you to write\n   SQL (something that looks like SQL anyways), to find assets in the ledger.\n   This bring better performance to finding out what cars were sold at auction\n   for more than $10,000 but less than $15,000.\n   \n   \n * Access: Last but not least is an access control list (ACL). This is not\n   particularly different from any other ACL you may have encountered with other\n   systems, and defines who has access to what data and operations.\n   \n   \n\n [https://composer-playground.mybluemix.net/editor]\n\nWant to take Hyperledger Composer for a spin without installing it locally? The\nComposer team hosts a \"playground\n[https://composer-playground.mybluemix.net/editor]\" which allows you to develop\nand test business networks in the browser (against local storage). Once you have\nfinished testing, you can download the \"banana file\" from the playground, and\ndeploy it onto Hyperledger Fabric.\n\nGoing Bananas\nHyperledger Composer provides a command-line interface (CLI), and SDK, to\ncontinue to make the process of deploying blockchain applications as easy as\npossible. With Fabric running, and your BNA file downloaded, the following\nComposer CLI command will deploy the file.\n\ncomposer network deploy -a real-time-stocks.bna -p hlfv1 -i PeerAdmin -s randomString\n\n\nFrom there, you still need to interact with your application on Hyperledger\nFabric. You have two ways to go about this:\n\n * Node.js SDK: Ideal for integrating Fabric directly with your existing Node.js\n   infrastructure.\n   \n   \n * REST server: Get up and going, quickly! This exposes all your assets and\n   transactions (defined in the model and logic files) to any application that\n   can speak REST.\n   \n   \n\nYou can invoke the REST server using the Composer CLI. It looks at your\ndeployment, and generates the API, and even provides a Swagger\n[https://swagger.io/] interface for you to further test out your application.\n\ncomposer-rest-server -p hlfv1 -n real-time-stocks -i admin -s adminpw -N never -w\n\n\nThe thing about a REST interface is that it is a request/response operation.\nGiven the earlier vehicle auction, if I use the REST interface to add a car, or\nchange the property on a car, that is in the ledger, that is all that happens.\nNotice that last little \"-w\" in the CLI? That actually adds a WebSocket \nimplementation to the REST server. Anything that can speak WebSocket can then\nlisten for events that happen on the blockchain.\n\nJust One Catch\nIf you will recall from when I broke down the pieces earlier in this post,\nevents can only be triggered from transactions. If you use the REST API to\nchange the price of a car that is up for auction, that call has no associated\nlogic, and will not emit any events - changing the value of a property does not\nrequire a transaction to be implemented.\n\nWhile transactions are generally for when assets change hands, there is nothing\nstopping you from writing transactions that emulate CRUD operations on your\nassets when you need to emit events. Instead of calling the REST interface for\nassets, you call the REST interface to invoke a transaction. The transaction\ninvokes your logic file, and it is there that you can emit events.\n\nWhy Do This\nComing full circle, this functionality to emit events is particularly useful to \nsend events, via WebSocket, to other systems when ledger values have changed (or\nmeet some criteria). For example, you may have to print a bill of sale when a\ncar is sold. A printer could listen for a \"PrintBillOfSale\" event, and automate\nthat task.\n\nAnother common use of notifications when things change on the ledger is a\ndashboard.\n\nStock Trader Dashboard\nTaking our newly auctioned car for a spin, take a sharp left at Completely\nUnrelated Avenue. When I was exploring event notifications from blockchain\ntransactions (specifically Composer transactions on Fabric), the concept of a\nstock trading dashboard came to mind. The car auction is just easier to\ncommunicate the parts and pieces of a Composer/Fabric system 😁.\n\nnamespace org.acme.market\n\nasset Stock identified by symbol {\n  o String symbol\n  o String name\n  o Double low\n  o Double high\n  o Double open\n  o Double last\n  o Double change\n}\n\nparticipant Trader identified by id {\n  o String id\n  --> Stock[] portfolio\n}\n\ntransaction Trade {\n  --> Stock stock\n  o Double price\n}\n\nevent TradeComplete {\n  o String symbol\n  o Double low\n  o Double high\n  o Double open\n  o Double last\n  o Double change\n}\n\n\nIn this Composer model file, we see a \"Stock\" asset with various properties.\nThere is a \"Trader\" participant that goes unused in this application outside of\nthe ACL. Then there is the transaction \"Trade\". This is what we will call to\nchange the price of a stock asset. That transaction can emit a \"TradeComplete\" \nevent, which has various properties describing the latest valuation.\n\n/**\n * Change stock values as transaction to get event notification.\n * @param {org.acme.market.Trade} tx Transaction instance.\n * @transaction\n */\nfunction trade( tx ) {\n  tx.stock.low = Math.min( tx.price, tx.stock.low );\n  tx.stock.high = Math.max( tx.price, tx.stock.high );\n  tx.stock.change = Math.round( ( tx.price - tx.stock.last ) * 100 ) / 100;\n  tx.stock.last = tx.price;\n  \n  // Get the asset registry\n  return getAssetRegistry( 'org.acme.market.Stock' )\n    .then( function( registry ) {\n      // Update the asset\n      return registry.update( tx.stock );\n    } )\n\t.then( function() {\n      // Generate event\n      var event = getFactory().newEvent( \n        'org.acme.market', \n        'TradeComplete' \n      );\n    \n      // Set properties\n      event.symbol = tx.stock.symbol;\n      event.low = tx.stock.low;\n      event.high = tx.stock.high;\n      event.open = tx.stock.open;\n      event.last = tx.stock.last;\n      event.change = tx.stock.change;\n    \n      // Emit\n      emit( event );\n    } );\n}  \n\n\nIn the logic file (JavaScript), the \"Trade\" transaction is further defined. It\nmakes changes to the stock asset, and then saves it to the ledger. After that it\nemits the \"TradeComplete\" event with the latest valuation.\n\n> Yes, this is really a blockchain application, and yes, this is really\nJavaScript. Composer is pretty cool!\n\n\nUsing the CLI commands from earlier, I can deploy this business network, and\nstand up a REST server (including WebSocket support for the events), with just\ntwo commands.\n\nTo submit changes to the stock assets, I have a Python script that picks a\nrandom stock, randomly changes the price to be within the high/low values (just\nkeeping things easy here), and then submits that stock, along with the new\nprice, to the \"Trade\" transaction. This happens once per second to demonstrate\nemitting and handle events from blockchain.\n\nimport csv\nimport requests\nimport time\n\nfrom random import randint\nfrom random import random\n\nwith open( 'short.list.csv', 'rb' ) as financials:\n  reader = csv.reader( financials )\n  portfolio = list( reader )\n\nwhile True:\n  stock = portfolio[randint( 0, len( portfolio ) - 1 )]\n\n  if len( stock[8] ) == 0:\n    continue\n\n  last = randint( \n    int( float( stock[8] ) * 100 ), \n    int( float( stock[9] ) * 100 ) \n  )\n\n  trade = {\n    '$class': 'org.acme.market.Trade',\n    'stock': stock[0],\n    'price': float( last ) / 100,\n  }\n  requests.post( 'http://localhost:3000/api/Trade', json = trade )\n  time.sleep( 1 )  \n\n\nOver on the web-based dashboard, a WebSocket listens for events. When it gets\nthe \"TradeComplete\" event, it updates the respective parts of the user interface\n(data table, and chart). And with that we have built an application on\nblockchain, and handles event notifications in secondary systems.\n\nWhat's Next\nIf you find any of this compelling, I would strongly encourage you to spend some\ntime kicking around Composer in the browser-based \"playground\". There is\n==nothing to install=, and only knowledge of building blockchain applications\n(with JavaScript nonetheless) to gain.\n\nWhen you are testing your application, you will see an option in the left\nsidebar to see all the changes to the ledger. If you have submitted a\ntransaction that emits an event, you can even see the event and its associated\ndetails in the list (so you know you are getting what you want before\ndeployment).\n\nThe source code for this project is in the IBM repository of my GitHub\n[https://github.com/krhoyt/IBM/tree/master/blockchain/events] account.","feature_image":"__GHOST_URL__/content/images/2019/01/chain.close.up.jpg","featured":0,"status":"published","locale":null,"visibility":"public","author_id":"1","created_at":"2017-09-28T20:51:53.000Z","updated_at":"2019-01-17T00:34:34.000Z","published_at":"2017-09-29T20:05:59.000Z","custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"type":"post","email_recipient_filter":"none"},{"id":"5adb8922351ffe0018a5775a","uuid":"b586ac5b-7046-4c27-9415-177bf35ced63","title":"LightBlue Bean to IBM Cloud","slug":"light-blue-bean-to-ibm-cloud","mobiledoc":"{\"version\":\"0.3.1\",\"markups\":[],\"atoms\":[],\"cards\":[[\"markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\"The LightBlue [Bean](https://punchthrough.com/bean), by Punch Through, is one of my favorite IoT development platforms. The Bean has many parts, including an iOS application called the [LightBlue Explorer](https://itunes.apple.com/us/app/lightblue-explorer-bluetooth-low-energy/id557428110?mt=8), that lets you view and control nearby Beans. The gang over at Punch Through recently added the ability for the Explorer application to send Bean data to [cloud services](https://punchthrough.com/blog/posts/introducing-cloud-connect-for-lightblue-explorer). Unfortunately, IBM was not included, so what follows is how to do Bean to IBM Cloud connectivity via iOS.\\n\\n####Bean\\n\\nThe Bean is a [BLE](https://en.wikipedia.org/wiki/Bluetooth_Low_Energy) development platform. As such it sports a BLE stack. You program the Bean via an [Arduino](https://www.arduino.cc/)-compatible workflow. Various features of the BLE stack are exposed to your Arduino program via an [API](https://punchthrough.com/bean/reference). As the Bean also has an accelerometer, temperature sensor, and RGB LED, the API also exposes those details to your Arduino program.\\n\\n```\\n// Reporting rate\\nunsigned long last = 0;\\nint rate = 1000;\\n\\n// Setup\\nvoid setup() {\\n  // Make sure LED is off\\n  // Save battery\\n  Bean.setLed( 0, 0, 0 );\\n\\n  // Deep sleep until needed\\n  Bean.enableWakeOnConnect( true );\\n}\\n```\\n\\nAs we look at the \\\"setup\\\" function, we can see that there really is not much going on. I use the Bean API to turn off the LED. Then I tell the Bean ==wake up when a device is connected==.\\n\\nWe will be reporting data on a regular basis. I do not want to put the Bean to sleep during that, nor do I want the Arduino to block execution of BLE communication, so I am using the tried and true method of ==counting ticks from the clock==.\\n\\n```\\n// Loop\\nvoid loop() {\\n  unsigned long now = 0;\\n  uint8_t scratch[20];\\n  \\n  // Connected to a device\\n  if( Bean.getConnectionState() ) {\\n    // Show connection\\n    Bean.setLed( 0, 255, 0 );    \\n    \\n    // Get clock\\n    now = millis();\\n    \\n    // Check timer\\n    if( ( now - last ) >= rate ) {\\n      // Update clock\\n      last = now;\\n      \\n      // Set sensor values\\n      output();      \\n    }\\n  } else {\\n    // Nobody connected\\n    // Turn off LED\\n    Bean.setLed( 0, 0, 0 );    \\n    \\n    // Deep sleep\\n    Bean.sleep( 0xFFFFFFFF );   \\n  }\\n}\\n```\\n\\nAs the Bean enters the \\\"loop\\\" function, a call to \\\"Bean.getConnectionState()\\\" reveals that no devices are currently connected, so \\\"Bean.sleep( 0xFFFFFFFF )\\\" is called to ==power down the ATmega== until a device connects. The \\\"Bean.sleep()\\\" call takes a number of milliseconds, but if you pass \\\"0xFFFFFFFF\\\" the Bean will ==sleep indefinitely== - in this case, until a device connects.\\n\\nOnce a device, like the iPhone, connects, the Bean springs to life. To show connectivity, I set the on-board RGB LED to green. Next, we start watching ticks from the clock to report sensor data at a one second interval (1000 ms). Then we call an \\\"output()\\\" function to actually report the data.\\n\\n```\\n// Sensor values on characteristic\\nvoid output() {\\n  AccelerationReading acceleration;\\n  char content[20];\\n  uint8_t scratch[20];\\n\\n  // Read accelerometer\\n  acceleration = Bean.getAcceleration();\\n\\n  // Format values as string\\n  sprintf(\\n    content,\\n    \\\"%d,%d,%d,%d\\\",\\n    acceleration.xAxis,\\n    acceleration.yAxis,\\n    acceleration.zAxis,\\t\\t\\n    Bean.getTemperature()\\n  );\\n\\n  // Put string into scratch format\\n  for( int i = 0; i < strlen( content ); i++ ) {\\n    scratch[i] = content[i];\\n  }\\n\\n  // Set scratch data for various sensors\\n  Bean.setScratchData( 1, scratch, strlen( content ) );\\n}\\n```\\n\\nThe Bean API exposes \\\"scratch\\\" data areas - also known as ==characteristic data==. You can put a finite amount of data into these areas - ==20 whole bytes==. In this case, that is enough room. The side effect is that the connected device (iPhone for example) can ask to be ==notified== when scratch data changes.\\n\\nThere are a few different ways to shove four numbers into bytes, but I like \\\"sprintf()\\\" to make a ==CSV string==. That makes it easy to deal when it gets to the device. There are multiple scratch data areas on the Bean, but it all fits in one, so one will do here.\\n\\n> Architecturally, I suppose putting each of the four values we want to have reported to the connected device, into their own scratch areas would be more sound. But that makes for more code on the device, and as a lazy programmer, why do four times, what you could do once?\\n\\nThat is all the code for the Bean. From here, we power up the Bean, connect to it using the \\\"[Bean Loader](https://punchthrough.com/bean/docs/guides/getting-started/os-x/)\\\" tool, compile the Arduino code, and upload it to the Bean. Once there, the Bean will go to sleep, and wait for a device to connect.\\n\\n####iOS (Swift 3)\\n\\nThe semantics of connecting a BLE device to iOS is easily, far and away, the single most popular [post](http://www.kevinhoyt.com/2016/05/20/the-12-steps-of-bluetooth-swift/) on my blog. There are a lot of little nuances to BLE, and it does not help that Apple keeps changing the CoreBluetooth interface. I will not repeat all of that information here, just the pertinent bits.\\n\\n```\\n// Data arrived from peripheral\\nfunc peripheral(\\n  _ peripheral: CBPeripheral, \\n  didUpdateValueFor characteristic: \\n  CBCharacteristic, error: Error?) {\\n    \\n  // Make sure it is the characteristic we want\\n  if characteristic.uuid == BEAN_SCRATCH_UUID {\\n    // Get bytes into string\\n    let content = String(\\n      data: characteristic.value!, \\n      encoding: String.Encoding.utf8\\n    )\\n      \\n    // Formalize\\n    let reading = Reading(raw: content!)\\n\\n    // Delegate\\n    delegate?.didGet(reading: reading)\\n  }\\n}\\n```\\n\\nWhen the Arduino code on the Bean updates the specified characteristic data (scratch), the iPhone is ==notified==. The data comes across as ==bytes==, which I format back into the ==CSV string== that was sent over. Then I pass the CSV string onto a value object where it is parsed, and various convenience methods are exposed (temperature in fahrenheit or celcius as an example).\\n\\n> To keep the nightmare of interfaces that need to be implemented for \\\"CBCentralManagerDelegate\\\" and \\\"CBPeripheralDelegate\\\" out of my view controller, I have wrapped all the pertinent BLE bits into a class all their own, with a single, concise ==delegate== method.\\n\\n####Cloudant\\n\\n[Cloudant](http://cloudant.com), [CouchDB](http://couchdb.apache.org/) on IBM Cloud, is a document store. ==It conveniently aligns CRUD operations with HTTP verbs.== This means that there is no \\\"driver\\\" or special protocol that your application needs to know. If you are working on a device that supports HTTP, then you can store data in the cloud. iOS speaks HTTP.\\n\\n```\\nfunc save(reading:Reading) {\\n    \\n  // Authentication\\n  let user_pass = \\\"\\\\(self.key):\\\\(self.password)\\\".data(using: .utf8)\\n  let encoded = user_pass!.base64EncodedString(options: Data.Base64EncodingOptions.init(rawValue: 0))\\n  let authorization = \\\"Basic \\\\(encoded)\\\"\\n\\n  // Build request\\n  let url = URL(string: \\\"https://\\\\(account).cloudant.com/\\\\(database)\\\")\\n  var request = URLRequest(url: url!)\\n  request.httpMethod = \\\"POST\\\"\\n  request.addValue(\\\"application/json\\\", forHTTPHeaderField: \\\"Content-Type\\\")\\n  request.addValue(authorization, forHTTPHeaderField: \\\"Authorization\\\")\\n  request.httpBody = reading.json();\\n    \\n  // Make request\\n  // Handle response\\n  URLSession.shared.dataTask(with: request) { (data, response, error) in\\n    if error != nil {\\n      debugPrint(error!)\\n    } else {\\n      do {\\n        let json = try JSONSerialization.jsonObject(with: data!) as! [String:Any]\\n      } catch {\\n        debugPrint(error)\\n      }\\n    }\\n  }.resume()\\n}\\n```\\n\\nMaking an HTTP request to Cloudant using Swift take a few steps. The first step is to ==Base64== encode the username and password pair, and to send them as \\\"==Basic Auth==\\\". ==Creating== a document is a ==POST== operation, and we will be sending the document to store as a ==JSON== string. Then we use ==URLSession== to fire off the request. The response is a pretty slender JSON string that includes the \\\"id\\\" of the created document.\\n\\n>Cloudant has a \\\"Permissions\\\" feature that allows you to generate additional key/token combinations for API access with various rights. ==Do not send your Cloudant administrator username/password.==\\n\\nWait. What? Just an HTTP request to work with the database? Yup! that was easy, right? Perhaps we should go for a bonus round!\\n\\n####Watson IoT\\n\\nAnother common endpoint for IoT data on IBM Cloud is [Watson IoT](https://www.ibm.com/internet-of-things/platform/watson-iot-platform/). At its core, Watson IoT is an ==MQTT broker== (MQTT being created, and largely [maintained](http://www.eclipse.org/paho/), by IBM). MQTT is now a nearly ==de facto standard== for IoT devices. This particularly application however, is just publishing data. We do not really need to subscribe to any topics. As it turns out, Watson IoT also supports an ==HTTP interface== to publish device events.\\n\\n```\\nfunc publish(reading:Reading) {\\n    \\n  // Authentication\\n  let user_pass = \\\"\\\\(self.username):\\\\(self.password)\\\".data(using: .utf8)\\n  let encoded = user_pass!.base64EncodedString(options: Data.Base64EncodingOptions.init(rawValue: 0))\\n  let authorization = \\\"Basic \\\\(encoded)\\\"\\n    \\n  // Build request\\n  let url = URL(string:\\n    \\\"https://\\\\(organization).\\\\(host):\\\\(port)\\\" +\\n    \\\"/api/v0002/application/\\\" +\\n    \\\"types/\\\\(device_type)\\\" +\\n    \\\"/devices/\\\\(device_id)\\\" +\\n    \\\"/events/\\\\(event)\\\"\\n  )\\n  var request = URLRequest(url: url!)\\n  request.httpMethod = \\\"POST\\\"\\n  request.addValue(\\\"application/json\\\", forHTTPHeaderField: \\\"Content-Type\\\")\\n  request.addValue(authorization, forHTTPHeaderField: \\\"Authorization\\\")\\n  request.httpBody = reading.json();\\n    \\n  // Make request\\n  // Handle response\\n  URLSession.shared.dataTask(with: request) { (data, response, error) in\\n    if error != nil {\\n      debugPrint(error!)\\n    }\\n  }.resume()\\n}\\n```\\n\\nThis is almost identical to the Cloudant example, just with a few nuances in terminology.\\n\\n- Within Watson IoT, you device categories of devices that will be connecting - this is the \\\"==device type==\\\".\\n- Each device in that category gets its own ID (as well as security token) - this is the \\\"==device ID==\\\".\\n- A device can trigger an \\\"==event==\\\" when it has something to report. This can be named whatever you want, and generally incorporated into the MQTT ==topic== string.\\n- The \\\"==username==\\\" and \\\"==password==\\\" here are keys generated for applications to access Watson IoT as though they were devices.\\n\\nKnowing the terminology, and where to put them, makes all the difference in using Watson IoT.\\n\\n####Next Steps\\n\\n<iframe width=\\\"560\\\" height=\\\"315\\\" src=\\\"https://www.youtube.com/embed/keFmb_ZXma0\\\" frameborder=\\\"0\\\" allowfullscreen></iframe>\\n\\nThere are a lot of other services on IBM Cloud that support an HTTP interface, and that could be useful in this scenario. One example that comes to mind, and that is growing in relevance for IoT architectures, is [Kafka](https://kafka.apache.org/). Kafka on IBM Cloud is called \\\"[Message Hub](http://www-03.ibm.com/software/products/en/ibm-message-hub)\\\". I think that would make an interesting next step.\\n\\nConnecting to Watson IoT over MQTT is possible of course, so I should probably update the application to provide for programmatic channel selection. If you want to see the code in its entirety, or just grab snippets for your own application, check out the [GitHub repository](https://github.com/krhoyt/IBM/tree/master/iot/bluetooth/cloud) for this project.\"}]],\"sections\":[[10,0]],\"ghostVersion\":\"3.0\"}","html":"<!--kg-card-begin: markdown--><p>The LightBlue <a href=\"https://punchthrough.com/bean\">Bean</a>, by Punch Through, is one of my favorite IoT development platforms. The Bean has many parts, including an iOS application called the <a href=\"https://itunes.apple.com/us/app/lightblue-explorer-bluetooth-low-energy/id557428110?mt=8\">LightBlue Explorer</a>, that lets you view and control nearby Beans. The gang over at Punch Through recently added the ability for the Explorer application to send Bean data to <a href=\"https://punchthrough.com/blog/posts/introducing-cloud-connect-for-lightblue-explorer\">cloud services</a>. Unfortunately, IBM was not included, so what follows is how to do Bean to IBM Cloud connectivity via iOS.</p>\n<h4 id=\"bean\">Bean</h4>\n<p>The Bean is a <a href=\"https://en.wikipedia.org/wiki/Bluetooth_Low_Energy\">BLE</a> development platform. As such it sports a BLE stack. You program the Bean via an <a href=\"https://www.arduino.cc/\">Arduino</a>-compatible workflow. Various features of the BLE stack are exposed to your Arduino program via an <a href=\"https://punchthrough.com/bean/reference\">API</a>. As the Bean also has an accelerometer, temperature sensor, and RGB LED, the API also exposes those details to your Arduino program.</p>\n<pre><code>// Reporting rate\nunsigned long last = 0;\nint rate = 1000;\n\n// Setup\nvoid setup() {\n  // Make sure LED is off\n  // Save battery\n  Bean.setLed( 0, 0, 0 );\n\n  // Deep sleep until needed\n  Bean.enableWakeOnConnect( true );\n}\n</code></pre>\n<p>As we look at the &quot;setup&quot; function, we can see that there really is not much going on. I use the Bean API to turn off the LED. Then I tell the Bean <mark>wake up when a device is connected</mark>.</p>\n<p>We will be reporting data on a regular basis. I do not want to put the Bean to sleep during that, nor do I want the Arduino to block execution of BLE communication, so I am using the tried and true method of <mark>counting ticks from the clock</mark>.</p>\n<pre><code>// Loop\nvoid loop() {\n  unsigned long now = 0;\n  uint8_t scratch[20];\n  \n  // Connected to a device\n  if( Bean.getConnectionState() ) {\n    // Show connection\n    Bean.setLed( 0, 255, 0 );    \n    \n    // Get clock\n    now = millis();\n    \n    // Check timer\n    if( ( now - last ) &gt;= rate ) {\n      // Update clock\n      last = now;\n      \n      // Set sensor values\n      output();      \n    }\n  } else {\n    // Nobody connected\n    // Turn off LED\n    Bean.setLed( 0, 0, 0 );    \n    \n    // Deep sleep\n    Bean.sleep( 0xFFFFFFFF );   \n  }\n}\n</code></pre>\n<p>As the Bean enters the &quot;loop&quot; function, a call to &quot;Bean.getConnectionState()&quot; reveals that no devices are currently connected, so &quot;Bean.sleep( 0xFFFFFFFF )&quot; is called to <mark>power down the ATmega</mark> until a device connects. The &quot;Bean.sleep()&quot; call takes a number of milliseconds, but if you pass &quot;0xFFFFFFFF&quot; the Bean will <mark>sleep indefinitely</mark> - in this case, until a device connects.</p>\n<p>Once a device, like the iPhone, connects, the Bean springs to life. To show connectivity, I set the on-board RGB LED to green. Next, we start watching ticks from the clock to report sensor data at a one second interval (1000 ms). Then we call an &quot;output()&quot; function to actually report the data.</p>\n<pre><code>// Sensor values on characteristic\nvoid output() {\n  AccelerationReading acceleration;\n  char content[20];\n  uint8_t scratch[20];\n\n  // Read accelerometer\n  acceleration = Bean.getAcceleration();\n\n  // Format values as string\n  sprintf(\n    content,\n    &quot;%d,%d,%d,%d&quot;,\n    acceleration.xAxis,\n    acceleration.yAxis,\n    acceleration.zAxis,\t\t\n    Bean.getTemperature()\n  );\n\n  // Put string into scratch format\n  for( int i = 0; i &lt; strlen( content ); i++ ) {\n    scratch[i] = content[i];\n  }\n\n  // Set scratch data for various sensors\n  Bean.setScratchData( 1, scratch, strlen( content ) );\n}\n</code></pre>\n<p>The Bean API exposes &quot;scratch&quot; data areas - also known as <mark>characteristic data</mark>. You can put a finite amount of data into these areas - <mark>20 whole bytes</mark>. In this case, that is enough room. The side effect is that the connected device (iPhone for example) can ask to be <mark>notified</mark> when scratch data changes.</p>\n<p>There are a few different ways to shove four numbers into bytes, but I like &quot;sprintf()&quot; to make a <mark>CSV string</mark>. That makes it easy to deal when it gets to the device. There are multiple scratch data areas on the Bean, but it all fits in one, so one will do here.</p>\n<blockquote>\n<p>Architecturally, I suppose putting each of the four values we want to have reported to the connected device, into their own scratch areas would be more sound. But that makes for more code on the device, and as a lazy programmer, why do four times, what you could do once?</p>\n</blockquote>\n<p>That is all the code for the Bean. From here, we power up the Bean, connect to it using the &quot;<a href=\"https://punchthrough.com/bean/docs/guides/getting-started/os-x/\">Bean Loader</a>&quot; tool, compile the Arduino code, and upload it to the Bean. Once there, the Bean will go to sleep, and wait for a device to connect.</p>\n<h4 id=\"iosswift3\">iOS (Swift 3)</h4>\n<p>The semantics of connecting a BLE device to iOS is easily, far and away, the single most popular <a href=\"http://www.kevinhoyt.com/2016/05/20/the-12-steps-of-bluetooth-swift/\">post</a> on my blog. There are a lot of little nuances to BLE, and it does not help that Apple keeps changing the CoreBluetooth interface. I will not repeat all of that information here, just the pertinent bits.</p>\n<pre><code>// Data arrived from peripheral\nfunc peripheral(\n  _ peripheral: CBPeripheral, \n  didUpdateValueFor characteristic: \n  CBCharacteristic, error: Error?) {\n    \n  // Make sure it is the characteristic we want\n  if characteristic.uuid == BEAN_SCRATCH_UUID {\n    // Get bytes into string\n    let content = String(\n      data: characteristic.value!, \n      encoding: String.Encoding.utf8\n    )\n      \n    // Formalize\n    let reading = Reading(raw: content!)\n\n    // Delegate\n    delegate?.didGet(reading: reading)\n  }\n}\n</code></pre>\n<p>When the Arduino code on the Bean updates the specified characteristic data (scratch), the iPhone is <mark>notified</mark>. The data comes across as <mark>bytes</mark>, which I format back into the <mark>CSV string</mark> that was sent over. Then I pass the CSV string onto a value object where it is parsed, and various convenience methods are exposed (temperature in fahrenheit or celcius as an example).</p>\n<blockquote>\n<p>To keep the nightmare of interfaces that need to be implemented for &quot;CBCentralManagerDelegate&quot; and &quot;CBPeripheralDelegate&quot; out of my view controller, I have wrapped all the pertinent BLE bits into a class all their own, with a single, concise <mark>delegate</mark> method.</p>\n</blockquote>\n<h4 id=\"cloudant\">Cloudant</h4>\n<p><a href=\"http://cloudant.com\">Cloudant</a>, <a href=\"http://couchdb.apache.org/\">CouchDB</a> on IBM Cloud, is a document store. <mark>It conveniently aligns CRUD operations with HTTP verbs.</mark> This means that there is no &quot;driver&quot; or special protocol that your application needs to know. If you are working on a device that supports HTTP, then you can store data in the cloud. iOS speaks HTTP.</p>\n<pre><code>func save(reading:Reading) {\n    \n  // Authentication\n  let user_pass = &quot;\\(self.key):\\(self.password)&quot;.data(using: .utf8)\n  let encoded = user_pass!.base64EncodedString(options: Data.Base64EncodingOptions.init(rawValue: 0))\n  let authorization = &quot;Basic \\(encoded)&quot;\n\n  // Build request\n  let url = URL(string: &quot;https://\\(account).cloudant.com/\\(database)&quot;)\n  var request = URLRequest(url: url!)\n  request.httpMethod = &quot;POST&quot;\n  request.addValue(&quot;application/json&quot;, forHTTPHeaderField: &quot;Content-Type&quot;)\n  request.addValue(authorization, forHTTPHeaderField: &quot;Authorization&quot;)\n  request.httpBody = reading.json();\n    \n  // Make request\n  // Handle response\n  URLSession.shared.dataTask(with: request) { (data, response, error) in\n    if error != nil {\n      debugPrint(error!)\n    } else {\n      do {\n        let json = try JSONSerialization.jsonObject(with: data!) as! [String:Any]\n      } catch {\n        debugPrint(error)\n      }\n    }\n  }.resume()\n}\n</code></pre>\n<p>Making an HTTP request to Cloudant using Swift take a few steps. The first step is to <mark>Base64</mark> encode the username and password pair, and to send them as &quot;<mark>Basic Auth</mark>&quot;. <mark>Creating</mark> a document is a <mark>POST</mark> operation, and we will be sending the document to store as a <mark>JSON</mark> string. Then we use <mark>URLSession</mark> to fire off the request. The response is a pretty slender JSON string that includes the &quot;id&quot; of the created document.</p>\n<blockquote>\n<p>Cloudant has a &quot;Permissions&quot; feature that allows you to generate additional key/token combinations for API access with various rights. <mark>Do not send your Cloudant administrator username/password.</mark></p>\n</blockquote>\n<p>Wait. What? Just an HTTP request to work with the database? Yup! that was easy, right? Perhaps we should go for a bonus round!</p>\n<h4 id=\"watsoniot\">Watson IoT</h4>\n<p>Another common endpoint for IoT data on IBM Cloud is <a href=\"https://www.ibm.com/internet-of-things/platform/watson-iot-platform/\">Watson IoT</a>. At its core, Watson IoT is an <mark>MQTT broker</mark> (MQTT being created, and largely <a href=\"http://www.eclipse.org/paho/\">maintained</a>, by IBM). MQTT is now a nearly <mark>de facto standard</mark> for IoT devices. This particularly application however, is just publishing data. We do not really need to subscribe to any topics. As it turns out, Watson IoT also supports an <mark>HTTP interface</mark> to publish device events.</p>\n<pre><code>func publish(reading:Reading) {\n    \n  // Authentication\n  let user_pass = &quot;\\(self.username):\\(self.password)&quot;.data(using: .utf8)\n  let encoded = user_pass!.base64EncodedString(options: Data.Base64EncodingOptions.init(rawValue: 0))\n  let authorization = &quot;Basic \\(encoded)&quot;\n    \n  // Build request\n  let url = URL(string:\n    &quot;https://\\(organization).\\(host):\\(port)&quot; +\n    &quot;/api/v0002/application/&quot; +\n    &quot;types/\\(device_type)&quot; +\n    &quot;/devices/\\(device_id)&quot; +\n    &quot;/events/\\(event)&quot;\n  )\n  var request = URLRequest(url: url!)\n  request.httpMethod = &quot;POST&quot;\n  request.addValue(&quot;application/json&quot;, forHTTPHeaderField: &quot;Content-Type&quot;)\n  request.addValue(authorization, forHTTPHeaderField: &quot;Authorization&quot;)\n  request.httpBody = reading.json();\n    \n  // Make request\n  // Handle response\n  URLSession.shared.dataTask(with: request) { (data, response, error) in\n    if error != nil {\n      debugPrint(error!)\n    }\n  }.resume()\n}\n</code></pre>\n<p>This is almost identical to the Cloudant example, just with a few nuances in terminology.</p>\n<ul>\n<li>Within Watson IoT, you device categories of devices that will be connecting - this is the &quot;<mark>device type</mark>&quot;.</li>\n<li>Each device in that category gets its own ID (as well as security token) - this is the &quot;<mark>device ID</mark>&quot;.</li>\n<li>A device can trigger an &quot;<mark>event</mark>&quot; when it has something to report. This can be named whatever you want, and generally incorporated into the MQTT <mark>topic</mark> string.</li>\n<li>The &quot;<mark>username</mark>&quot; and &quot;<mark>password</mark>&quot; here are keys generated for applications to access Watson IoT as though they were devices.</li>\n</ul>\n<p>Knowing the terminology, and where to put them, makes all the difference in using Watson IoT.</p>\n<h4 id=\"nextsteps\">Next Steps</h4>\n<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/keFmb_ZXma0\" frameborder=\"0\" allowfullscreen></iframe>\n<p>There are a lot of other services on IBM Cloud that support an HTTP interface, and that could be useful in this scenario. One example that comes to mind, and that is growing in relevance for IoT architectures, is <a href=\"https://kafka.apache.org/\">Kafka</a>. Kafka on IBM Cloud is called &quot;<a href=\"http://www-03.ibm.com/software/products/en/ibm-message-hub\">Message Hub</a>&quot;. I think that would make an interesting next step.</p>\n<p>Connecting to Watson IoT over MQTT is possible of course, so I should probably update the application to provide for programmatic channel selection. If you want to see the code in its entirety, or just grab snippets for your own application, check out the <a href=\"https://github.com/krhoyt/IBM/tree/master/iot/bluetooth/cloud\">GitHub repository</a> for this project.</p>\n<!--kg-card-end: markdown-->","comment_id":"83","plaintext":"The LightBlue Bean [https://punchthrough.com/bean], by Punch Through, is one of\nmy favorite IoT development platforms. The Bean has many parts, including an iOS\napplication called the LightBlue Explorer\n[https://itunes.apple.com/us/app/lightblue-explorer-bluetooth-low-energy/id557428110?mt=8]\n, that lets you view and control nearby Beans. The gang over at Punch Through\nrecently added the ability for the Explorer application to send Bean data to \ncloud services\n[https://punchthrough.com/blog/posts/introducing-cloud-connect-for-lightblue-explorer]\n. Unfortunately, IBM was not included, so what follows is how to do Bean to IBM\nCloud connectivity via iOS.\n\nBean\nThe Bean is a BLE [https://en.wikipedia.org/wiki/Bluetooth_Low_Energy] \ndevelopment platform. As such it sports a BLE stack. You program the Bean via an \nArduino [https://www.arduino.cc/]-compatible workflow. Various features of the\nBLE stack are exposed to your Arduino program via an API\n[https://punchthrough.com/bean/reference]. As the Bean also has an\naccelerometer, temperature sensor, and RGB LED, the API also exposes those\ndetails to your Arduino program.\n\n// Reporting rate\nunsigned long last = 0;\nint rate = 1000;\n\n// Setup\nvoid setup() {\n  // Make sure LED is off\n  // Save battery\n  Bean.setLed( 0, 0, 0 );\n\n  // Deep sleep until needed\n  Bean.enableWakeOnConnect( true );\n}\n\n\nAs we look at the \"setup\" function, we can see that there really is not much\ngoing on. I use the Bean API to turn off the LED. Then I tell the Bean wake up\nwhen a device is connected.\n\nWe will be reporting data on a regular basis. I do not want to put the Bean to\nsleep during that, nor do I want the Arduino to block execution of BLE\ncommunication, so I am using the tried and true method of counting ticks from\nthe clock.\n\n// Loop\nvoid loop() {\n  unsigned long now = 0;\n  uint8_t scratch[20];\n  \n  // Connected to a device\n  if( Bean.getConnectionState() ) {\n    // Show connection\n    Bean.setLed( 0, 255, 0 );    \n    \n    // Get clock\n    now = millis();\n    \n    // Check timer\n    if( ( now - last ) >= rate ) {\n      // Update clock\n      last = now;\n      \n      // Set sensor values\n      output();      \n    }\n  } else {\n    // Nobody connected\n    // Turn off LED\n    Bean.setLed( 0, 0, 0 );    \n    \n    // Deep sleep\n    Bean.sleep( 0xFFFFFFFF );   \n  }\n}\n\n\nAs the Bean enters the \"loop\" function, a call to \"Bean.getConnectionState()\"\nreveals that no devices are currently connected, so \"Bean.sleep( 0xFFFFFFFF )\"\nis called to power down the ATmega until a device connects. The \"Bean.sleep()\"\ncall takes a number of milliseconds, but if you pass \"0xFFFFFFFF\" the Bean will \nsleep indefinitely - in this case, until a device connects.\n\nOnce a device, like the iPhone, connects, the Bean springs to life. To show\nconnectivity, I set the on-board RGB LED to green. Next, we start watching ticks\nfrom the clock to report sensor data at a one second interval (1000 ms). Then we\ncall an \"output()\" function to actually report the data.\n\n// Sensor values on characteristic\nvoid output() {\n  AccelerationReading acceleration;\n  char content[20];\n  uint8_t scratch[20];\n\n  // Read accelerometer\n  acceleration = Bean.getAcceleration();\n\n  // Format values as string\n  sprintf(\n    content,\n    \"%d,%d,%d,%d\",\n    acceleration.xAxis,\n    acceleration.yAxis,\n    acceleration.zAxis,\t\t\n    Bean.getTemperature()\n  );\n\n  // Put string into scratch format\n  for( int i = 0; i < strlen( content ); i++ ) {\n    scratch[i] = content[i];\n  }\n\n  // Set scratch data for various sensors\n  Bean.setScratchData( 1, scratch, strlen( content ) );\n}\n\n\nThe Bean API exposes \"scratch\" data areas - also known as characteristic data.\nYou can put a finite amount of data into these areas - 20 whole bytes. In this\ncase, that is enough room. The side effect is that the connected device (iPhone\nfor example) can ask to be notified when scratch data changes.\n\nThere are a few different ways to shove four numbers into bytes, but I like\n\"sprintf()\" to make a CSV string. That makes it easy to deal when it gets to the\ndevice. There are multiple scratch data areas on the Bean, but it all fits in\none, so one will do here.\n\n> Architecturally, I suppose putting each of the four values we want to have\nreported to the connected device, into their own scratch areas would be more\nsound. But that makes for more code on the device, and as a lazy programmer, why\ndo four times, what you could do once?\n\n\nThat is all the code for the Bean. From here, we power up the Bean, connect to\nit using the \"Bean Loader\n[https://punchthrough.com/bean/docs/guides/getting-started/os-x/]\" tool, compile\nthe Arduino code, and upload it to the Bean. Once there, the Bean will go to\nsleep, and wait for a device to connect.\n\niOS (Swift 3)\nThe semantics of connecting a BLE device to iOS is easily, far and away, the\nsingle most popular post\n[http://www.kevinhoyt.com/2016/05/20/the-12-steps-of-bluetooth-swift/] on my\nblog. There are a lot of little nuances to BLE, and it does not help that Apple\nkeeps changing the CoreBluetooth interface. I will not repeat all of that\ninformation here, just the pertinent bits.\n\n// Data arrived from peripheral\nfunc peripheral(\n  _ peripheral: CBPeripheral, \n  didUpdateValueFor characteristic: \n  CBCharacteristic, error: Error?) {\n    \n  // Make sure it is the characteristic we want\n  if characteristic.uuid == BEAN_SCRATCH_UUID {\n    // Get bytes into string\n    let content = String(\n      data: characteristic.value!, \n      encoding: String.Encoding.utf8\n    )\n      \n    // Formalize\n    let reading = Reading(raw: content!)\n\n    // Delegate\n    delegate?.didGet(reading: reading)\n  }\n}\n\n\nWhen the Arduino code on the Bean updates the specified characteristic data\n(scratch), the iPhone is notified. The data comes across as bytes, which I\nformat back into the CSV string that was sent over. Then I pass the CSV string\nonto a value object where it is parsed, and various convenience methods are\nexposed (temperature in fahrenheit or celcius as an example).\n\n> To keep the nightmare of interfaces that need to be implemented for\n\"CBCentralManagerDelegate\" and \"CBPeripheralDelegate\" out of my view controller,\nI have wrapped all the pertinent BLE bits into a class all their own, with a\nsingle, concise delegate method.\n\n\nCloudant\nCloudant [http://cloudant.com], CouchDB [http://couchdb.apache.org/] on IBM\nCloud, is a document store. It conveniently aligns CRUD operations with HTTP\nverbs. This means that there is no \"driver\" or special protocol that your\napplication needs to know. If you are working on a device that supports HTTP,\nthen you can store data in the cloud. iOS speaks HTTP.\n\nfunc save(reading:Reading) {\n    \n  // Authentication\n  let user_pass = \"\\(self.key):\\(self.password)\".data(using: .utf8)\n  let encoded = user_pass!.base64EncodedString(options: Data.Base64EncodingOptions.init(rawValue: 0))\n  let authorization = \"Basic \\(encoded)\"\n\n  // Build request\n  let url = URL(string: \"https://\\(account).cloudant.com/\\(database)\")\n  var request = URLRequest(url: url!)\n  request.httpMethod = \"POST\"\n  request.addValue(\"application/json\", forHTTPHeaderField: \"Content-Type\")\n  request.addValue(authorization, forHTTPHeaderField: \"Authorization\")\n  request.httpBody = reading.json();\n    \n  // Make request\n  // Handle response\n  URLSession.shared.dataTask(with: request) { (data, response, error) in\n    if error != nil {\n      debugPrint(error!)\n    } else {\n      do {\n        let json = try JSONSerialization.jsonObject(with: data!) as! [String:Any]\n      } catch {\n        debugPrint(error)\n      }\n    }\n  }.resume()\n}\n\n\nMaking an HTTP request to Cloudant using Swift take a few steps. The first step\nis to Base64 encode the username and password pair, and to send them as \"Basic\nAuth\". Creating a document is a POST operation, and we will be sending the\ndocument to store as a JSON string. Then we use URLSession to fire off the\nrequest. The response is a pretty slender JSON string that includes the \"id\" of\nthe created document.\n\n> Cloudant has a \"Permissions\" feature that allows you to generate additional\nkey/token combinations for API access with various rights. Do not send your\nCloudant administrator username/password.\n\n\nWait. What? Just an HTTP request to work with the database? Yup! that was easy,\nright? Perhaps we should go for a bonus round!\n\nWatson IoT\nAnother common endpoint for IoT data on IBM Cloud is Watson IoT\n[https://www.ibm.com/internet-of-things/platform/watson-iot-platform/]. At its\ncore, Watson IoT is an MQTT broker (MQTT being created, and largely maintained\n[http://www.eclipse.org/paho/], by IBM). MQTT is now a nearly de facto standard \nfor IoT devices. This particularly application however, is just publishing data.\nWe do not really need to subscribe to any topics. As it turns out, Watson IoT\nalso supports an HTTP interface to publish device events.\n\nfunc publish(reading:Reading) {\n    \n  // Authentication\n  let user_pass = \"\\(self.username):\\(self.password)\".data(using: .utf8)\n  let encoded = user_pass!.base64EncodedString(options: Data.Base64EncodingOptions.init(rawValue: 0))\n  let authorization = \"Basic \\(encoded)\"\n    \n  // Build request\n  let url = URL(string:\n    \"https://\\(organization).\\(host):\\(port)\" +\n    \"/api/v0002/application/\" +\n    \"types/\\(device_type)\" +\n    \"/devices/\\(device_id)\" +\n    \"/events/\\(event)\"\n  )\n  var request = URLRequest(url: url!)\n  request.httpMethod = \"POST\"\n  request.addValue(\"application/json\", forHTTPHeaderField: \"Content-Type\")\n  request.addValue(authorization, forHTTPHeaderField: \"Authorization\")\n  request.httpBody = reading.json();\n    \n  // Make request\n  // Handle response\n  URLSession.shared.dataTask(with: request) { (data, response, error) in\n    if error != nil {\n      debugPrint(error!)\n    }\n  }.resume()\n}\n\n\nThis is almost identical to the Cloudant example, just with a few nuances in\nterminology.\n\n * Within Watson IoT, you device categories of devices that will be connecting -\n   this is the \"device type\".\n * Each device in that category gets its own ID (as well as security token) -\n   this is the \"device ID\".\n * A device can trigger an \"event\" when it has something to report. This can be\n   named whatever you want, and generally incorporated into the MQTT topic \n   string.\n * The \"username\" and \"password\" here are keys generated for applications to\n   access Watson IoT as though they were devices.\n\nKnowing the terminology, and where to put them, makes all the difference in\nusing Watson IoT.\n\nNext Steps\nThere are a lot of other services on IBM Cloud that support an HTTP interface,\nand that could be useful in this scenario. One example that comes to mind, and\nthat is growing in relevance for IoT architectures, is Kafka\n[https://kafka.apache.org/]. Kafka on IBM Cloud is called \"Message Hub\n[http://www-03.ibm.com/software/products/en/ibm-message-hub]\". I think that\nwould make an interesting next step.\n\nConnecting to Watson IoT over MQTT is possible of course, so I should probably\nupdate the application to provide for programmatic channel selection. If you\nwant to see the code in its entirety, or just grab snippets for your own\napplication, check out the GitHub repository\n[https://github.com/krhoyt/IBM/tree/master/iot/bluetooth/cloud] for this\nproject.","feature_image":"__GHOST_URL__/content/images/2019/01/blue.teeth.jpg","featured":0,"status":"published","locale":null,"visibility":"public","author_id":"1","created_at":"2017-10-09T17:13:44.000Z","updated_at":"2019-01-17T00:33:45.000Z","published_at":"2017-10-13T15:07:05.000Z","custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"type":"post","email_recipient_filter":"none"},{"id":"5adb8922351ffe0018a5775b","uuid":"9c22e90e-3ec6-4b05-9c13-e83df5685055","title":"Tips for IoT in the Cloud(ant)","slug":"tip-for-iot-in-the-cloud-ant","mobiledoc":"{\"version\":\"0.3.1\",\"markups\":[],\"atoms\":[],\"cards\":[[\"markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\"Most IoT platforms support HTTP. Cloudant (IBMs hosted CouchDB environment) is a data store that speaks HTTP. It is in fact the only way to interact with the data store. At first glance, this makes Cloudant/CouchDB an easy choice for IoT applications. But how do you manage IoT-scale data in a document data store? Here are some tips I have learned along the way.\\n\\n####The Document Problem\\n\\nIt makes sense that there are so many document storage options from which to choose, given that we view so many documents every day. A social media post. A news article. A screen of an application. \\n\\nAs a guide, a document should contain the data to present a discreet view. If you have a view that lets you see the current status of an IoT sensor, that document will likely have properties along the lines of device, current value, date updated, etc. \\n\\nWhat do you do if that view also contains a historical chart of the sensor readings?\\n\\nYou might consider adding an array property to the document containing a history of past readings. If you are keeping a small number of historical readings, say one per day, then this might work out. Though bear in mind that five (5) years of daily readings is 1,825 data points. That is a lot to load for a single view. What if the chart in that view is only weekly (0.004% of the data), monthly (0.02%), or even yearly (still, only 20%)? \\n\\nDo you really want to load all 1,825 data points if you just want 31 of them?\\n\\nOf course, many IoT sensors update far more frequently. Every minute gives you 1,440 readings per day. Every second gives you 86,400 readings per day. The cost (data, time) to load that view is going to become immense by the time the same five years pass. It goes without saying that you cannot easily view that many data points on a single screen in most cases - certainly not a mobile device.\\n\\n####Tip #1 - One Document Per Reading\\n\\nIf you are coming from using document data stores to populate web pages in a blog, news web site, etc. it is easy to make the mistake of putting the view of the screen first. The best way to scale, and manage access to, your IoT sensor data however is to create a document for each reading. This is still a document per view, but it is the view of the sensor at a moment in time, not the view of the mobile application presenting that data.\\n\\n>Note that I am glossing over many hardware considerations to focus on data storage/access in this post. The hardware aspect is another topic entirely.\\n\\nThe \\\"one document per reading\\\" approach also saves you from write collisions. Let us say for example that the screen view of the data contains multiple sensors. If we put the screen view of the data first, we might then have multiple array properties - one for each sensor we want to display.\\n\\nIn Cloudant/CouchDB, you have to read (at least part of) a document in order to update it. If two sensors read the same original document, and then try to update it, one of them is going to fail because the revision string will be out of order. When you put the view of the IoT device first, you avoid this problem entirely.\\n\\n####Tip #2 - Design Documents are Your Friend\\n\\nOften an IoT device will have more than one sensor - or at least more than one value to present. In the case of an environmental sensor, this could be as simple as temperature and battery level. More realistically however, you will also have humidity, light, CO2, air particulates, and other data points.\\n\\nPutting all of these values in a single document that represents the view of the IoT device is still great for storage. It is the function of design documents to take that bulk data and place it into a screen-oriented view. I will generally have a design view for the sensor on the whole, but also for each of the individual properties. As an example, this gives me easy access to a history of just temperature readings.\\n\\n```\\nfunction( doc ) {\\n  if( doc.type && doc.type == 'sensor' ) {\\n    emit( doc.device, doc.temperature );\\n  }\\n}\\n```\\n\\n####Tip #3 - Emit Arrays for Querying\\n\\nAs previously mentioned, a sensor that updates once per second will give you 86,400 documents per day - per sensor. How do you query the documents (readings) for 9 AM - 10 AM for a given sensor? The answer here is to lean into the features that the \\\"emit( key, value )\\\" function provides.\\n\\nThe first parameter that gets passed to the \\\"emit()\\\" function is a \\\"key\\\" value. While you will often see this as a single value, you can also pass an array of values.\\n\\n```\\nfunction( doc ) {\\n  if( doc.type && doc.type == 'sensor' ) {\\n    var created = new Date( doc.created_at );\\n  \\n    emit( [\\n      doc.device,\\n      created.getFullYear(),\\n      created.getMonth(),\\n      created.getDate(),\\n      created.getHours(),\\n      created.getMinutes()\\n    ], doc.temperature );\\n  }\\n}\\n```\\n\\nIn the above example, I parse a millisecond since the epoch timestamp, and break down each date part as an individual element of an array provided as the key (along with the device identifier). For the \\\"value\\\" parameter, I pass the recorded temperature value. This allows me to query readings for a specific sensor, in a given date range.\\n\\n```\\nhttps://krhoyt.cloudant.com/mydb/_design/environment/_view/readings?startkey=[\\\"device_1\\\",2017,10,10,9,0]&endkey=[\\\"device_1\\\",2017,10,10,9,59]\\n```\\n\\nUsing the above HTTP GET, I can access all the temperature readings for \\\"device_1\\\" between 9 AM and 10 AM. Each element of the array effectively becomes a query parameter - the device ID, year, month, date, hour, and minute. The results will even tell you where you are in the entire dataset, which can be useful for paging in the screen view.\\n\\nThe \\\"value\\\" of the \\\"emit()\\\" function can be a JavaScript object as well. We could emit temperature and humidity as an example. One of the way I like to use this is to provide a celcius and fahrenheit value rather than just the temperature in a specific unit of measure (internationalization). \\n\\n>Depending on your needs, you might defer this calculation to the client; trading a few bytes on the wire for CPU cycles on the client.\\n\\nBy leaning into design documents to define our views, we have a robust means of getting just the specific data we are interested in viewing - in this case, getting 3,600 readings (one hour of once per second readings) from the 86,400 that make up a day (0.04% of the readings for the day).\\n\\n####Tip #4 - Flip the Design\\n\\nI have used dates in the previous design document, because that is a common need - especially for charting. What if you wanted to see sensors reporting values outside of a given range? For this, you could flip the design. Instead of emitting an array of date values, emit an array of reading values.\\n\\n```\\nfunction( doc ) {\\n  if( doc.type && doc.type == 'sensor' ) {\\n    emit( [\\n      doc.device,\\n      doc.temperature,\\n      doc.humidity,\\n      doc.light\\n    ], doc.created );\\n  }\\n}\\n```\\n\\nUsing this approach you can query all the readings and to find temperatures in a given range. Or even temperatures in a given range, when humidity is in another given range. Having more design documents views may seem counterintuitive for somebody coming from a relational database. The result however is a powerful means to view your data however you need it, for whatever purpose.\\n\\n####Tip #5 - Indexes Y'All\\n\\nMaybe the \\\"startkey\\\" and \\\"endkey\\\" formation on the URL gives you the heebie jeebies. Maybe you prefer to POST your search criteria similar to how you might submit a SQL statement over a given database driver. Cloudant/CouchDB provides a functional equivalent of design document view called, indexes.\\n\\n```\\n{\\n  \\\"_id\\\": \\\"_design/by_sensor_in_range\\\",\\n  \\\"_rev\\\": \\\"3-8467bf107ce1d12950f3e1517a93f974\\\",\\n  \\\"language\\\": \\\"query\\\",\\n  \\\"views\\\": {\\n    \\\"by_sensor_in_range\\\": {\\n      \\\"map\\\": {\\n        \\\"fields\\\": {\\n          \\\"created_at\\\": \\\"asc\\\"\\n        }\\n      },\\n      \\\"reduce\\\": \\\"_count\\\",\\n      \\\"options\\\": {\\n        \\\"def\\\": {\\n          \\\"fields\\\": [\\n            \\\"created_at\\\"\\n          ]\\n        }\\n      }\\n    }\\n  }\\n}\\n```\\n\\nBack to querying date ranges, this design document allows me to POST a query to Cloudant/CouchDB. The results are the documents that meet the query criteria.\\n\\n```\\n{\\n  \\\"fields\\\": [\\n    \\\"_id\\\",\\n    \\\"_rev\\\",\\n    \\\"device\\\",\\n    \\\"created_at\\\",\\n    \\\"temperature\\\",\\n    \\\"humidity\\\",\\n    \\\"light\\\"\\n  ],\\n  \\\"sort\\\": [\\n    {\\n      \\\"created_at\\\": \\\"asc\\\"\\n    }\\n  ],\\n  \\\"selector\\\": {\\n    \\\"created_at\\\": {\\n      \\\"$gt\\\": 1507615200000,\\n      \\\"$lt\\\": 1507682065701\\n    },\\n    \\\"device\\\": \\\"device_1\\\"\\n  }\\n}\\n```\\n\\nIn this query, I am telling Cloudant/CouchDB to get me a specific set of fields from the resulting documents, perform an ascending sort on the index (date), between a given date range (in milliseconds), for a specific device. That is a whole lot of granularity, made easily accessible. The documentation for indexes covers the other criteria you can use to query the store (and there are a lot of them).\\n\\nLet me translate that for you relational folks:\\n\\n```\\nSELECT device, created_at, temperature, humidity, light\\nFROM  readings\\nWHERE created_at > 1507615200000\\nAND   created_at < 1507682065701\\nAND   device = 'device_1'\\n```\\n\\n####Tip #6 - Reduce the Noise\\n\\nTBD\\n\\n####Tip #7 - Use the Update Document\\n\\nOften times the document you want to store for an individual sensor reading has properties that may differ from the data that is presented from the IoT device. \\n\\nFor example, when using the Particle WebHook integration, you are going to get a JSON document with a \\\"coreid\\\" property. That property represents the unique device identifier. That is an essential piece of information to have, but I would rather it be reflected as \\\"device\\\" in my documents.\\n\\nThe data that I send in a Particle Function call is often in CSV format. This saves me he headache of trying to manage JSON on the MCU. Because \\\"sprintf()\\\" on Particle devices can be a struggle  with float values (read: not supported), I will also often multiply floats by 100 to get an integer value. I then in turn need to do the division before putting the document for the reading into the store.\\n\\n```\\n{\\n  \\\"coreid\\\": \\\"1234567890\\\",\\n  \\\"name\\\": \\\"environment\\\",\\n  \\\"data\\\": \\\"12,34,56\\\",\\n  \\\"published_at\\\": \\\"2017-10-12T22:47:59Z\\\"\\n}\\n```\\n\\nThis is an example of what I might get out of a Particle Function call with a WebHook. You might think that you will have to send this to a server (Node.js, Java, etc.) for further processing. Cloudant/CouchDB however has a special type of design document called the \\\"update\\\" document.\\n\\n```\\n{\\n  \\\"_id\\\": \\\"_design/create_sensor_reading\\\",\\n  \\\"_rev\\\": \\\"23-cffe42b0a0cb75fda6edbf4744c1e946\\\",\\n  \\\"updates\\\": {\\n    \\\"create_sensor_reading\\\": \\\"function( doc, req ) { ... }\\n  }\\n}\\n```\\n\\nIf you POST (create) to the update document, you can take additional processing actions before actually saving the reading document to the data store. The function you provide takes a document to update (null in the case of POST for document creation), and details about the HTTP request. The request will contain the body of the POST along with other details (such as a convenient UUID for your new document).\\n\\nNow I can customize how I want the reading document to actually be stored.\\n\\n```\\nfunction( doc, req ) { \\n  var data = JSON.parse( req.body );\\n  var parts = data.data.split( ',' );\\n\\t\\n  doc = {\\n    _id: req.uuid,\\n    device: data.coreid,\\n    type: 'sensor',\\n    event: data.name,\\n    raw: data.data,\\n    temperature: parseFloat( parts[0] ) / 100,\\n    humidity: parseFloat( parts[1] ),\\n    light: parseFloat( parts[2] ),\\n    created_at: Date.parse( data.published_at )\\n  };\\t\\n\\n  return [doc, JSON.stringify( doc )];\\n}\\n```\\n\\nUsing this approach I parse the JSON data from the request body, and then make my own document to be stored. I can do the division to get the float back out of the integer, parse an ISO-8601 date string into a timestamp, and change the \\\"coreid\\\" to \\\"device\\\" as per my liking.\\n\\nThe return of this function is an array with two elements. The first is the document to be saved, and the second is the body of the HTTP response. I like to turn my document back into a JSON string for the response for the purposes of debugging, but you can return any value you would like.\\n\\n####Tip #8 - Get Permission\\n\\nWhen you create a Cloudant account, wether via the IBM Cloud console, or directly with the Cloudant service, you will have an account username and password. You can use these credentials to access every database and document within your account - including deletion, or even changing the access credentials. Do not put your account credentials on an IoT device.\\n\\n![Permissions view in Cloudant.](http://images.kevinhoyt.com/cloudant.permissions.png)\\n\\nEach database has its own permissions, which you can access via the \\\"Permissions\\\" menu option. Use this feature to create two API keys - one for read, and one for write. Put the write credentials on your IoT device, and put the read credentials into your application. Should your credentials become compromised, you can disable that key with one click.\\n\\nSome devices and applications require both read and write permissions. Depending on how your architecture, you might choose to deploy both a read and a write credential, and have the device/application use the appropriate key for the appropriate operation. This can create challenges architecturally, and can potentially increase your attack surface. \\n\\nIn these situations, I generally create an API key with read/write for the given database. Disabling the credentials is still a click away, damage will be limited to a single database, and your account credentials are still protected.\\n\\n####Next Steps\\n\\nGetting your head around storing IoT data using documents can take time and practice. Hopefully these five tips will put you in a good position to hit the ground running. Keep in mind that these are \\\"tips\\\" not \\\"rules\\\". Your reporting needs may vary, but I am confident that Cloudant/CouchDB can make an ideal storage solution for your IoT needs.\\n\\nOne interesting aspect of all this is that I do not need any special database drivers. As I mentioned earlier, everything about Cloudant/CouchDB is HTTP-based. This means that I no point have I needed to rely on running a Node.js (or Java, or whatever) server. In fact, as I once heard somebody so eloquently put it \\\"the database is the server!\\\"\\n\\nIf you run into a situation where you do need additional server processing, you should definitely check out the emerging serverless offerings such as IBM Cloud Functions. Serverless, or functions as a service (FaaS), give you server processing when you need it, but you only pay for the time your function is actually running (versus paying for a cluster of servers running 24 x 7).\\n\\nWhen paired, these two solutions (IBM Cloud Functions, Cloudant) give you immensely powerful infrastructure, at an extremely low cost of entry for even the most demanding IoT needs.\"}]],\"sections\":[[10,0]],\"ghostVersion\":\"3.0\"}","html":"<!--kg-card-begin: markdown--><p>Most IoT platforms support HTTP. Cloudant (IBMs hosted CouchDB environment) is a data store that speaks HTTP. It is in fact the only way to interact with the data store. At first glance, this makes Cloudant/CouchDB an easy choice for IoT applications. But how do you manage IoT-scale data in a document data store? Here are some tips I have learned along the way.</p>\n<h4 id=\"thedocumentproblem\">The Document Problem</h4>\n<p>It makes sense that there are so many document storage options from which to choose, given that we view so many documents every day. A social media post. A news article. A screen of an application.</p>\n<p>As a guide, a document should contain the data to present a discreet view. If you have a view that lets you see the current status of an IoT sensor, that document will likely have properties along the lines of device, current value, date updated, etc.</p>\n<p>What do you do if that view also contains a historical chart of the sensor readings?</p>\n<p>You might consider adding an array property to the document containing a history of past readings. If you are keeping a small number of historical readings, say one per day, then this might work out. Though bear in mind that five (5) years of daily readings is 1,825 data points. That is a lot to load for a single view. What if the chart in that view is only weekly (0.004% of the data), monthly (0.02%), or even yearly (still, only 20%)?</p>\n<p>Do you really want to load all 1,825 data points if you just want 31 of them?</p>\n<p>Of course, many IoT sensors update far more frequently. Every minute gives you 1,440 readings per day. Every second gives you 86,400 readings per day. The cost (data, time) to load that view is going to become immense by the time the same five years pass. It goes without saying that you cannot easily view that many data points on a single screen in most cases - certainly not a mobile device.</p>\n<h4 id=\"tip1onedocumentperreading\">Tip #1 - One Document Per Reading</h4>\n<p>If you are coming from using document data stores to populate web pages in a blog, news web site, etc. it is easy to make the mistake of putting the view of the screen first. The best way to scale, and manage access to, your IoT sensor data however is to create a document for each reading. This is still a document per view, but it is the view of the sensor at a moment in time, not the view of the mobile application presenting that data.</p>\n<blockquote>\n<p>Note that I am glossing over many hardware considerations to focus on data storage/access in this post. The hardware aspect is another topic entirely.</p>\n</blockquote>\n<p>The &quot;one document per reading&quot; approach also saves you from write collisions. Let us say for example that the screen view of the data contains multiple sensors. If we put the screen view of the data first, we might then have multiple array properties - one for each sensor we want to display.</p>\n<p>In Cloudant/CouchDB, you have to read (at least part of) a document in order to update it. If two sensors read the same original document, and then try to update it, one of them is going to fail because the revision string will be out of order. When you put the view of the IoT device first, you avoid this problem entirely.</p>\n<h4 id=\"tip2designdocumentsareyourfriend\">Tip #2 - Design Documents are Your Friend</h4>\n<p>Often an IoT device will have more than one sensor - or at least more than one value to present. In the case of an environmental sensor, this could be as simple as temperature and battery level. More realistically however, you will also have humidity, light, CO2, air particulates, and other data points.</p>\n<p>Putting all of these values in a single document that represents the view of the IoT device is still great for storage. It is the function of design documents to take that bulk data and place it into a screen-oriented view. I will generally have a design view for the sensor on the whole, but also for each of the individual properties. As an example, this gives me easy access to a history of just temperature readings.</p>\n<pre><code>function( doc ) {\n  if( doc.type &amp;&amp; doc.type == 'sensor' ) {\n    emit( doc.device, doc.temperature );\n  }\n}\n</code></pre>\n<h4 id=\"tip3emitarraysforquerying\">Tip #3 - Emit Arrays for Querying</h4>\n<p>As previously mentioned, a sensor that updates once per second will give you 86,400 documents per day - per sensor. How do you query the documents (readings) for 9 AM - 10 AM for a given sensor? The answer here is to lean into the features that the &quot;emit( key, value )&quot; function provides.</p>\n<p>The first parameter that gets passed to the &quot;emit()&quot; function is a &quot;key&quot; value. While you will often see this as a single value, you can also pass an array of values.</p>\n<pre><code>function( doc ) {\n  if( doc.type &amp;&amp; doc.type == 'sensor' ) {\n    var created = new Date( doc.created_at );\n  \n    emit( [\n      doc.device,\n      created.getFullYear(),\n      created.getMonth(),\n      created.getDate(),\n      created.getHours(),\n      created.getMinutes()\n    ], doc.temperature );\n  }\n}\n</code></pre>\n<p>In the above example, I parse a millisecond since the epoch timestamp, and break down each date part as an individual element of an array provided as the key (along with the device identifier). For the &quot;value&quot; parameter, I pass the recorded temperature value. This allows me to query readings for a specific sensor, in a given date range.</p>\n<pre><code>https://krhoyt.cloudant.com/mydb/_design/environment/_view/readings?startkey=[&quot;device_1&quot;,2017,10,10,9,0]&amp;endkey=[&quot;device_1&quot;,2017,10,10,9,59]\n</code></pre>\n<p>Using the above HTTP GET, I can access all the temperature readings for &quot;device_1&quot; between 9 AM and 10 AM. Each element of the array effectively becomes a query parameter - the device ID, year, month, date, hour, and minute. The results will even tell you where you are in the entire dataset, which can be useful for paging in the screen view.</p>\n<p>The &quot;value&quot; of the &quot;emit()&quot; function can be a JavaScript object as well. We could emit temperature and humidity as an example. One of the way I like to use this is to provide a celcius and fahrenheit value rather than just the temperature in a specific unit of measure (internationalization).</p>\n<blockquote>\n<p>Depending on your needs, you might defer this calculation to the client; trading a few bytes on the wire for CPU cycles on the client.</p>\n</blockquote>\n<p>By leaning into design documents to define our views, we have a robust means of getting just the specific data we are interested in viewing - in this case, getting 3,600 readings (one hour of once per second readings) from the 86,400 that make up a day (0.04% of the readings for the day).</p>\n<h4 id=\"tip4flipthedesign\">Tip #4 - Flip the Design</h4>\n<p>I have used dates in the previous design document, because that is a common need - especially for charting. What if you wanted to see sensors reporting values outside of a given range? For this, you could flip the design. Instead of emitting an array of date values, emit an array of reading values.</p>\n<pre><code>function( doc ) {\n  if( doc.type &amp;&amp; doc.type == 'sensor' ) {\n    emit( [\n      doc.device,\n      doc.temperature,\n      doc.humidity,\n      doc.light\n    ], doc.created );\n  }\n}\n</code></pre>\n<p>Using this approach you can query all the readings and to find temperatures in a given range. Or even temperatures in a given range, when humidity is in another given range. Having more design documents views may seem counterintuitive for somebody coming from a relational database. The result however is a powerful means to view your data however you need it, for whatever purpose.</p>\n<h4 id=\"tip5indexesyall\">Tip #5 - Indexes Y'All</h4>\n<p>Maybe the &quot;startkey&quot; and &quot;endkey&quot; formation on the URL gives you the heebie jeebies. Maybe you prefer to POST your search criteria similar to how you might submit a SQL statement over a given database driver. Cloudant/CouchDB provides a functional equivalent of design document view called, indexes.</p>\n<pre><code>{\n  &quot;_id&quot;: &quot;_design/by_sensor_in_range&quot;,\n  &quot;_rev&quot;: &quot;3-8467bf107ce1d12950f3e1517a93f974&quot;,\n  &quot;language&quot;: &quot;query&quot;,\n  &quot;views&quot;: {\n    &quot;by_sensor_in_range&quot;: {\n      &quot;map&quot;: {\n        &quot;fields&quot;: {\n          &quot;created_at&quot;: &quot;asc&quot;\n        }\n      },\n      &quot;reduce&quot;: &quot;_count&quot;,\n      &quot;options&quot;: {\n        &quot;def&quot;: {\n          &quot;fields&quot;: [\n            &quot;created_at&quot;\n          ]\n        }\n      }\n    }\n  }\n}\n</code></pre>\n<p>Back to querying date ranges, this design document allows me to POST a query to Cloudant/CouchDB. The results are the documents that meet the query criteria.</p>\n<pre><code>{\n  &quot;fields&quot;: [\n    &quot;_id&quot;,\n    &quot;_rev&quot;,\n    &quot;device&quot;,\n    &quot;created_at&quot;,\n    &quot;temperature&quot;,\n    &quot;humidity&quot;,\n    &quot;light&quot;\n  ],\n  &quot;sort&quot;: [\n    {\n      &quot;created_at&quot;: &quot;asc&quot;\n    }\n  ],\n  &quot;selector&quot;: {\n    &quot;created_at&quot;: {\n      &quot;$gt&quot;: 1507615200000,\n      &quot;$lt&quot;: 1507682065701\n    },\n    &quot;device&quot;: &quot;device_1&quot;\n  }\n}\n</code></pre>\n<p>In this query, I am telling Cloudant/CouchDB to get me a specific set of fields from the resulting documents, perform an ascending sort on the index (date), between a given date range (in milliseconds), for a specific device. That is a whole lot of granularity, made easily accessible. The documentation for indexes covers the other criteria you can use to query the store (and there are a lot of them).</p>\n<p>Let me translate that for you relational folks:</p>\n<pre><code>SELECT device, created_at, temperature, humidity, light\nFROM  readings\nWHERE created_at &gt; 1507615200000\nAND   created_at &lt; 1507682065701\nAND   device = 'device_1'\n</code></pre>\n<h4 id=\"tip6reducethenoise\">Tip #6 - Reduce the Noise</h4>\n<p>TBD</p>\n<h4 id=\"tip7usetheupdatedocument\">Tip #7 - Use the Update Document</h4>\n<p>Often times the document you want to store for an individual sensor reading has properties that may differ from the data that is presented from the IoT device.</p>\n<p>For example, when using the Particle WebHook integration, you are going to get a JSON document with a &quot;coreid&quot; property. That property represents the unique device identifier. That is an essential piece of information to have, but I would rather it be reflected as &quot;device&quot; in my documents.</p>\n<p>The data that I send in a Particle Function call is often in CSV format. This saves me he headache of trying to manage JSON on the MCU. Because &quot;sprintf()&quot; on Particle devices can be a struggle  with float values (read: not supported), I will also often multiply floats by 100 to get an integer value. I then in turn need to do the division before putting the document for the reading into the store.</p>\n<pre><code>{\n  &quot;coreid&quot;: &quot;1234567890&quot;,\n  &quot;name&quot;: &quot;environment&quot;,\n  &quot;data&quot;: &quot;12,34,56&quot;,\n  &quot;published_at&quot;: &quot;2017-10-12T22:47:59Z&quot;\n}\n</code></pre>\n<p>This is an example of what I might get out of a Particle Function call with a WebHook. You might think that you will have to send this to a server (Node.js, Java, etc.) for further processing. Cloudant/CouchDB however has a special type of design document called the &quot;update&quot; document.</p>\n<pre><code>{\n  &quot;_id&quot;: &quot;_design/create_sensor_reading&quot;,\n  &quot;_rev&quot;: &quot;23-cffe42b0a0cb75fda6edbf4744c1e946&quot;,\n  &quot;updates&quot;: {\n    &quot;create_sensor_reading&quot;: &quot;function( doc, req ) { ... }\n  }\n}\n</code></pre>\n<p>If you POST (create) to the update document, you can take additional processing actions before actually saving the reading document to the data store. The function you provide takes a document to update (null in the case of POST for document creation), and details about the HTTP request. The request will contain the body of the POST along with other details (such as a convenient UUID for your new document).</p>\n<p>Now I can customize how I want the reading document to actually be stored.</p>\n<pre><code>function( doc, req ) { \n  var data = JSON.parse( req.body );\n  var parts = data.data.split( ',' );\n\t\n  doc = {\n    _id: req.uuid,\n    device: data.coreid,\n    type: 'sensor',\n    event: data.name,\n    raw: data.data,\n    temperature: parseFloat( parts[0] ) / 100,\n    humidity: parseFloat( parts[1] ),\n    light: parseFloat( parts[2] ),\n    created_at: Date.parse( data.published_at )\n  };\t\n\n  return [doc, JSON.stringify( doc )];\n}\n</code></pre>\n<p>Using this approach I parse the JSON data from the request body, and then make my own document to be stored. I can do the division to get the float back out of the integer, parse an ISO-8601 date string into a timestamp, and change the &quot;coreid&quot; to &quot;device&quot; as per my liking.</p>\n<p>The return of this function is an array with two elements. The first is the document to be saved, and the second is the body of the HTTP response. I like to turn my document back into a JSON string for the response for the purposes of debugging, but you can return any value you would like.</p>\n<h4 id=\"tip8getpermission\">Tip #8 - Get Permission</h4>\n<p>When you create a Cloudant account, wether via the IBM Cloud console, or directly with the Cloudant service, you will have an account username and password. You can use these credentials to access every database and document within your account - including deletion, or even changing the access credentials. Do not put your account credentials on an IoT device.</p>\n<p><img src=\"http://images.kevinhoyt.com/cloudant.permissions.png\" alt=\"Permissions view in Cloudant.\" loading=\"lazy\"></p>\n<p>Each database has its own permissions, which you can access via the &quot;Permissions&quot; menu option. Use this feature to create two API keys - one for read, and one for write. Put the write credentials on your IoT device, and put the read credentials into your application. Should your credentials become compromised, you can disable that key with one click.</p>\n<p>Some devices and applications require both read and write permissions. Depending on how your architecture, you might choose to deploy both a read and a write credential, and have the device/application use the appropriate key for the appropriate operation. This can create challenges architecturally, and can potentially increase your attack surface.</p>\n<p>In these situations, I generally create an API key with read/write for the given database. Disabling the credentials is still a click away, damage will be limited to a single database, and your account credentials are still protected.</p>\n<h4 id=\"nextsteps\">Next Steps</h4>\n<p>Getting your head around storing IoT data using documents can take time and practice. Hopefully these five tips will put you in a good position to hit the ground running. Keep in mind that these are &quot;tips&quot; not &quot;rules&quot;. Your reporting needs may vary, but I am confident that Cloudant/CouchDB can make an ideal storage solution for your IoT needs.</p>\n<p>One interesting aspect of all this is that I do not need any special database drivers. As I mentioned earlier, everything about Cloudant/CouchDB is HTTP-based. This means that I no point have I needed to rely on running a Node.js (or Java, or whatever) server. In fact, as I once heard somebody so eloquently put it &quot;the database is the server!&quot;</p>\n<p>If you run into a situation where you do need additional server processing, you should definitely check out the emerging serverless offerings such as IBM Cloud Functions. Serverless, or functions as a service (FaaS), give you server processing when you need it, but you only pay for the time your function is actually running (versus paying for a cluster of servers running 24 x 7).</p>\n<p>When paired, these two solutions (IBM Cloud Functions, Cloudant) give you immensely powerful infrastructure, at an extremely low cost of entry for even the most demanding IoT needs.</p>\n<!--kg-card-end: markdown-->","comment_id":"84","plaintext":"Most IoT platforms support HTTP. Cloudant (IBMs hosted CouchDB environment) is a\ndata store that speaks HTTP. It is in fact the only way to interact with the\ndata store. At first glance, this makes Cloudant/CouchDB an easy choice for IoT\napplications. But how do you manage IoT-scale data in a document data store?\nHere are some tips I have learned along the way.\n\nThe Document Problem\nIt makes sense that there are so many document storage options from which to\nchoose, given that we view so many documents every day. A social media post. A\nnews article. A screen of an application.\n\nAs a guide, a document should contain the data to present a discreet view. If\nyou have a view that lets you see the current status of an IoT sensor, that\ndocument will likely have properties along the lines of device, current value,\ndate updated, etc.\n\nWhat do you do if that view also contains a historical chart of the sensor\nreadings?\n\nYou might consider adding an array property to the document containing a history\nof past readings. If you are keeping a small number of historical readings, say\none per day, then this might work out. Though bear in mind that five (5) years\nof daily readings is 1,825 data points. That is a lot to load for a single view.\nWhat if the chart in that view is only weekly (0.004% of the data), monthly\n(0.02%), or even yearly (still, only 20%)?\n\nDo you really want to load all 1,825 data points if you just want 31 of them?\n\nOf course, many IoT sensors update far more frequently. Every minute gives you\n1,440 readings per day. Every second gives you 86,400 readings per day. The cost\n(data, time) to load that view is going to become immense by the time the same\nfive years pass. It goes without saying that you cannot easily view that many\ndata points on a single screen in most cases - certainly not a mobile device.\n\nTip #1 - One Document Per Reading\nIf you are coming from using document data stores to populate web pages in a\nblog, news web site, etc. it is easy to make the mistake of putting the view of\nthe screen first. The best way to scale, and manage access to, your IoT sensor\ndata however is to create a document for each reading. This is still a document\nper view, but it is the view of the sensor at a moment in time, not the view of\nthe mobile application presenting that data.\n\n> Note that I am glossing over many hardware considerations to focus on data\nstorage/access in this post. The hardware aspect is another topic entirely.\n\n\nThe \"one document per reading\" approach also saves you from write collisions.\nLet us say for example that the screen view of the data contains multiple\nsensors. If we put the screen view of the data first, we might then have\nmultiple array properties - one for each sensor we want to display.\n\nIn Cloudant/CouchDB, you have to read (at least part of) a document in order to\nupdate it. If two sensors read the same original document, and then try to\nupdate it, one of them is going to fail because the revision string will be out\nof order. When you put the view of the IoT device first, you avoid this problem\nentirely.\n\nTip #2 - Design Documents are Your Friend\nOften an IoT device will have more than one sensor - or at least more than one\nvalue to present. In the case of an environmental sensor, this could be as\nsimple as temperature and battery level. More realistically however, you will\nalso have humidity, light, CO2, air particulates, and other data points.\n\nPutting all of these values in a single document that represents the view of the\nIoT device is still great for storage. It is the function of design documents to\ntake that bulk data and place it into a screen-oriented view. I will generally\nhave a design view for the sensor on the whole, but also for each of the\nindividual properties. As an example, this gives me easy access to a history of\njust temperature readings.\n\nfunction( doc ) {\n  if( doc.type && doc.type == 'sensor' ) {\n    emit( doc.device, doc.temperature );\n  }\n}\n\n\nTip #3 - Emit Arrays for Querying\nAs previously mentioned, a sensor that updates once per second will give you\n86,400 documents per day - per sensor. How do you query the documents (readings)\nfor 9 AM - 10 AM for a given sensor? The answer here is to lean into the\nfeatures that the \"emit( key, value )\" function provides.\n\nThe first parameter that gets passed to the \"emit()\" function is a \"key\" value.\nWhile you will often see this as a single value, you can also pass an array of\nvalues.\n\nfunction( doc ) {\n  if( doc.type && doc.type == 'sensor' ) {\n    var created = new Date( doc.created_at );\n  \n    emit( [\n      doc.device,\n      created.getFullYear(),\n      created.getMonth(),\n      created.getDate(),\n      created.getHours(),\n      created.getMinutes()\n    ], doc.temperature );\n  }\n}\n\n\nIn the above example, I parse a millisecond since the epoch timestamp, and break\ndown each date part as an individual element of an array provided as the key\n(along with the device identifier). For the \"value\" parameter, I pass the\nrecorded temperature value. This allows me to query readings for a specific\nsensor, in a given date range.\n\nhttps://krhoyt.cloudant.com/mydb/_design/environment/_view/readings?startkey=[\"device_1\",2017,10,10,9,0]&endkey=[\"device_1\",2017,10,10,9,59]\n\n\nUsing the above HTTP GET, I can access all the temperature readings for\n\"device_1\" between 9 AM and 10 AM. Each element of the array effectively becomes\na query parameter - the device ID, year, month, date, hour, and minute. The\nresults will even tell you where you are in the entire dataset, which can be\nuseful for paging in the screen view.\n\nThe \"value\" of the \"emit()\" function can be a JavaScript object as well. We\ncould emit temperature and humidity as an example. One of the way I like to use\nthis is to provide a celcius and fahrenheit value rather than just the\ntemperature in a specific unit of measure (internationalization).\n\n> Depending on your needs, you might defer this calculation to the client; trading\na few bytes on the wire for CPU cycles on the client.\n\n\nBy leaning into design documents to define our views, we have a robust means of\ngetting just the specific data we are interested in viewing - in this case,\ngetting 3,600 readings (one hour of once per second readings) from the 86,400\nthat make up a day (0.04% of the readings for the day).\n\nTip #4 - Flip the Design\nI have used dates in the previous design document, because that is a common need\n- especially for charting. What if you wanted to see sensors reporting values\noutside of a given range? For this, you could flip the design. Instead of\nemitting an array of date values, emit an array of reading values.\n\nfunction( doc ) {\n  if( doc.type && doc.type == 'sensor' ) {\n    emit( [\n      doc.device,\n      doc.temperature,\n      doc.humidity,\n      doc.light\n    ], doc.created );\n  }\n}\n\n\nUsing this approach you can query all the readings and to find temperatures in a\ngiven range. Or even temperatures in a given range, when humidity is in another\ngiven range. Having more design documents views may seem counterintuitive for\nsomebody coming from a relational database. The result however is a powerful\nmeans to view your data however you need it, for whatever purpose.\n\nTip #5 - Indexes Y'All\nMaybe the \"startkey\" and \"endkey\" formation on the URL gives you the heebie\njeebies. Maybe you prefer to POST your search criteria similar to how you might\nsubmit a SQL statement over a given database driver. Cloudant/CouchDB provides a\nfunctional equivalent of design document view called, indexes.\n\n{\n  \"_id\": \"_design/by_sensor_in_range\",\n  \"_rev\": \"3-8467bf107ce1d12950f3e1517a93f974\",\n  \"language\": \"query\",\n  \"views\": {\n    \"by_sensor_in_range\": {\n      \"map\": {\n        \"fields\": {\n          \"created_at\": \"asc\"\n        }\n      },\n      \"reduce\": \"_count\",\n      \"options\": {\n        \"def\": {\n          \"fields\": [\n            \"created_at\"\n          ]\n        }\n      }\n    }\n  }\n}\n\n\nBack to querying date ranges, this design document allows me to POST a query to\nCloudant/CouchDB. The results are the documents that meet the query criteria.\n\n{\n  \"fields\": [\n    \"_id\",\n    \"_rev\",\n    \"device\",\n    \"created_at\",\n    \"temperature\",\n    \"humidity\",\n    \"light\"\n  ],\n  \"sort\": [\n    {\n      \"created_at\": \"asc\"\n    }\n  ],\n  \"selector\": {\n    \"created_at\": {\n      \"$gt\": 1507615200000,\n      \"$lt\": 1507682065701\n    },\n    \"device\": \"device_1\"\n  }\n}\n\n\nIn this query, I am telling Cloudant/CouchDB to get me a specific set of fields\nfrom the resulting documents, perform an ascending sort on the index (date),\nbetween a given date range (in milliseconds), for a specific device. That is a\nwhole lot of granularity, made easily accessible. The documentation for indexes\ncovers the other criteria you can use to query the store (and there are a lot of\nthem).\n\nLet me translate that for you relational folks:\n\nSELECT device, created_at, temperature, humidity, light\nFROM  readings\nWHERE created_at > 1507615200000\nAND   created_at < 1507682065701\nAND   device = 'device_1'\n\n\nTip #6 - Reduce the Noise\nTBD\n\nTip #7 - Use the Update Document\nOften times the document you want to store for an individual sensor reading has\nproperties that may differ from the data that is presented from the IoT device.\n\nFor example, when using the Particle WebHook integration, you are going to get a\nJSON document with a \"coreid\" property. That property represents the unique\ndevice identifier. That is an essential piece of information to have, but I\nwould rather it be reflected as \"device\" in my documents.\n\nThe data that I send in a Particle Function call is often in CSV format. This\nsaves me he headache of trying to manage JSON on the MCU. Because \"sprintf()\" on\nParticle devices can be a struggle with float values (read: not supported), I\nwill also often multiply floats by 100 to get an integer value. I then in turn\nneed to do the division before putting the document for the reading into the\nstore.\n\n{\n  \"coreid\": \"1234567890\",\n  \"name\": \"environment\",\n  \"data\": \"12,34,56\",\n  \"published_at\": \"2017-10-12T22:47:59Z\"\n}\n\n\nThis is an example of what I might get out of a Particle Function call with a\nWebHook. You might think that you will have to send this to a server (Node.js,\nJava, etc.) for further processing. Cloudant/CouchDB however has a special type\nof design document called the \"update\" document.\n\n{\n  \"_id\": \"_design/create_sensor_reading\",\n  \"_rev\": \"23-cffe42b0a0cb75fda6edbf4744c1e946\",\n  \"updates\": {\n    \"create_sensor_reading\": \"function( doc, req ) { ... }\n  }\n}\n\n\nIf you POST (create) to the update document, you can take additional processing\nactions before actually saving the reading document to the data store. The\nfunction you provide takes a document to update (null in the case of POST for\ndocument creation), and details about the HTTP request. The request will contain\nthe body of the POST along with other details (such as a convenient UUID for\nyour new document).\n\nNow I can customize how I want the reading document to actually be stored.\n\nfunction( doc, req ) { \n  var data = JSON.parse( req.body );\n  var parts = data.data.split( ',' );\n\t\n  doc = {\n    _id: req.uuid,\n    device: data.coreid,\n    type: 'sensor',\n    event: data.name,\n    raw: data.data,\n    temperature: parseFloat( parts[0] ) / 100,\n    humidity: parseFloat( parts[1] ),\n    light: parseFloat( parts[2] ),\n    created_at: Date.parse( data.published_at )\n  };\t\n\n  return [doc, JSON.stringify( doc )];\n}\n\n\nUsing this approach I parse the JSON data from the request body, and then make\nmy own document to be stored. I can do the division to get the float back out of\nthe integer, parse an ISO-8601 date string into a timestamp, and change the\n\"coreid\" to \"device\" as per my liking.\n\nThe return of this function is an array with two elements. The first is the\ndocument to be saved, and the second is the body of the HTTP response. I like to\nturn my document back into a JSON string for the response for the purposes of\ndebugging, but you can return any value you would like.\n\nTip #8 - Get Permission\nWhen you create a Cloudant account, wether via the IBM Cloud console, or\ndirectly with the Cloudant service, you will have an account username and\npassword. You can use these credentials to access every database and document\nwithin your account - including deletion, or even changing the access\ncredentials. Do not put your account credentials on an IoT device.\n\n\n\nEach database has its own permissions, which you can access via the\n\"Permissions\" menu option. Use this feature to create two API keys - one for\nread, and one for write. Put the write credentials on your IoT device, and put\nthe read credentials into your application. Should your credentials become\ncompromised, you can disable that key with one click.\n\nSome devices and applications require both read and write permissions. Depending\non how your architecture, you might choose to deploy both a read and a write\ncredential, and have the device/application use the appropriate key for the\nappropriate operation. This can create challenges architecturally, and can\npotentially increase your attack surface.\n\nIn these situations, I generally create an API key with read/write for the given\ndatabase. Disabling the credentials is still a click away, damage will be\nlimited to a single database, and your account credentials are still protected.\n\nNext Steps\nGetting your head around storing IoT data using documents can take time and\npractice. Hopefully these five tips will put you in a good position to hit the\nground running. Keep in mind that these are \"tips\" not \"rules\". Your reporting\nneeds may vary, but I am confident that Cloudant/CouchDB can make an ideal\nstorage solution for your IoT needs.\n\nOne interesting aspect of all this is that I do not need any special database\ndrivers. As I mentioned earlier, everything about Cloudant/CouchDB is\nHTTP-based. This means that I no point have I needed to rely on running a\nNode.js (or Java, or whatever) server. In fact, as I once heard somebody so\neloquently put it \"the database is the server!\"\n\nIf you run into a situation where you do need additional server processing, you\nshould definitely check out the emerging serverless offerings such as IBM Cloud\nFunctions. Serverless, or functions as a service (FaaS), give you server\nprocessing when you need it, but you only pay for the time your function is\nactually running (versus paying for a cluster of servers running 24 x 7).\n\nWhen paired, these two solutions (IBM Cloud Functions, Cloudant) give you\nimmensely powerful infrastructure, at an extremely low cost of entry for even\nthe most demanding IoT needs.","feature_image":"http://images.kevinhoyt.com/the.cloud.jpg","featured":0,"status":"draft","locale":null,"visibility":"public","author_id":"1","created_at":"2017-10-12T19:55:47.000Z","updated_at":"2018-05-08T20:10:41.000Z","published_at":null,"custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"type":"post","email_recipient_filter":"none"},{"id":"5adb8922351ffe0018a5775c","uuid":"c2080e19-02a3-42bb-b1bc-7e9b7a5d03ff","title":"OCR with Tesseract.JS","slug":"ocr-with-tesseract-js","mobiledoc":"{\"version\":\"0.3.1\",\"markups\":[],\"atoms\":[],\"cards\":[[\"markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\"I recently ran across a [DataTurks blog post](https://dataturks.com/blog/compare-image-text-recognition-apis.php) that did a high-level comparison of the OCR (optical character recognition) APIs offered by Google, Microsoft, and Amazon. At various points throughout the article, you are prompted to give them your email address in exchange for the test dataset; which I did. Now to see how the Web would perform.\\n\\n####Tesseract\\n\\nPer the project README, the [Tesseract OCR tool](https://github.com/tesseract-ocr/tesseract), was originally by Hewlett-Packard circa 1985 - 1994. In 2005, Tesseract was released into open source. It has been maintained by ==Google== since then, with the latest release at the time of this writing being ==June 2017==.\\n\\n[Tesseract.JS](https://github.com/naptha/tesseract.js#tesseractjs) is an [Emscripten](https://github.com/kripken/emscripten) port of the original project, making it available to the browser and other JavaScript runtimes. It is relatively full featured, including ==60+ languages==.\\n\\n####Annotations\\n\\nThe DataTurks dataset includes a ==CSV file== with the various tests. It includes the paths to ==958 images==, which are included, and an ==uppercase output== of the text in the images. Most of the images are relatively small in size (a few hundreds in either dimension).\\n\\n```\\ntest/122_10.png,PAYLOADS\\ntest/2319_3.png,DONT\\ntest/414_1.png,7023083507\\ntest/5019_1.png,3D\\ntest/5115_3.png,TWILIGHT\\ntest/5116_3.png,BEN\\ntest/2155_1.png,30\\ntest/1021_3.png,THAT\\ntest/138_10.png,THE\\ntest/5013_1.png,THOR\\n```\\n\\n![Sample test image, containing the word \\\"payloads\\\".](http://images.kevinhoyt.com/payloads.ocr.png)\\n\\nLoading the annotations, and parsing them, is relatively straightforward task. While I thought about putting some ==async/await== goodness to the test on this type of asynchronous task, it turned out to be a little cleaner \\\\[for me\\\\] using callbacks.\\n\\n```\\nconstructor() {\\n  // Test cases\\n  this.annotations = [];\\n\\n  // Which test\\n  this.index = 0;\\n\\n  // How many passed\\n  this.pass = 0;\\n\\n  // Reference to test image\\n  this.image = document.querySelector( '#test' );\\n  this.image.addEventListener( \\n    'load', \\n    ( evt ) => this.doImageLoaded( evt ) \\n  );\\n\\n  // Load the annotations\\n  // Test cases\\n  this.xhr = new XMLHttpRequest();\\n  this.xhr.addEventListener( \\n    'load', \\n    ( evt ) => this.doAnnotationsLoaded( evt ) \\n  );\\n  this.xhr.open( \\n    'GET', \\n    'dataset/annotations.txt', \\n    true \\n  );\\n  this.xhr.send( null );\\n}\\n\\n...\\n\\ndoAnnotationsLoaded( evt ) {\\n  // Split off the rows\\n  let rows = this.xhr.responseText.split( '\\\\n' );\\n\\n  // Split test and result\\n  for( let r = 0; r < rows.length; r++ ) {\\n    let pair = rows[r].split( ',' );\\n\\n    this.annotations.push( {\\n      source: pair[0],\\n      text: pair[1]\\n    } );\\n  }\\n\\n  // Start analysis\\n  this.analyze();\\n}\\n```\\n\\n####Analysis\\n\\nKicking off each round of analysis is the loading the test image. Once the image is loaded, Tesseract.recognize() is run using the default settings. The API is Promise-ified, which is what drove my temptation to use async/await. In this case, in the \\\".then()\\\" handler, I compare what Tesseract.JS found against the annotation for the test case.\\n\\n```\\n// Load test image\\nanalyze() {\\n  this.image.src = 'dataset/' + this.annotations[this.index].source;\\n}\\n\\n...\\n\\n// Test image loaded\\ndoImageLoaded( evt ) {\\n  // Run recognition\\n  Tesseract.recognize( this.image )\\n    .then( ( result ) => {\\n      // Put results back into annotation\\n      this.annotations[this.index].found = result.text.trim();\\n      this.annotations[this.index].confidence = result.confidence;\\n\\n      // Check for match\\n      if( this.annotations[this.index].text.toLowerCase() == result.text.trim().toLowerCase() ) {\\n        this.pass = this.pass + 1;\\n        this.annotations[this.index].pass = true;\\n      } else {\\n        this.annotations[this.index].pass = false;\\n      }\\n\\n      // Keep going so long as there are test cases\\n      if( this.index < ( this.annotations.length - 1 ) ) {\\n        this.index = this.index + 1;\\n        this.analyze();\\n      } else {\\n        console.log( this.pass );\\n        console.log( this.annotations );\\n      }\\n    } );\\n}\\n```\\n\\n####Results\\n\\nOf the ==958 images== processed, without any additional training, Tesseract.JS landed ==290 correct== - ==about 30%==. That turns out to be ==better than AWS Rekognition== based on the DataTurks testing. Not too bad!\\n\\nA couple caveats on my results ...\\n\\n- The DataTurks dataset is actually ==500 test cases==. It is unclear as to what those 500 test cases are exactly, so my testing uses the entire 958. Would the results be better or worse using the same 500?\\n\\n- It is also worth mentioning that there are cases where Tesseract.JS was actually correct, but marked wrong. For example, a result of \\\"(702) 308-3507\\\" did not match the annotation of \\\"7023083507\\\" though the Tesseract.JS result is ==clearly correct==.\\n\\n####Next Steps\\n\\nIt would be interesting to refine the analysis code to ==exclude special characters==. The DataTurks testing also counted where no result was indicated. This happens with Tesseract.JS as well, but I did not make any effort to record the count.\\n\\nAdditionally, I have to wonder how the code would be cleaned up using ==async/await==. It is a new JavaScript language feature which I do not feel as comfortable yet compared to callbacks. On top of that then would be removing XHR, in place of the \\\"==fetch()==\\\" API, which also works on promises.\\n\\nI put my code in a little [GitHub Gist](https://gist.github.com/krhoyt/573ee665a921e952c51863a761943266), and you are welcome to check it out, make changes, and let me know what you think.\\n\\n*Header image from [DeviantArt](https://stak1073.deviantart.com/art/Loki-Tesseract-338237561).*\"}]],\"sections\":[[10,0]],\"ghostVersion\":\"3.0\"}","html":"<!--kg-card-begin: markdown--><p>I recently ran across a <a href=\"https://dataturks.com/blog/compare-image-text-recognition-apis.php\">DataTurks blog post</a> that did a high-level comparison of the OCR (optical character recognition) APIs offered by Google, Microsoft, and Amazon. At various points throughout the article, you are prompted to give them your email address in exchange for the test dataset; which I did. Now to see how the Web would perform.</p>\n<h4 id=\"tesseract\">Tesseract</h4>\n<p>Per the project README, the <a href=\"https://github.com/tesseract-ocr/tesseract\">Tesseract OCR tool</a>, was originally by Hewlett-Packard circa 1985 - 1994. In 2005, Tesseract was released into open source. It has been maintained by <mark>Google</mark> since then, with the latest release at the time of this writing being <mark>June 2017</mark>.</p>\n<p><a href=\"https://github.com/naptha/tesseract.js#tesseractjs\">Tesseract.JS</a> is an <a href=\"https://github.com/kripken/emscripten\">Emscripten</a> port of the original project, making it available to the browser and other JavaScript runtimes. It is relatively full featured, including <mark>60+ languages</mark>.</p>\n<h4 id=\"annotations\">Annotations</h4>\n<p>The DataTurks dataset includes a <mark>CSV file</mark> with the various tests. It includes the paths to <mark>958 images</mark>, which are included, and an <mark>uppercase output</mark> of the text in the images. Most of the images are relatively small in size (a few hundreds in either dimension).</p>\n<pre><code>test/122_10.png,PAYLOADS\ntest/2319_3.png,DONT\ntest/414_1.png,7023083507\ntest/5019_1.png,3D\ntest/5115_3.png,TWILIGHT\ntest/5116_3.png,BEN\ntest/2155_1.png,30\ntest/1021_3.png,THAT\ntest/138_10.png,THE\ntest/5013_1.png,THOR\n</code></pre>\n<p><img src=\"http://images.kevinhoyt.com/payloads.ocr.png\" alt=\"Sample test image, containing the word &quot;payloads&quot;.\" loading=\"lazy\"></p>\n<p>Loading the annotations, and parsing them, is relatively straightforward task. While I thought about putting some <mark>async/await</mark> goodness to the test on this type of asynchronous task, it turned out to be a little cleaner [for me] using callbacks.</p>\n<pre><code>constructor() {\n  // Test cases\n  this.annotations = [];\n\n  // Which test\n  this.index = 0;\n\n  // How many passed\n  this.pass = 0;\n\n  // Reference to test image\n  this.image = document.querySelector( '#test' );\n  this.image.addEventListener( \n    'load', \n    ( evt ) =&gt; this.doImageLoaded( evt ) \n  );\n\n  // Load the annotations\n  // Test cases\n  this.xhr = new XMLHttpRequest();\n  this.xhr.addEventListener( \n    'load', \n    ( evt ) =&gt; this.doAnnotationsLoaded( evt ) \n  );\n  this.xhr.open( \n    'GET', \n    'dataset/annotations.txt', \n    true \n  );\n  this.xhr.send( null );\n}\n\n...\n\ndoAnnotationsLoaded( evt ) {\n  // Split off the rows\n  let rows = this.xhr.responseText.split( '\\n' );\n\n  // Split test and result\n  for( let r = 0; r &lt; rows.length; r++ ) {\n    let pair = rows[r].split( ',' );\n\n    this.annotations.push( {\n      source: pair[0],\n      text: pair[1]\n    } );\n  }\n\n  // Start analysis\n  this.analyze();\n}\n</code></pre>\n<h4 id=\"analysis\">Analysis</h4>\n<p>Kicking off each round of analysis is the loading the test image. Once the image is loaded, Tesseract.recognize() is run using the default settings. The API is Promise-ified, which is what drove my temptation to use async/await. In this case, in the &quot;.then()&quot; handler, I compare what Tesseract.JS found against the annotation for the test case.</p>\n<pre><code>// Load test image\nanalyze() {\n  this.image.src = 'dataset/' + this.annotations[this.index].source;\n}\n\n...\n\n// Test image loaded\ndoImageLoaded( evt ) {\n  // Run recognition\n  Tesseract.recognize( this.image )\n    .then( ( result ) =&gt; {\n      // Put results back into annotation\n      this.annotations[this.index].found = result.text.trim();\n      this.annotations[this.index].confidence = result.confidence;\n\n      // Check for match\n      if( this.annotations[this.index].text.toLowerCase() == result.text.trim().toLowerCase() ) {\n        this.pass = this.pass + 1;\n        this.annotations[this.index].pass = true;\n      } else {\n        this.annotations[this.index].pass = false;\n      }\n\n      // Keep going so long as there are test cases\n      if( this.index &lt; ( this.annotations.length - 1 ) ) {\n        this.index = this.index + 1;\n        this.analyze();\n      } else {\n        console.log( this.pass );\n        console.log( this.annotations );\n      }\n    } );\n}\n</code></pre>\n<h4 id=\"results\">Results</h4>\n<p>Of the <mark>958 images</mark> processed, without any additional training, Tesseract.JS landed <mark>290 correct</mark> - <mark>about 30%</mark>. That turns out to be <mark>better than AWS Rekognition</mark> based on the DataTurks testing. Not too bad!</p>\n<p>A couple caveats on my results ...</p>\n<ul>\n<li>\n<p>The DataTurks dataset is actually <mark>500 test cases</mark>. It is unclear as to what those 500 test cases are exactly, so my testing uses the entire 958. Would the results be better or worse using the same 500?</p>\n</li>\n<li>\n<p>It is also worth mentioning that there are cases where Tesseract.JS was actually correct, but marked wrong. For example, a result of &quot;(702) 308-3507&quot; did not match the annotation of &quot;7023083507&quot; though the Tesseract.JS result is <mark>clearly correct</mark>.</p>\n</li>\n</ul>\n<h4 id=\"nextsteps\">Next Steps</h4>\n<p>It would be interesting to refine the analysis code to <mark>exclude special characters</mark>. The DataTurks testing also counted where no result was indicated. This happens with Tesseract.JS as well, but I did not make any effort to record the count.</p>\n<p>Additionally, I have to wonder how the code would be cleaned up using <mark>async/await</mark>. It is a new JavaScript language feature which I do not feel as comfortable yet compared to callbacks. On top of that then would be removing XHR, in place of the &quot;<mark>fetch()</mark>&quot; API, which also works on promises.</p>\n<p>I put my code in a little <a href=\"https://gist.github.com/krhoyt/573ee665a921e952c51863a761943266\">GitHub Gist</a>, and you are welcome to check it out, make changes, and let me know what you think.</p>\n<p><em>Header image from <a href=\"https://stak1073.deviantart.com/art/Loki-Tesseract-338237561\">DeviantArt</a>.</em></p>\n<!--kg-card-end: markdown-->","comment_id":"85","plaintext":"I recently ran across a DataTurks blog post\n[https://dataturks.com/blog/compare-image-text-recognition-apis.php] that did a\nhigh-level comparison of the OCR (optical character recognition) APIs offered by\nGoogle, Microsoft, and Amazon. At various points throughout the article, you are\nprompted to give them your email address in exchange for the test dataset; which\nI did. Now to see how the Web would perform.\n\nTesseract\nPer the project README, the Tesseract OCR tool\n[https://github.com/tesseract-ocr/tesseract], was originally by Hewlett-Packard\ncirca 1985 - 1994. In 2005, Tesseract was released into open source. It has been\nmaintained by Google since then, with the latest release at the time of this\nwriting being June 2017.\n\nTesseract.JS [https://github.com/naptha/tesseract.js#tesseractjs] is an \nEmscripten [https://github.com/kripken/emscripten] port of the original project,\nmaking it available to the browser and other JavaScript runtimes. It is\nrelatively full featured, including 60+ languages.\n\nAnnotations\nThe DataTurks dataset includes a CSV file with the various tests. It includes\nthe paths to 958 images, which are included, and an uppercase output of the text\nin the images. Most of the images are relatively small in size (a few hundreds\nin either dimension).\n\ntest/122_10.png,PAYLOADS\ntest/2319_3.png,DONT\ntest/414_1.png,7023083507\ntest/5019_1.png,3D\ntest/5115_3.png,TWILIGHT\ntest/5116_3.png,BEN\ntest/2155_1.png,30\ntest/1021_3.png,THAT\ntest/138_10.png,THE\ntest/5013_1.png,THOR\n\n\n\n\nLoading the annotations, and parsing them, is relatively straightforward task.\nWhile I thought about putting some async/await goodness to the test on this type\nof asynchronous task, it turned out to be a little cleaner [for me] using\ncallbacks.\n\nconstructor() {\n  // Test cases\n  this.annotations = [];\n\n  // Which test\n  this.index = 0;\n\n  // How many passed\n  this.pass = 0;\n\n  // Reference to test image\n  this.image = document.querySelector( '#test' );\n  this.image.addEventListener( \n    'load', \n    ( evt ) => this.doImageLoaded( evt ) \n  );\n\n  // Load the annotations\n  // Test cases\n  this.xhr = new XMLHttpRequest();\n  this.xhr.addEventListener( \n    'load', \n    ( evt ) => this.doAnnotationsLoaded( evt ) \n  );\n  this.xhr.open( \n    'GET', \n    'dataset/annotations.txt', \n    true \n  );\n  this.xhr.send( null );\n}\n\n...\n\ndoAnnotationsLoaded( evt ) {\n  // Split off the rows\n  let rows = this.xhr.responseText.split( '\\n' );\n\n  // Split test and result\n  for( let r = 0; r < rows.length; r++ ) {\n    let pair = rows[r].split( ',' );\n\n    this.annotations.push( {\n      source: pair[0],\n      text: pair[1]\n    } );\n  }\n\n  // Start analysis\n  this.analyze();\n}\n\n\nAnalysis\nKicking off each round of analysis is the loading the test image. Once the image\nis loaded, Tesseract.recognize() is run using the default settings. The API is\nPromise-ified, which is what drove my temptation to use async/await. In this\ncase, in the \".then()\" handler, I compare what Tesseract.JS found against the\nannotation for the test case.\n\n// Load test image\nanalyze() {\n  this.image.src = 'dataset/' + this.annotations[this.index].source;\n}\n\n...\n\n// Test image loaded\ndoImageLoaded( evt ) {\n  // Run recognition\n  Tesseract.recognize( this.image )\n    .then( ( result ) => {\n      // Put results back into annotation\n      this.annotations[this.index].found = result.text.trim();\n      this.annotations[this.index].confidence = result.confidence;\n\n      // Check for match\n      if( this.annotations[this.index].text.toLowerCase() == result.text.trim().toLowerCase() ) {\n        this.pass = this.pass + 1;\n        this.annotations[this.index].pass = true;\n      } else {\n        this.annotations[this.index].pass = false;\n      }\n\n      // Keep going so long as there are test cases\n      if( this.index < ( this.annotations.length - 1 ) ) {\n        this.index = this.index + 1;\n        this.analyze();\n      } else {\n        console.log( this.pass );\n        console.log( this.annotations );\n      }\n    } );\n}\n\n\nResults\nOf the 958 images processed, without any additional training, Tesseract.JS\nlanded 290 correct - about 30%. That turns out to be better than AWS Rekognition \nbased on the DataTurks testing. Not too bad!\n\nA couple caveats on my results ...\n\n * The DataTurks dataset is actually 500 test cases. It is unclear as to what\n   those 500 test cases are exactly, so my testing uses the entire 958. Would\n   the results be better or worse using the same 500?\n   \n   \n * It is also worth mentioning that there are cases where Tesseract.JS was\n   actually correct, but marked wrong. For example, a result of \"(702) 308-3507\"\n   did not match the annotation of \"7023083507\" though the Tesseract.JS result\n   is clearly correct.\n   \n   \n\nNext Steps\nIt would be interesting to refine the analysis code to exclude special\ncharacters. The DataTurks testing also counted where no result was indicated.\nThis happens with Tesseract.JS as well, but I did not make any effort to record\nthe count.\n\nAdditionally, I have to wonder how the code would be cleaned up using \nasync/await. It is a new JavaScript language feature which I do not feel as\ncomfortable yet compared to callbacks. On top of that then would be removing\nXHR, in place of the \"fetch()\" API, which also works on promises.\n\nI put my code in a little GitHub Gist\n[https://gist.github.com/krhoyt/573ee665a921e952c51863a761943266], and you are\nwelcome to check it out, make changes, and let me know what you think.\n\nHeader image from DeviantArt\n[https://stak1073.deviantart.com/art/Loki-Tesseract-338237561].","feature_image":"__GHOST_URL__/content/images/2019/01/loki.tesseract.jpg","featured":0,"status":"published","locale":null,"visibility":"public","author_id":"1","created_at":"2017-12-23T21:11:22.000Z","updated_at":"2019-01-17T00:32:55.000Z","published_at":"2017-12-27T22:25:00.000Z","custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"type":"post","email_recipient_filter":"none"},{"id":"5adb8922351ffe0018a5775d","uuid":"befb2341-5133-4944-af64-b1b61057d087","title":"Index Conference","slug":"index-conference","mobiledoc":"{\"version\":\"0.3.1\",\"markups\":[],\"atoms\":[],\"cards\":[[\"markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\"It has been a number of years since I have been involved in running a conference. When a chance to chair a track around future technology at Index, I jumped at the opportunity. Beyond just my track however, here are some of the exciting sessions that I am looking to attend.\\n\\n####Index?\\n\\n[Index](http://indexconf.com) is a ==developer conference in San Fransisco, CA on February 20 - 22, 2018==. While it is being largely organized by IBM, Index is run almost entirely by industry leaders from Amazon, Nexmo (Vonage), Teradata, GoDaddy, and others. Index consists of eight different tracks from AI to Web, and containers to blockchain.\\n\\nIf you are a developer in the Bay Area, it is easy access to some of amazing speakers - a great chance to bolster existing skills, and check out new ones. \\n\\nHere are some of the session I am most looking forward to seeing ...\\n\\n#####Node.js: What's Next\\n\\n**What:** New features, major changes, and key initiatives, oh my!\\n\\n**Why:** [Michael Dawson](https://twitter.com/mhdawson1) is the ==chair of the technical steering committee== for Node.js. Few are closer what would be next with Node.js than Michael.\\n\\n#####Easy, Component-based, Serverless Applications with JavaScript\\n\\n**What:** A closer look at [Architect](https://arc.codes), a CLI for serverless deployments, and [Marko](https://markojs.com), a component UI library from the JS Foundation.\\n\\n**Why:** [Kris Borchers](https://twitter.com/kborchers) is the Executive Director of the [JS Foundation](https://js.foundation/) - keepers of projects like Grunt, Mocha, Webpack, and so much more.\\n\\n#####Moving 75,000 Microsofties to DevOps on the Public Cloud\\n\\n**What:** From culture to code, this is the story of how you take an entire organization to the cloud, from ... one of the main cloud vendors, Microsoft.\\n\\n**Why:** [Sam Guckenheimer](https://twitter.com/SamGuckenheimer) is the Product Owner of Visual Studio Team Services, at ==Microsoft==, will be giving a first-hand account of the process.\\n\\n#####Tensorflow, deep learning and modern RNN architectures, without a PhD\\n\\n**What:** Learning TensorFlow without the PhD. Enough said.\\n\\n**Why:** TensorFlow comes out of Google, and while they have not yet moved to open governance, the best place to learn more is from Google themselves - in this case, [Martin Gorner](https://twitter.com/martin_gorner), from the ==Google== Developer Relations team. \\n\\n#####Getting out of the bubble with global developer communities\\n\\n**What:** All about planning and executing on a global developer relations tour, including coverage of those regions where you may not know your developers quite as well.\\n\\n**Why:** I have been in developer relations for over a decade now, and I always love hearing stories of how others approach the space. In this case, [Bear Douglas](https://twitter.com/beardigsit) from ==Slack==.\\n\\n#####Discover how to include natural language generation in your applications\\n\\n**What:** You may have heard of Natural Language Processing (NLP) - taking raw text, and getting data about the content. Natural Language Generation is the opposite - taking data about something and generating raw, cohesive, text. A Chief Scientist from [Arria](https://www.arria.com) - the leader in this space - will be presenting.\\n\\n**Why:** I had a project recently in the finance vertical where we took financial data, and made what was effectively a customer prospectus, using NLG. The world has no shortage of data, but communicating it effectively is becoming a real challenge.\\n\\n#####Controlling a Quantum Computer with Code\\n\\n**What:** Quantum computers are a thing, but the tooling to take advantage of them is ... well ... not. This session by ==Rigetti== Software Engineer, [Steven Heidel](https://twitter.com/stevenheidel) lets us know where they need help.\\n\\n**Why:** As an IBMer, I am somewhat familiar with the concept of quantum computing - at least what we are doing in the space. It will be interesting to see what the competition is up to.\\n\\n####Next Steps\\n\\nRegister for [Index](http://indexconf.com), ==duh==! Amazon, Google, IBM, Microsoft, and so many more, all pushing the boundaries of development. You would be hard pressed to get this amount of technical depth and diversity in a single conference, anywhere. \"}]],\"sections\":[[10,0]],\"ghostVersion\":\"3.0\"}","html":"<!--kg-card-begin: markdown--><p>It has been a number of years since I have been involved in running a conference. When a chance to chair a track around future technology at Index, I jumped at the opportunity. Beyond just my track however, here are some of the exciting sessions that I am looking to attend.</p>\n<h4 id=\"index\">Index?</h4>\n<p><a href=\"http://indexconf.com\">Index</a> is a <mark>developer conference in San Fransisco, CA on February 20 - 22, 2018</mark>. While it is being largely organized by IBM, Index is run almost entirely by industry leaders from Amazon, Nexmo (Vonage), Teradata, GoDaddy, and others. Index consists of eight different tracks from AI to Web, and containers to blockchain.</p>\n<p>If you are a developer in the Bay Area, it is easy access to some of amazing speakers - a great chance to bolster existing skills, and check out new ones.</p>\n<p>Here are some of the session I am most looking forward to seeing ...</p>\n<h5 id=\"nodejswhatsnext\">Node.js: What's Next</h5>\n<p><strong>What:</strong> New features, major changes, and key initiatives, oh my!</p>\n<p><strong>Why:</strong> <a href=\"https://twitter.com/mhdawson1\">Michael Dawson</a> is the <mark>chair of the technical steering committee</mark> for Node.js. Few are closer what would be next with Node.js than Michael.</p>\n<h5 id=\"easycomponentbasedserverlessapplicationswithjavascript\">Easy, Component-based, Serverless Applications with JavaScript</h5>\n<p><strong>What:</strong> A closer look at <a href=\"https://arc.codes\">Architect</a>, a CLI for serverless deployments, and <a href=\"https://markojs.com\">Marko</a>, a component UI library from the JS Foundation.</p>\n<p><strong>Why:</strong> <a href=\"https://twitter.com/kborchers\">Kris Borchers</a> is the Executive Director of the <a href=\"https://js.foundation/\">JS Foundation</a> - keepers of projects like Grunt, Mocha, Webpack, and so much more.</p>\n<h5 id=\"moving75000microsoftiestodevopsonthepubliccloud\">Moving 75,000 Microsofties to DevOps on the Public Cloud</h5>\n<p><strong>What:</strong> From culture to code, this is the story of how you take an entire organization to the cloud, from ... one of the main cloud vendors, Microsoft.</p>\n<p><strong>Why:</strong> <a href=\"https://twitter.com/SamGuckenheimer\">Sam Guckenheimer</a> is the Product Owner of Visual Studio Team Services, at <mark>Microsoft</mark>, will be giving a first-hand account of the process.</p>\n<h5 id=\"tensorflowdeeplearningandmodernrnnarchitectureswithoutaphd\">Tensorflow, deep learning and modern RNN architectures, without a PhD</h5>\n<p><strong>What:</strong> Learning TensorFlow without the PhD. Enough said.</p>\n<p><strong>Why:</strong> TensorFlow comes out of Google, and while they have not yet moved to open governance, the best place to learn more is from Google themselves - in this case, <a href=\"https://twitter.com/martin_gorner\">Martin Gorner</a>, from the <mark>Google</mark> Developer Relations team.</p>\n<h5 id=\"gettingoutofthebubblewithglobaldevelopercommunities\">Getting out of the bubble with global developer communities</h5>\n<p><strong>What:</strong> All about planning and executing on a global developer relations tour, including coverage of those regions where you may not know your developers quite as well.</p>\n<p><strong>Why:</strong> I have been in developer relations for over a decade now, and I always love hearing stories of how others approach the space. In this case, <a href=\"https://twitter.com/beardigsit\">Bear Douglas</a> from <mark>Slack</mark>.</p>\n<h5 id=\"discoverhowtoincludenaturallanguagegenerationinyourapplications\">Discover how to include natural language generation in your applications</h5>\n<p><strong>What:</strong> You may have heard of Natural Language Processing (NLP) - taking raw text, and getting data about the content. Natural Language Generation is the opposite - taking data about something and generating raw, cohesive, text. A Chief Scientist from <a href=\"https://www.arria.com\">Arria</a> - the leader in this space - will be presenting.</p>\n<p><strong>Why:</strong> I had a project recently in the finance vertical where we took financial data, and made what was effectively a customer prospectus, using NLG. The world has no shortage of data, but communicating it effectively is becoming a real challenge.</p>\n<h5 id=\"controllingaquantumcomputerwithcode\">Controlling a Quantum Computer with Code</h5>\n<p><strong>What:</strong> Quantum computers are a thing, but the tooling to take advantage of them is ... well ... not. This session by <mark>Rigetti</mark> Software Engineer, <a href=\"https://twitter.com/stevenheidel\">Steven Heidel</a> lets us know where they need help.</p>\n<p><strong>Why:</strong> As an IBMer, I am somewhat familiar with the concept of quantum computing - at least what we are doing in the space. It will be interesting to see what the competition is up to.</p>\n<h4 id=\"nextsteps\">Next Steps</h4>\n<p>Register for <a href=\"http://indexconf.com\">Index</a>, <mark>duh</mark>! Amazon, Google, IBM, Microsoft, and so many more, all pushing the boundaries of development. You would be hard pressed to get this amount of technical depth and diversity in a single conference, anywhere.</p>\n<!--kg-card-end: markdown-->","comment_id":"86","plaintext":"It has been a number of years since I have been involved in running a\nconference. When a chance to chair a track around future technology at Index, I\njumped at the opportunity. Beyond just my track however, here are some of the\nexciting sessions that I am looking to attend.\n\nIndex?\nIndex [http://indexconf.com] is a developer conference in San Fransisco, CA on\nFebruary 20 - 22, 2018. While it is being largely organized by IBM, Index is run\nalmost entirely by industry leaders from Amazon, Nexmo (Vonage), Teradata,\nGoDaddy, and others. Index consists of eight different tracks from AI to Web,\nand containers to blockchain.\n\nIf you are a developer in the Bay Area, it is easy access to some of amazing\nspeakers - a great chance to bolster existing skills, and check out new ones.\n\nHere are some of the session I am most looking forward to seeing ...\n\nNode.js: What's Next\nWhat: New features, major changes, and key initiatives, oh my!\n\nWhy: Michael Dawson [https://twitter.com/mhdawson1] is the chair of the\ntechnical steering committee for Node.js. Few are closer what would be next with\nNode.js than Michael.\n\nEasy, Component-based, Serverless Applications with JavaScript\nWhat: A closer look at Architect [https://arc.codes], a CLI for serverless\ndeployments, and Marko [https://markojs.com], a component UI library from the JS\nFoundation.\n\nWhy: Kris Borchers [https://twitter.com/kborchers] is the Executive Director of\nthe JS Foundation [https://js.foundation/] - keepers of projects like Grunt,\nMocha, Webpack, and so much more.\n\nMoving 75,000 Microsofties to DevOps on the Public Cloud\nWhat: From culture to code, this is the story of how you take an entire\norganization to the cloud, from ... one of the main cloud vendors, Microsoft.\n\nWhy: Sam Guckenheimer [https://twitter.com/SamGuckenheimer] is the Product Owner\nof Visual Studio Team Services, at Microsoft, will be giving a first-hand\naccount of the process.\n\nTensorflow, deep learning and modern RNN architectures, without a PhD\nWhat: Learning TensorFlow without the PhD. Enough said.\n\nWhy: TensorFlow comes out of Google, and while they have not yet moved to open\ngovernance, the best place to learn more is from Google themselves - in this\ncase, Martin Gorner [https://twitter.com/martin_gorner], from the Google \nDeveloper Relations team.\n\nGetting out of the bubble with global developer communities\nWhat: All about planning and executing on a global developer relations tour,\nincluding coverage of those regions where you may not know your developers quite\nas well.\n\nWhy: I have been in developer relations for over a decade now, and I always love\nhearing stories of how others approach the space. In this case, Bear Douglas\n[https://twitter.com/beardigsit] from Slack.\n\nDiscover how to include natural language generation in your applications\nWhat: You may have heard of Natural Language Processing (NLP) - taking raw text,\nand getting data about the content. Natural Language Generation is the opposite\n- taking data about something and generating raw, cohesive, text. A Chief\nScientist from Arria [https://www.arria.com] - the leader in this space - will\nbe presenting.\n\nWhy: I had a project recently in the finance vertical where we took financial\ndata, and made what was effectively a customer prospectus, using NLG. The world\nhas no shortage of data, but communicating it effectively is becoming a real\nchallenge.\n\nControlling a Quantum Computer with Code\nWhat: Quantum computers are a thing, but the tooling to take advantage of them\nis ... well ... not. This session by Rigetti Software Engineer, Steven Heidel\n[https://twitter.com/stevenheidel] lets us know where they need help.\n\nWhy: As an IBMer, I am somewhat familiar with the concept of quantum computing -\nat least what we are doing in the space. It will be interesting to see what the\ncompetition is up to.\n\nNext Steps\nRegister for Index [http://indexconf.com], duh! Amazon, Google, IBM, Microsoft,\nand so many more, all pushing the boundaries of development. You would be hard\npressed to get this amount of technical depth and diversity in a single\nconference, anywhere.","feature_image":"__GHOST_URL__/content/images/2019/01/index.conference.jpg","featured":0,"status":"published","locale":null,"visibility":"public","author_id":"1","created_at":"2018-01-22T20:30:28.000Z","updated_at":"2019-01-17T00:32:00.000Z","published_at":"2018-01-22T23:43:00.000Z","custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"type":"post","email_recipient_filter":"none"},{"id":"5adb8922351ffe0018a5775e","uuid":"94f9041a-b74c-4de7-b25a-1532b7db0535","title":"Developer Advocacy Questions","slug":"developer-advocacy-questions","mobiledoc":"{\"version\":\"0.3.1\",\"markups\":[],\"atoms\":[],\"cards\":[[\"markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\"Ashley McNamara recently published her thoughts around \\\"[What is Developer Advocacy?](https://medium.com/@ashleymcnamara/what-is-developer-advocacy-3a92442b627c)\\\" Having worked in the developer relations space for over a decade, I always enjoy reading the perspective of others. How companies approach developer relations is often reflective of their internal cultures, which is probably more telling than they would like.\\n\\nPerhaps what I enjoyed even more were the questions that came after the article. There were a few of them where I really wanted to join the conversation, but I had a hard time keeping the content concise. Instead, I figured I would just tackle them here, as the next installment of my series on developer relations ...\\n\\n- [Evangelist, Advocate, Community](http://www.kevinhoyt.com/2016/06/08/evangelist-advocate-community/)\\n- [Measuring Developer Relations](http://www.kevinhoyt.com/2016/08/26/measuring-developer-relations/)\\n- [Developer Advocates and Ideas](http://www.kevinhoyt.com/2017/06/13/conference-abstract-ideas/)\\n- [Developer Advocates and Content](http://www.kevinhoyt.com/2017/08/16/developer-advocates-and-content/)\\n\\n####More Than Getting Started\\n\\n[Chen Shou](https://medium.com/@shou3301) was wondering why so many advocates seem to be focused on getting started demonstrations.\\n\\nA lot could be said here about the pace of technology, or what kind of content most conferences accept, but I like to think of this as a matter of personality.\\n\\nI had a sociology professor in college that stated something along the lines of \\\"humans tend to chase bright, shiny objects\\\". I do not even remember the context of the lecture, but that phrase has stuck with me, mostly because it very much defines one of my key personality traits (smile). If I am not kicking the tires on a new technology, I am not happy. I care a lot less about how to scale something, than taking it apart and figuring out what makes it tick in the first place - and my career has echoed this trait from consulting projects to proof-of-concept work in technical sales.\\n\\nImagine my surprise then when I discovered that many developers are very passionate about getting the implementation exactly right! Gasp! \\n\\nWithin the context of developer relations, I break these personality traits down into the \\\"art of the possible\\\" and the \\\"reality of production\\\". \\n\\n**Art of the Possible**\\n\\n- Describes new and potential technology scenarios\\n- Focuses on the cool, new, and bleeding edge\\n- Helps developers visualize the potential\\n\\n**Reality of Production**\\n\\n- Describes solving real problems with new technology\\n- Focused helping in production\\n- Help developer bridge the gap between demo and production\\n\\n>I should point out that my thoughts around personality traits is inspired by \\\"[Making the Technical Sale](https://www.amazon.com/Making-Technical-Sale-Successful-Consultant/dp/B01FKU4YG0)\\\" which goes into some length about matching your personality with how you approach the technical sale.\\n\\n**Corporate Culture**\\n\\nWhile being self-aware, and working in a manner that best suits your personality are important, I think there is something deeper happening in the current context of developer relations - we are watering it down.\\n\\nAn evangelist is one who seeks to convert others [to the Christian faith]. At the end of the day, this is very much what most companies want from developer relations - to convert you to their technology (faith). In order to convert somebody, you must first introduce them to the concept. Translated into conference sessions - getting started.\\n\\n==*This is a corporate culture tell.*==\\n\\nIf you see most presentations from a specific company focused on \\\"getting started\\\" then what they are telling you is that they view themselves at the \\\"innovation trigger\\\" stage of the Gartner Hype Cycle, and pushing up the \\\"peak of inflated expectations\\\". It also tells you that the company likely does not want to build community, or have a relationship with the developers that use its technology.\"}]],\"sections\":[[10,0]],\"ghostVersion\":\"3.0\"}","html":"<!--kg-card-begin: markdown--><p>Ashley McNamara recently published her thoughts around &quot;<a href=\"https://medium.com/@ashleymcnamara/what-is-developer-advocacy-3a92442b627c\">What is Developer Advocacy?</a>&quot; Having worked in the developer relations space for over a decade, I always enjoy reading the perspective of others. How companies approach developer relations is often reflective of their internal cultures, which is probably more telling than they would like.</p>\n<p>Perhaps what I enjoyed even more were the questions that came after the article. There were a few of them where I really wanted to join the conversation, but I had a hard time keeping the content concise. Instead, I figured I would just tackle them here, as the next installment of my series on developer relations ...</p>\n<ul>\n<li><a href=\"http://www.kevinhoyt.com/2016/06/08/evangelist-advocate-community/\">Evangelist, Advocate, Community</a></li>\n<li><a href=\"http://www.kevinhoyt.com/2016/08/26/measuring-developer-relations/\">Measuring Developer Relations</a></li>\n<li><a href=\"http://www.kevinhoyt.com/2017/06/13/conference-abstract-ideas/\">Developer Advocates and Ideas</a></li>\n<li><a href=\"http://www.kevinhoyt.com/2017/08/16/developer-advocates-and-content/\">Developer Advocates and Content</a></li>\n</ul>\n<h4 id=\"morethangettingstarted\">More Than Getting Started</h4>\n<p><a href=\"https://medium.com/@shou3301\">Chen Shou</a> was wondering why so many advocates seem to be focused on getting started demonstrations.</p>\n<p>A lot could be said here about the pace of technology, or what kind of content most conferences accept, but I like to think of this as a matter of personality.</p>\n<p>I had a sociology professor in college that stated something along the lines of &quot;humans tend to chase bright, shiny objects&quot;. I do not even remember the context of the lecture, but that phrase has stuck with me, mostly because it very much defines one of my key personality traits (smile). If I am not kicking the tires on a new technology, I am not happy. I care a lot less about how to scale something, than taking it apart and figuring out what makes it tick in the first place - and my career has echoed this trait from consulting projects to proof-of-concept work in technical sales.</p>\n<p>Imagine my surprise then when I discovered that many developers are very passionate about getting the implementation exactly right! Gasp!</p>\n<p>Within the context of developer relations, I break these personality traits down into the &quot;art of the possible&quot; and the &quot;reality of production&quot;.</p>\n<p><strong>Art of the Possible</strong></p>\n<ul>\n<li>Describes new and potential technology scenarios</li>\n<li>Focuses on the cool, new, and bleeding edge</li>\n<li>Helps developers visualize the potential</li>\n</ul>\n<p><strong>Reality of Production</strong></p>\n<ul>\n<li>Describes solving real problems with new technology</li>\n<li>Focused helping in production</li>\n<li>Help developer bridge the gap between demo and production</li>\n</ul>\n<blockquote>\n<p>I should point out that my thoughts around personality traits is inspired by &quot;<a href=\"https://www.amazon.com/Making-Technical-Sale-Successful-Consultant/dp/B01FKU4YG0\">Making the Technical Sale</a>&quot; which goes into some length about matching your personality with how you approach the technical sale.</p>\n</blockquote>\n<p><strong>Corporate Culture</strong></p>\n<p>While being self-aware, and working in a manner that best suits your personality are important, I think there is something deeper happening in the current context of developer relations - we are watering it down.</p>\n<p>An evangelist is one who seeks to convert others [to the Christian faith]. At the end of the day, this is very much what most companies want from developer relations - to convert you to their technology (faith). In order to convert somebody, you must first introduce them to the concept. Translated into conference sessions - getting started.</p>\n<p><mark><em>This is a corporate culture tell.</em></mark></p>\n<p>If you see most presentations from a specific company focused on &quot;getting started&quot; then what they are telling you is that they view themselves at the &quot;innovation trigger&quot; stage of the Gartner Hype Cycle, and pushing up the &quot;peak of inflated expectations&quot;. It also tells you that the company likely does not want to build community, or have a relationship with the developers that use its technology.</p>\n<!--kg-card-end: markdown-->","comment_id":"87","plaintext":"Ashley McNamara recently published her thoughts around \"What is Developer\nAdvocacy?\n[https://medium.com/@ashleymcnamara/what-is-developer-advocacy-3a92442b627c]\"\nHaving worked in the developer relations space for over a decade, I always enjoy\nreading the perspective of others. How companies approach developer relations is\noften reflective of their internal cultures, which is probably more telling than\nthey would like.\n\nPerhaps what I enjoyed even more were the questions that came after the article.\nThere were a few of them where I really wanted to join the conversation, but I\nhad a hard time keeping the content concise. Instead, I figured I would just\ntackle them here, as the next installment of my series on developer relations\n...\n\n * Evangelist, Advocate, Community\n   [http://www.kevinhoyt.com/2016/06/08/evangelist-advocate-community/]\n * Measuring Developer Relations\n   [http://www.kevinhoyt.com/2016/08/26/measuring-developer-relations/]\n * Developer Advocates and Ideas\n   [http://www.kevinhoyt.com/2017/06/13/conference-abstract-ideas/]\n * Developer Advocates and Content\n   [http://www.kevinhoyt.com/2017/08/16/developer-advocates-and-content/]\n\nMore Than Getting Started\nChen Shou [https://medium.com/@shou3301] was wondering why so many advocates\nseem to be focused on getting started demonstrations.\n\nA lot could be said here about the pace of technology, or what kind of content\nmost conferences accept, but I like to think of this as a matter of personality.\n\nI had a sociology professor in college that stated something along the lines of\n\"humans tend to chase bright, shiny objects\". I do not even remember the context\nof the lecture, but that phrase has stuck with me, mostly because it very much\ndefines one of my key personality traits (smile). If I am not kicking the tires\non a new technology, I am not happy. I care a lot less about how to scale\nsomething, than taking it apart and figuring out what makes it tick in the first\nplace - and my career has echoed this trait from consulting projects to\nproof-of-concept work in technical sales.\n\nImagine my surprise then when I discovered that many developers are very\npassionate about getting the implementation exactly right! Gasp!\n\nWithin the context of developer relations, I break these personality traits down\ninto the \"art of the possible\" and the \"reality of production\".\n\nArt of the Possible\n\n * Describes new and potential technology scenarios\n * Focuses on the cool, new, and bleeding edge\n * Helps developers visualize the potential\n\nReality of Production\n\n * Describes solving real problems with new technology\n * Focused helping in production\n * Help developer bridge the gap between demo and production\n\n> I should point out that my thoughts around personality traits is inspired by \"\nMaking the Technical Sale\n[https://www.amazon.com/Making-Technical-Sale-Successful-Consultant/dp/B01FKU4YG0]\n\" which goes into some length about matching your personality with how you\napproach the technical sale.\n\n\nCorporate Culture\n\nWhile being self-aware, and working in a manner that best suits your personality\nare important, I think there is something deeper happening in the current\ncontext of developer relations - we are watering it down.\n\nAn evangelist is one who seeks to convert others [to the Christian faith]. At\nthe end of the day, this is very much what most companies want from developer\nrelations - to convert you to their technology (faith). In order to convert\nsomebody, you must first introduce them to the concept. Translated into\nconference sessions - getting started.\n\nThis is a corporate culture tell.\n\nIf you see most presentations from a specific company focused on \"getting\nstarted\" then what they are telling you is that they view themselves at the\n\"innovation trigger\" stage of the Gartner Hype Cycle, and pushing up the \"peak\nof inflated expectations\". It also tells you that the company likely does not\nwant to build community, or have a relationship with the developers that use its\ntechnology.","feature_image":null,"featured":0,"status":"draft","locale":null,"visibility":"public","author_id":"1","created_at":"2018-01-23T17:00:48.000Z","updated_at":"2018-01-23T17:55:25.000Z","published_at":null,"custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"type":"post","email_recipient_filter":"none"},{"id":"5adb8922351ffe0018a5775f","uuid":"ccd99917-3e74-46e4-b94a-645e20fd75eb","title":"Developer Advocacy and the Project Triangle","slug":"developer-advocacy-and-the-iron-triange","mobiledoc":"{\"version\":\"0.3.1\",\"markups\":[],\"atoms\":[],\"cards\":[[\"markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\"If you have been around software development long enough, or even just the business world in general, then you will likely have heard of the Project Triangle. This is the saying of: \\\"Good, Fast, or Cheap. Pick two.\\\" It is unclear as to where this phrase originates, but it has been used in project management circles since at least the 1950s - and it applies to developer relations as well.\\n\\n####The Project Triangle\\n\\nThe general idea behind the project triangle goes something like:\\n\\n* To get something done quickly, and with high quality, will be very costly\\n* To get something done quickly, and with low cost, means it will not be of great quality\\n* To get something of high quality, done at a low cost, will take a long time\\n\\nThere are a lot of humorous takes on this, usually depicted as a Venn diagram. Where the circles overlap is usually marked as \\\"does not exist\\\" or the likes.\\n\\nWhen it comes to developer relations, the \\\"something\\\" is usually building community. Building community is generally viewed as a slow, and intentional process - the long game. It does not have to take a long time, but that means a shift in how the other two constants relate. Let us take a look at how these break down.\\n\\n####Fast and Good, Not Cheap\\n\\nIf you want to build a community quickly, and you want it to be a quality community - one in which you have a meaningful dialog with your customers - then it will not be cheap. While there are certainly a list of ways to spend that money, two specific scenarios I have seen in recent years come to mind.\\n\\nThe first scenario is to hire well-known talent (good, but not cheap) in the areas that you are looking for growth. As an example, these people might be well-known contributors on popular open source projects. Said people will most certainly come with their own community, which quickly (fast) gives you an audience of interested developers with which to dialog.\\n\\nAnother scenario might be to acquire (good, but not cheap) a company that has already built a community of its own. These developers will most certainly want to know what it is that you have planned for their favorite product(s) after the closing, giving you an open door to productive dialog (fast).\\n\\n####Fast and Cheap, Not Good\\n\\nThis is what I consider to be the marketing-heavy approach. \\n\\nSocial media gives just about anybody a massive megaphone through which to shout their message - or in the case of developer relations, about their product. A few dollars will go a long ways to boost search engine rankings. You might take a stab at a viral video, or project. Or maybe you just cross-link the heck out of a several dozen blog posts on different media outlets (channels). There are a myriad of [fast and cheap] approaches to take here.\\n\\nInevitably, some developers will hear your megaphone. Some may find your message compelling, and give your product a try. These developers however, will not generally be particularly invested in your offering (not good) - at least not at first. This means that having a dialog with them will be difficult, and that the feedback you receive will be just as shallow as your initial marketing blast.\\n\\nThat is not to say that this is a bad approach; indeed it is ideal for startups and smaller companies that cannot afford a dedicated developer relations staff. You do however have to be realistic about the resulting quality of community.\\n\\n####Good and Cheap, Not Quick\\n\\nDeveloper relations does not have to be expensive (cheap), but building a quality dialog (good) with your customers will take time (not quick). \\n\\nThat time investment means more than hosting a monthly meetup, or speaking at the occasional event - it means building quality documentation, it means making sure the barrier to entry is as low as possible. It means putting the time into the quality of your product up front. It means hearing, and addressing every single piece of feedback, through every possible channel. And so much more.\\n\\nDevelopers do not often have a lot of time to experiment with new technologies. This usually happens in what most companies call \\\"10% time\\\" which might mean a Friday afternoon (four hours of a forty-hour week is ten percent). If you have invested the time in your product (including documentation), then using it will be like magic. Problems that might have seemed too big for an afternoon, suddenly become child's play.\\n\\nCongratulations, you are half-way there ...\\n\\nNow comes the time to start a dialog with those developers entranced by the magic of your product. Eventually (not fast), a passionate (good) community will form.\\n\\n####A Friend in Need ...\\n\\nI sometimes take issue with the term \\\"developer\\\" as used by most companies. The term can dehumanize the very real people spending valuable hours of their lives using your product. Regardless of the situation, do you like it when a company (or person) wastes your time? Same difference.\\n\\nI like to think of developer communities as my friends - after all, it is only in that setting where I can truly get my geek on, and in which we can have an open exchange of ideas. I would never intentionally waste the time of a friend. To that end, how good can a friendship be if ... \\n\\n* You spend your lunch hour together every day at the finest restaurant in town?\\n* You buy them a Happy Meal at McDonald's once a month?\\n* You share a bag of granola while spending the weekend on a hiking trip?\\n\\nFurther yet, how does one even quantify a friendship? Can you say that Bob is worth $10 but Linda is worth $100? Answering \\\"yes\\\" to this question is where many companies go wrong with developer relations. If you commit to these types of quantifications, then you have just turned a relationship into a number, and taken a sharp turn into the world of sales.\\n\\n####Back to that Triangle Thing ...\\n\\nTo be sure, none of this is quite as cut and dry as we might like it to be. Sometimes you may find yourself spending more time on product, and other times on marketing blasts. At other times, you may look into acquisition. Each of these are integral parts of doing business, but it is important to recognize that each comes with trade-offs - even in developer relations.\\n\"}]],\"sections\":[[10,0]],\"ghostVersion\":\"3.0\"}","html":"<!--kg-card-begin: markdown--><p>If you have been around software development long enough, or even just the business world in general, then you will likely have heard of the Project Triangle. This is the saying of: &quot;Good, Fast, or Cheap. Pick two.&quot; It is unclear as to where this phrase originates, but it has been used in project management circles since at least the 1950s - and it applies to developer relations as well.</p>\n<h4 id=\"theprojecttriangle\">The Project Triangle</h4>\n<p>The general idea behind the project triangle goes something like:</p>\n<ul>\n<li>To get something done quickly, and with high quality, will be very costly</li>\n<li>To get something done quickly, and with low cost, means it will not be of great quality</li>\n<li>To get something of high quality, done at a low cost, will take a long time</li>\n</ul>\n<p>There are a lot of humorous takes on this, usually depicted as a Venn diagram. Where the circles overlap is usually marked as &quot;does not exist&quot; or the likes.</p>\n<p>When it comes to developer relations, the &quot;something&quot; is usually building community. Building community is generally viewed as a slow, and intentional process - the long game. It does not have to take a long time, but that means a shift in how the other two constants relate. Let us take a look at how these break down.</p>\n<h4 id=\"fastandgoodnotcheap\">Fast and Good, Not Cheap</h4>\n<p>If you want to build a community quickly, and you want it to be a quality community - one in which you have a meaningful dialog with your customers - then it will not be cheap. While there are certainly a list of ways to spend that money, two specific scenarios I have seen in recent years come to mind.</p>\n<p>The first scenario is to hire well-known talent (good, but not cheap) in the areas that you are looking for growth. As an example, these people might be well-known contributors on popular open source projects. Said people will most certainly come with their own community, which quickly (fast) gives you an audience of interested developers with which to dialog.</p>\n<p>Another scenario might be to acquire (good, but not cheap) a company that has already built a community of its own. These developers will most certainly want to know what it is that you have planned for their favorite product(s) after the closing, giving you an open door to productive dialog (fast).</p>\n<h4 id=\"fastandcheapnotgood\">Fast and Cheap, Not Good</h4>\n<p>This is what I consider to be the marketing-heavy approach.</p>\n<p>Social media gives just about anybody a massive megaphone through which to shout their message - or in the case of developer relations, about their product. A few dollars will go a long ways to boost search engine rankings. You might take a stab at a viral video, or project. Or maybe you just cross-link the heck out of a several dozen blog posts on different media outlets (channels). There are a myriad of [fast and cheap] approaches to take here.</p>\n<p>Inevitably, some developers will hear your megaphone. Some may find your message compelling, and give your product a try. These developers however, will not generally be particularly invested in your offering (not good) - at least not at first. This means that having a dialog with them will be difficult, and that the feedback you receive will be just as shallow as your initial marketing blast.</p>\n<p>That is not to say that this is a bad approach; indeed it is ideal for startups and smaller companies that cannot afford a dedicated developer relations staff. You do however have to be realistic about the resulting quality of community.</p>\n<h4 id=\"goodandcheapnotquick\">Good and Cheap, Not Quick</h4>\n<p>Developer relations does not have to be expensive (cheap), but building a quality dialog (good) with your customers will take time (not quick).</p>\n<p>That time investment means more than hosting a monthly meetup, or speaking at the occasional event - it means building quality documentation, it means making sure the barrier to entry is as low as possible. It means putting the time into the quality of your product up front. It means hearing, and addressing every single piece of feedback, through every possible channel. And so much more.</p>\n<p>Developers do not often have a lot of time to experiment with new technologies. This usually happens in what most companies call &quot;10% time&quot; which might mean a Friday afternoon (four hours of a forty-hour week is ten percent). If you have invested the time in your product (including documentation), then using it will be like magic. Problems that might have seemed too big for an afternoon, suddenly become child's play.</p>\n<p>Congratulations, you are half-way there ...</p>\n<p>Now comes the time to start a dialog with those developers entranced by the magic of your product. Eventually (not fast), a passionate (good) community will form.</p>\n<h4 id=\"afriendinneed\">A Friend in Need ...</h4>\n<p>I sometimes take issue with the term &quot;developer&quot; as used by most companies. The term can dehumanize the very real people spending valuable hours of their lives using your product. Regardless of the situation, do you like it when a company (or person) wastes your time? Same difference.</p>\n<p>I like to think of developer communities as my friends - after all, it is only in that setting where I can truly get my geek on, and in which we can have an open exchange of ideas. I would never intentionally waste the time of a friend. To that end, how good can a friendship be if ...</p>\n<ul>\n<li>You spend your lunch hour together every day at the finest restaurant in town?</li>\n<li>You buy them a Happy Meal at McDonald's once a month?</li>\n<li>You share a bag of granola while spending the weekend on a hiking trip?</li>\n</ul>\n<p>Further yet, how does one even quantify a friendship? Can you say that Bob is worth $10 but Linda is worth $100? Answering &quot;yes&quot; to this question is where many companies go wrong with developer relations. If you commit to these types of quantifications, then you have just turned a relationship into a number, and taken a sharp turn into the world of sales.</p>\n<h4 id=\"backtothattrianglething\">Back to that Triangle Thing ...</h4>\n<p>To be sure, none of this is quite as cut and dry as we might like it to be. Sometimes you may find yourself spending more time on product, and other times on marketing blasts. At other times, you may look into acquisition. Each of these are integral parts of doing business, but it is important to recognize that each comes with trade-offs - even in developer relations.</p>\n<!--kg-card-end: markdown-->","comment_id":"88","plaintext":"If you have been around software development long enough, or even just the\nbusiness world in general, then you will likely have heard of the Project\nTriangle. This is the saying of: \"Good, Fast, or Cheap. Pick two.\" It is unclear\nas to where this phrase originates, but it has been used in project management\ncircles since at least the 1950s - and it applies to developer relations as\nwell.\n\nThe Project Triangle\nThe general idea behind the project triangle goes something like:\n\n * To get something done quickly, and with high quality, will be very costly\n * To get something done quickly, and with low cost, means it will not be of\n   great quality\n * To get something of high quality, done at a low cost, will take a long time\n\nThere are a lot of humorous takes on this, usually depicted as a Venn diagram.\nWhere the circles overlap is usually marked as \"does not exist\" or the likes.\n\nWhen it comes to developer relations, the \"something\" is usually building\ncommunity. Building community is generally viewed as a slow, and intentional\nprocess - the long game. It does not have to take a long time, but that means a\nshift in how the other two constants relate. Let us take a look at how these\nbreak down.\n\nFast and Good, Not Cheap\nIf you want to build a community quickly, and you want it to be a quality\ncommunity - one in which you have a meaningful dialog with your customers - then\nit will not be cheap. While there are certainly a list of ways to spend that\nmoney, two specific scenarios I have seen in recent years come to mind.\n\nThe first scenario is to hire well-known talent (good, but not cheap) in the\nareas that you are looking for growth. As an example, these people might be\nwell-known contributors on popular open source projects. Said people will most\ncertainly come with their own community, which quickly (fast) gives you an\naudience of interested developers with which to dialog.\n\nAnother scenario might be to acquire (good, but not cheap) a company that has\nalready built a community of its own. These developers will most certainly want\nto know what it is that you have planned for their favorite product(s) after the\nclosing, giving you an open door to productive dialog (fast).\n\nFast and Cheap, Not Good\nThis is what I consider to be the marketing-heavy approach.\n\nSocial media gives just about anybody a massive megaphone through which to shout\ntheir message - or in the case of developer relations, about their product. A\nfew dollars will go a long ways to boost search engine rankings. You might take\na stab at a viral video, or project. Or maybe you just cross-link the heck out\nof a several dozen blog posts on different media outlets (channels). There are a\nmyriad of [fast and cheap] approaches to take here.\n\nInevitably, some developers will hear your megaphone. Some may find your message\ncompelling, and give your product a try. These developers however, will not\ngenerally be particularly invested in your offering (not good) - at least not at\nfirst. This means that having a dialog with them will be difficult, and that the\nfeedback you receive will be just as shallow as your initial marketing blast.\n\nThat is not to say that this is a bad approach; indeed it is ideal for startups\nand smaller companies that cannot afford a dedicated developer relations staff.\nYou do however have to be realistic about the resulting quality of community.\n\nGood and Cheap, Not Quick\nDeveloper relations does not have to be expensive (cheap), but building a\nquality dialog (good) with your customers will take time (not quick).\n\nThat time investment means more than hosting a monthly meetup, or speaking at\nthe occasional event - it means building quality documentation, it means making\nsure the barrier to entry is as low as possible. It means putting the time into\nthe quality of your product up front. It means hearing, and addressing every\nsingle piece of feedback, through every possible channel. And so much more.\n\nDevelopers do not often have a lot of time to experiment with new technologies.\nThis usually happens in what most companies call \"10% time\" which might mean a\nFriday afternoon (four hours of a forty-hour week is ten percent). If you have\ninvested the time in your product (including documentation), then using it will\nbe like magic. Problems that might have seemed too big for an afternoon,\nsuddenly become child's play.\n\nCongratulations, you are half-way there ...\n\nNow comes the time to start a dialog with those developers entranced by the\nmagic of your product. Eventually (not fast), a passionate (good) community will\nform.\n\nA Friend in Need ...\nI sometimes take issue with the term \"developer\" as used by most companies. The\nterm can dehumanize the very real people spending valuable hours of their lives\nusing your product. Regardless of the situation, do you like it when a company\n(or person) wastes your time? Same difference.\n\nI like to think of developer communities as my friends - after all, it is only\nin that setting where I can truly get my geek on, and in which we can have an\nopen exchange of ideas. I would never intentionally waste the time of a friend.\nTo that end, how good can a friendship be if ...\n\n * You spend your lunch hour together every day at the finest restaurant in\n   town?\n * You buy them a Happy Meal at McDonald's once a month?\n * You share a bag of granola while spending the weekend on a hiking trip?\n\nFurther yet, how does one even quantify a friendship? Can you say that Bob is\nworth $10 but Linda is worth $100? Answering \"yes\" to this question is where\nmany companies go wrong with developer relations. If you commit to these types\nof quantifications, then you have just turned a relationship into a number, and\ntaken a sharp turn into the world of sales.\n\nBack to that Triangle Thing ...\nTo be sure, none of this is quite as cut and dry as we might like it to be.\nSometimes you may find yourself spending more time on product, and other times\non marketing blasts. At other times, you may look into acquisition. Each of\nthese are integral parts of doing business, but it is important to recognize\nthat each comes with trade-offs - even in developer relations.","feature_image":null,"featured":0,"status":"draft","locale":null,"visibility":"public","author_id":"1","created_at":"2018-02-08T07:34:48.000Z","updated_at":"2018-02-08T10:03:30.000Z","published_at":null,"custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"type":"post","email_recipient_filter":"none"},{"id":"5adb8922351ffe0018a57760","uuid":"7b33c7a7-d5f5-4317-a77a-03ad7e017f19","title":"Teaching Watson to See My Garage","slug":"teaching-watson-to-see-my-garage","mobiledoc":"{\"version\":\"0.3.1\",\"markups\":[],\"atoms\":[],\"cards\":[[\"markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\"After a long day of work, I like to head out to my garage and tinker, binge the latest series, listen to a podcast, or settle in with a book. If the weather is right, I will open the garage door. Eventually I will head to bed. Then it hits me - did I close the garage door?\\n\\nThe last thing I want to do is get up, put on some clothes, head downstairs to the garage, and check it. By the time I get back to bed, I have to settle back down before drifting off to sleep. If only Watson could watch the garage for me.\\n\\n###Teaching Watson\\n\\n[Watson Visual Recognition](https://www.ibm.com/watson/services/visual-recognition/) can \\\"see\\\" a lot of things right out of the box - including some ==brand logos==, and ==celebrities==. The real power of Visual Recognition however surfaces when you train Watson to see what it is that you want it to see. This feature is called \\\"==Custom Classifiers==\\\". Creating a custom classifier has a few steps.\\n\\n1. **Capture** or assemble a set of images that contain the content you want to teach (train) Watson to see.\\n\\n2. **Manipulate** the images to best represent your content (optional). There is a lot of content in some images, and you want to pare down to your specific content.\\n\\n3. **Bundle and train** Watson on your images. The creates the classifier, and will be available only to your account.\\n\\n4. **Send** Watson an image you want to classify. This may require additional image tuning en route.\\n\\n###Capture\\n\\nTo capture images of my garage I needed a camera, so I stuck a [Raspberry Pi Zero](https://www.sparkfun.com/products/14329) with ==camera module== on the wall of my garage. I wrote a ==Python== script to take a picture once every five (5) minutes for an entire day, and upload it to [IBM Cloud Object Storage](https://www.ibm.com/cloud/object-storage) (==S3-compatible==).\\n\\n![The whole garage.](http://images.kevinhoyt.com/garagepi.whole.jpg)\\n\\nWhy one image every five minutes for an entire day? The garage does not change that much over time. The Watson documentation on custom classifiers suggest that a training sample will yield the best results at around ==300 images==. There are **24 hours** in a day, or **1,440 minutes**. Divided by **300**, that gives us **4.8** - so once every five minutes should yield enough images.\\n\\nI also wanted to capture the garage door ==open and closed==, during both the ==day time and during the night time==. To get this I needed at least a full day to start. All said and done, I got enough images to yield an accurate training model. If I needed more, I could run the script again, until I had all the images I needed.\\n\\n###Manipulate\\n\\nIn the case of the above photo of my garage, there is a lot to \\\"see\\\". There are two cars. There is a wall with a bunch of objects on and near it. The ceiling with the actual garage door opener. And somewhere, in the distance is the actual garage door. ==What is it that we want to actually teach Watson?==\\n\\n>To the human eye, it is easy to see that the garage door is open, but Watson needs little more direction. \\n\\nTo solve this problem, I wrote a Python script to download the images from IBM Cloud Object Storage, and further process them.\\n\\nThe documentation states that a good image is ==200x200 pixels==. It turns out that you do not need the full resolution of the camera image. The first step then is to resize the entire image down to ==640x480 pixels==. The second step then extracts only a ==specific portion== of the image - the area behind the cars, where the human eye can see the garage door as open or closed.\\n\\n![The part of the image we are interested in using.](http://images.kevinhoyt.com/garagepi.crop.jpg)\\n\\nI arrived at 640x480 through ==trial and error==. I tried 1024x768, and then looked at the cropped area. It was still really big - far larger than the 200-ish the documentation recommends. Then 800x600. Still a big on the large side. When I got down to 640x480, the portion of the image which shows the state of the garage door was about ==350x85==. Close enough.\\n\\n![Just the garage, please.](http://images.kevinhoyt.com/garagepi.garage.jpg)\\n\\nI should mention that [Pillow](https://pillow.readthedocs.io/en/5.1.x/), a fork of the [Python Image Library](http://www.pythonware.com/products/pil/) (PIL) made the work of sizing and cropping just a few lines of code.\\n\\n###Bundle and Train\\n\\nOnce I had all the samples sized and cropped, I pulled them (manually) into folders for \\\"==open day==\\\", \\\"==open night==\\\", \\\"==closed day==\\\", and \\\"==closed night==\\\". From there I bundled each into an ==archive file== (zip).  From there, a little ==cURL== action against the Watson API for training, was a single call.\\n\\n```\\n## Create Classifier\\ncurl -X \\\"POST\\\" \\\"https://gateway-a.watsonplatform.net/visual-recognition/api/v3/classifiers?api_key=YOUR_API_KEY&version=2016-05-20\\\" \\\\\\n  -H 'Content-Type: multipart/form-data; charset=utf-8;' \\\\\\n  -F \\\"closed_day_positive_examples=@closed.day.zip\\\" \\\\\\n  -F \\\"open_day_positive_examples=@open.day.zip\\\" \\\\\\n  -F \\\"closed_night_positive_examples=@closed.night.zip\\\" \\\\\\n  -F \\\"open_night_positive_examples=@open.night.zip\\\" \\\\\\n  -F \\\"name=garage\\\"\\n```\\n\\nIt can take some time for Watson to build the fully trained model. This makes sense if you are looking at 200-ish images per state, with four (4) states. That is about 800 images for Watson to process. For my dataset it took about five minutes. You can run the following script to ==check the progress== along the way. You should not train or test against the classifier while it is being built.\\n\\n```\\n## Classifier Details\\ncurl \\\"https://gateway-a.watsonplatform.net/visual-recognition/api/v3/classifiers/CLASSIFIER_ID?api_key=YOUR_API_KEY&version=2016-05-20\\\"\\n```\\n\\nWhile I have put the cURL here for review, I personally recommend the awesome [Paw](https://paw.cloud/) tool. I have folders in Paw for nearly a dozen difference IBM Cloud APIs - each folder with as many \\\"requests\\\" as needed to fully utilize that API. Whenever I need to use that API, I just open Paw, and all the hard work is done. What is better is that Paw will ==generate== cURL, or Python, or ... take your pick ... ==sample code== for you to use.\\n\\n###Send To Watson\\n\\nNow back to the ==Raspberry Pi== setup. Here I put one more Python script. The script serves two purposes.\\n\\nThe first thing it does is ==take a picture, and store it locally, every five minutes== (keeping with the aforementioned maths). The Pi camera can take a moment start up, and I did not want to incur that in my calls to Watson. I also did not want to process images when I did not need to - using the API call limits of my ==Lite== (**free**) tier.\\n\\nThe second thing the Python script on the Pi does is connect to [Watson IoT](https://www.ibm.com/internet-of-things) (MQTT). This lets the Pi sit there and ==wait== for a command to process the most recently captured image.\\n\\nWhen the command comes in, the script ==sends the image to Watson Visual Recognition for processing==. When a result has arrived from Watson, the Pi sends an ==MQTT event== with the pertinent data. I leave it up to the client implementation to decide what that means, and what to do with the results.\\n\\n```\\n## Classify Image\\ncurl -X \\\"POST\\\" \\\"https://gateway-a.watsonplatform.net/visual-recognition/api/v3/classify?api_key=YOUR_API_KEY&version=2016-05-20\\\" \\\\\\n  -H 'Content-Type: multipart/form-data; charset=utf-8;' \\\\\\n  -F \\\"images_file=@garage.jpg\\\" \\\\\\n  -F \\\"classifier_ids=CLASSIFIER_ID\\\"\\n```\\n\\nDecoupling the device (Pi) from the client implementation, allows me to integrate with a Progressive Web Application (PWA), native application, and/or voice systems such as Amazon Alexa.\\n\\n###Next Steps\\n\\nNow when I am lying in bed, and want to know if the garage is open, I can simply say \\\"Alexa, ask Tennyson about the garage door.\\\" The result, on a good day, is \\\"The garage door is closed.\\\" ==Off to sleep I go.==\\n\\nThe next problem to solve with this project is to ==close the garage door for me== if I am lying in bed, and find that I forgot to close it.  This would surface as another command for the Pi to monitor. \\\"Alexa, ask Tennyson to close the garage door.\\\" Then it would activate a relay hooked to the garage door opener itself.\\n\\nWhere things start getting really interesting is in running the classifier when every image is taken, and ==storing the results in a database==. Then I could teach Watson to know that the garage door should not be open in the first place, and ==close it for me automatically==.\\n\\nWhile the ==Lite== (**free**) account for Watson Visual Recognition has a limit of a ==single custom classifier==, I want to eventually add ==two more== classifiers - one for where ==my car== is in the image, and one for where my ==wife's car== is in the image. Then I can even ask Watson if my wife is home from anywhere in the world.\\n\\n###What About ... ?\\n\\nYou might be asking \\\"Why not just use **IoT** sensors to detect the state of the garage?\\\" I thought about this for a long time. Maybe some magnets and Hall sensors on the rails of the door? Maybe an ultrasonic range sensor that would yield one distance when the door was open, and another if it was closed?\\n\\nSure! These are possibilities, but powering the sensors, and integrating them with a master involves a lot more engineering. \\n\\n==In the end, I wanted to best model how I actually work as a human.== How I work as a human is look out to the garage, and see, with my eyes, the state of the door. A single Pi with a camera, mounted on the wall is the closest approximation to that - and it costs about $70 USD to get going.\\n\"}]],\"sections\":[[10,0]],\"ghostVersion\":\"3.0\"}","html":"<!--kg-card-begin: markdown--><p>After a long day of work, I like to head out to my garage and tinker, binge the latest series, listen to a podcast, or settle in with a book. If the weather is right, I will open the garage door. Eventually I will head to bed. Then it hits me - did I close the garage door?</p>\n<p>The last thing I want to do is get up, put on some clothes, head downstairs to the garage, and check it. By the time I get back to bed, I have to settle back down before drifting off to sleep. If only Watson could watch the garage for me.</p>\n<h3 id=\"teachingwatson\">Teaching Watson</h3>\n<p><a href=\"https://www.ibm.com/watson/services/visual-recognition/\">Watson Visual Recognition</a> can &quot;see&quot; a lot of things right out of the box - including some <mark>brand logos</mark>, and <mark>celebrities</mark>. The real power of Visual Recognition however surfaces when you train Watson to see what it is that you want it to see. This feature is called &quot;<mark>Custom Classifiers</mark>&quot;. Creating a custom classifier has a few steps.</p>\n<ol>\n<li>\n<p><strong>Capture</strong> or assemble a set of images that contain the content you want to teach (train) Watson to see.</p>\n</li>\n<li>\n<p><strong>Manipulate</strong> the images to best represent your content (optional). There is a lot of content in some images, and you want to pare down to your specific content.</p>\n</li>\n<li>\n<p><strong>Bundle and train</strong> Watson on your images. The creates the classifier, and will be available only to your account.</p>\n</li>\n<li>\n<p><strong>Send</strong> Watson an image you want to classify. This may require additional image tuning en route.</p>\n</li>\n</ol>\n<h3 id=\"capture\">Capture</h3>\n<p>To capture images of my garage I needed a camera, so I stuck a <a href=\"https://www.sparkfun.com/products/14329\">Raspberry Pi Zero</a> with <mark>camera module</mark> on the wall of my garage. I wrote a <mark>Python</mark> script to take a picture once every five (5) minutes for an entire day, and upload it to <a href=\"https://www.ibm.com/cloud/object-storage\">IBM Cloud Object Storage</a> (<mark>S3-compatible</mark>).</p>\n<p><img src=\"http://images.kevinhoyt.com/garagepi.whole.jpg\" alt=\"The whole garage.\" loading=\"lazy\"></p>\n<p>Why one image every five minutes for an entire day? The garage does not change that much over time. The Watson documentation on custom classifiers suggest that a training sample will yield the best results at around <mark>300 images</mark>. There are <strong>24 hours</strong> in a day, or <strong>1,440 minutes</strong>. Divided by <strong>300</strong>, that gives us <strong>4.8</strong> - so once every five minutes should yield enough images.</p>\n<p>I also wanted to capture the garage door <mark>open and closed</mark>, during both the <mark>day time and during the night time</mark>. To get this I needed at least a full day to start. All said and done, I got enough images to yield an accurate training model. If I needed more, I could run the script again, until I had all the images I needed.</p>\n<h3 id=\"manipulate\">Manipulate</h3>\n<p>In the case of the above photo of my garage, there is a lot to &quot;see&quot;. There are two cars. There is a wall with a bunch of objects on and near it. The ceiling with the actual garage door opener. And somewhere, in the distance is the actual garage door. <mark>What is it that we want to actually teach Watson?</mark></p>\n<blockquote>\n<p>To the human eye, it is easy to see that the garage door is open, but Watson needs little more direction.</p>\n</blockquote>\n<p>To solve this problem, I wrote a Python script to download the images from IBM Cloud Object Storage, and further process them.</p>\n<p>The documentation states that a good image is <mark>200x200 pixels</mark>. It turns out that you do not need the full resolution of the camera image. The first step then is to resize the entire image down to <mark>640x480 pixels</mark>. The second step then extracts only a <mark>specific portion</mark> of the image - the area behind the cars, where the human eye can see the garage door as open or closed.</p>\n<p><img src=\"http://images.kevinhoyt.com/garagepi.crop.jpg\" alt=\"The part of the image we are interested in using.\" loading=\"lazy\"></p>\n<p>I arrived at 640x480 through <mark>trial and error</mark>. I tried 1024x768, and then looked at the cropped area. It was still really big - far larger than the 200-ish the documentation recommends. Then 800x600. Still a big on the large side. When I got down to 640x480, the portion of the image which shows the state of the garage door was about <mark>350x85</mark>. Close enough.</p>\n<p><img src=\"http://images.kevinhoyt.com/garagepi.garage.jpg\" alt=\"Just the garage, please.\" loading=\"lazy\"></p>\n<p>I should mention that <a href=\"https://pillow.readthedocs.io/en/5.1.x/\">Pillow</a>, a fork of the <a href=\"http://www.pythonware.com/products/pil/\">Python Image Library</a> (PIL) made the work of sizing and cropping just a few lines of code.</p>\n<h3 id=\"bundleandtrain\">Bundle and Train</h3>\n<p>Once I had all the samples sized and cropped, I pulled them (manually) into folders for &quot;<mark>open day</mark>&quot;, &quot;<mark>open night</mark>&quot;, &quot;<mark>closed day</mark>&quot;, and &quot;<mark>closed night</mark>&quot;. From there I bundled each into an <mark>archive file</mark> (zip).  From there, a little <mark>cURL</mark> action against the Watson API for training, was a single call.</p>\n<pre><code>## Create Classifier\ncurl -X &quot;POST&quot; &quot;https://gateway-a.watsonplatform.net/visual-recognition/api/v3/classifiers?api_key=YOUR_API_KEY&amp;version=2016-05-20&quot; \\\n  -H 'Content-Type: multipart/form-data; charset=utf-8;' \\\n  -F &quot;closed_day_positive_examples=@closed.day.zip&quot; \\\n  -F &quot;open_day_positive_examples=@open.day.zip&quot; \\\n  -F &quot;closed_night_positive_examples=@closed.night.zip&quot; \\\n  -F &quot;open_night_positive_examples=@open.night.zip&quot; \\\n  -F &quot;name=garage&quot;\n</code></pre>\n<p>It can take some time for Watson to build the fully trained model. This makes sense if you are looking at 200-ish images per state, with four (4) states. That is about 800 images for Watson to process. For my dataset it took about five minutes. You can run the following script to <mark>check the progress</mark> along the way. You should not train or test against the classifier while it is being built.</p>\n<pre><code>## Classifier Details\ncurl &quot;https://gateway-a.watsonplatform.net/visual-recognition/api/v3/classifiers/CLASSIFIER_ID?api_key=YOUR_API_KEY&amp;version=2016-05-20&quot;\n</code></pre>\n<p>While I have put the cURL here for review, I personally recommend the awesome <a href=\"https://paw.cloud/\">Paw</a> tool. I have folders in Paw for nearly a dozen difference IBM Cloud APIs - each folder with as many &quot;requests&quot; as needed to fully utilize that API. Whenever I need to use that API, I just open Paw, and all the hard work is done. What is better is that Paw will <mark>generate</mark> cURL, or Python, or ... take your pick ... <mark>sample code</mark> for you to use.</p>\n<h3 id=\"sendtowatson\">Send To Watson</h3>\n<p>Now back to the <mark>Raspberry Pi</mark> setup. Here I put one more Python script. The script serves two purposes.</p>\n<p>The first thing it does is <mark>take a picture, and store it locally, every five minutes</mark> (keeping with the aforementioned maths). The Pi camera can take a moment start up, and I did not want to incur that in my calls to Watson. I also did not want to process images when I did not need to - using the API call limits of my <mark>Lite</mark> (<strong>free</strong>) tier.</p>\n<p>The second thing the Python script on the Pi does is connect to <a href=\"https://www.ibm.com/internet-of-things\">Watson IoT</a> (MQTT). This lets the Pi sit there and <mark>wait</mark> for a command to process the most recently captured image.</p>\n<p>When the command comes in, the script <mark>sends the image to Watson Visual Recognition for processing</mark>. When a result has arrived from Watson, the Pi sends an <mark>MQTT event</mark> with the pertinent data. I leave it up to the client implementation to decide what that means, and what to do with the results.</p>\n<pre><code>## Classify Image\ncurl -X &quot;POST&quot; &quot;https://gateway-a.watsonplatform.net/visual-recognition/api/v3/classify?api_key=YOUR_API_KEY&amp;version=2016-05-20&quot; \\\n  -H 'Content-Type: multipart/form-data; charset=utf-8;' \\\n  -F &quot;images_file=@garage.jpg&quot; \\\n  -F &quot;classifier_ids=CLASSIFIER_ID&quot;\n</code></pre>\n<p>Decoupling the device (Pi) from the client implementation, allows me to integrate with a Progressive Web Application (PWA), native application, and/or voice systems such as Amazon Alexa.</p>\n<h3 id=\"nextsteps\">Next Steps</h3>\n<p>Now when I am lying in bed, and want to know if the garage is open, I can simply say &quot;Alexa, ask Tennyson about the garage door.&quot; The result, on a good day, is &quot;The garage door is closed.&quot; <mark>Off to sleep I go.</mark></p>\n<p>The next problem to solve with this project is to <mark>close the garage door for me</mark> if I am lying in bed, and find that I forgot to close it.  This would surface as another command for the Pi to monitor. &quot;Alexa, ask Tennyson to close the garage door.&quot; Then it would activate a relay hooked to the garage door opener itself.</p>\n<p>Where things start getting really interesting is in running the classifier when every image is taken, and <mark>storing the results in a database</mark>. Then I could teach Watson to know that the garage door should not be open in the first place, and <mark>close it for me automatically</mark>.</p>\n<p>While the <mark>Lite</mark> (<strong>free</strong>) account for Watson Visual Recognition has a limit of a <mark>single custom classifier</mark>, I want to eventually add <mark>two more</mark> classifiers - one for where <mark>my car</mark> is in the image, and one for where my <mark>wife's car</mark> is in the image. Then I can even ask Watson if my wife is home from anywhere in the world.</p>\n<h3 id=\"whatabout\">What About ... ?</h3>\n<p>You might be asking &quot;Why not just use <strong>IoT</strong> sensors to detect the state of the garage?&quot; I thought about this for a long time. Maybe some magnets and Hall sensors on the rails of the door? Maybe an ultrasonic range sensor that would yield one distance when the door was open, and another if it was closed?</p>\n<p>Sure! These are possibilities, but powering the sensors, and integrating them with a master involves a lot more engineering.</p>\n<p><mark>In the end, I wanted to best model how I actually work as a human.</mark> How I work as a human is look out to the garage, and see, with my eyes, the state of the door. A single Pi with a camera, mounted on the wall is the closest approximation to that - and it costs about $70 USD to get going.</p>\n<!--kg-card-end: markdown-->","comment_id":"89","plaintext":"After a long day of work, I like to head out to my garage and tinker, binge the\nlatest series, listen to a podcast, or settle in with a book. If the weather is\nright, I will open the garage door. Eventually I will head to bed. Then it hits\nme - did I close the garage door?\n\nThe last thing I want to do is get up, put on some clothes, head downstairs to\nthe garage, and check it. By the time I get back to bed, I have to settle back\ndown before drifting off to sleep. If only Watson could watch the garage for me.\n\nTeaching Watson\nWatson Visual Recognition\n[https://www.ibm.com/watson/services/visual-recognition/] can \"see\" a lot of\nthings right out of the box - including some brand logos, and celebrities. The\nreal power of Visual Recognition however surfaces when you train Watson to see\nwhat it is that you want it to see. This feature is called \"Custom Classifiers\".\nCreating a custom classifier has a few steps.\n\n 1. Capture or assemble a set of images that contain the content you want to\n    teach (train) Watson to see.\n    \n    \n 2. Manipulate the images to best represent your content (optional). There is a\n    lot of content in some images, and you want to pare down to your specific\n    content.\n    \n    \n 3. Bundle and train Watson on your images. The creates the classifier, and will\n    be available only to your account.\n    \n    \n 4. Send Watson an image you want to classify. This may require additional image\n    tuning en route.\n    \n    \n\nCapture\nTo capture images of my garage I needed a camera, so I stuck a Raspberry Pi Zero\n[https://www.sparkfun.com/products/14329] with camera module on the wall of my\ngarage. I wrote a Python script to take a picture once every five (5) minutes\nfor an entire day, and upload it to IBM Cloud Object Storage\n[https://www.ibm.com/cloud/object-storage] (S3-compatible).\n\n\n\nWhy one image every five minutes for an entire day? The garage does not change\nthat much over time. The Watson documentation on custom classifiers suggest that\na training sample will yield the best results at around 300 images. There are 24\nhours in a day, or 1,440 minutes. Divided by 300, that gives us 4.8 - so once\nevery five minutes should yield enough images.\n\nI also wanted to capture the garage door open and closed, during both the day\ntime and during the night time. To get this I needed at least a full day to\nstart. All said and done, I got enough images to yield an accurate training\nmodel. If I needed more, I could run the script again, until I had all the\nimages I needed.\n\nManipulate\nIn the case of the above photo of my garage, there is a lot to \"see\". There are\ntwo cars. There is a wall with a bunch of objects on and near it. The ceiling\nwith the actual garage door opener. And somewhere, in the distance is the actual\ngarage door. What is it that we want to actually teach Watson?\n\n> To the human eye, it is easy to see that the garage door is open, but Watson\nneeds little more direction.\n\n\nTo solve this problem, I wrote a Python script to download the images from IBM\nCloud Object Storage, and further process them.\n\nThe documentation states that a good image is 200x200 pixels. It turns out that\nyou do not need the full resolution of the camera image. The first step then is\nto resize the entire image down to 640x480 pixels. The second step then extracts\nonly a specific portion of the image - the area behind the cars, where the human\neye can see the garage door as open or closed.\n\n\n\nI arrived at 640x480 through trial and error. I tried 1024x768, and then looked\nat the cropped area. It was still really big - far larger than the 200-ish the\ndocumentation recommends. Then 800x600. Still a big on the large side. When I\ngot down to 640x480, the portion of the image which shows the state of the\ngarage door was about 350x85. Close enough.\n\n\n\nI should mention that Pillow [https://pillow.readthedocs.io/en/5.1.x/], a fork\nof the Python Image Library [http://www.pythonware.com/products/pil/] (PIL) made\nthe work of sizing and cropping just a few lines of code.\n\nBundle and Train\nOnce I had all the samples sized and cropped, I pulled them (manually) into\nfolders for \"open day\", \"open night\", \"closed day\", and \"closed night\". From\nthere I bundled each into an archive file (zip). From there, a little cURL \naction against the Watson API for training, was a single call.\n\n## Create Classifier\ncurl -X \"POST\" \"https://gateway-a.watsonplatform.net/visual-recognition/api/v3/classifiers?api_key=YOUR_API_KEY&version=2016-05-20\" \\\n  -H 'Content-Type: multipart/form-data; charset=utf-8;' \\\n  -F \"closed_day_positive_examples=@closed.day.zip\" \\\n  -F \"open_day_positive_examples=@open.day.zip\" \\\n  -F \"closed_night_positive_examples=@closed.night.zip\" \\\n  -F \"open_night_positive_examples=@open.night.zip\" \\\n  -F \"name=garage\"\n\n\nIt can take some time for Watson to build the fully trained model. This makes\nsense if you are looking at 200-ish images per state, with four (4) states. That\nis about 800 images for Watson to process. For my dataset it took about five\nminutes. You can run the following script to check the progress along the way.\nYou should not train or test against the classifier while it is being built.\n\n## Classifier Details\ncurl \"https://gateway-a.watsonplatform.net/visual-recognition/api/v3/classifiers/CLASSIFIER_ID?api_key=YOUR_API_KEY&version=2016-05-20\"\n\n\nWhile I have put the cURL here for review, I personally recommend the awesome \nPaw [https://paw.cloud/] tool. I have folders in Paw for nearly a dozen\ndifference IBM Cloud APIs - each folder with as many \"requests\" as needed to\nfully utilize that API. Whenever I need to use that API, I just open Paw, and\nall the hard work is done. What is better is that Paw will generate cURL, or\nPython, or ... take your pick ... sample code for you to use.\n\nSend To Watson\nNow back to the Raspberry Pi setup. Here I put one more Python script. The\nscript serves two purposes.\n\nThe first thing it does is take a picture, and store it locally, every five\nminutes (keeping with the aforementioned maths). The Pi camera can take a moment\nstart up, and I did not want to incur that in my calls to Watson. I also did not\nwant to process images when I did not need to - using the API call limits of my \nLite (free) tier.\n\nThe second thing the Python script on the Pi does is connect to Watson IoT\n[https://www.ibm.com/internet-of-things] (MQTT). This lets the Pi sit there and \nwait for a command to process the most recently captured image.\n\nWhen the command comes in, the script sends the image to Watson Visual\nRecognition for processing. When a result has arrived from Watson, the Pi sends\nan MQTT event with the pertinent data. I leave it up to the client\nimplementation to decide what that means, and what to do with the results.\n\n## Classify Image\ncurl -X \"POST\" \"https://gateway-a.watsonplatform.net/visual-recognition/api/v3/classify?api_key=YOUR_API_KEY&version=2016-05-20\" \\\n  -H 'Content-Type: multipart/form-data; charset=utf-8;' \\\n  -F \"images_file=@garage.jpg\" \\\n  -F \"classifier_ids=CLASSIFIER_ID\"\n\n\nDecoupling the device (Pi) from the client implementation, allows me to\nintegrate with a Progressive Web Application (PWA), native application, and/or\nvoice systems such as Amazon Alexa.\n\nNext Steps\nNow when I am lying in bed, and want to know if the garage is open, I can simply\nsay \"Alexa, ask Tennyson about the garage door.\" The result, on a good day, is\n\"The garage door is closed.\" Off to sleep I go.\n\nThe next problem to solve with this project is to close the garage door for me \nif I am lying in bed, and find that I forgot to close it. This would surface as\nanother command for the Pi to monitor. \"Alexa, ask Tennyson to close the garage\ndoor.\" Then it would activate a relay hooked to the garage door opener itself.\n\nWhere things start getting really interesting is in running the classifier when\nevery image is taken, and storing the results in a database. Then I could teach\nWatson to know that the garage door should not be open in the first place, and \nclose it for me automatically.\n\nWhile the Lite (free) account for Watson Visual Recognition has a limit of a \nsingle custom classifier, I want to eventually add two more classifiers - one\nfor where my car is in the image, and one for where my wife's car is in the\nimage. Then I can even ask Watson if my wife is home from anywhere in the world.\n\nWhat About ... ?\nYou might be asking \"Why not just use IoT sensors to detect the state of the\ngarage?\" I thought about this for a long time. Maybe some magnets and Hall\nsensors on the rails of the door? Maybe an ultrasonic range sensor that would\nyield one distance when the door was open, and another if it was closed?\n\nSure! These are possibilities, but powering the sensors, and integrating them\nwith a master involves a lot more engineering.\n\nIn the end, I wanted to best model how I actually work as a human. How I work as\na human is look out to the garage, and see, with my eyes, the state of the door.\nA single Pi with a camera, mounted on the wall is the closest approximation to\nthat - and it costs about $70 USD to get going.","feature_image":"__GHOST_URL__/content/images/2019/01/watson.jpg","featured":0,"status":"published","locale":null,"visibility":"public","author_id":"1","created_at":"2018-04-17T21:04:57.000Z","updated_at":"2019-01-17T00:31:04.000Z","published_at":"2018-04-17T22:59:05.000Z","custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"type":"post","email_recipient_filter":"none"},{"id":"5adb8abd68339a00221450c0","uuid":"b3eb7d74-01b6-48e2-b70e-c0e9a611ad6f","title":"Cigar Lounges","slug":"lounge","mobiledoc":"{\"version\":\"0.3.1\",\"atoms\":[],\"cards\":[[\"markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\"In this day of negative sentiment towards smoking you might be inclined to think that the cigar lounge would be a dying breed. As bars, restaurants, and public spaces alike ban smoking, brothers of the leaf (BOTL) find sanctuary in their local cigar lounge. \\n\\nAs a frequent traveler, I find myself in the position of getting to experience the character of these fine establishments all over the world. This then is my ode to the character, and characters, of the places I have encountered along the way.\\n\\n###Boisdale of Canary Wharf\\n*15 Cabot Square, Canary Wharf, London E14 4QT, UK*\\nLast visited: July 2, 2017\\n\\n![Go Scottish with Scotch.](http://images.kevinhoyt.com/boisdale.canary.jpg)\\n\\nThere are several [Boisdale](https://www.boisdale.co.uk) locations around London, all of which are smoker friendly. The Canary Wharf location features a plush, enclosed, lounge where you can get high-end food, drink, and cigars. The Scottish decor begs a single malt pairing. An attendant will see to your every need along the way.\\n\\n###Cigar Bar and Grill\\n*850 Montgomery St, San Francisco, CA 94133*\\nLast visited: April 24, 2018\\n\\n![Cigars. Downtown. SF.](http://images.kevinhoyt.com/cigar.bar.sf.jpg)\\n\\nCigar smoking, food, and liquor in downtown San Francisco, CA? You bet! Formally a restaurant with live music, salsa lessons, and more, attached to an open-air, heated, patio. The food is really good, if a little on the expenseive side. And the same could be said for the small humidor.\\n\\n###Cigars at No. 10\\n*10 Manchester St, Marylebone, London W1U 4DG, UK*\\nLast visited: July 17, 2017\\n\\n![High class. Or. Not.](http://images.kevinhoyt.com/cigars.at.no.ten.jpg)\\n\\nTechnically a hotel, with an outstanding restaurant, there is a small-ish, but very comfortable outdoor smoking area (COSA) in the back. The area is heated, with a privacy screen for your pleasure. The staff is very attentive, and highly trained. They will escort you to their humidor, which is well-stocked with a large assortment of Cuban cigars. Just be prepared for the bespoke price tag.\\n\\n###Havana Cigar Lounge\\n*4850 W Flamingo Rd #18, Las Vegas, NV 89103*\\nLast visited: February 21, 2018\\n\\n![Dark. Plush. Classic.](http://images.kevinhoyt.com/havana.cigar.vegas.jpg)\\n\\nOn a rare, rainy, late afternoon in Las Vegas, I slipped off the strip to Havana Cigar Lounge. The doors at the front and the back of the store were propped open. A cool breeze drifted through keeping the air fresh. The humidor was lightly stocked, but had a respectable variety. The host popped in a video showcasing James J. Fox in London - another lounge in this list.\\n\\n###Havana Garage\\n*1008 Howard St, Omaha, NE 68102*\\nLast visited: September 6, 2018\\n\\n![People watching on the Old Market.](http://images.kevinhoyt.com/havana.garage.omaha.jpg)\\n\\nThe Old Market in downtown Omaha, Nebraska, is a trendy, night-life destination pretty much any day of the week. Nestled on the brick-lined main street, is Havana Garage, where you can have a cigar, choose from a wide variety of alcoholic beverages, and people watch the night away. Well stocked, if a little pricey, humidor in the basement.\\n\\n###Habana House Cigar Lounge\\n*13729 Research Blvd #1075, Austin, TX 78750*\\nLast visited: August 26, 2019\\n\\n![Walk right in, sit right down.](http://images.kevinhoyt.com/habana.house.austin.jpg)\\n\\nThis lounge has three distinct seating areas, each with its own set of televisions. Pick your area - news, sports, history, fiction. The massive walk-in humidor will be sure to offer something to fit your palate. A variety of food options are within walking distance (even in the Austin summer). Spacious patio in the front.\\n\\n###James J. Fox\\n*19 St James's St, St. James's, London SW1A 1ES, UK*\\nLast visited: August 26, 2017\\n\\n![Sit in the same chair as Churchill himself.](http://images.kevinhoyt.com/james.j.fox.jpg)\\n\\nDo you like a churchill? Would you like to sit in the chair, in the lounge, where Winston Churchill would visit for a cigar? Welcome home! With a cigar museum, fantastic selection and service, and a spacious lounge, it is hard to beat the historic pleasure of [James J. Fox](https://www.jjfox.co.uk/).\\n\\n###Maduros\\n*2997 Cumberland Blvd SE, Atlanta, GA 30339*\\nLast visited: March 6, 2019\\n\\n![Get your funky drink on.](http://images.kevinhoyt.com/maduros.jpg)\\n\\nIf you are catching a Braves game, or are in the Galleria area for a conference, and you want to grab a cigar, an adult beverage, and get your funk on, then you have come to the right spot. Part night club, part cigar lounge, [Maduros](https://maduroscigarshop.com) has a smallish humidor, but with enough variety for every stogie lover. And did I mention adult beverages?\\n\\n###Orlando Cigar and Tobacco\\n*9924 Universal Blvd #218, Orlando, FL 32819*\\nLast visited: July 21, 2019\\n\\n![Theme park getaway for the suburban dad.](http://images.kevinhoyt.com/orlando.cigars.tobacco.jpg)\\n\\nIf you are in Orlando, FL, and looking for the quintessential cigar lounge - this is not it. If you are a suburban dad, booked in the VRBO at [Vista Cay](https://www.vistacayholidays.com) across the street, and looking for a little escape from theme park madness and kids - you have found your nirvana. Pick up a beverage at the Publix on the way in, or have some tea at the shop.\\n\\n###Slo Burn\\n*10449 S Parker Rd, Parker, CO 80134*\\nLast visited: All the time\\n\\n![The testosterone overfloweth.](http://images.kevinhoyt.com/slo.burn.jpg)\\n\\nVeteran owned and operated, I consider [Slo Burn](https://www.sloburncigars.com/) to be my \\\"home\\\" lounge. If you are ever on the south side of Denver, drop on by, and you will likely find me here. Public seating in the front, private lounge with stocked minibar in the back. Televisions and leather recliners all around. Sizable walk-in humidor with a wide selection.\\n\\n###Smoking Cave\\n*5435 Boatworks Dr, Highlands Ranch, CO 80126*\\nLast visited: Every Thursday\\n\\n![A cave with a mermaid view.](http://images.kevinhoyt.com/smoking.cave.jpg)\\n\\nProbably the most well-known cigar lounge in the Denver metro, where the humidor has a large selection and the best prices in town. The front of the store is open to the public and has a dozen or so leather seats for your smoking pleasure. The members lounge has another two-dozen leather seats. Special events abound, usually with free food. Just watch out that Quang does not steal your stick.\\n\\n###Tobacco Leaf\\n*6870 S Rainbow Blvd #118, Las Vegas, NV 89118*\\nLast visited: February 22, 2018\\n\\n![Lounge in the back.](http://images.kevinhoyt.com/tobacco.leaf.4.jpg)\\n\\nWhat?! You go to Las Vegas and you head to a shop off the strip? Yes. Yes I do. The tiled room in the back with a dozen or so leather chairs is a perfect escape from the constant barrage of casino noise. A lengthy, and well-categorized, humidor runs along the length of most of the store where you are sure to find a stick of your liking. \\n\"}]],\"markups\":[],\"sections\":[[10,0]],\"ghostVersion\":\"3.0\"}","html":"<!--kg-card-begin: markdown--><p>In this day of negative sentiment towards smoking you might be inclined to think that the cigar lounge would be a dying breed. As bars, restaurants, and public spaces alike ban smoking, brothers of the leaf (BOTL) find sanctuary in their local cigar lounge.</p>\n<p>As a frequent traveler, I find myself in the position of getting to experience the character of these fine establishments all over the world. This then is my ode to the character, and characters, of the places I have encountered along the way.</p>\n<h3 id=\"boisdaleofcanarywharf\">Boisdale of Canary Wharf</h3>\n<p><em>15 Cabot Square, Canary Wharf, London E14 4QT, UK</em><br>\nLast visited: July 2, 2017</p>\n<p><img src=\"http://images.kevinhoyt.com/boisdale.canary.jpg\" alt=\"Go Scottish with Scotch.\" loading=\"lazy\"></p>\n<p>There are several <a href=\"https://www.boisdale.co.uk\">Boisdale</a> locations around London, all of which are smoker friendly. The Canary Wharf location features a plush, enclosed, lounge where you can get high-end food, drink, and cigars. The Scottish decor begs a single malt pairing. An attendant will see to your every need along the way.</p>\n<h3 id=\"cigarbarandgrill\">Cigar Bar and Grill</h3>\n<p><em>850 Montgomery St, San Francisco, CA 94133</em><br>\nLast visited: April 24, 2018</p>\n<p><img src=\"http://images.kevinhoyt.com/cigar.bar.sf.jpg\" alt=\"Cigars. Downtown. SF.\" loading=\"lazy\"></p>\n<p>Cigar smoking, food, and liquor in downtown San Francisco, CA? You bet! Formally a restaurant with live music, salsa lessons, and more, attached to an open-air, heated, patio. The food is really good, if a little on the expenseive side. And the same could be said for the small humidor.</p>\n<h3 id=\"cigarsatno10\">Cigars at No. 10</h3>\n<p><em>10 Manchester St, Marylebone, London W1U 4DG, UK</em><br>\nLast visited: July 17, 2017</p>\n<p><img src=\"http://images.kevinhoyt.com/cigars.at.no.ten.jpg\" alt=\"High class. Or. Not.\" loading=\"lazy\"></p>\n<p>Technically a hotel, with an outstanding restaurant, there is a small-ish, but very comfortable outdoor smoking area (COSA) in the back. The area is heated, with a privacy screen for your pleasure. The staff is very attentive, and highly trained. They will escort you to their humidor, which is well-stocked with a large assortment of Cuban cigars. Just be prepared for the bespoke price tag.</p>\n<h3 id=\"havanacigarlounge\">Havana Cigar Lounge</h3>\n<p><em>4850 W Flamingo Rd #18, Las Vegas, NV 89103</em><br>\nLast visited: February 21, 2018</p>\n<p><img src=\"http://images.kevinhoyt.com/havana.cigar.vegas.jpg\" alt=\"Dark. Plush. Classic.\" loading=\"lazy\"></p>\n<p>On a rare, rainy, late afternoon in Las Vegas, I slipped off the strip to Havana Cigar Lounge. The doors at the front and the back of the store were propped open. A cool breeze drifted through keeping the air fresh. The humidor was lightly stocked, but had a respectable variety. The host popped in a video showcasing James J. Fox in London - another lounge in this list.</p>\n<h3 id=\"havanagarage\">Havana Garage</h3>\n<p><em>1008 Howard St, Omaha, NE 68102</em><br>\nLast visited: September 6, 2018</p>\n<p><img src=\"http://images.kevinhoyt.com/havana.garage.omaha.jpg\" alt=\"People watching on the Old Market.\" loading=\"lazy\"></p>\n<p>The Old Market in downtown Omaha, Nebraska, is a trendy, night-life destination pretty much any day of the week. Nestled on the brick-lined main street, is Havana Garage, where you can have a cigar, choose from a wide variety of alcoholic beverages, and people watch the night away. Well stocked, if a little pricey, humidor in the basement.</p>\n<h3 id=\"habanahousecigarlounge\">Habana House Cigar Lounge</h3>\n<p><em>13729 Research Blvd #1075, Austin, TX 78750</em><br>\nLast visited: August 26, 2019</p>\n<p><img src=\"http://images.kevinhoyt.com/habana.house.austin.jpg\" alt=\"Walk right in, sit right down.\" loading=\"lazy\"></p>\n<p>This lounge has three distinct seating areas, each with its own set of televisions. Pick your area - news, sports, history, fiction. The massive walk-in humidor will be sure to offer something to fit your palate. A variety of food options are within walking distance (even in the Austin summer). Spacious patio in the front.</p>\n<h3 id=\"jamesjfox\">James J. Fox</h3>\n<p><em>19 St James's St, St. James's, London SW1A 1ES, UK</em><br>\nLast visited: August 26, 2017</p>\n<p><img src=\"http://images.kevinhoyt.com/james.j.fox.jpg\" alt=\"Sit in the same chair as Churchill himself.\" loading=\"lazy\"></p>\n<p>Do you like a churchill? Would you like to sit in the chair, in the lounge, where Winston Churchill would visit for a cigar? Welcome home! With a cigar museum, fantastic selection and service, and a spacious lounge, it is hard to beat the historic pleasure of <a href=\"https://www.jjfox.co.uk/\">James J. Fox</a>.</p>\n<h3 id=\"maduros\">Maduros</h3>\n<p><em>2997 Cumberland Blvd SE, Atlanta, GA 30339</em><br>\nLast visited: March 6, 2019</p>\n<p><img src=\"http://images.kevinhoyt.com/maduros.jpg\" alt=\"Get your funky drink on.\" loading=\"lazy\"></p>\n<p>If you are catching a Braves game, or are in the Galleria area for a conference, and you want to grab a cigar, an adult beverage, and get your funk on, then you have come to the right spot. Part night club, part cigar lounge, <a href=\"https://maduroscigarshop.com\">Maduros</a> has a smallish humidor, but with enough variety for every stogie lover. And did I mention adult beverages?</p>\n<h3 id=\"orlandocigarandtobacco\">Orlando Cigar and Tobacco</h3>\n<p><em>9924 Universal Blvd #218, Orlando, FL 32819</em><br>\nLast visited: July 21, 2019</p>\n<p><img src=\"http://images.kevinhoyt.com/orlando.cigars.tobacco.jpg\" alt=\"Theme park getaway for the suburban dad.\" loading=\"lazy\"></p>\n<p>If you are in Orlando, FL, and looking for the quintessential cigar lounge - this is not it. If you are a suburban dad, booked in the VRBO at <a href=\"https://www.vistacayholidays.com\">Vista Cay</a> across the street, and looking for a little escape from theme park madness and kids - you have found your nirvana. Pick up a beverage at the Publix on the way in, or have some tea at the shop.</p>\n<h3 id=\"sloburn\">Slo Burn</h3>\n<p><em>10449 S Parker Rd, Parker, CO 80134</em><br>\nLast visited: All the time</p>\n<p><img src=\"http://images.kevinhoyt.com/slo.burn.jpg\" alt=\"The testosterone overfloweth.\" loading=\"lazy\"></p>\n<p>Veteran owned and operated, I consider <a href=\"https://www.sloburncigars.com/\">Slo Burn</a> to be my &quot;home&quot; lounge. If you are ever on the south side of Denver, drop on by, and you will likely find me here. Public seating in the front, private lounge with stocked minibar in the back. Televisions and leather recliners all around. Sizable walk-in humidor with a wide selection.</p>\n<h3 id=\"smokingcave\">Smoking Cave</h3>\n<p><em>5435 Boatworks Dr, Highlands Ranch, CO 80126</em><br>\nLast visited: Every Thursday</p>\n<p><img src=\"http://images.kevinhoyt.com/smoking.cave.jpg\" alt=\"A cave with a mermaid view.\" loading=\"lazy\"></p>\n<p>Probably the most well-known cigar lounge in the Denver metro, where the humidor has a large selection and the best prices in town. The front of the store is open to the public and has a dozen or so leather seats for your smoking pleasure. The members lounge has another two-dozen leather seats. Special events abound, usually with free food. Just watch out that Quang does not steal your stick.</p>\n<h3 id=\"tobaccoleaf\">Tobacco Leaf</h3>\n<p><em>6870 S Rainbow Blvd #118, Las Vegas, NV 89118</em><br>\nLast visited: February 22, 2018</p>\n<p><img src=\"http://images.kevinhoyt.com/tobacco.leaf.4.jpg\" alt=\"Lounge in the back.\" loading=\"lazy\"></p>\n<p>What?! You go to Las Vegas and you head to a shop off the strip? Yes. Yes I do. The tiled room in the back with a dozen or so leather chairs is a perfect escape from the constant barrage of casino noise. A lengthy, and well-categorized, humidor runs along the length of most of the store where you are sure to find a stick of your liking.</p>\n<!--kg-card-end: markdown-->","comment_id":"5adb8abd68339a00221450c0","plaintext":"In this day of negative sentiment towards smoking you might be inclined to think\nthat the cigar lounge would be a dying breed. As bars, restaurants, and public\nspaces alike ban smoking, brothers of the leaf (BOTL) find sanctuary in their\nlocal cigar lounge.\n\nAs a frequent traveler, I find myself in the position of getting to experience\nthe character of these fine establishments all over the world. This then is my\node to the character, and characters, of the places I have encountered along the\nway.\n\nBoisdale of Canary Wharf\n15 Cabot Square, Canary Wharf, London E14 4QT, UK\nLast visited: July 2, 2017\n\n\n\nThere are several Boisdale [https://www.boisdale.co.uk] locations around London,\nall of which are smoker friendly. The Canary Wharf location features a plush,\nenclosed, lounge where you can get high-end food, drink, and cigars. The\nScottish decor begs a single malt pairing. An attendant will see to your every\nneed along the way.\n\nCigar Bar and Grill\n850 Montgomery St, San Francisco, CA 94133\nLast visited: April 24, 2018\n\n\n\nCigar smoking, food, and liquor in downtown San Francisco, CA? You bet! Formally\na restaurant with live music, salsa lessons, and more, attached to an open-air,\nheated, patio. The food is really good, if a little on the expenseive side. And\nthe same could be said for the small humidor.\n\nCigars at No. 10\n10 Manchester St, Marylebone, London W1U 4DG, UK\nLast visited: July 17, 2017\n\n\n\nTechnically a hotel, with an outstanding restaurant, there is a small-ish, but\nvery comfortable outdoor smoking area (COSA) in the back. The area is heated,\nwith a privacy screen for your pleasure. The staff is very attentive, and highly\ntrained. They will escort you to their humidor, which is well-stocked with a\nlarge assortment of Cuban cigars. Just be prepared for the bespoke price tag.\n\nHavana Cigar Lounge\n4850 W Flamingo Rd #18, Las Vegas, NV 89103\nLast visited: February 21, 2018\n\n\n\nOn a rare, rainy, late afternoon in Las Vegas, I slipped off the strip to Havana\nCigar Lounge. The doors at the front and the back of the store were propped\nopen. A cool breeze drifted through keeping the air fresh. The humidor was\nlightly stocked, but had a respectable variety. The host popped in a video\nshowcasing James J. Fox in London - another lounge in this list.\n\nHavana Garage\n1008 Howard St, Omaha, NE 68102\nLast visited: September 6, 2018\n\n\n\nThe Old Market in downtown Omaha, Nebraska, is a trendy, night-life destination\npretty much any day of the week. Nestled on the brick-lined main street, is\nHavana Garage, where you can have a cigar, choose from a wide variety of\nalcoholic beverages, and people watch the night away. Well stocked, if a little\npricey, humidor in the basement.\n\nHabana House Cigar Lounge\n13729 Research Blvd #1075, Austin, TX 78750\nLast visited: August 26, 2019\n\n\n\nThis lounge has three distinct seating areas, each with its own set of\ntelevisions. Pick your area - news, sports, history, fiction. The massive\nwalk-in humidor will be sure to offer something to fit your palate. A variety of\nfood options are within walking distance (even in the Austin summer). Spacious\npatio in the front.\n\nJames J. Fox\n19 St James's St, St. James's, London SW1A 1ES, UK\nLast visited: August 26, 2017\n\n\n\nDo you like a churchill? Would you like to sit in the chair, in the lounge,\nwhere Winston Churchill would visit for a cigar? Welcome home! With a cigar\nmuseum, fantastic selection and service, and a spacious lounge, it is hard to\nbeat the historic pleasure of James J. Fox [https://www.jjfox.co.uk/].\n\nMaduros\n2997 Cumberland Blvd SE, Atlanta, GA 30339\nLast visited: March 6, 2019\n\n\n\nIf you are catching a Braves game, or are in the Galleria area for a conference,\nand you want to grab a cigar, an adult beverage, and get your funk on, then you\nhave come to the right spot. Part night club, part cigar lounge, Maduros\n[https://maduroscigarshop.com] has a smallish humidor, but with enough variety\nfor every stogie lover. And did I mention adult beverages?\n\nOrlando Cigar and Tobacco\n9924 Universal Blvd #218, Orlando, FL 32819\nLast visited: July 21, 2019\n\n\n\nIf you are in Orlando, FL, and looking for the quintessential cigar lounge -\nthis is not it. If you are a suburban dad, booked in the VRBO at Vista Cay\n[https://www.vistacayholidays.com] across the street, and looking for a little\nescape from theme park madness and kids - you have found your nirvana. Pick up a\nbeverage at the Publix on the way in, or have some tea at the shop.\n\nSlo Burn\n10449 S Parker Rd, Parker, CO 80134\nLast visited: All the time\n\n\n\nVeteran owned and operated, I consider Slo Burn [https://www.sloburncigars.com/] \nto be my \"home\" lounge. If you are ever on the south side of Denver, drop on by,\nand you will likely find me here. Public seating in the front, private lounge\nwith stocked minibar in the back. Televisions and leather recliners all around.\nSizable walk-in humidor with a wide selection.\n\nSmoking Cave\n5435 Boatworks Dr, Highlands Ranch, CO 80126\nLast visited: Every Thursday\n\n\n\nProbably the most well-known cigar lounge in the Denver metro, where the humidor\nhas a large selection and the best prices in town. The front of the store is\nopen to the public and has a dozen or so leather seats for your smoking\npleasure. The members lounge has another two-dozen leather seats. Special events\nabound, usually with free food. Just watch out that Quang does not steal your\nstick.\n\nTobacco Leaf\n6870 S Rainbow Blvd #118, Las Vegas, NV 89118\nLast visited: February 22, 2018\n\n\n\nWhat?! You go to Las Vegas and you head to a shop off the strip? Yes. Yes I do.\nThe tiled room in the back with a dozen or so leather chairs is a perfect escape\nfrom the constant barrage of casino noise. A lengthy, and well-categorized,\nhumidor runs along the length of most of the store where you are sure to find a\nstick of your liking.","feature_image":"__GHOST_URL__/content/images/2018/04/cohiba.cigars.jpg","featured":0,"status":"published","locale":null,"visibility":"public","author_id":"1","created_at":"2018-04-21T19:02:21.000Z","updated_at":"2019-08-27T14:18:05.000Z","published_at":"2018-04-21T19:02:29.000Z","custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"type":"page","email_recipient_filter":"none"},{"id":"5bce15ea9b7c4e00bfd17f20","uuid":"3d1b47f9-e0a9-41c2-9dc4-8fe17fcc58f8","title":"Image Processing in a Web Worker","slug":"image-processing-in-a-web-worker","mobiledoc":"{\"version\":\"0.3.1\",\"markups\":[],\"atoms\":[],\"cards\":[[\"markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\"WebRTC (Real-Time Communication) has many great uses, much of which is still relatively untapped. One of my favorite side effects of WebRTC arriving in browsers is the ability to put a live web camera feed into a video element. Once those two are paired, you can start performing real-time image processing on the content via a canvas element and the ImageData object.\\n\\n## What Performance\\n\\nAnalysis of a web camera stream can be very demanding on the browser ...\\n\\nConsider a single image from a web camera stream that is 640x480 pixels - relatively small by modern standards. That is 307,200 pixels. The first step in many image processing algorithms is to remove color (noise) from an image by making it grayscale. This means a loop with 307,200 iterations before we can really start analyzing the image content.\\n\\nThe next step in image processing is often to perform a softening of the image detail through a Gaussian blur (more denoising). A Gaussian blur represents O(kernel * width * height). This means that applying a blur on the above image (640x480), with a kernel of three (3) yields 921,600 operations. That is on top of the 307,200 we needed to move from color to gray.\\n\\nIf we approach this work in a requestAnimationFrame() callback, then we are going to crush rendering performance. If we approach this work in a setTimeout() callback, we will crush the event loop. Either way, we are going to make the page unresponsive to user interaction. Ideally, we need a way to offload this work to another thread.\\n\\n## Enter Web Workers\\n\\nWeb Workers give us a separate thread for working with data, but they come with a trade-off - no access to the DOM. In the case of image processing however, this does not matter. Image processing involves running a lot of maths on a specific set of data. The end result is generally a geometric description of the content (lines, boxes, etc). This makes image processing with Web Workers a perfect fit.\\n\\nIn this example, we will take frames from the video element via canvas, get an ImageData object representing half of the frame, process that frame to grayscale in a Web Worker, and then display the results over the video.\\n\\n### A Tale of Two Threads\\n\\nTo process a frame of the video, we will need to place it onto a canvas element. This is what will allow us to get the ImageData for processing. To show the frame, we can overlay a canvas on the video, where the canvas will be otherwise transparent unless we paint something on it. This then is actually two separate threads. One for the processing and one for the rendering.\\n\\nWhen it comes to rendering, that is the realm of requestAnimationFrame(). This will help us to stay in sync with the browser refresh and optimize painting operations. We should generally strive to keep the requestAnimationFrame() callback aligned with rendering only. We do not want to capture frames of the video, or process the frame to grayscale. If there are grayscale results, we want to render them, but we do not want requestAnimationFrame() beholden to the processing itself.\\n\\nWhen it comes to the processing, we can grab a frame of the video, and put it on a canvas, without impacting the rendering cycle. The processing to gray scale will happen in a separate thread, so that will not impact rendering either. Then when we have the resulting grayscale bytes we need to render them, but we do not want to tie the two together. We want to offload the resulting ImageData, and move on to processing the next frame.\\n\\nWhat does all this mean? It means that we need a shared property that the requestAnimationFrame() and worker results can access. Whew! We will come back to this concept in a moment, but for now, all that boils down to:\\n\\n    class WorkIt {\\n      constructor() {\\n        this.grayscale = null;\\n      }\\n    }\\n\\n> Decoupling two parts of an application is a powerful technique I have also used successfully in IoT.\\n\\n### Setup the Web Worker\\n\\nDeclaring a Web Worker is easy enough, but we will want to make sure the scope is at a place where we can refer to the instance throughout our class. To get data from the main thread over to the worker, we call Worker.postMessage(). On the worker side, we will implement an event listener for the message being posted. Likewise, when the worker is finished, it will call Worker.postMessage() to send data back to the main thread. Then to handle it, we need to add a listener for the message being posted.\\n\\n    class WorkIt {\\n      constructor() {\\n        this.grayscale = null;\\n        \\n        this.worker = new Worker( 'grayscale.js' );\\n        this.worker.addEventListener( 'message', ( evt ) => this.doWorkerMessage( evt ) );\\n      }\\n    }\\n\\n> There are some constraints on the data that can be passed. Namely that the object needs to conform to the [structured clone algorithm](https://developer.mozilla.org/en-US/docs/Web/API/Web_Workers_API/Structured_clone_algorithm). Most built-in data types, including ImageData, conform to this algorithm.\\n\\n### Canvas the Neighborhood\\n\\nAs previously mentioned, we will need two canvas elements. One which gets a full frame from the video, and one which renders the resulting grayscale image. This means two canvas element references, and two context instances. In the interest of convenience, I like to bundle these two together in a custom class. This keeps my code cleaner.\\n\\n    class ImageCanvas {\\n      constructor( path ) {\\n        this.canvas = document.querySelector( path );\\n        this.context = this.canvas.getContext( '2d' );\\n      }\\n    }\\n    \\n    class WorkIt {\\n      constructor() {\\n        this.grayscale = null;\\n        \\n        this.worker = new Worker( 'grayscale.js' );\\n        this.worker.addEventListener( 'message', ( evt ) => this.doWorkerMessage( evt ) );\\n        \\n        this.input = new ImageCanvas( '#input' );\\n        this.output = new ImageCanvas( '#output' );\\n      }\\n    }\\n\\n### I See You\\n\\nNow for some video. Once the stream loads, you will notice a call to two different methods. These are the method calls that kick off our rendering and processing hooks. As we will see in a moment, the draw() method will invoke requestAnimationFrame() and assign itself as the callback. This effectively sets up the rendering loop inline with the actual rendering process of the browser. The process() method will capture the frame and post it as a message over to the worker. When the work finishes, we will land back in the main thread. More on that in a moment.\\n\\n    this.video = document.querySelector( 'video' );\\n    navigator.mediaDevices.getUserMedia({video: true, audio: false} )\\n    .then( ( stream ) => {\\n      this.video.srcObject = stream;\\n      this.video.play();\\n      \\n      this.draw();\\n      this.process();\\n    } )\\n    .catch( ( err ) => {\\n      console.log( err );\\n    } );\\n\\n### Draw the Frame\\n\\nThe draw() method contains very little code, and that is by design. We could grab the frame, process it, and overlay it on the video all in the draw() method. That would work. The problem is that all those operations would need to happen before the browser rendered anything. With enough processing, this will bring our frame rate down to close to the single digits.\\n\\n    draw() {\\n      if( this.grayscale !== null ) {\\n        this.output.context.putImageData(\\n          this.grayscale,\\n          this.output.canvas.clientWidth / 2,\\n          0\\n        );\\n      }\\n      \\n      requestAnimationFrame( () => {return this.draw();} );\\n    }\\n\\nWhat we will do instead is look for that touch point between rendering and processing, which in this case is the \\\"grayscale\\\" property. If an ImageData value is in the grayscale property, it will be rendered on the next pass. And that is it. Again, try and keep the activities in requestAnimationFrame() oriented around the things that actually need to be rendered.\\n\\nAt this point, the rendering is happily going about its work. Likely at around a nice, buttery smooth, 60 frames per second. It is just sitting there, waiting for something to render. Now let us give it something to render.\\n\\n### Capture the Frame\\n\\nThe process() method is also pretty slender - three whole lines. First we draw the currently displayed video frame to an offscreen canvas. Next we get an ImageData object representing the right portion of that frame. Then we send the ImageData over to the worker via Worker.postMessage().\\n\\n    process() {\\n      this.input.context.drawImage( this.video, 0, 0 );\\n      \\n      const pixels = this.input.context.getImageData(\\n        this.input.canvas.clientWidth / 2,\\n        0,\\n        this.input.canvas.clientWidth / 2,\\n        this.input.canvas.clientHeight\\n      );\\n      \\n      this.worker.postMessage( pixels );\\n    }\\n\\n> As it turns out, moving 307,200 values around in memory to construct the ImageData object still takes some time. In fact, it will be the most significant part of the work on the main thread in this case. What would be nice is something like ImageData that is optimized for rendering. It turns out that the [ImageBitmap](https://caniuse.com/#feat=createimagebitmap) object is that class, but it also turns out that ImageBitmap is not broadly implemented in browser yet (October 2018).\\n\\n### Get to Worker\\n\\nTurning an image from color to grayscale can be accomplished in a few different ways - some more scientific than others. My preferred method for image processing is to average the red, green, and blue channels. Then you place that resulting average value back into the red, green, and blue channels. This is where we make those 307,200 iterations. It is detached from the rendering thread and callback event loop.\\n\\n    self.addEventListener( 'message', ( evt ) => {\\n      let pixels = evt.data;\\n      \\n      for( let x = 0; x < pixels.data.length; x += 4 ) {\\n        let average = (\\n          pixels.data[x] +\\n          pixels.data[x + 1] +\\n          pixels.data[x + 2]\\n        ) / 3;\\n        \\n        pixels.data[x] = average;\\n        pixels.data[x + 1] = average;\\n        pixels.data[x + 2] = average;\\n      }\\n      \\n      self.postMessage( pixels );\\n    } );\\n\\nWith the averaging of the pixels complete (a.k.a. grayscale), we call Worker.postMessage() to send the result back to the main thread. Imagine a Gaussian blur happening here. Edge detection. Polygon extraction (object detection). All happening away from the rendering.\\n\\n### And We Are Done\\n\\nBack in the main thread, our message callback gets, um, called back. Here we offload the processed ImageData object off to the \\\"grayscale\\\" property. After that, we call process() again to do the same to whatever frame is being displayed by the camera stream (on the video element) at the moment.\\n\\n    doWorkerMessage( evt ) {\\n      this.grayscale = evt.data;\\n      this.process();\\n    }\\n\\nYou might be asking how the ImageData object gets rendered. Well, now that there is an ImageData object to render, the next time the requestAnimationFrame() handler is called, it will see that object and paint it onto the canvas that overlaid on the video.\\n\\n## The Results Are In\\n\\nIf we start looking at the performance in detail, we will see that rendering stays around 60 fps. It does dip from time to time (to around 40 fps on my machine), and if we look at the operations taking the most time, what we will discover is that it has to do with getting the ImageData from the main thread over to the worker. That 307,200 element array inside the ImageData object is pretty beefy no matter how we feel about it.\\n\\n![Resulting half color, half grayscale image.](http://images.kevinhoyt.com/two.face.jpg)\\n\\nAs it turns out, the browser vendors noticed this as well, and there is an approach we can use to cut that time almost in half. I will cover that in the next installment of this series.\\n\"}]],\"sections\":[[10,0]],\"ghostVersion\":\"3.0\"}","html":"<!--kg-card-begin: markdown--><p>WebRTC (Real-Time Communication) has many great uses, much of which is still relatively untapped. One of my favorite side effects of WebRTC arriving in browsers is the ability to put a live web camera feed into a video element. Once those two are paired, you can start performing real-time image processing on the content via a canvas element and the ImageData object.</p>\n<h2 id=\"whatperformance\">What Performance</h2>\n<p>Analysis of a web camera stream can be very demanding on the browser ...</p>\n<p>Consider a single image from a web camera stream that is 640x480 pixels - relatively small by modern standards. That is 307,200 pixels. The first step in many image processing algorithms is to remove color (noise) from an image by making it grayscale. This means a loop with 307,200 iterations before we can really start analyzing the image content.</p>\n<p>The next step in image processing is often to perform a softening of the image detail through a Gaussian blur (more denoising). A Gaussian blur represents O(kernel * width * height). This means that applying a blur on the above image (640x480), with a kernel of three (3) yields 921,600 operations. That is on top of the 307,200 we needed to move from color to gray.</p>\n<p>If we approach this work in a requestAnimationFrame() callback, then we are going to crush rendering performance. If we approach this work in a setTimeout() callback, we will crush the event loop. Either way, we are going to make the page unresponsive to user interaction. Ideally, we need a way to offload this work to another thread.</p>\n<h2 id=\"enterwebworkers\">Enter Web Workers</h2>\n<p>Web Workers give us a separate thread for working with data, but they come with a trade-off - no access to the DOM. In the case of image processing however, this does not matter. Image processing involves running a lot of maths on a specific set of data. The end result is generally a geometric description of the content (lines, boxes, etc). This makes image processing with Web Workers a perfect fit.</p>\n<p>In this example, we will take frames from the video element via canvas, get an ImageData object representing half of the frame, process that frame to grayscale in a Web Worker, and then display the results over the video.</p>\n<h3 id=\"ataleoftwothreads\">A Tale of Two Threads</h3>\n<p>To process a frame of the video, we will need to place it onto a canvas element. This is what will allow us to get the ImageData for processing. To show the frame, we can overlay a canvas on the video, where the canvas will be otherwise transparent unless we paint something on it. This then is actually two separate threads. One for the processing and one for the rendering.</p>\n<p>When it comes to rendering, that is the realm of requestAnimationFrame(). This will help us to stay in sync with the browser refresh and optimize painting operations. We should generally strive to keep the requestAnimationFrame() callback aligned with rendering only. We do not want to capture frames of the video, or process the frame to grayscale. If there are grayscale results, we want to render them, but we do not want requestAnimationFrame() beholden to the processing itself.</p>\n<p>When it comes to the processing, we can grab a frame of the video, and put it on a canvas, without impacting the rendering cycle. The processing to gray scale will happen in a separate thread, so that will not impact rendering either. Then when we have the resulting grayscale bytes we need to render them, but we do not want to tie the two together. We want to offload the resulting ImageData, and move on to processing the next frame.</p>\n<p>What does all this mean? It means that we need a shared property that the requestAnimationFrame() and worker results can access. Whew! We will come back to this concept in a moment, but for now, all that boils down to:</p>\n<pre><code>class WorkIt {\n  constructor() {\n    this.grayscale = null;\n  }\n}\n</code></pre>\n<blockquote>\n<p>Decoupling two parts of an application is a powerful technique I have also used successfully in IoT.</p>\n</blockquote>\n<h3 id=\"setupthewebworker\">Setup the Web Worker</h3>\n<p>Declaring a Web Worker is easy enough, but we will want to make sure the scope is at a place where we can refer to the instance throughout our class. To get data from the main thread over to the worker, we call Worker.postMessage(). On the worker side, we will implement an event listener for the message being posted. Likewise, when the worker is finished, it will call Worker.postMessage() to send data back to the main thread. Then to handle it, we need to add a listener for the message being posted.</p>\n<pre><code>class WorkIt {\n  constructor() {\n    this.grayscale = null;\n    \n    this.worker = new Worker( 'grayscale.js' );\n    this.worker.addEventListener( 'message', ( evt ) =&gt; this.doWorkerMessage( evt ) );\n  }\n}\n</code></pre>\n<blockquote>\n<p>There are some constraints on the data that can be passed. Namely that the object needs to conform to the <a href=\"https://developer.mozilla.org/en-US/docs/Web/API/Web_Workers_API/Structured_clone_algorithm\">structured clone algorithm</a>. Most built-in data types, including ImageData, conform to this algorithm.</p>\n</blockquote>\n<h3 id=\"canvastheneighborhood\">Canvas the Neighborhood</h3>\n<p>As previously mentioned, we will need two canvas elements. One which gets a full frame from the video, and one which renders the resulting grayscale image. This means two canvas element references, and two context instances. In the interest of convenience, I like to bundle these two together in a custom class. This keeps my code cleaner.</p>\n<pre><code>class ImageCanvas {\n  constructor( path ) {\n    this.canvas = document.querySelector( path );\n    this.context = this.canvas.getContext( '2d' );\n  }\n}\n\nclass WorkIt {\n  constructor() {\n    this.grayscale = null;\n    \n    this.worker = new Worker( 'grayscale.js' );\n    this.worker.addEventListener( 'message', ( evt ) =&gt; this.doWorkerMessage( evt ) );\n    \n    this.input = new ImageCanvas( '#input' );\n    this.output = new ImageCanvas( '#output' );\n  }\n}\n</code></pre>\n<h3 id=\"iseeyou\">I See You</h3>\n<p>Now for some video. Once the stream loads, you will notice a call to two different methods. These are the method calls that kick off our rendering and processing hooks. As we will see in a moment, the draw() method will invoke requestAnimationFrame() and assign itself as the callback. This effectively sets up the rendering loop inline with the actual rendering process of the browser. The process() method will capture the frame and post it as a message over to the worker. When the work finishes, we will land back in the main thread. More on that in a moment.</p>\n<pre><code>this.video = document.querySelector( 'video' );\nnavigator.mediaDevices.getUserMedia({video: true, audio: false} )\n.then( ( stream ) =&gt; {\n  this.video.srcObject = stream;\n  this.video.play();\n  \n  this.draw();\n  this.process();\n} )\n.catch( ( err ) =&gt; {\n  console.log( err );\n} );\n</code></pre>\n<h3 id=\"drawtheframe\">Draw the Frame</h3>\n<p>The draw() method contains very little code, and that is by design. We could grab the frame, process it, and overlay it on the video all in the draw() method. That would work. The problem is that all those operations would need to happen before the browser rendered anything. With enough processing, this will bring our frame rate down to close to the single digits.</p>\n<pre><code>draw() {\n  if( this.grayscale !== null ) {\n    this.output.context.putImageData(\n      this.grayscale,\n      this.output.canvas.clientWidth / 2,\n      0\n    );\n  }\n  \n  requestAnimationFrame( () =&gt; {return this.draw();} );\n}\n</code></pre>\n<p>What we will do instead is look for that touch point between rendering and processing, which in this case is the &quot;grayscale&quot; property. If an ImageData value is in the grayscale property, it will be rendered on the next pass. And that is it. Again, try and keep the activities in requestAnimationFrame() oriented around the things that actually need to be rendered.</p>\n<p>At this point, the rendering is happily going about its work. Likely at around a nice, buttery smooth, 60 frames per second. It is just sitting there, waiting for something to render. Now let us give it something to render.</p>\n<h3 id=\"capturetheframe\">Capture the Frame</h3>\n<p>The process() method is also pretty slender - three whole lines. First we draw the currently displayed video frame to an offscreen canvas. Next we get an ImageData object representing the right portion of that frame. Then we send the ImageData over to the worker via Worker.postMessage().</p>\n<pre><code>process() {\n  this.input.context.drawImage( this.video, 0, 0 );\n  \n  const pixels = this.input.context.getImageData(\n    this.input.canvas.clientWidth / 2,\n    0,\n    this.input.canvas.clientWidth / 2,\n    this.input.canvas.clientHeight\n  );\n  \n  this.worker.postMessage( pixels );\n}\n</code></pre>\n<blockquote>\n<p>As it turns out, moving 307,200 values around in memory to construct the ImageData object still takes some time. In fact, it will be the most significant part of the work on the main thread in this case. What would be nice is something like ImageData that is optimized for rendering. It turns out that the <a href=\"https://caniuse.com/#feat=createimagebitmap\">ImageBitmap</a> object is that class, but it also turns out that ImageBitmap is not broadly implemented in browser yet (October 2018).</p>\n</blockquote>\n<h3 id=\"gettoworker\">Get to Worker</h3>\n<p>Turning an image from color to grayscale can be accomplished in a few different ways - some more scientific than others. My preferred method for image processing is to average the red, green, and blue channels. Then you place that resulting average value back into the red, green, and blue channels. This is where we make those 307,200 iterations. It is detached from the rendering thread and callback event loop.</p>\n<pre><code>self.addEventListener( 'message', ( evt ) =&gt; {\n  let pixels = evt.data;\n  \n  for( let x = 0; x &lt; pixels.data.length; x += 4 ) {\n    let average = (\n      pixels.data[x] +\n      pixels.data[x + 1] +\n      pixels.data[x + 2]\n    ) / 3;\n    \n    pixels.data[x] = average;\n    pixels.data[x + 1] = average;\n    pixels.data[x + 2] = average;\n  }\n  \n  self.postMessage( pixels );\n} );\n</code></pre>\n<p>With the averaging of the pixels complete (a.k.a. grayscale), we call Worker.postMessage() to send the result back to the main thread. Imagine a Gaussian blur happening here. Edge detection. Polygon extraction (object detection). All happening away from the rendering.</p>\n<h3 id=\"andwearedone\">And We Are Done</h3>\n<p>Back in the main thread, our message callback gets, um, called back. Here we offload the processed ImageData object off to the &quot;grayscale&quot; property. After that, we call process() again to do the same to whatever frame is being displayed by the camera stream (on the video element) at the moment.</p>\n<pre><code>doWorkerMessage( evt ) {\n  this.grayscale = evt.data;\n  this.process();\n}\n</code></pre>\n<p>You might be asking how the ImageData object gets rendered. Well, now that there is an ImageData object to render, the next time the requestAnimationFrame() handler is called, it will see that object and paint it onto the canvas that overlaid on the video.</p>\n<h2 id=\"theresultsarein\">The Results Are In</h2>\n<p>If we start looking at the performance in detail, we will see that rendering stays around 60 fps. It does dip from time to time (to around 40 fps on my machine), and if we look at the operations taking the most time, what we will discover is that it has to do with getting the ImageData from the main thread over to the worker. That 307,200 element array inside the ImageData object is pretty beefy no matter how we feel about it.</p>\n<p><img src=\"http://images.kevinhoyt.com/two.face.jpg\" alt=\"Resulting half color, half grayscale image.\" loading=\"lazy\"></p>\n<p>As it turns out, the browser vendors noticed this as well, and there is an approach we can use to cut that time almost in half. I will cover that in the next installment of this series.</p>\n<!--kg-card-end: markdown-->","comment_id":"5bce15ea9b7c4e00bfd17f20","plaintext":"WebRTC (Real-Time Communication) has many great uses, much of which is still\nrelatively untapped. One of my favorite side effects of WebRTC arriving in\nbrowsers is the ability to put a live web camera feed into a video element. Once\nthose two are paired, you can start performing real-time image processing on the\ncontent via a canvas element and the ImageData object.\n\nWhat Performance\nAnalysis of a web camera stream can be very demanding on the browser ...\n\nConsider a single image from a web camera stream that is 640x480 pixels -\nrelatively small by modern standards. That is 307,200 pixels. The first step in\nmany image processing algorithms is to remove color (noise) from an image by\nmaking it grayscale. This means a loop with 307,200 iterations before we can\nreally start analyzing the image content.\n\nThe next step in image processing is often to perform a softening of the image\ndetail through a Gaussian blur (more denoising). A Gaussian blur represents\nO(kernel * width * height). This means that applying a blur on the above image\n(640x480), with a kernel of three (3) yields 921,600 operations. That is on top\nof the 307,200 we needed to move from color to gray.\n\nIf we approach this work in a requestAnimationFrame() callback, then we are\ngoing to crush rendering performance. If we approach this work in a setTimeout()\ncallback, we will crush the event loop. Either way, we are going to make the\npage unresponsive to user interaction. Ideally, we need a way to offload this\nwork to another thread.\n\nEnter Web Workers\nWeb Workers give us a separate thread for working with data, but they come with\na trade-off - no access to the DOM. In the case of image processing however,\nthis does not matter. Image processing involves running a lot of maths on a\nspecific set of data. The end result is generally a geometric description of the\ncontent (lines, boxes, etc). This makes image processing with Web Workers a\nperfect fit.\n\nIn this example, we will take frames from the video element via canvas, get an\nImageData object representing half of the frame, process that frame to grayscale\nin a Web Worker, and then display the results over the video.\n\nA Tale of Two Threads\nTo process a frame of the video, we will need to place it onto a canvas element.\nThis is what will allow us to get the ImageData for processing. To show the\nframe, we can overlay a canvas on the video, where the canvas will be otherwise\ntransparent unless we paint something on it. This then is actually two separate\nthreads. One for the processing and one for the rendering.\n\nWhen it comes to rendering, that is the realm of requestAnimationFrame(). This\nwill help us to stay in sync with the browser refresh and optimize painting\noperations. We should generally strive to keep the requestAnimationFrame()\ncallback aligned with rendering only. We do not want to capture frames of the\nvideo, or process the frame to grayscale. If there are grayscale results, we\nwant to render them, but we do not want requestAnimationFrame() beholden to the\nprocessing itself.\n\nWhen it comes to the processing, we can grab a frame of the video, and put it on\na canvas, without impacting the rendering cycle. The processing to gray scale\nwill happen in a separate thread, so that will not impact rendering either. Then\nwhen we have the resulting grayscale bytes we need to render them, but we do not\nwant to tie the two together. We want to offload the resulting ImageData, and\nmove on to processing the next frame.\n\nWhat does all this mean? It means that we need a shared property that the\nrequestAnimationFrame() and worker results can access. Whew! We will come back\nto this concept in a moment, but for now, all that boils down to:\n\nclass WorkIt {\n  constructor() {\n    this.grayscale = null;\n  }\n}\n\n\n> Decoupling two parts of an application is a powerful technique I have also used\nsuccessfully in IoT.\n\n\nSetup the Web Worker\nDeclaring a Web Worker is easy enough, but we will want to make sure the scope\nis at a place where we can refer to the instance throughout our class. To get\ndata from the main thread over to the worker, we call Worker.postMessage(). On\nthe worker side, we will implement an event listener for the message being\nposted. Likewise, when the worker is finished, it will call Worker.postMessage()\nto send data back to the main thread. Then to handle it, we need to add a\nlistener for the message being posted.\n\nclass WorkIt {\n  constructor() {\n    this.grayscale = null;\n    \n    this.worker = new Worker( 'grayscale.js' );\n    this.worker.addEventListener( 'message', ( evt ) => this.doWorkerMessage( evt ) );\n  }\n}\n\n\n> There are some constraints on the data that can be passed. Namely that the\nobject needs to conform to the structured clone algorithm\n[https://developer.mozilla.org/en-US/docs/Web/API/Web_Workers_API/Structured_clone_algorithm]\n. Most built-in data types, including ImageData, conform to this algorithm.\n\n\nCanvas the Neighborhood\nAs previously mentioned, we will need two canvas elements. One which gets a full\nframe from the video, and one which renders the resulting grayscale image. This\nmeans two canvas element references, and two context instances. In the interest\nof convenience, I like to bundle these two together in a custom class. This\nkeeps my code cleaner.\n\nclass ImageCanvas {\n  constructor( path ) {\n    this.canvas = document.querySelector( path );\n    this.context = this.canvas.getContext( '2d' );\n  }\n}\n\nclass WorkIt {\n  constructor() {\n    this.grayscale = null;\n    \n    this.worker = new Worker( 'grayscale.js' );\n    this.worker.addEventListener( 'message', ( evt ) => this.doWorkerMessage( evt ) );\n    \n    this.input = new ImageCanvas( '#input' );\n    this.output = new ImageCanvas( '#output' );\n  }\n}\n\n\nI See You\nNow for some video. Once the stream loads, you will notice a call to two\ndifferent methods. These are the method calls that kick off our rendering and\nprocessing hooks. As we will see in a moment, the draw() method will invoke\nrequestAnimationFrame() and assign itself as the callback. This effectively sets\nup the rendering loop inline with the actual rendering process of the browser.\nThe process() method will capture the frame and post it as a message over to the\nworker. When the work finishes, we will land back in the main thread. More on\nthat in a moment.\n\nthis.video = document.querySelector( 'video' );\nnavigator.mediaDevices.getUserMedia({video: true, audio: false} )\n.then( ( stream ) => {\n  this.video.srcObject = stream;\n  this.video.play();\n  \n  this.draw();\n  this.process();\n} )\n.catch( ( err ) => {\n  console.log( err );\n} );\n\n\nDraw the Frame\nThe draw() method contains very little code, and that is by design. We could\ngrab the frame, process it, and overlay it on the video all in the draw()\nmethod. That would work. The problem is that all those operations would need to\nhappen before the browser rendered anything. With enough processing, this will\nbring our frame rate down to close to the single digits.\n\ndraw() {\n  if( this.grayscale !== null ) {\n    this.output.context.putImageData(\n      this.grayscale,\n      this.output.canvas.clientWidth / 2,\n      0\n    );\n  }\n  \n  requestAnimationFrame( () => {return this.draw();} );\n}\n\n\nWhat we will do instead is look for that touch point between rendering and\nprocessing, which in this case is the \"grayscale\" property. If an ImageData\nvalue is in the grayscale property, it will be rendered on the next pass. And\nthat is it. Again, try and keep the activities in requestAnimationFrame()\noriented around the things that actually need to be rendered.\n\nAt this point, the rendering is happily going about its work. Likely at around a\nnice, buttery smooth, 60 frames per second. It is just sitting there, waiting\nfor something to render. Now let us give it something to render.\n\nCapture the Frame\nThe process() method is also pretty slender - three whole lines. First we draw\nthe currently displayed video frame to an offscreen canvas. Next we get an\nImageData object representing the right portion of that frame. Then we send the\nImageData over to the worker via Worker.postMessage().\n\nprocess() {\n  this.input.context.drawImage( this.video, 0, 0 );\n  \n  const pixels = this.input.context.getImageData(\n    this.input.canvas.clientWidth / 2,\n    0,\n    this.input.canvas.clientWidth / 2,\n    this.input.canvas.clientHeight\n  );\n  \n  this.worker.postMessage( pixels );\n}\n\n\n> As it turns out, moving 307,200 values around in memory to construct the\nImageData object still takes some time. In fact, it will be the most significant\npart of the work on the main thread in this case. What would be nice is\nsomething like ImageData that is optimized for rendering. It turns out that the \nImageBitmap [https://caniuse.com/#feat=createimagebitmap] object is that class,\nbut it also turns out that ImageBitmap is not broadly implemented in browser yet\n(October 2018).\n\n\nGet to Worker\nTurning an image from color to grayscale can be accomplished in a few different\nways - some more scientific than others. My preferred method for image\nprocessing is to average the red, green, and blue channels. Then you place that\nresulting average value back into the red, green, and blue channels. This is\nwhere we make those 307,200 iterations. It is detached from the rendering thread\nand callback event loop.\n\nself.addEventListener( 'message', ( evt ) => {\n  let pixels = evt.data;\n  \n  for( let x = 0; x < pixels.data.length; x += 4 ) {\n    let average = (\n      pixels.data[x] +\n      pixels.data[x + 1] +\n      pixels.data[x + 2]\n    ) / 3;\n    \n    pixels.data[x] = average;\n    pixels.data[x + 1] = average;\n    pixels.data[x + 2] = average;\n  }\n  \n  self.postMessage( pixels );\n} );\n\n\nWith the averaging of the pixels complete (a.k.a. grayscale), we call\nWorker.postMessage() to send the result back to the main thread. Imagine a\nGaussian blur happening here. Edge detection. Polygon extraction (object\ndetection). All happening away from the rendering.\n\nAnd We Are Done\nBack in the main thread, our message callback gets, um, called back. Here we\noffload the processed ImageData object off to the \"grayscale\" property. After\nthat, we call process() again to do the same to whatever frame is being\ndisplayed by the camera stream (on the video element) at the moment.\n\ndoWorkerMessage( evt ) {\n  this.grayscale = evt.data;\n  this.process();\n}\n\n\nYou might be asking how the ImageData object gets rendered. Well, now that there\nis an ImageData object to render, the next time the requestAnimationFrame()\nhandler is called, it will see that object and paint it onto the canvas that\noverlaid on the video.\n\nThe Results Are In\nIf we start looking at the performance in detail, we will see that rendering\nstays around 60 fps. It does dip from time to time (to around 40 fps on my\nmachine), and if we look at the operations taking the most time, what we will\ndiscover is that it has to do with getting the ImageData from the main thread\nover to the worker. That 307,200 element array inside the ImageData object is\npretty beefy no matter how we feel about it.\n\n\n\nAs it turns out, the browser vendors noticed this as well, and there is an\napproach we can use to cut that time almost in half. I will cover that in the\nnext installment of this series.","feature_image":"__GHOST_URL__/content/images/2018/10/needle.thread.jpg","featured":0,"status":"published","locale":null,"visibility":"public","author_id":"1","created_at":"2018-10-22T18:24:42.000Z","updated_at":"2018-10-29T17:15:23.000Z","published_at":"2018-10-23T20:45:59.000Z","custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"type":"post","email_recipient_filter":"none"},{"id":"5bd72d1da7d50d00bf13e9fa","uuid":"ff5884ca-ee14-4597-b3a2-7c187dcef646","title":"Transferable ImageData","slug":"transferable-imagedata","mobiledoc":"{\"version\":\"0.3.1\",\"markups\":[],\"atoms\":[],\"cards\":[[\"markdown\",{\"cardName\":\"card-markdown\",\"markdown\":\"In a [previous post](https://www.kevinhoyt.com/2018/10/23/image-processing-in-a-web-worker/), I covered sending ImageData from a canvas element, to a Web Worker. This allows for threaded image processing without impacting rendering performance of the browser ... mostly.\\n\\n## By Value\\n\\nThere is still a performance hit, af around 20 milliseconds in my *very* informal testing. This has to do with how data is passed from the main page to the worker - effectively by value. This means that a complete copy of the data is made. This may be fine when dealing with smaller datasets, but when a 640x480 image has 307,200 pixels, it takes a bit more processing.\\n\\n## By Reference\\n\\nShortly after workers were introduced, the concept of \\\"[transferable objects](https://developers.google.com/web/updates/2011/12/Transferable-Objects-Lightning-Fast)\\\" was introduced. The cited work does a good job of introducing the technical reasoning and function. Effectively, when passed as a transferable object (versus structured clone), the reference to the place in memory where the object resides is being handed of to the receiving thread. \\n\\n> Note the side effect that the object being transfered is no longer available to the main page.\\n\\n## Sending ImageData\\n\\nIn order to be transferable, the object being passed to the worker must be ArrayBuffer, MessagePort, or ImageBitmap. MessagePort does not really help us in the case of working on the raw image data. ImageBitmap would be great, but still lacks consistent [support](https://caniuse.com/#search=bitmap) (Nov 2018). This leaves us with ArrayBuffer.\\n\\nUsing into the code from the previous post in this series, we are going to look at the \\\"process()\\\" method. Originally, using ImageData, and passing by value, it looked like the following snippet of code.\\n\\n    process() {\\n      this.input.context.drawImage( this.video, 0, 0 );\\n      \\n      const pixels = this.input.context.getImageData(\\n        this.input.canvas.clientWidth / 2,\\n        0,\\n        this.input.canvas.clientWidth / 2,\\n        this.input.canvas.clientHeight\\n      );\\n      \\n      this.worker.postMessage( pixels );\\n    }\\n    \\nThe ImageData object has a \\\"data\\\" property which is of Uint8ClampedArray type. The data property, or \\\"Uint8ClampedArray\\\" instance, has a \\\"buffer\\\" property on it that results in an ArrayBuffer. And since ArrayBuffer is transferable, we are in business. Our process method updated for passing by reference, look as follows.\\n\\n    process() {\\n      this.input.context.drawImage( this.video, 0, 0 );\\n      \\n      const pixels = this.input.context.getImageData(\\n        this.input.canvas.clientWidth / 2,\\n        0,\\n        this.input.canvas.clientWidth / 2,\\n        this.input.canvas.clientHeight\\n      );\\n      \\n      this.worker.postMessage( {\\n        pixels: pixels.data.buffer,\\n        width: this.input.canvas.clientWidth / 2,\\n        height: this.input.canvas.clientHeight,\\n        channels: 4\\n      }, [pixels.data.buffer] );      \\n    }\\n    \\nSince we will be passing an ArrayBuffer, we will lose some of the information about the ImageData along the way. Specifically, the width and height properties which will be needed on the worker side to reconstruct the ImageData object from the ArrayBuffer. To remedy this problem, we pass an Object instance with the width and height properties - and of course the pixels.\\n\\nIn order to mark the pixels as transferable, we pass an additional argument to the \\\"Worker.postMessage()\\\" method that indicates the data we want to send. Note that this is an array, and that you can provide many different bits of data as needed. Also keep in mind that once the data is sent to the worker, it is no longer available to the sending thread. This is still an ideal setup for image processing, as we want something entirely different back in most cases.\\n\\n## Receiving ImageData\\n\\nThe call to \\\"postMessage()\\\" looks the same wether you are sending ImageData pixels to a worker, or back from a worker to the main thread. Effectively \\\"pixels.data.buffer\\\" with enough additional information to handle reconstructing the ImageData object. On either side of the call, the next challenge is in getting the ImageData back from the ArrayBuffer instance.\\n\\n    let pixels = new ImageData( \\n      new Uint8ClampedArray( evt.data.pixels ),\\n      evt.data.width,\\n      evt.data.height \\n    );\\n\\nThe ImageData class has two constructors. The first takes a width and height, and creates a black retangle. The second takes an Uint8ClampedArray, width, and height. The Uint8ClampedArray has four different constructors, one of which takes an ArrayBuffer. Since we passed in an ArrayBuffer (by reference), we can use that to create the Uint8ClampedArray as needed by the ImageData, in addition to the width and height properties we passed along.\\n\\n## The Results\\n\\nFrom an image processing perspective, we are back in business, now with an extra 10 milliseconds. That right, this techniques cut an already pretty light 20 milliseconds in half (50%). The impact on frame rate is still beyond the human eye, but this leaves 10 milliseconds for us to perform other image processing. I will start leaning into those specifics in my next post.\\n\"}]],\"sections\":[[10,0]],\"ghostVersion\":\"3.0\"}","html":"<!--kg-card-begin: markdown--><p>In a <a href=\"https://www.kevinhoyt.com/2018/10/23/image-processing-in-a-web-worker/\">previous post</a>, I covered sending ImageData from a canvas element, to a Web Worker. This allows for threaded image processing without impacting rendering performance of the browser ... mostly.</p>\n<h2 id=\"byvalue\">By Value</h2>\n<p>There is still a performance hit, af around 20 milliseconds in my <em>very</em> informal testing. This has to do with how data is passed from the main page to the worker - effectively by value. This means that a complete copy of the data is made. This may be fine when dealing with smaller datasets, but when a 640x480 image has 307,200 pixels, it takes a bit more processing.</p>\n<h2 id=\"byreference\">By Reference</h2>\n<p>Shortly after workers were introduced, the concept of &quot;<a href=\"https://developers.google.com/web/updates/2011/12/Transferable-Objects-Lightning-Fast\">transferable objects</a>&quot; was introduced. The cited work does a good job of introducing the technical reasoning and function. Effectively, when passed as a transferable object (versus structured clone), the reference to the place in memory where the object resides is being handed of to the receiving thread.</p>\n<blockquote>\n<p>Note the side effect that the object being transfered is no longer available to the main page.</p>\n</blockquote>\n<h2 id=\"sendingimagedata\">Sending ImageData</h2>\n<p>In order to be transferable, the object being passed to the worker must be ArrayBuffer, MessagePort, or ImageBitmap. MessagePort does not really help us in the case of working on the raw image data. ImageBitmap would be great, but still lacks consistent <a href=\"https://caniuse.com/#search=bitmap\">support</a> (Nov 2018). This leaves us with ArrayBuffer.</p>\n<p>Using into the code from the previous post in this series, we are going to look at the &quot;process()&quot; method. Originally, using ImageData, and passing by value, it looked like the following snippet of code.</p>\n<pre><code>process() {\n  this.input.context.drawImage( this.video, 0, 0 );\n  \n  const pixels = this.input.context.getImageData(\n    this.input.canvas.clientWidth / 2,\n    0,\n    this.input.canvas.clientWidth / 2,\n    this.input.canvas.clientHeight\n  );\n  \n  this.worker.postMessage( pixels );\n}\n</code></pre>\n<p>The ImageData object has a &quot;data&quot; property which is of Uint8ClampedArray type. The data property, or &quot;Uint8ClampedArray&quot; instance, has a &quot;buffer&quot; property on it that results in an ArrayBuffer. And since ArrayBuffer is transferable, we are in business. Our process method updated for passing by reference, look as follows.</p>\n<pre><code>process() {\n  this.input.context.drawImage( this.video, 0, 0 );\n  \n  const pixels = this.input.context.getImageData(\n    this.input.canvas.clientWidth / 2,\n    0,\n    this.input.canvas.clientWidth / 2,\n    this.input.canvas.clientHeight\n  );\n  \n  this.worker.postMessage( {\n    pixels: pixels.data.buffer,\n    width: this.input.canvas.clientWidth / 2,\n    height: this.input.canvas.clientHeight,\n    channels: 4\n  }, [pixels.data.buffer] );      \n}\n</code></pre>\n<p>Since we will be passing an ArrayBuffer, we will lose some of the information about the ImageData along the way. Specifically, the width and height properties which will be needed on the worker side to reconstruct the ImageData object from the ArrayBuffer. To remedy this problem, we pass an Object instance with the width and height properties - and of course the pixels.</p>\n<p>In order to mark the pixels as transferable, we pass an additional argument to the &quot;Worker.postMessage()&quot; method that indicates the data we want to send. Note that this is an array, and that you can provide many different bits of data as needed. Also keep in mind that once the data is sent to the worker, it is no longer available to the sending thread. This is still an ideal setup for image processing, as we want something entirely different back in most cases.</p>\n<h2 id=\"receivingimagedata\">Receiving ImageData</h2>\n<p>The call to &quot;postMessage()&quot; looks the same wether you are sending ImageData pixels to a worker, or back from a worker to the main thread. Effectively &quot;pixels.data.buffer&quot; with enough additional information to handle reconstructing the ImageData object. On either side of the call, the next challenge is in getting the ImageData back from the ArrayBuffer instance.</p>\n<pre><code>let pixels = new ImageData( \n  new Uint8ClampedArray( evt.data.pixels ),\n  evt.data.width,\n  evt.data.height \n);\n</code></pre>\n<p>The ImageData class has two constructors. The first takes a width and height, and creates a black retangle. The second takes an Uint8ClampedArray, width, and height. The Uint8ClampedArray has four different constructors, one of which takes an ArrayBuffer. Since we passed in an ArrayBuffer (by reference), we can use that to create the Uint8ClampedArray as needed by the ImageData, in addition to the width and height properties we passed along.</p>\n<h2 id=\"theresults\">The Results</h2>\n<p>From an image processing perspective, we are back in business, now with an extra 10 milliseconds. That right, this techniques cut an already pretty light 20 milliseconds in half (50%). The impact on frame rate is still beyond the human eye, but this leaves 10 milliseconds for us to perform other image processing. I will start leaning into those specifics in my next post.</p>\n<!--kg-card-end: markdown-->","comment_id":"5bd72d1da7d50d00bf13e9fa","plaintext":"In a previous post\n[https://www.kevinhoyt.com/2018/10/23/image-processing-in-a-web-worker/], I\ncovered sending ImageData from a canvas element, to a Web Worker. This allows\nfor threaded image processing without impacting rendering performance of the\nbrowser ... mostly.\n\nBy Value\nThere is still a performance hit, af around 20 milliseconds in my very informal\ntesting. This has to do with how data is passed from the main page to the worker\n- effectively by value. This means that a complete copy of the data is made.\nThis may be fine when dealing with smaller datasets, but when a 640x480 image\nhas 307,200 pixels, it takes a bit more processing.\n\nBy Reference\nShortly after workers were introduced, the concept of \"transferable objects\n[https://developers.google.com/web/updates/2011/12/Transferable-Objects-Lightning-Fast]\n\" was introduced. The cited work does a good job of introducing the technical\nreasoning and function. Effectively, when passed as a transferable object\n(versus structured clone), the reference to the place in memory where the object\nresides is being handed of to the receiving thread.\n\n> Note the side effect that the object being transfered is no longer available to\nthe main page.\n\n\nSending ImageData\nIn order to be transferable, the object being passed to the worker must be\nArrayBuffer, MessagePort, or ImageBitmap. MessagePort does not really help us in\nthe case of working on the raw image data. ImageBitmap would be great, but still\nlacks consistent support [https://caniuse.com/#search=bitmap] (Nov 2018). This\nleaves us with ArrayBuffer.\n\nUsing into the code from the previous post in this series, we are going to look\nat the \"process()\" method. Originally, using ImageData, and passing by value, it\nlooked like the following snippet of code.\n\nprocess() {\n  this.input.context.drawImage( this.video, 0, 0 );\n  \n  const pixels = this.input.context.getImageData(\n    this.input.canvas.clientWidth / 2,\n    0,\n    this.input.canvas.clientWidth / 2,\n    this.input.canvas.clientHeight\n  );\n  \n  this.worker.postMessage( pixels );\n}\n\n\nThe ImageData object has a \"data\" property which is of Uint8ClampedArray type.\nThe data property, or \"Uint8ClampedArray\" instance, has a \"buffer\" property on\nit that results in an ArrayBuffer. And since ArrayBuffer is transferable, we are\nin business. Our process method updated for passing by reference, look as\nfollows.\n\nprocess() {\n  this.input.context.drawImage( this.video, 0, 0 );\n  \n  const pixels = this.input.context.getImageData(\n    this.input.canvas.clientWidth / 2,\n    0,\n    this.input.canvas.clientWidth / 2,\n    this.input.canvas.clientHeight\n  );\n  \n  this.worker.postMessage( {\n    pixels: pixels.data.buffer,\n    width: this.input.canvas.clientWidth / 2,\n    height: this.input.canvas.clientHeight,\n    channels: 4\n  }, [pixels.data.buffer] );      \n}\n\n\nSince we will be passing an ArrayBuffer, we will lose some of the information\nabout the ImageData along the way. Specifically, the width and height properties\nwhich will be needed on the worker side to reconstruct the ImageData object from\nthe ArrayBuffer. To remedy this problem, we pass an Object instance with the\nwidth and height properties - and of course the pixels.\n\nIn order to mark the pixels as transferable, we pass an additional argument to\nthe \"Worker.postMessage()\" method that indicates the data we want to send. Note\nthat this is an array, and that you can provide many different bits of data as\nneeded. Also keep in mind that once the data is sent to the worker, it is no\nlonger available to the sending thread. This is still an ideal setup for image\nprocessing, as we want something entirely different back in most cases.\n\nReceiving ImageData\nThe call to \"postMessage()\" looks the same wether you are sending ImageData\npixels to a worker, or back from a worker to the main thread. Effectively\n\"pixels.data.buffer\" with enough additional information to handle reconstructing\nthe ImageData object. On either side of the call, the next challenge is in\ngetting the ImageData back from the ArrayBuffer instance.\n\nlet pixels = new ImageData( \n  new Uint8ClampedArray( evt.data.pixels ),\n  evt.data.width,\n  evt.data.height \n);\n\n\nThe ImageData class has two constructors. The first takes a width and height,\nand creates a black retangle. The second takes an Uint8ClampedArray, width, and\nheight. The Uint8ClampedArray has four different constructors, one of which\ntakes an ArrayBuffer. Since we passed in an ArrayBuffer (by reference), we can\nuse that to create the Uint8ClampedArray as needed by the ImageData, in addition\nto the width and height properties we passed along.\n\nThe Results\nFrom an image processing perspective, we are back in business, now with an extra\n10 milliseconds. That right, this techniques cut an already pretty light 20\nmilliseconds in half (50%). The impact on frame rate is still beyond the human\neye, but this leaves 10 milliseconds for us to perform other image processing. I\nwill start leaning into those specifics in my next post.","feature_image":"__GHOST_URL__/content/images/2018/10/needle.thread-1.jpg","featured":0,"status":"published","locale":null,"visibility":"public","author_id":"1","created_at":"2018-10-29T15:54:05.000Z","updated_at":"2018-10-31T17:59:59.000Z","published_at":"2018-10-31T18:00:00.000Z","custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"type":"post","email_recipient_filter":"none"},{"id":"5c1a866ba40d3300bf69139f","uuid":"d8c4a2cd-1277-4bcb-9c1e-5b47bc552ed5","title":"What I Install","slug":"what-i-install","mobiledoc":"{\"version\":\"0.3.1\",\"atoms\":[],\"cards\":[],\"markups\":[],\"sections\":[[1,\"p\",[[0,[],0,\"I have been a Mac user for some time, and really committed to the platform with the release of Mac OS X. With every major release I completely wipe my machine, and reinstall everything from scratch. Why not just upgrade in-place? Because I like the minty cool freshness of a clean install. This of course means I need to install all my applications from scratch as well. Here is what I installed, and why, when I moved to Mojave (Nov 2018).\"]]],[1,\"p\",[[0,[],0,\"In the order in which installed ...\"]]],[1,\"h3\",[[0,[],0,\"Chrome\"]]],[1,\"p\",[[0,[],0,\"It is no secret that I am a massive supporter of Web Standards. It should be no surprise then that my first stop is for a browser. Google and Chrome take a lot of criticism, especially with Microsoft moving to Chromium. For my needs however, there is just no browser that pushes the standards as far and as hard - even including bleeding edge features like Web Bluetooth.\"]]],[1,\"h3\",[[0,[],0,\"TextMate\"]]],[1,\"p\",[[0,[],0,\"Simple, fast, robust, text editor. I do not always need an IDE. Sometimes I just want a developer-style clipboard for code snippets. The formatting features are a big help with XML and JSON data. Plug-ins for just about every technology under the sun. Command-line integration makes it easy to get to in a pinch. Purply-pink flower goodness.\"]]],[1,\"h3\",[[0,[],0,\"GitHub Desktop\"]]],[1,\"p\",[[0,[],0,\"I am not afraid of the command-line, but I really like the official UI-based tooling for managing all my code. After a clean install, this is how I restore all the various projects I have been working on. Used for repositories for conferences, personal use, and various corporate projects.\"]]],[1,\"h3\",[[0,[],0,\"Visual Studio Code\"]]],[1,\"p\",[[0,[],0,\"And sometimes you do want an IDE. Broad support for most languages/platforms, and extremely customizable.\"]]],[1,\"h3\",[[0,[],0,\"Adobe Creative Cloud\"]]],[1,\"p\",[[0,[],0,\"You might think I would be bitter here as I was laid off at Adobe after a nearly 15-year run. The company decided that developers were no longer a worthwhile investment, and cut all developer relations. All that time at the company however gave me a chance to be downright capable with many of the Creative Cloud products. Most notably, Photoshop, Illustration, Premiere, and Audition.\"]]],[1,\"h3\",[[0,[],0,\"Docker\"]]],[1,\"p\",[[0,[],0,\"While I generally do not have much interest in containerization, I can certainly appreciate the features it brings to developers. I used to develop with tooling installed into my user account on the operating system. Then I had to manage versions and conflicts. Now I develop everything in a variety of containers, keeping my machine clean, and code portable.\"]]],[1,\"h3\",[[0,[],0,\"Querious\"]]],[1,\"p\",[[0,[],0,\"MySQL? In the age of N0SQL? Yup! SQL makes sense to me. I am very capable with it, so when I need data storage, I tend to reach for MySQL. Having visual tooling installed locally gives me quick access to my data, and makes iterating on normalization a snap. Import and export data, easily define character sets, key references, and more.\"]]],[1,\"h3\",[[0,[],0,\"Xcode\"]]],[1,\"p\",[[0,[],0,\"From time-to-time I venture into native Mac and iOS development, these days using Swift. When I install Xcode however, I am doing so almost exclusively for the the iOS Simulator. Nothing beats testing your PWA on what effectively amounts to the actual device. Rendering is spot on across Simulator and device. Debugging integration with Safari is extremely useful.\"]]],[1,\"h3\",[[0,[],0,\"Paw\"]]],[1,\"p\",[[0,[],0,\"HTTP is everywhere. APIs are predominantly HTTP these days. Having top-notch tooling is key. Paw even supports more complicated authentication mechanisms such as OAuth. Code generation for a variety of languages and corresponding libraries, or just the raw HTTP. My favorite recent addition is the cloud sync functionality, which keeps all my calls in the cloud for quick restore on a clean machine.\"]]],[1,\"h3\",[[0,[],0,\"Microsoft Office\"]]],[1,\"p\",[[0,[],0,\"I am more of a Google Office user, and have been known to build presentations using it that others take for Keynote or PowerPoint. Just because I use Google Office does not mean everybody else does, and sometimes you do not want to import those random files - you just want a quick glance. Installed mostly because it is still hard not to.\"]]],[1,\"h3\",[[0,[],0,\"Transmit\"]]],[1,\"p\",[[0,[],0,\"FTP client par excellence. Great for working with IBM Cloud Object Storage, Amazon S3 buckets, and my local NAS.\"]]],[1,\"h3\",[[0,[],0,\"Day One\"]]],[1,\"p\",[[0,[],0,\"I use Day One for what is probably not a common case - for taking notes during meetings. Every meeting I have, especially those held virtually, has an entry in Day One. Easily searchable to remember who said what, and when. I also have journals for my cigar tasting, and movie watching past times.\"]]],[1,\"h3\",[[0,[],0,\"Cisco AnyConnect\"]]],[1,\"p\",[[0,[],0,\"Because sometimes I need to get on the VPN. Expenses and all that. They make me do it. It has been yeas since MacOS moved to black icons in the Finder. For the love of all that is good and holy, can you please make the icon black!\"]]],[1,\"h3\",[[0,[],0,\"Slack\"]]],[1,\"p\",[[0,[],0,\"They say email is dead. I like to say that I no longer have an email problem, I now have a Slack problem. I do not know what the Slack equivalent of inbox-zero is, but I am pretty sure that is never going to happen. The main tool used for collaboration across advocate teams at IBM.\"]]],[1,\"h3\",[[0,[],0,\"Zoom\"]]],[1,\"p\",[[0,[],0,\"IBM has a corporate standard on Webex, so why install Zoom? Some people just refuse the make the jump. Non-conformists!\"]]],[1,\"h3\",[[0,[],0,\"Box\"]]],[1,\"p\",[[0,[],0,\"The IBM corporate standard for document synchronization. People around IBM use it in ways that it probably really should not be used, which I suppose is a testament to the product's capabilities. Would much prefer Google Office.\"]]],[1,\"h3\",[[0,[],0,\"Webex\"]]],[1,\"p\",[[0,[],0,\"Last but not least - literally - I probably use Webex more than I do an IDE. So many meetings. Many many tiers of middle management. The IBM way. I am just a first-line manager, bottom of the pile nobody, and I spend an average of twenty (20) hours per week in Webex. Thankfully, bringing this list full circle, Webex has a web-based client now for participating in meetings.\"]]],[1,\"p\",[]]],\"ghostVersion\":\"3.0\"}","html":"<p>I have been a Mac user for some time, and really committed to the platform with the release of Mac OS X. With every major release I completely wipe my machine, and reinstall everything from scratch. Why not just upgrade in-place? Because I like the minty cool freshness of a clean install. This of course means I need to install all my applications from scratch as well. Here is what I installed, and why, when I moved to Mojave (Nov 2018).</p><p>In the order in which installed ...</p><h3 id=\"chrome\">Chrome</h3><p>It is no secret that I am a massive supporter of Web Standards. It should be no surprise then that my first stop is for a browser. Google and Chrome take a lot of criticism, especially with Microsoft moving to Chromium. For my needs however, there is just no browser that pushes the standards as far and as hard - even including bleeding edge features like Web Bluetooth.</p><h3 id=\"textmate\">TextMate</h3><p>Simple, fast, robust, text editor. I do not always need an IDE. Sometimes I just want a developer-style clipboard for code snippets. The formatting features are a big help with XML and JSON data. Plug-ins for just about every technology under the sun. Command-line integration makes it easy to get to in a pinch. Purply-pink flower goodness.</p><h3 id=\"github-desktop\">GitHub Desktop</h3><p>I am not afraid of the command-line, but I really like the official UI-based tooling for managing all my code. After a clean install, this is how I restore all the various projects I have been working on. Used for repositories for conferences, personal use, and various corporate projects.</p><h3 id=\"visual-studio-code\">Visual Studio Code</h3><p>And sometimes you do want an IDE. Broad support for most languages/platforms, and extremely customizable.</p><h3 id=\"adobe-creative-cloud\">Adobe Creative Cloud</h3><p>You might think I would be bitter here as I was laid off at Adobe after a nearly 15-year run. The company decided that developers were no longer a worthwhile investment, and cut all developer relations. All that time at the company however gave me a chance to be downright capable with many of the Creative Cloud products. Most notably, Photoshop, Illustration, Premiere, and Audition.</p><h3 id=\"docker\">Docker</h3><p>While I generally do not have much interest in containerization, I can certainly appreciate the features it brings to developers. I used to develop with tooling installed into my user account on the operating system. Then I had to manage versions and conflicts. Now I develop everything in a variety of containers, keeping my machine clean, and code portable.</p><h3 id=\"querious\">Querious</h3><p>MySQL? In the age of N0SQL? Yup! SQL makes sense to me. I am very capable with it, so when I need data storage, I tend to reach for MySQL. Having visual tooling installed locally gives me quick access to my data, and makes iterating on normalization a snap. Import and export data, easily define character sets, key references, and more.</p><h3 id=\"xcode\">Xcode</h3><p>From time-to-time I venture into native Mac and iOS development, these days using Swift. When I install Xcode however, I am doing so almost exclusively for the the iOS Simulator. Nothing beats testing your PWA on what effectively amounts to the actual device. Rendering is spot on across Simulator and device. Debugging integration with Safari is extremely useful.</p><h3 id=\"paw\">Paw</h3><p>HTTP is everywhere. APIs are predominantly HTTP these days. Having top-notch tooling is key. Paw even supports more complicated authentication mechanisms such as OAuth. Code generation for a variety of languages and corresponding libraries, or just the raw HTTP. My favorite recent addition is the cloud sync functionality, which keeps all my calls in the cloud for quick restore on a clean machine.</p><h3 id=\"microsoft-office\">Microsoft Office</h3><p>I am more of a Google Office user, and have been known to build presentations using it that others take for Keynote or PowerPoint. Just because I use Google Office does not mean everybody else does, and sometimes you do not want to import those random files - you just want a quick glance. Installed mostly because it is still hard not to.</p><h3 id=\"transmit\">Transmit</h3><p>FTP client par excellence. Great for working with IBM Cloud Object Storage, Amazon S3 buckets, and my local NAS.</p><h3 id=\"day-one\">Day One</h3><p>I use Day One for what is probably not a common case - for taking notes during meetings. Every meeting I have, especially those held virtually, has an entry in Day One. Easily searchable to remember who said what, and when. I also have journals for my cigar tasting, and movie watching past times.</p><h3 id=\"cisco-anyconnect\">Cisco AnyConnect</h3><p>Because sometimes I need to get on the VPN. Expenses and all that. They make me do it. It has been yeas since MacOS moved to black icons in the Finder. For the love of all that is good and holy, can you please make the icon black!</p><h3 id=\"slack\">Slack</h3><p>They say email is dead. I like to say that I no longer have an email problem, I now have a Slack problem. I do not know what the Slack equivalent of inbox-zero is, but I am pretty sure that is never going to happen. The main tool used for collaboration across advocate teams at IBM.</p><h3 id=\"zoom\">Zoom</h3><p>IBM has a corporate standard on Webex, so why install Zoom? Some people just refuse the make the jump. Non-conformists!</p><h3 id=\"box\">Box</h3><p>The IBM corporate standard for document synchronization. People around IBM use it in ways that it probably really should not be used, which I suppose is a testament to the product's capabilities. Would much prefer Google Office.</p><h3 id=\"webex\">Webex</h3><p>Last but not least - literally - I probably use Webex more than I do an IDE. So many meetings. Many many tiers of middle management. The IBM way. I am just a first-line manager, bottom of the pile nobody, and I spend an average of twenty (20) hours per week in Webex. Thankfully, bringing this list full circle, Webex has a web-based client now for participating in meetings.</p>","comment_id":"5c1a866ba40d3300bf69139f","plaintext":"I have been a Mac user for some time, and really committed to the platform with\nthe release of Mac OS X. With every major release I completely wipe my machine,\nand reinstall everything from scratch. Why not just upgrade in-place? Because I\nlike the minty cool freshness of a clean install. This of course means I need to\ninstall all my applications from scratch as well. Here is what I installed, and\nwhy, when I moved to Mojave (Nov 2018).\n\nIn the order in which installed ...\n\nChrome\nIt is no secret that I am a massive supporter of Web Standards. It should be no\nsurprise then that my first stop is for a browser. Google and Chrome take a lot\nof criticism, especially with Microsoft moving to Chromium. For my needs\nhowever, there is just no browser that pushes the standards as far and as hard -\neven including bleeding edge features like Web Bluetooth.\n\nTextMate\nSimple, fast, robust, text editor. I do not always need an IDE. Sometimes I just\nwant a developer-style clipboard for code snippets. The formatting features are\na big help with XML and JSON data. Plug-ins for just about every technology\nunder the sun. Command-line integration makes it easy to get to in a pinch.\nPurply-pink flower goodness.\n\nGitHub Desktop\nI am not afraid of the command-line, but I really like the official UI-based\ntooling for managing all my code. After a clean install, this is how I restore\nall the various projects I have been working on. Used for repositories for\nconferences, personal use, and various corporate projects.\n\nVisual Studio Code\nAnd sometimes you do want an IDE. Broad support for most languages/platforms,\nand extremely customizable.\n\nAdobe Creative Cloud\nYou might think I would be bitter here as I was laid off at Adobe after a nearly\n15-year run. The company decided that developers were no longer a worthwhile\ninvestment, and cut all developer relations. All that time at the company\nhowever gave me a chance to be downright capable with many of the Creative Cloud\nproducts. Most notably, Photoshop, Illustration, Premiere, and Audition.\n\nDocker\nWhile I generally do not have much interest in containerization, I can certainly\nappreciate the features it brings to developers. I used to develop with tooling\ninstalled into my user account on the operating system. Then I had to manage\nversions and conflicts. Now I develop everything in a variety of containers,\nkeeping my machine clean, and code portable.\n\nQuerious\nMySQL? In the age of N0SQL? Yup! SQL makes sense to me. I am very capable with\nit, so when I need data storage, I tend to reach for MySQL. Having visual\ntooling installed locally gives me quick access to my data, and makes iterating\non normalization a snap. Import and export data, easily define character sets,\nkey references, and more.\n\nXcode\nFrom time-to-time I venture into native Mac and iOS development, these days\nusing Swift. When I install Xcode however, I am doing so almost exclusively for\nthe the iOS Simulator. Nothing beats testing your PWA on what effectively\namounts to the actual device. Rendering is spot on across Simulator and device.\nDebugging integration with Safari is extremely useful.\n\nPaw\nHTTP is everywhere. APIs are predominantly HTTP these days. Having top-notch\ntooling is key. Paw even supports more complicated authentication mechanisms\nsuch as OAuth. Code generation for a variety of languages and corresponding\nlibraries, or just the raw HTTP. My favorite recent addition is the cloud sync\nfunctionality, which keeps all my calls in the cloud for quick restore on a\nclean machine.\n\nMicrosoft Office\nI am more of a Google Office user, and have been known to build presentations\nusing it that others take for Keynote or PowerPoint. Just because I use Google\nOffice does not mean everybody else does, and sometimes you do not want to\nimport those random files - you just want a quick glance. Installed mostly\nbecause it is still hard not to.\n\nTransmit\nFTP client par excellence. Great for working with IBM Cloud Object Storage,\nAmazon S3 buckets, and my local NAS.\n\nDay One\nI use Day One for what is probably not a common case - for taking notes during\nmeetings. Every meeting I have, especially those held virtually, has an entry in\nDay One. Easily searchable to remember who said what, and when. I also have\njournals for my cigar tasting, and movie watching past times.\n\nCisco AnyConnect\nBecause sometimes I need to get on the VPN. Expenses and all that. They make me\ndo it. It has been yeas since MacOS moved to black icons in the Finder. For the\nlove of all that is good and holy, can you please make the icon black!\n\nSlack\nThey say email is dead. I like to say that I no longer have an email problem, I\nnow have a Slack problem. I do not know what the Slack equivalent of inbox-zero\nis, but I am pretty sure that is never going to happen. The main tool used for\ncollaboration across advocate teams at IBM.\n\nZoom\nIBM has a corporate standard on Webex, so why install Zoom? Some people just\nrefuse the make the jump. Non-conformists!\n\nBox\nThe IBM corporate standard for document synchronization. People around IBM use\nit in ways that it probably really should not be used, which I suppose is a\ntestament to the product's capabilities. Would much prefer Google Office.\n\nWebex\nLast but not least - literally - I probably use Webex more than I do an IDE. So\nmany meetings. Many many tiers of middle management. The IBM way. I am just a\nfirst-line manager, bottom of the pile nobody, and I spend an average of twenty\n(20) hours per week in Webex. Thankfully, bringing this list full circle, Webex\nhas a web-based client now for participating in meetings.","feature_image":"__GHOST_URL__/content/images/2018/12/library.congress.jpg","featured":0,"status":"published","locale":null,"visibility":"public","author_id":"1","created_at":"2018-12-19T17:56:59.000Z","updated_at":"2018-12-19T18:57:03.000Z","published_at":"2018-12-19T18:53:54.000Z","custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"type":"post","email_recipient_filter":"none"},{"id":"5c3fbb70dff84900c0a5620b","uuid":"c3d6d7c4-cb08-4d8d-b74c-589b3264562c","title":"Serverless Upload to Object Storage","slug":"upload-files-to","mobiledoc":"{\"version\":\"0.3.1\",\"atoms\":[],\"cards\":[[\"markdown\",{\"markdown\":\"```\\nPOST /cgi-bin/qtest HTTP/1.1\\nHost: aram\\nUser-Agent: Mozilla/5.0 Gecko/2009042316 Firefox/3.0.10\\nAccept: text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\\nAccept-Language: en-us,en;q=0.5\\nAccept-Encoding: gzip,deflate\\nAccept-Charset: ISO-8859-1,utf-8;q=0.7,*;q=0.7\\nKeep-Alive: 300\\nConnection: keep-alive\\nReferer: http://aram/~martind/banner.htm\\nContent-Type: multipart/form-data; boundary=----------287032381131322\\nContent-Length: 514\\n\\n------------287032381131322\\nContent-Disposition: form-data; name=\\\"datafile1\\\"; filename=\\\"r.gif\\\"\\nContent-Type: image/gif\\n\\nGIF87a.............,...........D..;\\n------------287032381131322\\nContent-Disposition: form-data; name=\\\"datafile2\\\"; filename=\\\"g.gif\\\"\\nContent-Type: image/gif\\n\\nGIF87a.............,...........D..;\\n------------287032381131322\\nContent-Disposition: form-data; name=\\\"datafile3\\\"; filename=\\\"b.gif\\\"\\nContent-Type: image/gif\\n\\nGIF87a.............,...........D..;\\n------------287032381131322--\\n```\"}],[\"markdown\",{\"markdown\":\"```\\nibmcloud fn action create cos/post.object --kind nodejs:8 action.zip --web raw\\n```\"}],[\"image\",{\"src\":\"/content/images/2019/01/post.object.web.ui.png\"}],[\"markdown\",{\"markdown\":\"```\\nconst multipart = require( 'parted' ).multipart;\\nconst sts = require( 'string-to-stream' );\\n\\nlet decoded = new Buffer( params.__ow_body, 'base64' );\\nlet stream = sts( decoded );\\n\\nconst options = {\\n  limit: 30 * 1024,\\n  diskLimit: 30 * 1024 * 1024\\n};\\n\\nlet parser = new multipart( \\n  params.__ow_headers[ 'content-type' ],\\n  options\\n);\\nlet parts = {};\\n\\nparser.on( 'error', function( err ) {\\n  console.log( 'parser error', err );\\n} );\\n\\nparser.on( 'part', function( field, part ) {\\n  parts[field] = part;\\n} );\\n\\nparser.on( 'end', function() {\\n  console.log( 'File upload complete.' );\\n} );\\n\\nstream.pipe( parser );\\n```\"}],[\"markdown\",{\"markdown\":\"```\\nlet cos = new AWS.S3( {\\n  endpoint: '<endpoint>',\\n  apiKeyId: '<api-key>',\\n  ibmAuthEndpoint: 'https://iam.ng.bluemix.net/oidc/token',\\n  serviceInstanceId: '<resource-instance-id>',\\n} );\\n```\"}],[\"markdown\",{\"markdown\":\"```\\nlet obj = fs.readFileSync( parts.file );\\n\\nawait cos.putObject( {\\n  Body: obj,\\n  Bucket: 'file-upload',\\n  Key: parts.name   \\n} )\\n.promise()\\n.then( ( data ) => {\\n  console.log( 'File storage complete.' );\\n} );\\n```\"}],[\"markdown\",{\"markdown\":\"```\\nfunction upload( params ) {\\n  const AWS = require( 'ibm-cos-sdk' );\\n  const fs = require( 'fs' );\\n  const multipart = require( 'parted' ).multipart;\\n  const sts = require( 'string-to-stream' );\\n\\n  return new Promise( ( resolve, reject ) => {\\n    let decoded = new Buffer( params.__ow_body, 'base64' );\\n    let stream = sts( decoded );\\n\\n    let parser = new multipart(\\n      params.__ow_headers[ 'content-type' ], {\\n        limit: 30 * 1024,\\n        diskLimit: 30 * 1024 * 1024\\n      }\\n    );\\n    let parts = {};\\n\\n    parser.on( 'error', function( err ) {\\n      console.log( 'Whoops!', err );\\n    } );\\n\\n    parser.on( 'part', function( field, part ) {\\n      parts[field] = part;\\n    } );\\n\\n    parser.on( 'end', async function() {\\n      let cos = new AWS.S3( {\\n        endpoint: params.COS_ENDPOINT,\\n        apiKeyId: params.COS_API_KEY,\\n        ibmAuthEndpoint: params.COS_AUTH_ENDPOINT,\\n        serviceInstanceId: params.COS_SERVICE_INSTANCE\\n      } );\\n    \\n      let obj = fs.readFileSync( parts.file );\\n    \\n      await cos.putObject( {\\n        Body: obj,\\n        Bucket: params.COS_BUCKET,\\n        Key: parts.name\\n      } )\\n      .promise()\\n      .then( ( data ) => {\\n        resolve( {\\n          headers: {\\n            'Content-Type': 'application/json'\\n          },\\n          body: data\\n        } );\\n      } );\\n    } );\\n\\n    stream.pipe( parser );\\n  } );\\n}\\n\\n// Do not forget to export\\nexports.main = upload;\\n```\"}]],\"markups\":[[\"a\",[\"href\",\"https://console.bluemix.net/openwhisk/\"]],[\"a\",[\"href\",\"https://openwhisk.apache.org/\"]],[\"a\",[\"href\",\"https://www.ibm.com/cloud/object-storage\"]],[\"a\",[\"href\",\"https://stackoverflow.com/questions/913626/what-should-a-multipart-http-request-with-multiple-files-look-like\"]],[\"a\",[\"href\",\"https://github.com/chjj/parted\"]],[\"a\",[\"href\",\"https://github.com/feross/string-to-stream\"]],[\"a\",[\"href\",\"https://wiki.openstack.org/wiki/Swift\"]],[\"a\",[\"href\",\"https://github.com/ibm/ibm-cos-sdk-js\"]],[\"strong\"]],\"sections\":[[1,\"p\",[[0,[],0,\"So there you are, building the world's next breakthrough social platform. And of course you want to minimize cost, administration, etc. so you use \"],[0,[0],1,\"IBM Cloud Functions\"],[0,[],0,\" (\"],[0,[1],1,\"Apache OpenWhisk\"],[0,[],0,\"). Then you decide to let users manage their profile picture. Uploading a file to a Cloud Function takes some consideration - and then there is the problem that the environment will destroy itself, and the uploaded content, after execution. In this post, we combine \"],[0,[2],1,\"IBM Cloud Object Storage\"],[0,[],0,\" with Web Actions for robust, persistent storage.\"]]],[1,\"h2\",[[0,[],0,\"HTTP Multipart Request\"]]],[1,\"p\",[[0,[],0,\"Most web developers will think of this as submitting an HTML form - often for the purposes of uploading files. When we look at the content going across the wire, you will first see a typical HTTP header, with one notable addition. There will be an entry in the header for the \\\"boundary\\\" of the various pieces of content. Then as you look trough the rest of the request, you will see that boundary marker between each distinct item being sent - a form field, and a file, for example. \"]]],[1,\"p\",[[0,[],0,\"From an example shown on \"],[0,[3],1,\"Stack Overflow\"],[0,[],0,\":\"]]],[10,0],[1,\"p\",[[0,[],0,\"When this content arrives at the traditional middleware, such as Express on Node.js, the boundary label is found, and the request is parsed to get the parts. Parts are then placed in variables relative to how the middleware functions. In the context of a Cloud Function however, there is no Express to parse the request, so we need to (a) get access to the raw HTTP content and (b) parse it ourselves.\"]]],[1,\"h2\",[[0,[],0,\"Upload to Cloud Function\"]]],[1,\"p\",[[0,[],0,\"If you are familiar with the CLI for Cloud Functions, you may already be familiar with the \\\"--web\\\" argument, which allows you to expose the function to HTTP endpoints. There is a \\\"raw\\\" addition that can be made to this argument to enable raw handing of the HTTP content. With this change the function will receive the raw HTTP string, not an object that has already been parsed. And this is exactly what we want.\"]]],[10,1],[1,\"p\",[[0,[],0,\"Note that this setting can be found in the IBM Cloud Functions web-based UI as well. Select an action and then \\\"Endpoints\\\". In fact, I prefer the web UI for tweaking actions once they have been created.\"]]],[10,2],[1,\"h3\",[[0,[],0,\"Parsing the HTTP Content\"]]],[1,\"p\",[[0,[],0,\"When I originally started down this path, I was determined to parse the HTTP content myself. After all, the boundary is called out right there in the header. How hard could it be to find the boundary in the body and parse it myself? It turns out that there are a lot of edge cases that make this harder than it sounds. In the end, I turned to the \\\"\"],[0,[4],1,\"parted\"],[0,[],0,\"\\\" library for help, with a special assist from \\\"\"],[0,[5],1,\"string-to-stream\"],[0,[],0,\"\\\".\"]]],[10,3],[1,\"p\",[[0,[],0,\"The first step here is to decode the HTTP request body, and create a stream from it. Then we set aside a splash of disk space. Up next is the actual parsing of the body. Various events occur during the parsing, most notably, when the parsing process has ended. When the parsing ends you will have a \\\"parts\\\" variable with properties reflecting the various names of the fields that were provided when the request was made.\"]]],[1,\"p\",[[0,[],0,\"That is to say that if you have <input type=\\\"file\\\" name=\\\"picture\\\"> then you will have a \\\"parts.picture\\\" property containing the file contents. You can then choose to write it to disk, send it on to another service for further processing, etc.\"]]],[1,\"p\",[[0,[],0,\"Of course as a user profile picture, or other file type you may want to keep around for some duration, writing the contents to disk is not going to do you any good. That content will be destroyed along with the Cloud Function once the processing has finished. We really need to put the file somewhere reliable - such as IBM Cloud Object Storage.\"]]],[1,\"h2\",[[0,[],0,\"Upload to Object Storage\"]]],[1,\"p\",[[0,[],0,\"Under the covers, IBM Cloud Object Storage (COS) is \"],[0,[6],1,\"OpenStack Swift\"],[0,[],0,\", and conforms to the interface established by AWS S3. You create buckets, and put objects in the buckets. There are SDKs available in various languages, but we are going to continue along the Node.js path. There is a specific \"],[0,[7],1,\"fork\"],[0,[],0,\" of the S3 library for COS which I recommend using for the most consistent results.\"]]],[1,\"h3\",[[0,[],0,\"Instantiate COS\"]]],[1,\"p\",[[0,[],0,\"The first step in using the library is to instantiate it as a client. A quick look at the documentation for the library shows that we need \\\"endpoint\\\", \\\"apiKeyId\\\", and \\\"serviceInstanceId\\\" values. For me, finding the right pieces in the UI took some effort the first time.\"]]],[10,4],[1,\"p\",[[0,[8],1,\"Endpoint\"]]],[1,\"p\",[[0,[],0,\"When you are looking at the list of buckets, you will see a \\\"Location\\\" column. The bucket I am using for this example sits in \\\"us-east\\\". Click on \\\"Endpoint\\\" in the left sidebar. When I created the bucket, I selected \\\"Regional\\\" resiliency, so I will select \\\"Regional\\\". Then select the corresponding location such as \\\"us-east\\\". We will be using \\\"Public\\\" access, which in the case of \\\"us-east\\\" yields \\\"s3.us-east.cloud-object-storage.appdomain.cloud\\\".\"]]],[1,\"p\",[[0,[8],1,\"API Key\"]]],[1,\"p\",[[0,[],0,\"Next we will head to the \\\"Service credentials\\\" section by clicking on that link in the left sidebar. You may have a number of credentials, each will have the option to \\\"View credentials\\\". Expanding this section yields a section of JSON. Look for the field labeled \\\"apikey\\\".\"]]],[1,\"p\",[[0,[8],1,\"Service Instance ID\"]]],[1,\"p\",[[0,[],0,\"The \\\"resource-instance-id\\\" is found in this block of JSON-formatted credentials as well.\"]]],[1,\"h3\",[[0,[],0,\"Put an Object\"]]],[1,\"p\",[[0,[],0,\"Putting an object into COS looks exactly like it would if you were using the AWS S3 SDK. On the COS client you call \\\"putObject()\\\" with the content you want to store, the bucket in which you want to store it, and the name of the content itself.\"]]],[10,5],[1,\"p\",[[0,[],0,\"When we uploaded the file from the browser (or other client) to the Cloud Function, we ended up with a \\\"parts\\\" object holding the file contents in a \\\"file\\\" property (which came from the name of the HTML input). To get this ready for storage, we read it into a Buffer object and provide that as the \\\"Body\\\" parameter. \"]]],[1,\"p\",[[0,[],0,\"The \\\"Bucket\\\" name should match the desired bucket from your COS instance. You can name the file whatever you need to for your application. \\\"Key\\\" is equivalent to \\\"name\\\" in the COS/S3 model. For the purposes of flexibility, I like to send the name along with the file - effectively, I like to make the name a parameter on the service.\"]]],[1,\"blockquote\",[[0,[],0,\"The \\\"putObject()\\\" call is using \\\"async/await\\\". If we do not wait for the upload to finish, the function will terminate before the file ever gets uploaded.\"]]],[1,\"h2\",[[0,[],0,\"All Together Now\"]]],[1,\"p\",[[0,[],0,\"Now that we know how to upload, and parse, a file to a Cloud Function, and how to then put the file onto Cloud Object Storage, all that is left is to string the bits together.\"]]],[10,6],[1,\"p\",[[0,[],0,\"There are some particularly important details to consider when putting everything together. The first is that the main function returns a \\\"Promise\\\". This is done to keep the function running until a result is ready. Without this promise, the function would terminate and the file would (a) never get parsed and (b) never get stored.\"]]],[1,\"p\",[[0,[],0,\"The other important detail comes up in the \\\"end\\\" callback of the parsing operation. The function provided is marked as \\\"async\\\". The reason for this is that the COS operation is promise-based. And again, if we do not wait for it to finish, the function will return before the file is stored. The means we need to use the \\\"await\\\" keyword. The \\\"await\\\" keyword can only be used inside an \\\"async\\\" function.\"]]],[1,\"p\",[[0,[],0,\"It is probably worth noting that I put my COS credentials in parameters provided to the function. I do not however expose those parameters to the client. Those parameters are provided at the function level. You can do this on the CLI, or in the Cloud Functions UI tooling. I prefer the tooling when working with very long strings like the ones needed for COS connectivity. YMMV.\"]]],[1,\"h2\",[[0,[],0,\"Next Steps\"]]],[1,\"p\",[[0,[],0,\"Now that we know how to get a file up to IBM Cloud Object Storage via IBM Cloud Functions, the next step is to get it back down. I will go over what this looks like in my next post. This will give us the \\\"read\\\", \\\"edit\\\", and \\\"add\\\" operations of BREAD. A little further on, and we can \\\"browse\\\" and \\\"delete\\\". Then we can piece together a full file management system built using serverless.\"]]]],\"ghostVersion\":\"3.0\"}","html":"<p>So there you are, building the world's next breakthrough social platform. And of course you want to minimize cost, administration, etc. so you use <a href=\"https://console.bluemix.net/openwhisk/\">IBM Cloud Functions</a> (<a href=\"https://openwhisk.apache.org/\">Apache OpenWhisk</a>). Then you decide to let users manage their profile picture. Uploading a file to a Cloud Function takes some consideration - and then there is the problem that the environment will destroy itself, and the uploaded content, after execution. In this post, we combine <a href=\"https://www.ibm.com/cloud/object-storage\">IBM Cloud Object Storage</a> with Web Actions for robust, persistent storage.</p><h2 id=\"http-multipart-request\">HTTP Multipart Request</h2><p>Most web developers will think of this as submitting an HTML form - often for the purposes of uploading files. When we look at the content going across the wire, you will first see a typical HTTP header, with one notable addition. There will be an entry in the header for the \"boundary\" of the various pieces of content. Then as you look trough the rest of the request, you will see that boundary marker between each distinct item being sent - a form field, and a file, for example. </p><p>From an example shown on <a href=\"https://stackoverflow.com/questions/913626/what-should-a-multipart-http-request-with-multiple-files-look-like\">Stack Overflow</a>:</p><!--kg-card-begin: markdown--><pre><code>POST /cgi-bin/qtest HTTP/1.1\nHost: aram\nUser-Agent: Mozilla/5.0 Gecko/2009042316 Firefox/3.0.10\nAccept: text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\nAccept-Language: en-us,en;q=0.5\nAccept-Encoding: gzip,deflate\nAccept-Charset: ISO-8859-1,utf-8;q=0.7,*;q=0.7\nKeep-Alive: 300\nConnection: keep-alive\nReferer: http://aram/~martind/banner.htm\nContent-Type: multipart/form-data; boundary=----------287032381131322\nContent-Length: 514\n\n------------287032381131322\nContent-Disposition: form-data; name=&quot;datafile1&quot;; filename=&quot;r.gif&quot;\nContent-Type: image/gif\n\nGIF87a.............,...........D..;\n------------287032381131322\nContent-Disposition: form-data; name=&quot;datafile2&quot;; filename=&quot;g.gif&quot;\nContent-Type: image/gif\n\nGIF87a.............,...........D..;\n------------287032381131322\nContent-Disposition: form-data; name=&quot;datafile3&quot;; filename=&quot;b.gif&quot;\nContent-Type: image/gif\n\nGIF87a.............,...........D..;\n------------287032381131322--\n</code></pre>\n<!--kg-card-end: markdown--><p>When this content arrives at the traditional middleware, such as Express on Node.js, the boundary label is found, and the request is parsed to get the parts. Parts are then placed in variables relative to how the middleware functions. In the context of a Cloud Function however, there is no Express to parse the request, so we need to (a) get access to the raw HTTP content and (b) parse it ourselves.</p><h2 id=\"upload-to-cloud-function\">Upload to Cloud Function</h2><p>If you are familiar with the CLI for Cloud Functions, you may already be familiar with the \"--web\" argument, which allows you to expose the function to HTTP endpoints. There is a \"raw\" addition that can be made to this argument to enable raw handing of the HTTP content. With this change the function will receive the raw HTTP string, not an object that has already been parsed. And this is exactly what we want.</p><!--kg-card-begin: markdown--><pre><code>ibmcloud fn action create cos/post.object --kind nodejs:8 action.zip --web raw\n</code></pre>\n<!--kg-card-end: markdown--><p>Note that this setting can be found in the IBM Cloud Functions web-based UI as well. Select an action and then \"Endpoints\". In fact, I prefer the web UI for tweaking actions once they have been created.</p><figure class=\"kg-card kg-image-card\"><img src=\"/content/images/2019/01/post.object.web.ui.png\" class=\"kg-image\" alt loading=\"lazy\"></figure><h3 id=\"parsing-the-http-content\">Parsing the HTTP Content</h3><p>When I originally started down this path, I was determined to parse the HTTP content myself. After all, the boundary is called out right there in the header. How hard could it be to find the boundary in the body and parse it myself? It turns out that there are a lot of edge cases that make this harder than it sounds. In the end, I turned to the \"<a href=\"https://github.com/chjj/parted\">parted</a>\" library for help, with a special assist from \"<a href=\"https://github.com/feross/string-to-stream\">string-to-stream</a>\".</p><!--kg-card-begin: markdown--><pre><code>const multipart = require( 'parted' ).multipart;\nconst sts = require( 'string-to-stream' );\n\nlet decoded = new Buffer( params.__ow_body, 'base64' );\nlet stream = sts( decoded );\n\nconst options = {\n  limit: 30 * 1024,\n  diskLimit: 30 * 1024 * 1024\n};\n\nlet parser = new multipart( \n  params.__ow_headers[ 'content-type' ],\n  options\n);\nlet parts = {};\n\nparser.on( 'error', function( err ) {\n  console.log( 'parser error', err );\n} );\n\nparser.on( 'part', function( field, part ) {\n  parts[field] = part;\n} );\n\nparser.on( 'end', function() {\n  console.log( 'File upload complete.' );\n} );\n\nstream.pipe( parser );\n</code></pre>\n<!--kg-card-end: markdown--><p>The first step here is to decode the HTTP request body, and create a stream from it. Then we set aside a splash of disk space. Up next is the actual parsing of the body. Various events occur during the parsing, most notably, when the parsing process has ended. When the parsing ends you will have a \"parts\" variable with properties reflecting the various names of the fields that were provided when the request was made.</p><p>That is to say that if you have &lt;input type=\"file\" name=\"picture\"&gt; then you will have a \"parts.picture\" property containing the file contents. You can then choose to write it to disk, send it on to another service for further processing, etc.</p><p>Of course as a user profile picture, or other file type you may want to keep around for some duration, writing the contents to disk is not going to do you any good. That content will be destroyed along with the Cloud Function once the processing has finished. We really need to put the file somewhere reliable - such as IBM Cloud Object Storage.</p><h2 id=\"upload-to-object-storage\">Upload to Object Storage</h2><p>Under the covers, IBM Cloud Object Storage (COS) is <a href=\"https://wiki.openstack.org/wiki/Swift\">OpenStack Swift</a>, and conforms to the interface established by AWS S3. You create buckets, and put objects in the buckets. There are SDKs available in various languages, but we are going to continue along the Node.js path. There is a specific <a href=\"https://github.com/ibm/ibm-cos-sdk-js\">fork</a> of the S3 library for COS which I recommend using for the most consistent results.</p><h3 id=\"instantiate-cos\">Instantiate COS</h3><p>The first step in using the library is to instantiate it as a client. A quick look at the documentation for the library shows that we need \"endpoint\", \"apiKeyId\", and \"serviceInstanceId\" values. For me, finding the right pieces in the UI took some effort the first time.</p><!--kg-card-begin: markdown--><pre><code>let cos = new AWS.S3( {\n  endpoint: '&lt;endpoint&gt;',\n  apiKeyId: '&lt;api-key&gt;',\n  ibmAuthEndpoint: 'https://iam.ng.bluemix.net/oidc/token',\n  serviceInstanceId: '&lt;resource-instance-id&gt;',\n} );\n</code></pre>\n<!--kg-card-end: markdown--><p><strong>Endpoint</strong></p><p>When you are looking at the list of buckets, you will see a \"Location\" column. The bucket I am using for this example sits in \"us-east\". Click on \"Endpoint\" in the left sidebar. When I created the bucket, I selected \"Regional\" resiliency, so I will select \"Regional\". Then select the corresponding location such as \"us-east\". We will be using \"Public\" access, which in the case of \"us-east\" yields \"s3.us-east.cloud-object-storage.appdomain.cloud\".</p><p><strong>API Key</strong></p><p>Next we will head to the \"Service credentials\" section by clicking on that link in the left sidebar. You may have a number of credentials, each will have the option to \"View credentials\". Expanding this section yields a section of JSON. Look for the field labeled \"apikey\".</p><p><strong>Service Instance ID</strong></p><p>The \"resource-instance-id\" is found in this block of JSON-formatted credentials as well.</p><h3 id=\"put-an-object\">Put an Object</h3><p>Putting an object into COS looks exactly like it would if you were using the AWS S3 SDK. On the COS client you call \"putObject()\" with the content you want to store, the bucket in which you want to store it, and the name of the content itself.</p><!--kg-card-begin: markdown--><pre><code>let obj = fs.readFileSync( parts.file );\n\nawait cos.putObject( {\n  Body: obj,\n  Bucket: 'file-upload',\n  Key: parts.name   \n} )\n.promise()\n.then( ( data ) =&gt; {\n  console.log( 'File storage complete.' );\n} );\n</code></pre>\n<!--kg-card-end: markdown--><p>When we uploaded the file from the browser (or other client) to the Cloud Function, we ended up with a \"parts\" object holding the file contents in a \"file\" property (which came from the name of the HTML input). To get this ready for storage, we read it into a Buffer object and provide that as the \"Body\" parameter. </p><p>The \"Bucket\" name should match the desired bucket from your COS instance. You can name the file whatever you need to for your application. \"Key\" is equivalent to \"name\" in the COS/S3 model. For the purposes of flexibility, I like to send the name along with the file - effectively, I like to make the name a parameter on the service.</p><blockquote>The \"putObject()\" call is using \"async/await\". If we do not wait for the upload to finish, the function will terminate before the file ever gets uploaded.</blockquote><h2 id=\"all-together-now\">All Together Now</h2><p>Now that we know how to upload, and parse, a file to a Cloud Function, and how to then put the file onto Cloud Object Storage, all that is left is to string the bits together.</p><!--kg-card-begin: markdown--><pre><code>function upload( params ) {\n  const AWS = require( 'ibm-cos-sdk' );\n  const fs = require( 'fs' );\n  const multipart = require( 'parted' ).multipart;\n  const sts = require( 'string-to-stream' );\n\n  return new Promise( ( resolve, reject ) =&gt; {\n    let decoded = new Buffer( params.__ow_body, 'base64' );\n    let stream = sts( decoded );\n\n    let parser = new multipart(\n      params.__ow_headers[ 'content-type' ], {\n        limit: 30 * 1024,\n        diskLimit: 30 * 1024 * 1024\n      }\n    );\n    let parts = {};\n\n    parser.on( 'error', function( err ) {\n      console.log( 'Whoops!', err );\n    } );\n\n    parser.on( 'part', function( field, part ) {\n      parts[field] = part;\n    } );\n\n    parser.on( 'end', async function() {\n      let cos = new AWS.S3( {\n        endpoint: params.COS_ENDPOINT,\n        apiKeyId: params.COS_API_KEY,\n        ibmAuthEndpoint: params.COS_AUTH_ENDPOINT,\n        serviceInstanceId: params.COS_SERVICE_INSTANCE\n      } );\n    \n      let obj = fs.readFileSync( parts.file );\n    \n      await cos.putObject( {\n        Body: obj,\n        Bucket: params.COS_BUCKET,\n        Key: parts.name\n      } )\n      .promise()\n      .then( ( data ) =&gt; {\n        resolve( {\n          headers: {\n            'Content-Type': 'application/json'\n          },\n          body: data\n        } );\n      } );\n    } );\n\n    stream.pipe( parser );\n  } );\n}\n\n// Do not forget to export\nexports.main = upload;\n</code></pre>\n<!--kg-card-end: markdown--><p>There are some particularly important details to consider when putting everything together. The first is that the main function returns a \"Promise\". This is done to keep the function running until a result is ready. Without this promise, the function would terminate and the file would (a) never get parsed and (b) never get stored.</p><p>The other important detail comes up in the \"end\" callback of the parsing operation. The function provided is marked as \"async\". The reason for this is that the COS operation is promise-based. And again, if we do not wait for it to finish, the function will return before the file is stored. The means we need to use the \"await\" keyword. The \"await\" keyword can only be used inside an \"async\" function.</p><p>It is probably worth noting that I put my COS credentials in parameters provided to the function. I do not however expose those parameters to the client. Those parameters are provided at the function level. You can do this on the CLI, or in the Cloud Functions UI tooling. I prefer the tooling when working with very long strings like the ones needed for COS connectivity. YMMV.</p><h2 id=\"next-steps\">Next Steps</h2><p>Now that we know how to get a file up to IBM Cloud Object Storage via IBM Cloud Functions, the next step is to get it back down. I will go over what this looks like in my next post. This will give us the \"read\", \"edit\", and \"add\" operations of BREAD. A little further on, and we can \"browse\" and \"delete\". Then we can piece together a full file management system built using serverless.</p>","comment_id":"5c3fbb70dff84900c0a5620b","plaintext":"So there you are, building the world's next breakthrough social platform. And of\ncourse you want to minimize cost, administration, etc. so you use IBM Cloud\nFunctions [https://console.bluemix.net/openwhisk/] (Apache OpenWhisk\n[https://openwhisk.apache.org/]). Then you decide to let users manage their\nprofile picture. Uploading a file to a Cloud Function takes some consideration -\nand then there is the problem that the environment will destroy itself, and the\nuploaded content, after execution. In this post, we combine IBM Cloud Object\nStorage [https://www.ibm.com/cloud/object-storage] with Web Actions for robust,\npersistent storage.\n\nHTTP Multipart Request\nMost web developers will think of this as submitting an HTML form - often for\nthe purposes of uploading files. When we look at the content going across the\nwire, you will first see a typical HTTP header, with one notable addition. There\nwill be an entry in the header for the \"boundary\" of the various pieces of\ncontent. Then as you look trough the rest of the request, you will see that\nboundary marker between each distinct item being sent - a form field, and a\nfile, for example. \n\nFrom an example shown on Stack Overflow\n[https://stackoverflow.com/questions/913626/what-should-a-multipart-http-request-with-multiple-files-look-like]\n:\n\nPOST /cgi-bin/qtest HTTP/1.1\nHost: aram\nUser-Agent: Mozilla/5.0 Gecko/2009042316 Firefox/3.0.10\nAccept: text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\nAccept-Language: en-us,en;q=0.5\nAccept-Encoding: gzip,deflate\nAccept-Charset: ISO-8859-1,utf-8;q=0.7,*;q=0.7\nKeep-Alive: 300\nConnection: keep-alive\nReferer: http://aram/~martind/banner.htm\nContent-Type: multipart/form-data; boundary=----------287032381131322\nContent-Length: 514\n\n------------287032381131322\nContent-Disposition: form-data; name=\"datafile1\"; filename=\"r.gif\"\nContent-Type: image/gif\n\nGIF87a.............,...........D..;\n------------287032381131322\nContent-Disposition: form-data; name=\"datafile2\"; filename=\"g.gif\"\nContent-Type: image/gif\n\nGIF87a.............,...........D..;\n------------287032381131322\nContent-Disposition: form-data; name=\"datafile3\"; filename=\"b.gif\"\nContent-Type: image/gif\n\nGIF87a.............,...........D..;\n------------287032381131322--\n\n\nWhen this content arrives at the traditional middleware, such as Express on\nNode.js, the boundary label is found, and the request is parsed to get the\nparts. Parts are then placed in variables relative to how the middleware\nfunctions. In the context of a Cloud Function however, there is no Express to\nparse the request, so we need to (a) get access to the raw HTTP content and (b)\nparse it ourselves.\n\nUpload to Cloud Function\nIf you are familiar with the CLI for Cloud Functions, you may already be\nfamiliar with the \"--web\" argument, which allows you to expose the function to\nHTTP endpoints. There is a \"raw\" addition that can be made to this argument to\nenable raw handing of the HTTP content. With this change the function will\nreceive the raw HTTP string, not an object that has already been parsed. And\nthis is exactly what we want.\n\nibmcloud fn action create cos/post.object --kind nodejs:8 action.zip --web raw\n\n\nNote that this setting can be found in the IBM Cloud Functions web-based UI as\nwell. Select an action and then \"Endpoints\". In fact, I prefer the web UI for\ntweaking actions once they have been created.\n\nParsing the HTTP Content\nWhen I originally started down this path, I was determined to parse the HTTP\ncontent myself. After all, the boundary is called out right there in the header.\nHow hard could it be to find the boundary in the body and parse it myself? It\nturns out that there are a lot of edge cases that make this harder than it\nsounds. In the end, I turned to the \"parted [https://github.com/chjj/parted]\"\nlibrary for help, with a special assist from \"string-to-stream\n[https://github.com/feross/string-to-stream]\".\n\nconst multipart = require( 'parted' ).multipart;\nconst sts = require( 'string-to-stream' );\n\nlet decoded = new Buffer( params.__ow_body, 'base64' );\nlet stream = sts( decoded );\n\nconst options = {\n  limit: 30 * 1024,\n  diskLimit: 30 * 1024 * 1024\n};\n\nlet parser = new multipart( \n  params.__ow_headers[ 'content-type' ],\n  options\n);\nlet parts = {};\n\nparser.on( 'error', function( err ) {\n  console.log( 'parser error', err );\n} );\n\nparser.on( 'part', function( field, part ) {\n  parts[field] = part;\n} );\n\nparser.on( 'end', function() {\n  console.log( 'File upload complete.' );\n} );\n\nstream.pipe( parser );\n\n\nThe first step here is to decode the HTTP request body, and create a stream from\nit. Then we set aside a splash of disk space. Up next is the actual parsing of\nthe body. Various events occur during the parsing, most notably, when the\nparsing process has ended. When the parsing ends you will have a \"parts\"\nvariable with properties reflecting the various names of the fields that were\nprovided when the request was made.\n\nThat is to say that if you have <input type=\"file\" name=\"picture\"> then you will\nhave a \"parts.picture\" property containing the file contents. You can then\nchoose to write it to disk, send it on to another service for further\nprocessing, etc.\n\nOf course as a user profile picture, or other file type you may want to keep\naround for some duration, writing the contents to disk is not going to do you\nany good. That content will be destroyed along with the Cloud Function once the\nprocessing has finished. We really need to put the file somewhere reliable -\nsuch as IBM Cloud Object Storage.\n\nUpload to Object Storage\nUnder the covers, IBM Cloud Object Storage (COS) is OpenStack Swift\n[https://wiki.openstack.org/wiki/Swift], and conforms to the interface\nestablished by AWS S3. You create buckets, and put objects in the buckets. There\nare SDKs available in various languages, but we are going to continue along the\nNode.js path. There is a specific fork [https://github.com/ibm/ibm-cos-sdk-js] \nof the S3 library for COS which I recommend using for the most consistent\nresults.\n\nInstantiate COS\nThe first step in using the library is to instantiate it as a client. A quick\nlook at the documentation for the library shows that we need \"endpoint\",\n\"apiKeyId\", and \"serviceInstanceId\" values. For me, finding the right pieces in\nthe UI took some effort the first time.\n\nlet cos = new AWS.S3( {\n  endpoint: '<endpoint>',\n  apiKeyId: '<api-key>',\n  ibmAuthEndpoint: 'https://iam.ng.bluemix.net/oidc/token',\n  serviceInstanceId: '<resource-instance-id>',\n} );\n\n\nEndpoint\n\nWhen you are looking at the list of buckets, you will see a \"Location\" column.\nThe bucket I am using for this example sits in \"us-east\". Click on \"Endpoint\" in\nthe left sidebar. When I created the bucket, I selected \"Regional\" resiliency,\nso I will select \"Regional\". Then select the corresponding location such as\n\"us-east\". We will be using \"Public\" access, which in the case of \"us-east\"\nyields \"s3.us-east.cloud-object-storage.appdomain.cloud\".\n\nAPI Key\n\nNext we will head to the \"Service credentials\" section by clicking on that link\nin the left sidebar. You may have a number of credentials, each will have the\noption to \"View credentials\". Expanding this section yields a section of JSON.\nLook for the field labeled \"apikey\".\n\nService Instance ID\n\nThe \"resource-instance-id\" is found in this block of JSON-formatted credentials\nas well.\n\nPut an Object\nPutting an object into COS looks exactly like it would if you were using the AWS\nS3 SDK. On the COS client you call \"putObject()\" with the content you want to\nstore, the bucket in which you want to store it, and the name of the content\nitself.\n\nlet obj = fs.readFileSync( parts.file );\n\nawait cos.putObject( {\n  Body: obj,\n  Bucket: 'file-upload',\n  Key: parts.name   \n} )\n.promise()\n.then( ( data ) => {\n  console.log( 'File storage complete.' );\n} );\n\n\nWhen we uploaded the file from the browser (or other client) to the Cloud\nFunction, we ended up with a \"parts\" object holding the file contents in a\n\"file\" property (which came from the name of the HTML input). To get this ready\nfor storage, we read it into a Buffer object and provide that as the \"Body\"\nparameter. \n\nThe \"Bucket\" name should match the desired bucket from your COS instance. You\ncan name the file whatever you need to for your application. \"Key\" is equivalent\nto \"name\" in the COS/S3 model. For the purposes of flexibility, I like to send\nthe name along with the file - effectively, I like to make the name a parameter\non the service.\n\n> The \"putObject()\" call is using \"async/await\". If we do not wait for the upload\nto finish, the function will terminate before the file ever gets uploaded.\nAll Together Now\nNow that we know how to upload, and parse, a file to a Cloud Function, and how\nto then put the file onto Cloud Object Storage, all that is left is to string\nthe bits together.\n\nfunction upload( params ) {\n  const AWS = require( 'ibm-cos-sdk' );\n  const fs = require( 'fs' );\n  const multipart = require( 'parted' ).multipart;\n  const sts = require( 'string-to-stream' );\n\n  return new Promise( ( resolve, reject ) => {\n    let decoded = new Buffer( params.__ow_body, 'base64' );\n    let stream = sts( decoded );\n\n    let parser = new multipart(\n      params.__ow_headers[ 'content-type' ], {\n        limit: 30 * 1024,\n        diskLimit: 30 * 1024 * 1024\n      }\n    );\n    let parts = {};\n\n    parser.on( 'error', function( err ) {\n      console.log( 'Whoops!', err );\n    } );\n\n    parser.on( 'part', function( field, part ) {\n      parts[field] = part;\n    } );\n\n    parser.on( 'end', async function() {\n      let cos = new AWS.S3( {\n        endpoint: params.COS_ENDPOINT,\n        apiKeyId: params.COS_API_KEY,\n        ibmAuthEndpoint: params.COS_AUTH_ENDPOINT,\n        serviceInstanceId: params.COS_SERVICE_INSTANCE\n      } );\n    \n      let obj = fs.readFileSync( parts.file );\n    \n      await cos.putObject( {\n        Body: obj,\n        Bucket: params.COS_BUCKET,\n        Key: parts.name\n      } )\n      .promise()\n      .then( ( data ) => {\n        resolve( {\n          headers: {\n            'Content-Type': 'application/json'\n          },\n          body: data\n        } );\n      } );\n    } );\n\n    stream.pipe( parser );\n  } );\n}\n\n// Do not forget to export\nexports.main = upload;\n\n\nThere are some particularly important details to consider when putting\neverything together. The first is that the main function returns a \"Promise\".\nThis is done to keep the function running until a result is ready. Without this\npromise, the function would terminate and the file would (a) never get parsed\nand (b) never get stored.\n\nThe other important detail comes up in the \"end\" callback of the parsing\noperation. The function provided is marked as \"async\". The reason for this is\nthat the COS operation is promise-based. And again, if we do not wait for it to\nfinish, the function will return before the file is stored. The means we need to\nuse the \"await\" keyword. The \"await\" keyword can only be used inside an \"async\"\nfunction.\n\nIt is probably worth noting that I put my COS credentials in parameters provided\nto the function. I do not however expose those parameters to the client. Those\nparameters are provided at the function level. You can do this on the CLI, or in\nthe Cloud Functions UI tooling. I prefer the tooling when working with very long\nstrings like the ones needed for COS connectivity. YMMV.\n\nNext Steps\nNow that we know how to get a file up to IBM Cloud Object Storage via IBM Cloud\nFunctions, the next step is to get it back down. I will go over what this looks\nlike in my next post. This will give us the \"read\", \"edit\", and \"add\" operations\nof BREAD. A little further on, and we can \"browse\" and \"delete\". Then we can\npiece together a full file management system built using serverless.","feature_image":"__GHOST_URL__/content/images/2019/01/whisk.eggs.jpg","featured":0,"status":"published","locale":null,"visibility":"public","author_id":"1","created_at":"2019-01-16T23:17:04.000Z","updated_at":"2019-01-23T19:34:34.000Z","published_at":"2019-01-23T19:34:34.000Z","custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"type":"post","email_recipient_filter":"none"},{"id":"5c48c6a19d94d600c0e43dd8","uuid":"9a5efbcf-f614-43cb-80cb-5b6f5b235ecd","title":"Serverless Download from Object Storage","slug":"serverless-download-from-object-storage","mobiledoc":"{\"version\":\"0.3.1\",\"atoms\":[],\"cards\":[[\"markdown\",{\"markdown\":\"```\\nlet cos = new AWS.S3( {\\n  endpoint: params.COS_ENDPOINT,\\n  apiKeyId: params.COS_API_KEY,\\n  ibmAuthEndpoint: params.COS_AUTH_ENDPOINT,\\n  serviceInstanceId: params.COS_SERVICE_INSTANCE\\n} );\\n\\ncos.getObject( {\\n  Bucket: params.COS_BUCKET,\\n  Key: params.name\\n} )\\n.promise()\\n.then( ( data ) => {\\n  console.log( data );\\n} );\\n```\"}],[\"markdown\",{\"markdown\":\"```\\nfunction download( params ) {\\n  const AWS = require( 'ibm-cos-sdk' );\\n\\n  let cos = new AWS.S3( {\\n    endpoint: params.COS_ENDPOINT,\\n    apiKeyId: params.COS_API_KEY,\\n    ibmAuthEndpoint: params.COS_AUTH_ENDPOINT,\\n    serviceInstanceId: params.COS_SERVICE_INSTANCE\\n  } );\\n\\n  return cos.getObject( {\\n    Bucket: params.COS_BUCKET, \\n    Key: params.name\\n  } )\\n  .promise()\\n  .then( ( data ) => {\\n    return {\\n      headers: { \\n        'Content-Disposition': `attachment; filename=\\\"${params.name}\\\"`,\\n        'Content-Type': data.ContentType \\n      },\\n      statusCode: 200,\\n      body: Buffer.from( data.Body ).toString( 'base64' )\\n    }; \\n  } );\\n}\\n\\nexports.main = download;\\n```\"}],[\"markdown\",{\"markdown\":\"```\\nreturn cos.listObjectsV2( {\\n  Bucket: params.COS_BUCKET\\n} )\\n.promise()\\n.then( ( data ) => {\\n  let body = [];\\n\\n  for( let c = 0; c < data.Contents.length; c++ ) {\\n    body.push( {\\n      name: data.Contents[c].Key,\\n      etag: data.Contents[c].ETag.replace( /\\\"/g,\\\"\\\" ),\\n      modified: data.Contents[c].LastModified,\\n      size: data.Contents[c].Size\\n    } );\\n  }\\n  \\n  return {\\n    headers: {\\n      'Content-Type': 'application/json'          \\n    },\\n    body: body\\n  };      \\n} );\\n```\"}],[\"markdown\",{\"markdown\":\"```\\nreturn cos.deleteObject( {\\n  Bucket: params.COS_BUCKET,\\n  Key: params.name\\n} )\\n.promise()\\n.then( ( data ) => {\\n  return {\\n    headers: {\\n      'Content-Type': 'application/json'\\n    },\\n    statusCode: 200,\\n    body: {\\n      name: params.name\\n    }\\n  };\\n} );\\n```\"}]],\"markups\":[[\"a\",[\"href\",\"__GHOST_URL__/2019/01/23/upload-files-to/\"]],[\"a\",[\"href\",\"https://console.bluemix.net/openwhisk/\"]],[\"a\",[\"href\",\"https://www.ibm.com/cloud/object-storage\"]],[\"a\",[\"href\",\"https://www.youtube.com/watch?v=zNgcYGgtf8M\"]],[\"a\",[\"href\",\"https://github.com/ibm/ibm-cos-sdk-js\"]],[\"strong\"]],\"sections\":[[1,\"p\",[[0,[],0,\"In the \"],[0,[0],1,\"previous post\"],[0,[],0,\", I walked through how to upload a file from an HTTP multipart client, to an \"],[0,[1],1,\"IBM Cloud Function\"],[0,[],0,\", and how to persist that file on \"],[0,[2],1,\"IBM Cloud Object Storage\"],[0,[],0,\" (COS). In this post I will explore getting the file back out of COS and downloaded to an HTTP client.\"]]],[1,\"h2\",[[0,[],0,\"Get Outta My COS\"]]],[1,\"p\",[[0,[],0,\"And into my \"],[0,[3],1,\"car\"],[0,[],0,\"! Ahem ...\"]]],[1,\"p\",[[0,[],0,\"IBM Cloud Object Storage is compatible with AWS S3. Just as in the previous post we will use the IBM \"],[0,[4],1,\"fork\"],[0,[],0,\" of that SDK for Node.js. And the first step is to instantiate the client. If you are confused on where to get the various credentials, I cover that in detail in the previous post as well.\"]]],[10,0],[1,\"p\",[[0,[],0,\"The \\\"getObject()\\\" call on the COS client requires the name of the \\\"Bucket\\\" where the object resides, and the name of the object you want to get. Note that name and \\\"Key\\\" are synonymous. This is effectively the path and file name of the object you want to download.\"]]],[1,\"blockquote\",[[0,[],0,\"Watch out! If the file does not exist, the SDK will throw an error effectively halting the execution of the function. You may want to do a bit more error catching than is demonstrated in the snippet above.\"]]],[1,\"h2\",[[0,[],0,\"And Into My Function\"]]],[1,\"p\",[[0,[],0,\"There are a couple subtleties that we need to consider for the Cloud Function side of this equation. The first is that the COS client is promise-based, which means we need to wait for it to finish before terminating the function itself. The second is what exactly do we return?\"]]],[10,1],[1,\"p\",[[0,[],0,\"In the previous post we wrapped our callback-filled parsing code in a \\\"Promise\\\" and returned that reference. That kept the function running. In this case, the COS SDK is promise-based, and we can then return the promise generated by the call to \\\"getObject()\\\" itself.\"]]],[1,\"p\",[[0,[],0,\"As far as what to return, we have to stops. The first is in the headers of the return value. In order to prompt for a download, we need to set the header \\\"Content-Disposition\\\". The result from the \\\"getObject()\\\" call, will have the appropriate content type we can use in the header as well.\"]]],[1,\"p\",[[0,[],0,\"The \\\"body\\\" of the return object should be the contents of the file, Base-64 encoded. The bytes that make up the file are in the \\\"data\\\" object result from the \\\"getObject()\\\" call in a property labeled \\\"Body\\\". We can put that into a \\\"Buffer\\\" instance, and leverage \\\"toString( 'base64' )\\\" to get the Base-64 encoded content.\"]]],[1,\"h2\",[[0,[],0,\"Moar Integration!\"]]],[1,\"p\",[[0,[],0,\"At this point we can read, edit, and add for our BREAD operations. We are so close, and the code is so similar, that rather than make another blog post, I will round out the operations in the following code snippets. First up, browse.\"]]],[1,\"p\",[[0,[5],1,\"Browse the Objects in a Bucket\"]]],[1,\"p\",[[0,[],0,\"The AWS S3 documentation has \\\"listObjects()\\\" and \\\"listObjectsV2()\\\" and suggest that it prefers the later. The \\\"listObjectsV2()\\\" takes the argument \\\"Bucket\\\" name, and will only return 1,000 items. If you have more than 1,000 items in your bucket, you will need to page through them.\"]]],[10,2],[1,\"p\",[[0,[],0,\"The \\\"data\\\" object in this case is an array, where each object in the array has a \\\"Key\\\" property (file name), \\\"LastModified\\\", \\\"Size\\\", and \\\"ETag\\\". You can effectively return the data as-is. I am being picky here in forcing lowercase keys. The \\\"ETag\\\" value is also enclosed in nested quotes, so I clean that up to get get the raw string as the value.\"]]],[1,\"p\",[[0,[5],1,\"Delete an Object in a Bucket\"]]],[1,\"p\",[[0,[],0,\"Finally we come to removing an object from a bucket. This is pretty much the same as getting an object list from a bucket. And just like the listing, you can return the call to \\\"deleteObject()\\\" and the resulting promise to keep the function running. The return from a single deletion is ... nothing. I return the file name that was just deleted as a courtesy.\"]]],[10,3],[1,\"p\",[[0,[],0,\"The parameters passed to \\\"deleteObject()\\\" include the \\\"Bucket\\\" (file path) and \\\"Key\\\" (file name).\"]]],[1,\"h2\",[[0,[],0,\"Next Steps\"]]],[1,\"p\",[[0,[],0,\"We can now leverage all the BREAD operations (browse, read, edit, add, delete) from within an IBM Cloud Function, persisting files to IBM Cloud Object Storage. From here we could build our own serverless-based file manager. I am kicking around the idea of a VSCode plug-in to manage those files in my COS buckets.\"]]]],\"ghostVersion\":\"3.0\"}","html":"<p>In the <a href=\"__GHOST_URL__/2019/01/23/upload-files-to/\">previous post</a>, I walked through how to upload a file from an HTTP multipart client, to an <a href=\"https://console.bluemix.net/openwhisk/\">IBM Cloud Function</a>, and how to persist that file on <a href=\"https://www.ibm.com/cloud/object-storage\">IBM Cloud Object Storage</a> (COS). In this post I will explore getting the file back out of COS and downloaded to an HTTP client.</p><h2 id=\"get-outta-my-cos\">Get Outta My COS</h2><p>And into my <a href=\"https://www.youtube.com/watch?v=zNgcYGgtf8M\">car</a>! Ahem ...</p><p>IBM Cloud Object Storage is compatible with AWS S3. Just as in the previous post we will use the IBM <a href=\"https://github.com/ibm/ibm-cos-sdk-js\">fork</a> of that SDK for Node.js. And the first step is to instantiate the client. If you are confused on where to get the various credentials, I cover that in detail in the previous post as well.</p><!--kg-card-begin: markdown--><pre><code>let cos = new AWS.S3( {\n  endpoint: params.COS_ENDPOINT,\n  apiKeyId: params.COS_API_KEY,\n  ibmAuthEndpoint: params.COS_AUTH_ENDPOINT,\n  serviceInstanceId: params.COS_SERVICE_INSTANCE\n} );\n\ncos.getObject( {\n  Bucket: params.COS_BUCKET,\n  Key: params.name\n} )\n.promise()\n.then( ( data ) =&gt; {\n  console.log( data );\n} );\n</code></pre>\n<!--kg-card-end: markdown--><p>The \"getObject()\" call on the COS client requires the name of the \"Bucket\" where the object resides, and the name of the object you want to get. Note that name and \"Key\" are synonymous. This is effectively the path and file name of the object you want to download.</p><blockquote>Watch out! If the file does not exist, the SDK will throw an error effectively halting the execution of the function. You may want to do a bit more error catching than is demonstrated in the snippet above.</blockquote><h2 id=\"and-into-my-function\">And Into My Function</h2><p>There are a couple subtleties that we need to consider for the Cloud Function side of this equation. The first is that the COS client is promise-based, which means we need to wait for it to finish before terminating the function itself. The second is what exactly do we return?</p><!--kg-card-begin: markdown--><pre><code>function download( params ) {\n  const AWS = require( 'ibm-cos-sdk' );\n\n  let cos = new AWS.S3( {\n    endpoint: params.COS_ENDPOINT,\n    apiKeyId: params.COS_API_KEY,\n    ibmAuthEndpoint: params.COS_AUTH_ENDPOINT,\n    serviceInstanceId: params.COS_SERVICE_INSTANCE\n  } );\n\n  return cos.getObject( {\n    Bucket: params.COS_BUCKET, \n    Key: params.name\n  } )\n  .promise()\n  .then( ( data ) =&gt; {\n    return {\n      headers: { \n        'Content-Disposition': `attachment; filename=&quot;${params.name}&quot;`,\n        'Content-Type': data.ContentType \n      },\n      statusCode: 200,\n      body: Buffer.from( data.Body ).toString( 'base64' )\n    }; \n  } );\n}\n\nexports.main = download;\n</code></pre>\n<!--kg-card-end: markdown--><p>In the previous post we wrapped our callback-filled parsing code in a \"Promise\" and returned that reference. That kept the function running. In this case, the COS SDK is promise-based, and we can then return the promise generated by the call to \"getObject()\" itself.</p><p>As far as what to return, we have to stops. The first is in the headers of the return value. In order to prompt for a download, we need to set the header \"Content-Disposition\". The result from the \"getObject()\" call, will have the appropriate content type we can use in the header as well.</p><p>The \"body\" of the return object should be the contents of the file, Base-64 encoded. The bytes that make up the file are in the \"data\" object result from the \"getObject()\" call in a property labeled \"Body\". We can put that into a \"Buffer\" instance, and leverage \"toString( 'base64' )\" to get the Base-64 encoded content.</p><h2 id=\"moar-integration-\">Moar Integration!</h2><p>At this point we can read, edit, and add for our BREAD operations. We are so close, and the code is so similar, that rather than make another blog post, I will round out the operations in the following code snippets. First up, browse.</p><p><strong>Browse the Objects in a Bucket</strong></p><p>The AWS S3 documentation has \"listObjects()\" and \"listObjectsV2()\" and suggest that it prefers the later. The \"listObjectsV2()\" takes the argument \"Bucket\" name, and will only return 1,000 items. If you have more than 1,000 items in your bucket, you will need to page through them.</p><!--kg-card-begin: markdown--><pre><code>return cos.listObjectsV2( {\n  Bucket: params.COS_BUCKET\n} )\n.promise()\n.then( ( data ) =&gt; {\n  let body = [];\n\n  for( let c = 0; c &lt; data.Contents.length; c++ ) {\n    body.push( {\n      name: data.Contents[c].Key,\n      etag: data.Contents[c].ETag.replace( /&quot;/g,&quot;&quot; ),\n      modified: data.Contents[c].LastModified,\n      size: data.Contents[c].Size\n    } );\n  }\n  \n  return {\n    headers: {\n      'Content-Type': 'application/json'          \n    },\n    body: body\n  };      \n} );\n</code></pre>\n<!--kg-card-end: markdown--><p>The \"data\" object in this case is an array, where each object in the array has a \"Key\" property (file name), \"LastModified\", \"Size\", and \"ETag\". You can effectively return the data as-is. I am being picky here in forcing lowercase keys. The \"ETag\" value is also enclosed in nested quotes, so I clean that up to get get the raw string as the value.</p><p><strong>Delete an Object in a Bucket</strong></p><p>Finally we come to removing an object from a bucket. This is pretty much the same as getting an object list from a bucket. And just like the listing, you can return the call to \"deleteObject()\" and the resulting promise to keep the function running. The return from a single deletion is ... nothing. I return the file name that was just deleted as a courtesy.</p><!--kg-card-begin: markdown--><pre><code>return cos.deleteObject( {\n  Bucket: params.COS_BUCKET,\n  Key: params.name\n} )\n.promise()\n.then( ( data ) =&gt; {\n  return {\n    headers: {\n      'Content-Type': 'application/json'\n    },\n    statusCode: 200,\n    body: {\n      name: params.name\n    }\n  };\n} );\n</code></pre>\n<!--kg-card-end: markdown--><p>The parameters passed to \"deleteObject()\" include the \"Bucket\" (file path) and \"Key\" (file name).</p><h2 id=\"next-steps\">Next Steps</h2><p>We can now leverage all the BREAD operations (browse, read, edit, add, delete) from within an IBM Cloud Function, persisting files to IBM Cloud Object Storage. From here we could build our own serverless-based file manager. I am kicking around the idea of a VSCode plug-in to manage those files in my COS buckets.</p>","comment_id":"5c48c6a19d94d600c0e43dd8","plaintext":"In the previous post [__GHOST_URL__/2019/01/23/upload-files-to/], I walked\nthrough how to upload a file from an HTTP multipart client, to an IBM Cloud\nFunction [https://console.bluemix.net/openwhisk/], and how to persist that file\non IBM Cloud Object Storage [https://www.ibm.com/cloud/object-storage] (COS). In\nthis post I will explore getting the file back out of COS and downloaded to an\nHTTP client.\n\nGet Outta My COS\nAnd into my car [https://www.youtube.com/watch?v=zNgcYGgtf8M]! Ahem ...\n\nIBM Cloud Object Storage is compatible with AWS S3. Just as in the previous post\nwe will use the IBM fork [https://github.com/ibm/ibm-cos-sdk-js] of that SDK for\nNode.js. And the first step is to instantiate the client. If you are confused on\nwhere to get the various credentials, I cover that in detail in the previous\npost as well.\n\nlet cos = new AWS.S3( {\n  endpoint: params.COS_ENDPOINT,\n  apiKeyId: params.COS_API_KEY,\n  ibmAuthEndpoint: params.COS_AUTH_ENDPOINT,\n  serviceInstanceId: params.COS_SERVICE_INSTANCE\n} );\n\ncos.getObject( {\n  Bucket: params.COS_BUCKET,\n  Key: params.name\n} )\n.promise()\n.then( ( data ) => {\n  console.log( data );\n} );\n\n\nThe \"getObject()\" call on the COS client requires the name of the \"Bucket\" where\nthe object resides, and the name of the object you want to get. Note that name\nand \"Key\" are synonymous. This is effectively the path and file name of the\nobject you want to download.\n\n> Watch out! If the file does not exist, the SDK will throw an error effectively\nhalting the execution of the function. You may want to do a bit more error\ncatching than is demonstrated in the snippet above.\nAnd Into My Function\nThere are a couple subtleties that we need to consider for the Cloud Function\nside of this equation. The first is that the COS client is promise-based, which\nmeans we need to wait for it to finish before terminating the function itself.\nThe second is what exactly do we return?\n\nfunction download( params ) {\n  const AWS = require( 'ibm-cos-sdk' );\n\n  let cos = new AWS.S3( {\n    endpoint: params.COS_ENDPOINT,\n    apiKeyId: params.COS_API_KEY,\n    ibmAuthEndpoint: params.COS_AUTH_ENDPOINT,\n    serviceInstanceId: params.COS_SERVICE_INSTANCE\n  } );\n\n  return cos.getObject( {\n    Bucket: params.COS_BUCKET, \n    Key: params.name\n  } )\n  .promise()\n  .then( ( data ) => {\n    return {\n      headers: { \n        'Content-Disposition': `attachment; filename=\"${params.name}\"`,\n        'Content-Type': data.ContentType \n      },\n      statusCode: 200,\n      body: Buffer.from( data.Body ).toString( 'base64' )\n    }; \n  } );\n}\n\nexports.main = download;\n\n\nIn the previous post we wrapped our callback-filled parsing code in a \"Promise\"\nand returned that reference. That kept the function running. In this case, the\nCOS SDK is promise-based, and we can then return the promise generated by the\ncall to \"getObject()\" itself.\n\nAs far as what to return, we have to stops. The first is in the headers of the\nreturn value. In order to prompt for a download, we need to set the header\n\"Content-Disposition\". The result from the \"getObject()\" call, will have the\nappropriate content type we can use in the header as well.\n\nThe \"body\" of the return object should be the contents of the file, Base-64\nencoded. The bytes that make up the file are in the \"data\" object result from\nthe \"getObject()\" call in a property labeled \"Body\". We can put that into a\n\"Buffer\" instance, and leverage \"toString( 'base64' )\" to get the Base-64\nencoded content.\n\nMoar Integration!\nAt this point we can read, edit, and add for our BREAD operations. We are so\nclose, and the code is so similar, that rather than make another blog post, I\nwill round out the operations in the following code snippets. First up, browse.\n\nBrowse the Objects in a Bucket\n\nThe AWS S3 documentation has \"listObjects()\" and \"listObjectsV2()\" and suggest\nthat it prefers the later. The \"listObjectsV2()\" takes the argument \"Bucket\"\nname, and will only return 1,000 items. If you have more than 1,000 items in\nyour bucket, you will need to page through them.\n\nreturn cos.listObjectsV2( {\n  Bucket: params.COS_BUCKET\n} )\n.promise()\n.then( ( data ) => {\n  let body = [];\n\n  for( let c = 0; c < data.Contents.length; c++ ) {\n    body.push( {\n      name: data.Contents[c].Key,\n      etag: data.Contents[c].ETag.replace( /\"/g,\"\" ),\n      modified: data.Contents[c].LastModified,\n      size: data.Contents[c].Size\n    } );\n  }\n  \n  return {\n    headers: {\n      'Content-Type': 'application/json'          \n    },\n    body: body\n  };      \n} );\n\n\nThe \"data\" object in this case is an array, where each object in the array has a\n\"Key\" property (file name), \"LastModified\", \"Size\", and \"ETag\". You can\neffectively return the data as-is. I am being picky here in forcing lowercase\nkeys. The \"ETag\" value is also enclosed in nested quotes, so I clean that up to\nget get the raw string as the value.\n\nDelete an Object in a Bucket\n\nFinally we come to removing an object from a bucket. This is pretty much the\nsame as getting an object list from a bucket. And just like the listing, you can\nreturn the call to \"deleteObject()\" and the resulting promise to keep the\nfunction running. The return from a single deletion is ... nothing. I return the\nfile name that was just deleted as a courtesy.\n\nreturn cos.deleteObject( {\n  Bucket: params.COS_BUCKET,\n  Key: params.name\n} )\n.promise()\n.then( ( data ) => {\n  return {\n    headers: {\n      'Content-Type': 'application/json'\n    },\n    statusCode: 200,\n    body: {\n      name: params.name\n    }\n  };\n} );\n\n\nThe parameters passed to \"deleteObject()\" include the \"Bucket\" (file path) and\n\"Key\" (file name).\n\nNext Steps\nWe can now leverage all the BREAD operations (browse, read, edit, add, delete)\nfrom within an IBM Cloud Function, persisting files to IBM Cloud Object Storage.\nFrom here we could build our own serverless-based file manager. I am kicking\naround the idea of a VSCode plug-in to manage those files in my COS buckets.","feature_image":"__GHOST_URL__/content/images/2019/01/whisk.eggs-2.jpg","featured":0,"status":"published","locale":null,"visibility":"public","author_id":"1","created_at":"2019-01-23T19:55:13.000Z","updated_at":"2019-01-30T20:59:59.000Z","published_at":"2019-01-30T21:00:00.000Z","custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"type":"post","email_recipient_filter":"none"},{"id":"5c48ed0f9d94d600c0e43e6a","uuid":"e74097cd-3138-4e7b-a177-f12cf3a4ed68","title":"IBM and the Open Organization","slug":"ibm-to-acquire-an-open-organization","mobiledoc":"{\"version\":\"0.3.1\",\"atoms\":[],\"cards\":[],\"markups\":[[\"a\",[\"href\",\"https://www.amazon.com/Open-Organization-Igniting-Passion-Performance-ebook/dp/B00O92Q6CQ/ref=sr_1_1?crid=1F28T3RNM0UH9&keywords=open+organization&qid=1550107069&s=gateway&sprefix=open+org%2Caps%2C167&sr=8-1\"]],[\"a\",[\"href\",\"https://twitter.com/johnsheehan\"]],[\"a\",[\"href\",\"https://en.wikipedia.org/wiki/Unintended_consequences\"]],[\"a\",[\"href\",\"https://evansdata.com/reports/viewRelease.php?reportID=9\"]],[\"strong\"],[\"em\"],[\"a\",[\"href\",\"https://www.theatlantic.com/education/archive/2017/07/internalizing-the-myth-of-meritocracy/535035/\"]],[\"a\",[\"href\",\"https://www.theguardian.com/news/2018/oct/19/the-myth-of-meritocracy-who-really-gets-what-they-deserve\"]],[\"a\",[\"href\",\"https://www.the-american-interest.com/2018/11/02/on-the-merits/\"]],[\"a\",[\"href\",\"https://hbr.org/2011/04/coca-colas-marketing-shift-fro\"]]],\"sections\":[[1,\"p\",[[0,[],0,\"On October 28, 2018, IBM announced plans to acquire Red Hat. The CEO of Red Hat is Jim Whitehurst, and Mr. Whitehurst released a book titled \\\"\"],[0,[0],1,\"The Open Organization\"],[0,[],0,\"\\\" in 2015. The book details a management philosophy Mr. Whitehurst acquired when he joined Red Hat in 2007. The book is broken down into three parts: why, how, and what. Here I attempt to distill the contents of those sections into my favorite quotes.\"]]],[1,\"h2\",[[0,[],0,\"Forward\"]]],[1,\"p\",[[0,[],0,\"Nearly the first 20% of the book is the forward, and it is worth picking up for that content alone. In that section you will find Mr. Whitehurst's definition of the \\\"open organization.\\\"\"]]],[1,\"blockquote\",[[0,[],0,\"An “open organization” — which I define as an organization that engages participative communities both inside and out — responds to opportunities more quickly, has access to resources and talent outside the organization, and inspires, motivates, and empowers people at all levels to act with accountability.\"]]],[1,\"h2\",[[0,[],0,\"Why: Motivating and Inspiring\"]]],[1,\"p\",[[0,[],0,\"What is the purpose of your organization? In the developer relations group that I manage, we have one purpose: Make developers heroes for what it is that they are trying to achieve. While I would love to claim credit for that purpose, it comes from \"],[0,[1],1,\"John Sheehan\"],[0,[],0,\" while at Twilio. It does not define what we do per se, but certainly says why we do what we do, which is a distinction that Mr. Whitehurst makes about open organizations.\"]]],[1,\"blockquote\",[[0,[],0,\"Purpose is often misunderstood. It’s not what a group does but why it does what it does. It’s not a goal but a reason—the reason it exists, the need it fulfills, and the assistance it bestows. It is the answer to the question every group should ask itself: if we disappeared today, how would the world be different tomorrow?\"]]],[1,\"h3\",[[0,[],0,\"Igniting Passion\"]]],[1,\"p\",[[0,[],0,\"Passion is a topic that comes up repeatedly throughout the book, not just in this section. In my experience, passion is often replaced by metrics. The message is clear - do not go after what you passionately think will help our company, but rather, go after what I tell you will help our company. In short: You do not know, I do, shut up and do what I say. That is perhaps the quickest path to destroy passion.\"]]],[1,\"blockquote\",[[0,[],0,\"What sets open organizations apart, and what gives them a true competitive advantage, is that they also have embraced the idea that they need to activate the emotional passions and desires among their workers to actually reach that ultimate destination as defined by their purpose.\"]]],[1,\"p\",[[0,[],0,\"But ... metrics! You might be inclined to think that the metrics-driven approach would seem to obviously have a negative impact on igniting passion. And that subsequently, management would catch on and more carefully consider their approach. Yet, metrics persist. This is not a fault of their own, but rather what is taught.\"]]],[1,\"blockquote\",[[0,[],0,\"In management science, the simplifying assumption is that people are rational, value-maximizing, unemotional cogs.\"]]],[1,\"p\",[[0,[],0,\"As Mr. Whitehurst notes, the open organization should have metrics. To me it is about how those metrics are applied. Are they applied to drive behavior, or to measure the effectiveness of the purpose? In developer relations, we are all well aware of the \"],[0,[2],1,\"unintended consequences\"],[0,[],0,\" around \\\"vanity metrics.\\\" If I tell my team to blog n-times, then often I will end up with n-amount of half-baked posts. Conversely, what happens if we frame the goals of the organization in the context of content that helps developers be heroes?\"]]],[1,\"blockquote\",[[0,[],0,\"When you have passion inside your organization ... a leader and manager’s role becomes more about creating context and reinforcing what purpose and end goal the organization is working toward.\"]]],[1,\"h3\",[[0,[],0,\"Building Engagement\"]]],[1,\"p\",[[0,[],0,\"Engagement lies at the core of developer relations. You can hire an army of advocates, but reaching the 23 million \"],[0,[3],1,\"developers in the world\"],[0,[],0,\" is something you can simply not do by yourself. Developer relations organizations \"],[0,[4,5],2,\"must\"],[0,[],0,\" find those developers outside of the organization and engage them directly - igniting their passion.\"]]],[1,\"blockquote\",[[0,[],0,\"Additionally, the most engaged community members have taken on the role of community moderators and are leading the continued evolution of the site. These volunteer moderators don’t work for Red Hat, but have a personal interest in the open source way. They contribute their time to write articles, engage with commenters, and spread the word about content on social media.\"]]],[1,\"p\",[[0,[],0,\"How then does one engage developers? Mr. Whitehurst starts internally and then expands to outside of the organization. Many of his stated principles however apply equally. First things first, we need to let people know that they are not cogs - that we, as representatives of a given company, and thus the company itself, actually care.\"]]],[1,\"blockquote\",[[0,[],0,\"People need to believe that the company and its leaders care about them.\"]]],[1,\"p\",[[0,[],0,\"We then need to let them know what we are about. Many companies shy away from talking about roadmaps, but in successful open source projects, the roadmaps are transparent. I am a big fan of having product teams openly blog about what they are thinking, why they are thinking about those interests, and establishing an open dialog on social channels.\"]]],[1,\"blockquote\",[[0,[],0,\"People thirst for context; they want to know the whats and whys of their company’s direction, and they want to be part of making it successful.\"]]],[1,\"p\",[[0,[],0,\"Of course talking about yourself (your company) all the time can easily become what I like to call a marketing \\\"megaphone\\\". When you are IBM, with hundreds of thousands of employees, that megaphone is extremely loud. Talking about yourself all the time does not a relationship [engagement] make. We must also be willing to listen, and act.\"]]],[1,\"blockquote\",[[0,[],0,\"You won’t hear employees if you’ve delegated the task of listening.\"]]],[1,\"blockquote\",[[0,[],0,\"In my opinion, feedback is a gift.\"]]],[1,\"blockquote\",[[0,[],0,\"The worst possible thing is if employees contribute their best ideas and nothing happens. Listening is simply not enough.\"]]],[1,\"p\",[[0,[],0,\"These all talk of the company/employee relationship, but I think it is equally applicable to the company/customer relationship.\"]]],[1,\"h2\",[[0,[],0,\"How: Getting Things Done\"]]],[1,\"p\",[[0,[],0,\"As a fan of the \\\"Getting Things Done\\\" (GTD) methodology, the title of this part of the book drew me right in, and I kept on reading. Alas, this section is not about that methodology. It is about how you get things done if you have all these pieces (non-company developers) moving around and doing things based on their passions.\"]]],[1,\"blockquote\",[[0,[],0,\"To be a member of the Athenian politeia, according to the authors of A Company of Citizens, meant that you thought, argued, and acted with your fellow citizens, and that you learned through the practice of civic life.\"]]],[1,\"p\",[[0,[],0,\"It might seem that this is chaos to most managers. As Mr. Whitehurst argues repeatedly, the financial success of Red Hat would suggest otherwise.\"]]],[1,\"h3\",[[0,[],0,\"Meritocracy Not Democracy\"]]],[1,\"p\",[[0,[],0,\"As an American, I am pretty bullish on democracy, or at the very least, the republic, so the title of this section had me concerned at first. There is \"],[0,[6],1,\"substantial\"],[0,[],0,\" \"],[0,[7],1,\"research\"],[0,[],0,\" to show that meritocracy can become institutionalized \"],[0,[8],1,\"class warfare\"],[0,[],0,\". As I read through the concepts presented however, a strong case is made for the application of meritocracy within the open organization.\"]]],[1,\"blockquote\",[[0,[],0,\"But the path to building influence really is about quality over quantity.\"]]],[1,\"p\",[[0,[],0,\"Who does not want a quality product? Quality documentation? Quality engagement? Another John Sheehan concept is to make sure you have good product first and foremost. The argument made by The Open Organization then is that, via meritocracy, we must let those with the best knowledge of the product be free to innovate.\"]]],[1,\"blockquote\",[[0,[],0,\"Freeing up the thought leaders and rock stars can jumpstart innovation, something every company and business is trying to bottle.\"]]],[1,\"p\",[[0,[],0,\"You may have heard of 20% time (or 10% time, or other value) in which developers get time to work on whatever it is that inspires them. I have long held the view that advocates spend more like 80% innovating on gaps to lower barriers to entry with a given product - this can take the form of SDK development, or in an open source world, contribution directly to the code base. Mr. Whitehurst seems to agree, and rather than just leave this to advocates, opens it up across the entire organization.\"]]],[1,\"blockquote\",[[0,[],0,\"We want the people who have proven to be great innovators to spend all of their time innovating, not just 20 percent of it.\"]]],[1,\"p\",[[0,[],0,\"What about that Vice President of XYZ that dictates a specific direction - specific work to be done? Senior Vice President? Executive Vice President? The suggested answer to this seems to center around meritocracy - specifically, has that person earned the respect, or are they managing by title?\"]]],[1,\"blockquote\",[[0,[],0,\"Ask yourself if you command respect because people have to respect you or, rather, because you’ve truly earned respect.\"]]],[1,\"blockquote\",[[0,[],0,\"When people respect you only because of your authority, they will give you the minimum effort.\"]]],[1,\"p\",[[0,[],0,\"From my days in the military, I would coin this as \\\"leadership by example\\\" rather than meritocracy. Do not give me crap about the polish of my boots if yours do not look like they have been polished since they were issued. \"]]],[1,\"p\",[[0,[],0,\"Turning back to developer relations, internally you should be weary of demanding specific action if you cannot or have not demonstrated your ability to deliver on said action. Externally, we cannot expect non-company developers to be engaged with the product if we ourselves are not engaged. Eat your own dog food.\"]]],[1,\"h3\",[[0,[],0,\"Letting Sparks Fly\"]]],[1,\"p\",[[0,[],0,\"Now firmly back in the realm of developer relations, many organizations want to control the conversation of their product. This reminds me of one of my favorite Harvard Business Review (HBR) articles on \"],[0,[9],1,\"Coca-Cola\"],[0,[],0,\", where the company lets go of brand ownership, and effectively reignites financial success. Among my favorite lines from the article is \\\"accept that you don't own your brands; your consumers do\\\". Whoa!\"]]],[1,\"blockquote\",[[0,[],0,\"Our findings show that debate and criticism do not inhibit ideas but, rather, stimulate them relative to every other condition.\"]]],[1,\"p\",[[0,[],0,\"In developer relations, if developers are talking about your product, especially in a critical manner, then they have just opened the door for engagement. This is the chance to share the decisions you have made, and why you chose them. However, you must be prepared to be wrong, and to take corrective action. This cannot be simply more \\\"megaphone\\\".\"]]],[1,\"blockquote\",[[0,[],0,\"One of the biggest threats any executive faces is the notion that he or she is only hearing the good stuff about the business.\"]]],[1,\"p\",[[0,[],0,\"A colleague of mine at IBM, who shall remain nameless, once told me that monthly management reports excel at \\\"turning shit into diamonds\\\". This is a very real danger for developer relations. If what you are experiencing with non-company developers, or product, is shit, then you need to report it that way. If you polish it up, then those above you polish it up, then those above them, and so on. By the time it reaches the top of the organization, that shit shines like diamonds. Executive in turn think everything is going great, and make decisions that do no work in the best interests of the company.\"]]],[1,\"blockquote\",[[0,[],0,\"But you don’t want to strip away differences in trying to get to consensus, because that can lead to groupthink, poor decisions, and stalled innovation.\"]]],[1,\"blockquote\",[[0,[],0,\"If there is more truth in the hallways than in meetings, you have a problem.\"]]],[1,\"h2\",[[0,[],0,\"What: Setting Direction\"]]],[1,\"blockquote\",[[0,[],0,\"Putting people first, pursuing excellence, embracing change, acting with integrity, and serving our world.\"]]],[1,\"p\",[[0,[],0,\"I think I just heard managers in traditional hierarchical organizations just throw up in their mouths a little. Yes, let us all just gather round and hug one another now. How can I be expected to lead if we are all gathered around singing \\\"Kumbaya\\\"? This is where The Open Organization turns next, as it seeks to close out this persuasive argument on a structure markedly different than the traditional top-down approach\"]]],[1,\"h3\",[[0,[],0,\"Inclusive Decisions\"]]],[1,\"p\",[[0,[],0,\"From a developer relations perspective, how well does your company listen to developer feedback? Has it altered or changed its plans based on feedback coming from outside its walls?\"]]],[1,\"blockquote\",[[0,[],0,\"Getting people to engage in the decision-making process doesn’t have to be at just the corporate level. It’s a mind-set that leaders throughout the organization should wholeheartedly embrace.\"]]],[1,\"p\",[[0,[],0,\"Developers are risk averse. Changing your product can have monumental impacts on their lives - and financial livelihood of the companies for which they work. When you engage your customers in the conversation, you achieve results that are best for them, and in turn best for you. In lieu of having established a good dialog with developers, advocates are a great place for product teams to find out how their changes will impact their customers.\"]]],[1,\"blockquote\",[[0,[],0,\"... when you take the time to make your decision-making process transparent, you can still drive progress and get things done, even when your decision is an unpopular one.\"]]],[1,\"p\",[[0,[],0,\"If your product team is not actively engaging the developer relations team(s) then you have a problem. If you are on the product side, I get it - including advocates may seem as though you are inviting dissent. Advocates are there to help - after all they have a vested interest in seeing the product succeed as well. Nobody wants to get up on stage at a conference knowing full well that the product just pissed off a percentage of the audience - or is about to.\"]]],[1,\"blockquote\",[[0,[],0,\"\\\"Lance recognized that solving the problem was not about holding endless discussions or answering every concern before we took action,” DiMaio said. “It was more about acknowledging people’s concerns and then putting an exit plan in place if things didn’t work out.\\\"\"]]],[1,\"h3\",[[0,[],0,\"Catalyzing Direction\"]]],[1,\"p\",[[0,[],0,\"Advocates come in all \\\"shapes and sizes\\\". That is to say that some are master networkers, while others posses unparalleled insight into how the source code works. Both can contribute in their own ways. It is a matter of how they approach that work that impacts how well it relates to engagement.\"]]],[1,\"blockquote\",[[0,[],0,\"We contribute to our communities, doing what we believe is in their best long-term interest, by building up positions of thought leadership within.\"]]],[1,\"blockquote\",[[0,[],0,\"By having capable, engaged people recognize the importance of the goal and then expecting them to solve it in their own way, thousands of small tweaks can be made across the company. That’s much richer and subtler than the handful of big initiatives that a top-down planning process would generate.\"]]],[1,\"p\",[[0,[],0,\"With engagement in place, Mr. Whitehurst turns to experimenting your way to success. Any experienced advocate knows this process well - any developer, really. You think you can implement your solution in one way, only to find gaps that make you have to explore another direction. It is these gaps that developer relations should, through experimentation, seek out and fill.\"]]],[1,\"blockquote\",[[0,[],0,\"When the bosses make the decisions, decisions are made by politics, persuasion, and PowerPoint. When you make decisions through experiment, the best idea can prove itself.\"]]],[1,\"p\",[[0,[],0,\"The important implication for management is in fostering the culture that allows that experimentation to happen in the first place.\"]]],[1,\"blockquote\",[[0,[],0,\"I believe the new skill in leadership is leadership by experiment.\"]]],[1,\"h2\",[[0,[],0,\"Epiloge\"]]],[1,\"p\",[[0,[],0,\"The epilog of The Open Organization focuses on the journey once you have decided to embrace the presented concepts. It can get pretty touchy-feely at points, but contrary to what traditional management science may suggest - you unemotional cog - I think it really gets to the core of what an open organization is about, and frankly how developer relations organizations should function.\"]]],[1,\"blockquote\",[[0,[],0,\"We at Red Hat beat our competitors to market and react more quickly to threats and opportunities not because we pedal faster, but because the organization taps into powerful energy sources like purpose, passion, and community that make it move faster.\"]]],[1,\"h2\",[[0,[],0,\"Next Steps\"]]],[1,\"p\",[[0,[],0,\"I have clearly taken a very developer relations-centric view on The Open Organization. After more than a decade of working in developer relations, I have experienced many of the concepts presented first-hand - and I know all too well the traditional management perspective that just does not \\\"get it.\\\" To me The Open Organization was almost a formalizing of those lessons, presented by an old-school traditional manager. A thoroughly enjoyable read.\"]]],[1,\"p\",[[0,[],0,\"Time will tell what the acquisition of Red Hat by IBM will mean to the cultures of both companies. If The Open Organization is any indicator of what might be expected, then I think the combined forces have a great chance at success.\"]]]],\"ghostVersion\":\"3.0\"}","html":"<p>On October 28, 2018, IBM announced plans to acquire Red Hat. The CEO of Red Hat is Jim Whitehurst, and Mr. Whitehurst released a book titled \"<a href=\"https://www.amazon.com/Open-Organization-Igniting-Passion-Performance-ebook/dp/B00O92Q6CQ/ref=sr_1_1?crid=1F28T3RNM0UH9&amp;keywords=open+organization&amp;qid=1550107069&amp;s=gateway&amp;sprefix=open+org%2Caps%2C167&amp;sr=8-1\">The Open Organization</a>\" in 2015. The book details a management philosophy Mr. Whitehurst acquired when he joined Red Hat in 2007. The book is broken down into three parts: why, how, and what. Here I attempt to distill the contents of those sections into my favorite quotes.</p><h2 id=\"forward\">Forward</h2><p>Nearly the first 20% of the book is the forward, and it is worth picking up for that content alone. In that section you will find Mr. Whitehurst's definition of the \"open organization.\"</p><blockquote>An “open organization” — which I define as an organization that engages participative communities both inside and out — responds to opportunities more quickly, has access to resources and talent outside the organization, and inspires, motivates, and empowers people at all levels to act with accountability.</blockquote><h2 id=\"why-motivating-and-inspiring\">Why: Motivating and Inspiring</h2><p>What is the purpose of your organization? In the developer relations group that I manage, we have one purpose: Make developers heroes for what it is that they are trying to achieve. While I would love to claim credit for that purpose, it comes from <a href=\"https://twitter.com/johnsheehan\">John Sheehan</a> while at Twilio. It does not define what we do per se, but certainly says why we do what we do, which is a distinction that Mr. Whitehurst makes about open organizations.</p><blockquote>Purpose is often misunderstood. It’s not what a group does but why it does what it does. It’s not a goal but a reason—the reason it exists, the need it fulfills, and the assistance it bestows. It is the answer to the question every group should ask itself: if we disappeared today, how would the world be different tomorrow?</blockquote><h3 id=\"igniting-passion\">Igniting Passion</h3><p>Passion is a topic that comes up repeatedly throughout the book, not just in this section. In my experience, passion is often replaced by metrics. The message is clear - do not go after what you passionately think will help our company, but rather, go after what I tell you will help our company. In short: You do not know, I do, shut up and do what I say. That is perhaps the quickest path to destroy passion.</p><blockquote>What sets open organizations apart, and what gives them a true competitive advantage, is that they also have embraced the idea that they need to activate the emotional passions and desires among their workers to actually reach that ultimate destination as defined by their purpose.</blockquote><p>But ... metrics! You might be inclined to think that the metrics-driven approach would seem to obviously have a negative impact on igniting passion. And that subsequently, management would catch on and more carefully consider their approach. Yet, metrics persist. This is not a fault of their own, but rather what is taught.</p><blockquote>In management science, the simplifying assumption is that people are rational, value-maximizing, unemotional cogs.</blockquote><p>As Mr. Whitehurst notes, the open organization should have metrics. To me it is about how those metrics are applied. Are they applied to drive behavior, or to measure the effectiveness of the purpose? In developer relations, we are all well aware of the <a href=\"https://en.wikipedia.org/wiki/Unintended_consequences\">unintended consequences</a> around \"vanity metrics.\" If I tell my team to blog n-times, then often I will end up with n-amount of half-baked posts. Conversely, what happens if we frame the goals of the organization in the context of content that helps developers be heroes?</p><blockquote>When you have passion inside your organization ... a leader and manager’s role becomes more about creating context and reinforcing what purpose and end goal the organization is working toward.</blockquote><h3 id=\"building-engagement\">Building Engagement</h3><p>Engagement lies at the core of developer relations. You can hire an army of advocates, but reaching the 23 million <a href=\"https://evansdata.com/reports/viewRelease.php?reportID=9\">developers in the world</a> is something you can simply not do by yourself. Developer relations organizations <strong><em>must</em></strong> find those developers outside of the organization and engage them directly - igniting their passion.</p><blockquote>Additionally, the most engaged community members have taken on the role of community moderators and are leading the continued evolution of the site. These volunteer moderators don’t work for Red Hat, but have a personal interest in the open source way. They contribute their time to write articles, engage with commenters, and spread the word about content on social media.</blockquote><p>How then does one engage developers? Mr. Whitehurst starts internally and then expands to outside of the organization. Many of his stated principles however apply equally. First things first, we need to let people know that they are not cogs - that we, as representatives of a given company, and thus the company itself, actually care.</p><blockquote>People need to believe that the company and its leaders care about them.</blockquote><p>We then need to let them know what we are about. Many companies shy away from talking about roadmaps, but in successful open source projects, the roadmaps are transparent. I am a big fan of having product teams openly blog about what they are thinking, why they are thinking about those interests, and establishing an open dialog on social channels.</p><blockquote>People thirst for context; they want to know the whats and whys of their company’s direction, and they want to be part of making it successful.</blockquote><p>Of course talking about yourself (your company) all the time can easily become what I like to call a marketing \"megaphone\". When you are IBM, with hundreds of thousands of employees, that megaphone is extremely loud. Talking about yourself all the time does not a relationship [engagement] make. We must also be willing to listen, and act.</p><blockquote>You won’t hear employees if you’ve delegated the task of listening.</blockquote><blockquote>In my opinion, feedback is a gift.</blockquote><blockquote>The worst possible thing is if employees contribute their best ideas and nothing happens. Listening is simply not enough.</blockquote><p>These all talk of the company/employee relationship, but I think it is equally applicable to the company/customer relationship.</p><h2 id=\"how-getting-things-done\">How: Getting Things Done</h2><p>As a fan of the \"Getting Things Done\" (GTD) methodology, the title of this part of the book drew me right in, and I kept on reading. Alas, this section is not about that methodology. It is about how you get things done if you have all these pieces (non-company developers) moving around and doing things based on their passions.</p><blockquote>To be a member of the Athenian politeia, according to the authors of A Company of Citizens, meant that you thought, argued, and acted with your fellow citizens, and that you learned through the practice of civic life.</blockquote><p>It might seem that this is chaos to most managers. As Mr. Whitehurst argues repeatedly, the financial success of Red Hat would suggest otherwise.</p><h3 id=\"meritocracy-not-democracy\">Meritocracy Not Democracy</h3><p>As an American, I am pretty bullish on democracy, or at the very least, the republic, so the title of this section had me concerned at first. There is <a href=\"https://www.theatlantic.com/education/archive/2017/07/internalizing-the-myth-of-meritocracy/535035/\">substantial</a> <a href=\"https://www.theguardian.com/news/2018/oct/19/the-myth-of-meritocracy-who-really-gets-what-they-deserve\">research</a> to show that meritocracy can become institutionalized <a href=\"https://www.the-american-interest.com/2018/11/02/on-the-merits/\">class warfare</a>. As I read through the concepts presented however, a strong case is made for the application of meritocracy within the open organization.</p><blockquote>But the path to building influence really is about quality over quantity.</blockquote><p>Who does not want a quality product? Quality documentation? Quality engagement? Another John Sheehan concept is to make sure you have good product first and foremost. The argument made by The Open Organization then is that, via meritocracy, we must let those with the best knowledge of the product be free to innovate.</p><blockquote>Freeing up the thought leaders and rock stars can jumpstart innovation, something every company and business is trying to bottle.</blockquote><p>You may have heard of 20% time (or 10% time, or other value) in which developers get time to work on whatever it is that inspires them. I have long held the view that advocates spend more like 80% innovating on gaps to lower barriers to entry with a given product - this can take the form of SDK development, or in an open source world, contribution directly to the code base. Mr. Whitehurst seems to agree, and rather than just leave this to advocates, opens it up across the entire organization.</p><blockquote>We want the people who have proven to be great innovators to spend all of their time innovating, not just 20 percent of it.</blockquote><p>What about that Vice President of XYZ that dictates a specific direction - specific work to be done? Senior Vice President? Executive Vice President? The suggested answer to this seems to center around meritocracy - specifically, has that person earned the respect, or are they managing by title?</p><blockquote>Ask yourself if you command respect because people have to respect you or, rather, because you’ve truly earned respect.</blockquote><blockquote>When people respect you only because of your authority, they will give you the minimum effort.</blockquote><p>From my days in the military, I would coin this as \"leadership by example\" rather than meritocracy. Do not give me crap about the polish of my boots if yours do not look like they have been polished since they were issued. </p><p>Turning back to developer relations, internally you should be weary of demanding specific action if you cannot or have not demonstrated your ability to deliver on said action. Externally, we cannot expect non-company developers to be engaged with the product if we ourselves are not engaged. Eat your own dog food.</p><h3 id=\"letting-sparks-fly\">Letting Sparks Fly</h3><p>Now firmly back in the realm of developer relations, many organizations want to control the conversation of their product. This reminds me of one of my favorite Harvard Business Review (HBR) articles on <a href=\"https://hbr.org/2011/04/coca-colas-marketing-shift-fro\">Coca-Cola</a>, where the company lets go of brand ownership, and effectively reignites financial success. Among my favorite lines from the article is \"accept that you don't own your brands; your consumers do\". Whoa!</p><blockquote>Our findings show that debate and criticism do not inhibit ideas but, rather, stimulate them relative to every other condition.</blockquote><p>In developer relations, if developers are talking about your product, especially in a critical manner, then they have just opened the door for engagement. This is the chance to share the decisions you have made, and why you chose them. However, you must be prepared to be wrong, and to take corrective action. This cannot be simply more \"megaphone\".</p><blockquote>One of the biggest threats any executive faces is the notion that he or she is only hearing the good stuff about the business.</blockquote><p>A colleague of mine at IBM, who shall remain nameless, once told me that monthly management reports excel at \"turning shit into diamonds\". This is a very real danger for developer relations. If what you are experiencing with non-company developers, or product, is shit, then you need to report it that way. If you polish it up, then those above you polish it up, then those above them, and so on. By the time it reaches the top of the organization, that shit shines like diamonds. Executive in turn think everything is going great, and make decisions that do no work in the best interests of the company.</p><blockquote>But you don’t want to strip away differences in trying to get to consensus, because that can lead to groupthink, poor decisions, and stalled innovation.</blockquote><blockquote>If there is more truth in the hallways than in meetings, you have a problem.</blockquote><h2 id=\"what-setting-direction\">What: Setting Direction</h2><blockquote>Putting people first, pursuing excellence, embracing change, acting with integrity, and serving our world.</blockquote><p>I think I just heard managers in traditional hierarchical organizations just throw up in their mouths a little. Yes, let us all just gather round and hug one another now. How can I be expected to lead if we are all gathered around singing \"Kumbaya\"? This is where The Open Organization turns next, as it seeks to close out this persuasive argument on a structure markedly different than the traditional top-down approach</p><h3 id=\"inclusive-decisions\">Inclusive Decisions</h3><p>From a developer relations perspective, how well does your company listen to developer feedback? Has it altered or changed its plans based on feedback coming from outside its walls?</p><blockquote>Getting people to engage in the decision-making process doesn’t have to be at just the corporate level. It’s a mind-set that leaders throughout the organization should wholeheartedly embrace.</blockquote><p>Developers are risk averse. Changing your product can have monumental impacts on their lives - and financial livelihood of the companies for which they work. When you engage your customers in the conversation, you achieve results that are best for them, and in turn best for you. In lieu of having established a good dialog with developers, advocates are a great place for product teams to find out how their changes will impact their customers.</p><blockquote>... when you take the time to make your decision-making process transparent, you can still drive progress and get things done, even when your decision is an unpopular one.</blockquote><p>If your product team is not actively engaging the developer relations team(s) then you have a problem. If you are on the product side, I get it - including advocates may seem as though you are inviting dissent. Advocates are there to help - after all they have a vested interest in seeing the product succeed as well. Nobody wants to get up on stage at a conference knowing full well that the product just pissed off a percentage of the audience - or is about to.</p><blockquote>\"Lance recognized that solving the problem was not about holding endless discussions or answering every concern before we took action,” DiMaio said. “It was more about acknowledging people’s concerns and then putting an exit plan in place if things didn’t work out.\"</blockquote><h3 id=\"catalyzing-direction\">Catalyzing Direction</h3><p>Advocates come in all \"shapes and sizes\". That is to say that some are master networkers, while others posses unparalleled insight into how the source code works. Both can contribute in their own ways. It is a matter of how they approach that work that impacts how well it relates to engagement.</p><blockquote>We contribute to our communities, doing what we believe is in their best long-term interest, by building up positions of thought leadership within.</blockquote><blockquote>By having capable, engaged people recognize the importance of the goal and then expecting them to solve it in their own way, thousands of small tweaks can be made across the company. That’s much richer and subtler than the handful of big initiatives that a top-down planning process would generate.</blockquote><p>With engagement in place, Mr. Whitehurst turns to experimenting your way to success. Any experienced advocate knows this process well - any developer, really. You think you can implement your solution in one way, only to find gaps that make you have to explore another direction. It is these gaps that developer relations should, through experimentation, seek out and fill.</p><blockquote>When the bosses make the decisions, decisions are made by politics, persuasion, and PowerPoint. When you make decisions through experiment, the best idea can prove itself.</blockquote><p>The important implication for management is in fostering the culture that allows that experimentation to happen in the first place.</p><blockquote>I believe the new skill in leadership is leadership by experiment.</blockquote><h2 id=\"epiloge\">Epiloge</h2><p>The epilog of The Open Organization focuses on the journey once you have decided to embrace the presented concepts. It can get pretty touchy-feely at points, but contrary to what traditional management science may suggest - you unemotional cog - I think it really gets to the core of what an open organization is about, and frankly how developer relations organizations should function.</p><blockquote>We at Red Hat beat our competitors to market and react more quickly to threats and opportunities not because we pedal faster, but because the organization taps into powerful energy sources like purpose, passion, and community that make it move faster.</blockquote><h2 id=\"next-steps\">Next Steps</h2><p>I have clearly taken a very developer relations-centric view on The Open Organization. After more than a decade of working in developer relations, I have experienced many of the concepts presented first-hand - and I know all too well the traditional management perspective that just does not \"get it.\" To me The Open Organization was almost a formalizing of those lessons, presented by an old-school traditional manager. A thoroughly enjoyable read.</p><p>Time will tell what the acquisition of Red Hat by IBM will mean to the cultures of both companies. If The Open Organization is any indicator of what might be expected, then I think the combined forces have a great chance at success.</p>","comment_id":"5c48ed0f9d94d600c0e43e6a","plaintext":"On October 28, 2018, IBM announced plans to acquire Red Hat. The CEO of Red Hat\nis Jim Whitehurst, and Mr. Whitehurst released a book titled \"The Open\nOrganization\n[https://www.amazon.com/Open-Organization-Igniting-Passion-Performance-ebook/dp/B00O92Q6CQ/ref=sr_1_1?crid=1F28T3RNM0UH9&keywords=open+organization&qid=1550107069&s=gateway&sprefix=open+org%2Caps%2C167&sr=8-1]\n\" in 2015. The book details a management philosophy Mr. Whitehurst acquired when\nhe joined Red Hat in 2007. The book is broken down into three parts: why, how,\nand what. Here I attempt to distill the contents of those sections into my\nfavorite quotes.\n\nForward\nNearly the first 20% of the book is the forward, and it is worth picking up for\nthat content alone. In that section you will find Mr. Whitehurst's definition of\nthe \"open organization.\"\n\n> An “open organization” — which I define as an organization that engages\nparticipative communities both inside and out — responds to opportunities more\nquickly, has access to resources and talent outside the organization, and\ninspires, motivates, and empowers people at all levels to act with\naccountability.\nWhy: Motivating and Inspiring\nWhat is the purpose of your organization? In the developer relations group that\nI manage, we have one purpose: Make developers heroes for what it is that they\nare trying to achieve. While I would love to claim credit for that purpose, it\ncomes from John Sheehan [https://twitter.com/johnsheehan]  while at Twilio. It\ndoes not define what we do per se, but certainly says why we do what we do,\nwhich is a distinction that Mr. Whitehurst makes about open organizations.\n\n> Purpose is often misunderstood. It’s not what a group does but why it does what\nit does. It’s not a goal but a reason—the reason it exists, the need it\nfulfills, and the assistance it bestows. It is the answer to the question every\ngroup should ask itself: if we disappeared today, how would the world be\ndifferent tomorrow?\nIgniting Passion\nPassion is a topic that comes up repeatedly throughout the book, not just in\nthis section. In my experience, passion is often replaced by metrics. The\nmessage is clear - do not go after what you passionately think will help our\ncompany, but rather, go after what I tell you will help our company. In short:\nYou do not know, I do, shut up and do what I say. That is perhaps the quickest\npath to destroy passion.\n\n> What sets open organizations apart, and what gives them a true competitive\nadvantage, is that they also have embraced the idea that they need to activate\nthe emotional passions and desires among their workers to actually reach that\nultimate destination as defined by their purpose.\nBut ... metrics! You might be inclined to think that the metrics-driven approach\nwould seem to obviously have a negative impact on igniting passion. And that\nsubsequently, management would catch on and more carefully consider their\napproach. Yet, metrics persist. This is not a fault of their own, but rather\nwhat is taught.\n\n> In management science, the simplifying assumption is that people are rational,\nvalue-maximizing, unemotional cogs.\nAs Mr. Whitehurst notes, the open organization should have metrics. To me it is\nabout how those metrics are applied. Are they applied to drive behavior, or to\nmeasure the effectiveness of the purpose? In developer relations, we are all\nwell aware of the unintended consequences\n[https://en.wikipedia.org/wiki/Unintended_consequences]  around \"vanity\nmetrics.\" If I tell my team to blog n-times, then often I will end up with\nn-amount of half-baked posts. Conversely, what happens if we frame the goals of\nthe organization in the context of content that helps developers be heroes?\n\n> When you have passion inside your organization ... a leader and manager’s role\nbecomes more about creating context and reinforcing what purpose and end goal\nthe organization is working toward.\nBuilding Engagement\nEngagement lies at the core of developer relations. You can hire an army of\nadvocates, but reaching the 23 million developers in the world\n[https://evansdata.com/reports/viewRelease.php?reportID=9]  is something you can\nsimply not do by yourself. Developer relations organizations must  find those\ndevelopers outside of the organization and engage them directly - igniting their\npassion.\n\n> Additionally, the most engaged community members have taken on the role of\ncommunity moderators and are leading the continued evolution of the site. These\nvolunteer moderators don’t work for Red Hat, but have a personal interest in the\nopen source way. They contribute their time to write articles, engage with\ncommenters, and spread the word about content on social media.\nHow then does one engage developers? Mr. Whitehurst starts internally and then\nexpands to outside of the organization. Many of his stated principles however\napply equally. First things first, we need to let people know that they are not\ncogs - that we, as representatives of a given company, and thus the company\nitself, actually care.\n\n> People need to believe that the company and its leaders care about them.\nWe then need to let them know what we are about. Many companies shy away from\ntalking about roadmaps, but in successful open source projects, the roadmaps are\ntransparent. I am a big fan of having product teams openly blog about what they\nare thinking, why they are thinking about those interests, and establishing an\nopen dialog on social channels.\n\n> People thirst for context; they want to know the whats and whys of their\ncompany’s direction, and they want to be part of making it successful.\nOf course talking about yourself (your company) all the time can easily become\nwhat I like to call a marketing \"megaphone\". When you are IBM, with hundreds of\nthousands of employees, that megaphone is extremely loud. Talking about yourself\nall the time does not a relationship [engagement] make. We must also be willing\nto listen, and act.\n\n> You won’t hear employees if you’ve delegated the task of listening.\n> In my opinion, feedback is a gift.\n> The worst possible thing is if employees contribute their best ideas and nothing\nhappens. Listening is simply not enough.\nThese all talk of the company/employee relationship, but I think it is equally\napplicable to the company/customer relationship.\n\nHow: Getting Things Done\nAs a fan of the \"Getting Things Done\" (GTD) methodology, the title of this part\nof the book drew me right in, and I kept on reading. Alas, this section is not\nabout that methodology. It is about how you get things done if you have all\nthese pieces (non-company developers) moving around and doing things based on\ntheir passions.\n\n> To be a member of the Athenian politeia, according to the authors of A Company\nof Citizens, meant that you thought, argued, and acted with your fellow\ncitizens, and that you learned through the practice of civic life.\nIt might seem that this is chaos to most managers. As Mr. Whitehurst argues\nrepeatedly, the financial success of Red Hat would suggest otherwise.\n\nMeritocracy Not Democracy\nAs an American, I am pretty bullish on democracy, or at the very least, the\nrepublic, so the title of this section had me concerned at first. There is \nsubstantial\n[https://www.theatlantic.com/education/archive/2017/07/internalizing-the-myth-of-meritocracy/535035/] \n research\n[https://www.theguardian.com/news/2018/oct/19/the-myth-of-meritocracy-who-really-gets-what-they-deserve] \n to show that meritocracy can become institutionalized class warfare\n[https://www.the-american-interest.com/2018/11/02/on-the-merits/]. As I read\nthrough the concepts presented however, a strong case is made for the\napplication of meritocracy within the open organization.\n\n> But the path to building influence really is about quality over quantity.\nWho does not want a quality product? Quality documentation? Quality engagement?\nAnother John Sheehan concept is to make sure you have good product first and\nforemost. The argument made by The Open Organization then is that, via\nmeritocracy, we must let those with the best knowledge of the product be free to\ninnovate.\n\n> Freeing up the thought leaders and rock stars can jumpstart innovation,\nsomething every company and business is trying to bottle.\nYou may have heard of 20% time (or 10% time, or other value) in which developers\nget time to work on whatever it is that inspires them. I have long held the view\nthat advocates spend more like 80% innovating on gaps to lower barriers to entry\nwith a given product - this can take the form of SDK development, or in an open\nsource world, contribution directly to the code base. Mr. Whitehurst seems to\nagree, and rather than just leave this to advocates, opens it up across the\nentire organization.\n\n> We want the people who have proven to be great innovators to spend all of their\ntime innovating, not just 20 percent of it.\nWhat about that Vice President of XYZ that dictates a specific direction -\nspecific work to be done? Senior Vice President? Executive Vice President? The\nsuggested answer to this seems to center around meritocracy - specifically, has\nthat person earned the respect, or are they managing by title?\n\n> Ask yourself if you command respect because people have to respect you or,\nrather, because you’ve truly earned respect.\n> When people respect you only because of your authority, they will give you the\nminimum effort.\nFrom my days in the military, I would coin this as \"leadership by example\"\nrather than meritocracy. Do not give me crap about the polish of my boots if\nyours do not look like they have been polished since they were issued. \n\nTurning back to developer relations, internally you should be weary of demanding\nspecific action if you cannot or have not demonstrated your ability to deliver\non said action. Externally, we cannot expect non-company developers to be\nengaged with the product if we ourselves are not engaged. Eat your own dog food.\n\nLetting Sparks Fly\nNow firmly back in the realm of developer relations, many organizations want to\ncontrol the conversation of their product. This reminds me of one of my favorite\nHarvard Business Review (HBR) articles on Coca-Cola\n[https://hbr.org/2011/04/coca-colas-marketing-shift-fro], where the company lets\ngo of brand ownership, and effectively reignites financial success. Among my\nfavorite lines from the article is \"accept that you don't own your brands; your\nconsumers do\". Whoa!\n\n> Our findings show that debate and criticism do not inhibit ideas but, rather,\nstimulate them relative to every other condition.\nIn developer relations, if developers are talking about your product, especially\nin a critical manner, then they have just opened the door for engagement. This\nis the chance to share the decisions you have made, and why you chose them.\nHowever, you must be prepared to be wrong, and to take corrective action. This\ncannot be simply more \"megaphone\".\n\n> One of the biggest threats any executive faces is the notion that he or she is\nonly hearing the good stuff about the business.\nA colleague of mine at IBM, who shall remain nameless, once told me that monthly\nmanagement reports excel at \"turning shit into diamonds\". This is a very real\ndanger for developer relations. If what you are experiencing with non-company\ndevelopers, or product, is shit, then you need to report it that way. If you\npolish it up, then those above you polish it up, then those above them, and so\non. By the time it reaches the top of the organization, that shit shines like\ndiamonds. Executive in turn think everything is going great, and make decisions\nthat do no work in the best interests of the company.\n\n> But you don’t want to strip away differences in trying to get to consensus,\nbecause that can lead to groupthink, poor decisions, and stalled innovation.\n> If there is more truth in the hallways than in meetings, you have a problem.\nWhat: Setting Direction\n> Putting people first, pursuing excellence, embracing change, acting with\nintegrity, and serving our world.\nI think I just heard managers in traditional hierarchical organizations just\nthrow up in their mouths a little. Yes, let us all just gather round and hug one\nanother now. How can I be expected to lead if we are all gathered around singing\n\"Kumbaya\"? This is where The Open Organization turns next, as it seeks to close\nout this persuasive argument on a structure markedly different than the\ntraditional top-down approach\n\nInclusive Decisions\nFrom a developer relations perspective, how well does your company listen to\ndeveloper feedback? Has it altered or changed its plans based on feedback coming\nfrom outside its walls?\n\n> Getting people to engage in the decision-making process doesn’t have to be at\njust the corporate level. It’s a mind-set that leaders throughout the\norganization should wholeheartedly embrace.\nDevelopers are risk averse. Changing your product can have monumental impacts on\ntheir lives - and financial livelihood of the companies for which they work.\nWhen you engage your customers in the conversation, you achieve results that are\nbest for them, and in turn best for you. In lieu of having established a good\ndialog with developers, advocates are a great place for product teams to find\nout how their changes will impact their customers.\n\n> ... when you take the time to make your decision-making process transparent, you\ncan still drive progress and get things done, even when your decision is an\nunpopular one.\nIf your product team is not actively engaging the developer relations team(s)\nthen you have a problem. If you are on the product side, I get it - including\nadvocates may seem as though you are inviting dissent. Advocates are there to\nhelp - after all they have a vested interest in seeing the product succeed as\nwell. Nobody wants to get up on stage at a conference knowing full well that the\nproduct just pissed off a percentage of the audience - or is about to.\n\n> \"Lance recognized that solving the problem was not about holding endless\ndiscussions or answering every concern before we took action,” DiMaio said. “It\nwas more about acknowledging people’s concerns and then putting an exit plan in\nplace if things didn’t work out.\"\nCatalyzing Direction\nAdvocates come in all \"shapes and sizes\". That is to say that some are master\nnetworkers, while others posses unparalleled insight into how the source code\nworks. Both can contribute in their own ways. It is a matter of how they\napproach that work that impacts how well it relates to engagement.\n\n> We contribute to our communities, doing what we believe is in their best\nlong-term interest, by building up positions of thought leadership within.\n> By having capable, engaged people recognize the importance of the goal and then\nexpecting them to solve it in their own way, thousands of small tweaks can be\nmade across the company. That’s much richer and subtler than the handful of big\ninitiatives that a top-down planning process would generate.\nWith engagement in place, Mr. Whitehurst turns to experimenting your way to\nsuccess. Any experienced advocate knows this process well - any developer,\nreally. You think you can implement your solution in one way, only to find gaps\nthat make you have to explore another direction. It is these gaps that developer\nrelations should, through experimentation, seek out and fill.\n\n> When the bosses make the decisions, decisions are made by politics, persuasion,\nand PowerPoint. When you make decisions through experiment, the best idea can\nprove itself.\nThe important implication for management is in fostering the culture that allows\nthat experimentation to happen in the first place.\n\n> I believe the new skill in leadership is leadership by experiment.\nEpiloge\nThe epilog of The Open Organization focuses on the journey once you have decided\nto embrace the presented concepts. It can get pretty touchy-feely at points, but\ncontrary to what traditional management science may suggest - you unemotional\ncog - I think it really gets to the core of what an open organization is about,\nand frankly how developer relations organizations should function.\n\n> We at Red Hat beat our competitors to market and react more quickly to threats\nand opportunities not because we pedal faster, but because the organization taps\ninto powerful energy sources like purpose, passion, and community that make it\nmove faster.\nNext Steps\nI have clearly taken a very developer relations-centric view on The Open\nOrganization. After more than a decade of working in developer relations, I have\nexperienced many of the concepts presented first-hand - and I know all too well\nthe traditional management perspective that just does not \"get it.\" To me The\nOpen Organization was almost a formalizing of those lessons, presented by an\nold-school traditional manager. A thoroughly enjoyable read.\n\nTime will tell what the acquisition of Red Hat by IBM will mean to the cultures\nof both companies. If The Open Organization is any indicator of what might be\nexpected, then I think the combined forces have a great chance at success.","feature_image":"__GHOST_URL__/content/images/2019/02/red.hat.jpg","featured":0,"status":"published","locale":null,"visibility":"public","author_id":"1","created_at":"2019-01-23T22:39:11.000Z","updated_at":"2019-02-14T16:23:00.000Z","published_at":"2019-02-14T16:23:00.000Z","custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"type":"post","email_recipient_filter":"none"},{"id":"5c5b1a15b1b1f400c0b6e862","uuid":"54ef668d-a5c4-4b1b-be14-49cc1f00ef34","title":"Serverless Storage Redux","slug":"serverless-storage-redux","mobiledoc":"{\"version\":\"0.3.1\",\"atoms\":[],\"cards\":[[\"markdown\",{\"markdown\":\"```\\nrp( {\\n  url: 'https://iam.ng.bluemix.net/oidc/token',\\n  method: 'POST',\\n  form: {\\n    apikey: _YOUR_API_KEY_,\\n    response_type: 'cloud_iam',\\n    grant_type: 'urn:ibm:params:oauth:grant-type:apikey'\\n  },\\n  json: true\\n} )  \\n.then( ( data ) => {\\n  console.log( data.access_token );\\n} )\\n.catch( ( e ) => {\\n  console.log( 'Error getting token.' );\\n} );\\n```\"}],[\"markdown\",{\"markdown\":\"```\\n// Endpoints vary per region as well as public/private\\n// Check the documentation for the URL for your storage\\nconst COS_ENDPOINT = 's3.us-east.cloud-object-storage.appdomain.cloud';\\nconst FILE_TO_UPLOAD = fs.readFileSync( parts.file );\\n\\nrp( {\\n  url: `https://${COS_ENDPOINT}/${bucket}/${object}`,\\n  method: 'PUT',\\n  headers: {\\n    'ibm-service-instance-id': _YOUR_INSTANCE_ID_\\n  },\\n  auth: {\\n   bearer: _TOKEN_FROM_PREVIOUS_\\n  },\\n  body: FILE_TO_UPLOAD,\\n  resolveWithFullResponse: true      \\n} )\\n.then( ( response ) => {\\n  // ETag comes with nested quotes\\n  // Remove quotes\\n  let etag = response.headers['etag'].replace( /\\\"/g,\\\"\\\" );\\n\\n  console.log( etag );\\n} )\\n.catch( ( e ) => {\\n  console.log( 'Error uploading object.' );\\n} )\\n```\"}],[\"markdown\",{\"markdown\":\"```\\n// Endpoints vary per region as well as public/private\\n// Check the documentation for the URL for your storage\\nconst COS_ENDPOINT = 's3.us-east.cloud-object-storage.appdomain.cloud';\\n\\nrp( {\\n  url: `https://${COS_ENDPOINT}/${bucket}/${object}`,\\n  method: 'GET',\\n  headers: {\\n    'ibm-service-instance-id': _YOUR_INSTANCE_ID_\\n  },\\n  auth: {\\n    bearer: _TOKEN_FROM_PREVIOUS_\\n  },\\n  resolveWithFullResponse: true,\\n  encoding: null\\n} )\\n.then( ( response ) => {\\n  console.log( response.headers['content-type'] );\\n  console.log( response.body.toString( 'base64' );\\n} )\\n.catch( ( e ) => {\\n  console.log( 'Error getting object.' );\\n} );\\n```\"}],[\"markdown\",{\"markdown\":\"```\\nconst token = await cos.getToken( \\n  COS_AUTH_ENDPOINT, \\n  COS_API_KEY \\n);\\n\\nconst objects = await cos.getObjectList( \\n  token, \\n  COS_ENDPOINT, \\n  COS_SERVICE_INSTANCE, \\n  bucket \\n);\\n\\n// Mind blowing magic\\nfor( let f = 0; f < objects.length; f++ ) {\\n  await cos.deleteObject( \\n    token, \\n    COS_ENDPOINT, \\n    COS_SERVICE_INSTANCE, \\n    bucket, \\n    objects[f].name \\n  );\\n}\\n    \\nawait cos.deleteBucket( \\n  token, \\n  COS_ENDPOINT, \\n  COS_SERVICE_INSTANCE, \\n  bucket \\n); \\n```\"}]],\"markups\":[[\"a\",[\"href\",\"https://console.bluemix.net/openwhisk/\"]],[\"a\",[\"href\",\"https://www.ibm.com/cloud/object-storage\"]],[\"a\",[\"href\",\"https://github.com/ibm/ibm-cos-sdk-js\"]],[\"a\",[\"href\",\"__GHOST_URL__/2019/01/23/upload-files-to\"]],[\"a\",[\"href\",\"https://github.com/krhoyt/ServerlessStorage\"]]],\"sections\":[[1,\"p\",[[0,[],0,\"In my previous two posts I wrote about upload a file to \"],[0,[0],1,\"IBM Cloud Functions\"],[0,[],0,\" (serverless), and then how to download a file from \"],[0,[1],1,\"IBM Cloud Object Storage\"],[0,[],0,\" (COS) using serverless. Both of these posts used the the provided \"],[0,[2],1,\"IBM COS SDK for Node.js\"],[0,[],0,\". In this post I will rehash the upload and download process, but using the COS REST API.\"]]],[1,\"h2\",[[0,[],0,\"Et tu, SDK?\"]]],[1,\"p\",[[0,[],0,\"The IBM COS SDK for Node.js is a fork of the AWS S3 SDK for Node.js. As an IBM developer advocate, this has always felt a bit ... dirty. It is important that the interface to S3 is a de facto standard used by numerous storage providers. This comes with most of the benefits that would come with any other standardization. \"]]],[1,\"p\",[[0,[],0,\"That \\\"de facto\\\" bit, though. The S3 interface is not a standard in the full sense - Amazon could change the interface at any point, and the entire industry would have to shift. True standards help developers avoid these types of risks through structured feedback and oversight.\"]]],[1,\"h2\",[[0,[],0,\"Liberally Layered Libraries\"]]],[1,\"p\",[[0,[],0,\"Let me just come out and say this right up front - I am not a fan of frameworks, and generally try to leverage libraries a sparingly as possible. To be sure, frameworks have their place (large development teams), and libraries keep us from reinventing the wheel. I am not saying that developers should not use these tools, but that they should use them appropriately.\"]]],[1,\"p\",[[0,[],0,\"In this case, I just went right for the library (slaps self on wrist). I did not even stop to think about how the underlying transaction takes place, and whether or not I actually needed the library. Once I had wrapped up the previous posts, I went back to revisit those transactions. The interface to IBM COS is an easy-to-use, token-based, REST-based API. HTTP is a standard ... I am game!\"]]],[1,\"h2\",[[0,[],0,\"The Lion Sleeps Tonight\"]]],[1,\"p\",[[0,[],0,\"To get a token, you will need your API key from the IBM Cloud Object Storage dashboard as covered in my \"],[0,[3],1,\"previous post\"],[0,[],0,\". Plug that into an HTTP request against the authentication endpoint, and you have yourself a token.\"]]],[10,0],[1,\"p\",[[0,[],0,\"I am using \\\"request-promise\\\" here because functions operating on callbacks will terminate before the callback is ever reached. We need to return a promise that lets the system know that we are still executing code, and will get back to it with some data to return.\"]]],[1,\"h2\",[[0,[],0,\"Put An Object (Upload)\"]]],[1,\"p\",[[0,[],0,\"With our token in hand, we need one more request to upload the file to Cloud Object Storage. I talk about the \\\"instance ID\\\" in a previous post. You will find the ID you need in the credentials section of the Cloud Object Storage dashboard. Also before you can make the upload, you will need to read the file you want to upload.\"]]],[10,1],[1,\"p\",[[0,[],0,\"The HTTP request will return with no body. An \\\"ETag\\\" will be provided in the headers. The \\\"ETag\\\" is a hash of the file provided to allow you to verify that the contents of the file in the cloud match those of the files you have locally. Strangely, it comes wrapped in quotes, which we can remove with a splash of regular expressions. \"]]],[1,\"p\",[[0,[],0,\"To get access to the headers when using \\\"request-promise\\\" we need to include the \\\"resolveWithFullResponse\\\" property on the outgoing call. Otherwise the result of the promise is just the body of the request.\"]]],[1,\"h2\",[[0,[],0,\"Get an Object (Download)\"]]],[1,\"p\",[[0,[],0,\"The request to get a file from COS is almost identical. The only difference being that this is a \\\"GET\\\" request and as such has no body. We also do not want \\\"request-promise\\\" to be overly aggressive with our file contents, so we set \\\"encoding\\\" to \\\"null\\\" on the request.\"]]],[10,2],[1,\"p\",[[0,[],0,\"The \\\"resolveWithFullResponse\\\" property is useful here as well - we want both the headers and the body of the response. The headers are useful to get the content type of the file, and the body for the file itself. In the above example, I apply a Base-64 encoding to the file contents, which is needed for the HTTP response.\"]]],[1,\"h2\",[[0,[],0,\"Bonus: Delete All Teh Thingz\"]]],[1,\"p\",[[0,[],0,\"Gaining more insight into the underlying HTTP interface, I quickly started wrapping various operations in their own functions. And because I used \\\"request-promise\\\" everything surfaced as a promise. This then let me string what would otherwise be complex calls to COS together with \\\"async/await\\\".\"]]],[1,\"p\",[[0,[],0,\"The COS REST API provides a means to delete an object. Great! It also provides a means to delete a bucket. Great! ... If that bucket is empty ... Oh.\"]]],[1,\"p\",[[0,[],0,\"This means that you will first need to get a listing of objects in the given bucket. Then you will need to iterate over them - calling the REST API repeatedly to delete each object - using promises. If you have never had to do this before, then it may not seem like a big deal.  Trust me that the firing off of any quantity of asynchronous requests in a loop, that then must be completed in turn, is a real nightmare. Until \\\"async/await\\\". In fact, it is situations like this where \\\"async/await\\\" really shines.\"]]],[10,3],[1,\"p\",[[0,[],0,\"First, get your token. Next, the object list. With the object list in hand, iterate over them, deleting each in turn. Finally, delete the bucket itself. \"]]],[1,\"p\",[[0,[],0,\"You might notice that all you see here are the functions I have used to wrap the COS REST API. And you might otherwise call this a library. And then you might suggest that I said to beware of libraries just a few paragraphs ago. And, on the surface, you would be right.\"]]],[1,\"p\",[[0,[],0,\"What has changed here is that I now know why and how I am making each of the calls. I understand the transactions, and know that I do not need a whole separate wrapper. A wrapper that does some magic behind the scenes that I am otherwise oblivious to understanding.\"]]],[1,\"p\",[[0,[],0,\"In the previous post, you will see calls along the lines of \\\"COS_SDK.doSomething().promise().then()\\\". Where does that \\\"promise()\\\" bit come from? Why do I have to make another method call to get it? Should not the function just return a promise? Does the \\\"promise()\\\" call keep me from using \\\"async/await\\\" in the proper sense to solve complex, chained calls like deleting an entire bucket? Since it is not standard, and not an actual \\\"Promise\\\" object, I do not know how it will behave. This increases risk - effectively increased my dependency on the SDK - and makes me nervous about leveraging the SDK.\"]]],[1,\"p\",[[0,[],0,\"In short, I have distilled it down to the standards level, which gives me peace of mind and fine grained control.\"]]],[1,\"h2\",[[0,[],0,\"Next Steps\"]]],[1,\"p\",[[0,[],0,\"If you want to take a look at the functions I implemented to access the REST SDK, it is available in a GitHub \"],[0,[4],1,\"repository\"],[0,[],0,\" for this series of posts. You will also find a complete mobile UI (web standards) for managing your IBM Cloud Object Storage instance using IBM Cloud Functions.\"]]],[1,\"p\",[[0,[],0,\"There are two next steps I would like to pursue if I had more time. The first is to handle chunked file upload. Effectively upload a very large file by reading it in, breaking it apart at a specific point, and then treating each part as its own distinct file on the upload, then reassemble those back on the server. The COS REST API supports this, I just did not need it for my application.\"]]],[1,\"p\",[[0,[],0,\"The second step would be to handle paging of object lists. The COS REST API will only return 1,000 records at a time. If the bucket contains more than 1,000 items, you will have to page through them. This can be explicit or implicit, but currently it is something I do not account for at all.\"]]],[1,\"p\",[[0,[],0,\"Finally, I think it would be cool to use a bucket as a root, and use naming conventions to designate folders. Buckets can only be one layer deep. S3 gets around this by using names like \\\"my_folder/my_object.jpg\\\" and then logically display folders in the user interface. This would allow me to create a bucket for workshop attendees using their email address, and keep each students files/work separate from the others - all while using a single instance of IBM Cloud Object Storage.\"]]]],\"ghostVersion\":\"3.0\"}","html":"<p>In my previous two posts I wrote about upload a file to <a href=\"https://console.bluemix.net/openwhisk/\">IBM Cloud Functions</a> (serverless), and then how to download a file from <a href=\"https://www.ibm.com/cloud/object-storage\">IBM Cloud Object Storage</a> (COS) using serverless. Both of these posts used the the provided <a href=\"https://github.com/ibm/ibm-cos-sdk-js\">IBM COS SDK for Node.js</a>. In this post I will rehash the upload and download process, but using the COS REST API.</p><h2 id=\"et-tu-sdk\">Et tu, SDK?</h2><p>The IBM COS SDK for Node.js is a fork of the AWS S3 SDK for Node.js. As an IBM developer advocate, this has always felt a bit ... dirty. It is important that the interface to S3 is a de facto standard used by numerous storage providers. This comes with most of the benefits that would come with any other standardization. </p><p>That \"de facto\" bit, though. The S3 interface is not a standard in the full sense - Amazon could change the interface at any point, and the entire industry would have to shift. True standards help developers avoid these types of risks through structured feedback and oversight.</p><h2 id=\"liberally-layered-libraries\">Liberally Layered Libraries</h2><p>Let me just come out and say this right up front - I am not a fan of frameworks, and generally try to leverage libraries a sparingly as possible. To be sure, frameworks have their place (large development teams), and libraries keep us from reinventing the wheel. I am not saying that developers should not use these tools, but that they should use them appropriately.</p><p>In this case, I just went right for the library (slaps self on wrist). I did not even stop to think about how the underlying transaction takes place, and whether or not I actually needed the library. Once I had wrapped up the previous posts, I went back to revisit those transactions. The interface to IBM COS is an easy-to-use, token-based, REST-based API. HTTP is a standard ... I am game!</p><h2 id=\"the-lion-sleeps-tonight\">The Lion Sleeps Tonight</h2><p>To get a token, you will need your API key from the IBM Cloud Object Storage dashboard as covered in my <a href=\"__GHOST_URL__/2019/01/23/upload-files-to\">previous post</a>. Plug that into an HTTP request against the authentication endpoint, and you have yourself a token.</p><!--kg-card-begin: markdown--><pre><code>rp( {\n  url: 'https://iam.ng.bluemix.net/oidc/token',\n  method: 'POST',\n  form: {\n    apikey: _YOUR_API_KEY_,\n    response_type: 'cloud_iam',\n    grant_type: 'urn:ibm:params:oauth:grant-type:apikey'\n  },\n  json: true\n} )  \n.then( ( data ) =&gt; {\n  console.log( data.access_token );\n} )\n.catch( ( e ) =&gt; {\n  console.log( 'Error getting token.' );\n} );\n</code></pre>\n<!--kg-card-end: markdown--><p>I am using \"request-promise\" here because functions operating on callbacks will terminate before the callback is ever reached. We need to return a promise that lets the system know that we are still executing code, and will get back to it with some data to return.</p><h2 id=\"put-an-object-upload-\">Put An Object (Upload)</h2><p>With our token in hand, we need one more request to upload the file to Cloud Object Storage. I talk about the \"instance ID\" in a previous post. You will find the ID you need in the credentials section of the Cloud Object Storage dashboard. Also before you can make the upload, you will need to read the file you want to upload.</p><!--kg-card-begin: markdown--><pre><code>// Endpoints vary per region as well as public/private\n// Check the documentation for the URL for your storage\nconst COS_ENDPOINT = 's3.us-east.cloud-object-storage.appdomain.cloud';\nconst FILE_TO_UPLOAD = fs.readFileSync( parts.file );\n\nrp( {\n  url: `https://${COS_ENDPOINT}/${bucket}/${object}`,\n  method: 'PUT',\n  headers: {\n    'ibm-service-instance-id': _YOUR_INSTANCE_ID_\n  },\n  auth: {\n   bearer: _TOKEN_FROM_PREVIOUS_\n  },\n  body: FILE_TO_UPLOAD,\n  resolveWithFullResponse: true      \n} )\n.then( ( response ) =&gt; {\n  // ETag comes with nested quotes\n  // Remove quotes\n  let etag = response.headers['etag'].replace( /&quot;/g,&quot;&quot; );\n\n  console.log( etag );\n} )\n.catch( ( e ) =&gt; {\n  console.log( 'Error uploading object.' );\n} )\n</code></pre>\n<!--kg-card-end: markdown--><p>The HTTP request will return with no body. An \"ETag\" will be provided in the headers. The \"ETag\" is a hash of the file provided to allow you to verify that the contents of the file in the cloud match those of the files you have locally. Strangely, it comes wrapped in quotes, which we can remove with a splash of regular expressions. </p><p>To get access to the headers when using \"request-promise\" we need to include the \"resolveWithFullResponse\" property on the outgoing call. Otherwise the result of the promise is just the body of the request.</p><h2 id=\"get-an-object-download-\">Get an Object (Download)</h2><p>The request to get a file from COS is almost identical. The only difference being that this is a \"GET\" request and as such has no body. We also do not want \"request-promise\" to be overly aggressive with our file contents, so we set \"encoding\" to \"null\" on the request.</p><!--kg-card-begin: markdown--><pre><code>// Endpoints vary per region as well as public/private\n// Check the documentation for the URL for your storage\nconst COS_ENDPOINT = 's3.us-east.cloud-object-storage.appdomain.cloud';\n\nrp( {\n  url: `https://${COS_ENDPOINT}/${bucket}/${object}`,\n  method: 'GET',\n  headers: {\n    'ibm-service-instance-id': _YOUR_INSTANCE_ID_\n  },\n  auth: {\n    bearer: _TOKEN_FROM_PREVIOUS_\n  },\n  resolveWithFullResponse: true,\n  encoding: null\n} )\n.then( ( response ) =&gt; {\n  console.log( response.headers['content-type'] );\n  console.log( response.body.toString( 'base64' );\n} )\n.catch( ( e ) =&gt; {\n  console.log( 'Error getting object.' );\n} );\n</code></pre>\n<!--kg-card-end: markdown--><p>The \"resolveWithFullResponse\" property is useful here as well - we want both the headers and the body of the response. The headers are useful to get the content type of the file, and the body for the file itself. In the above example, I apply a Base-64 encoding to the file contents, which is needed for the HTTP response.</p><h2 id=\"bonus-delete-all-teh-thingz\">Bonus: Delete All Teh Thingz</h2><p>Gaining more insight into the underlying HTTP interface, I quickly started wrapping various operations in their own functions. And because I used \"request-promise\" everything surfaced as a promise. This then let me string what would otherwise be complex calls to COS together with \"async/await\".</p><p>The COS REST API provides a means to delete an object. Great! It also provides a means to delete a bucket. Great! ... If that bucket is empty ... Oh.</p><p>This means that you will first need to get a listing of objects in the given bucket. Then you will need to iterate over them - calling the REST API repeatedly to delete each object - using promises. If you have never had to do this before, then it may not seem like a big deal.  Trust me that the firing off of any quantity of asynchronous requests in a loop, that then must be completed in turn, is a real nightmare. Until \"async/await\". In fact, it is situations like this where \"async/await\" really shines.</p><!--kg-card-begin: markdown--><pre><code>const token = await cos.getToken( \n  COS_AUTH_ENDPOINT, \n  COS_API_KEY \n);\n\nconst objects = await cos.getObjectList( \n  token, \n  COS_ENDPOINT, \n  COS_SERVICE_INSTANCE, \n  bucket \n);\n\n// Mind blowing magic\nfor( let f = 0; f &lt; objects.length; f++ ) {\n  await cos.deleteObject( \n    token, \n    COS_ENDPOINT, \n    COS_SERVICE_INSTANCE, \n    bucket, \n    objects[f].name \n  );\n}\n    \nawait cos.deleteBucket( \n  token, \n  COS_ENDPOINT, \n  COS_SERVICE_INSTANCE, \n  bucket \n); \n</code></pre>\n<!--kg-card-end: markdown--><p>First, get your token. Next, the object list. With the object list in hand, iterate over them, deleting each in turn. Finally, delete the bucket itself. </p><p>You might notice that all you see here are the functions I have used to wrap the COS REST API. And you might otherwise call this a library. And then you might suggest that I said to beware of libraries just a few paragraphs ago. And, on the surface, you would be right.</p><p>What has changed here is that I now know why and how I am making each of the calls. I understand the transactions, and know that I do not need a whole separate wrapper. A wrapper that does some magic behind the scenes that I am otherwise oblivious to understanding.</p><p>In the previous post, you will see calls along the lines of \"COS_SDK.doSomething().promise().then()\". Where does that \"promise()\" bit come from? Why do I have to make another method call to get it? Should not the function just return a promise? Does the \"promise()\" call keep me from using \"async/await\" in the proper sense to solve complex, chained calls like deleting an entire bucket? Since it is not standard, and not an actual \"Promise\" object, I do not know how it will behave. This increases risk - effectively increased my dependency on the SDK - and makes me nervous about leveraging the SDK.</p><p>In short, I have distilled it down to the standards level, which gives me peace of mind and fine grained control.</p><h2 id=\"next-steps\">Next Steps</h2><p>If you want to take a look at the functions I implemented to access the REST SDK, it is available in a GitHub <a href=\"https://github.com/krhoyt/ServerlessStorage\">repository</a> for this series of posts. You will also find a complete mobile UI (web standards) for managing your IBM Cloud Object Storage instance using IBM Cloud Functions.</p><p>There are two next steps I would like to pursue if I had more time. The first is to handle chunked file upload. Effectively upload a very large file by reading it in, breaking it apart at a specific point, and then treating each part as its own distinct file on the upload, then reassemble those back on the server. The COS REST API supports this, I just did not need it for my application.</p><p>The second step would be to handle paging of object lists. The COS REST API will only return 1,000 records at a time. If the bucket contains more than 1,000 items, you will have to page through them. This can be explicit or implicit, but currently it is something I do not account for at all.</p><p>Finally, I think it would be cool to use a bucket as a root, and use naming conventions to designate folders. Buckets can only be one layer deep. S3 gets around this by using names like \"my_folder/my_object.jpg\" and then logically display folders in the user interface. This would allow me to create a bucket for workshop attendees using their email address, and keep each students files/work separate from the others - all while using a single instance of IBM Cloud Object Storage.</p>","comment_id":"5c5b1a15b1b1f400c0b6e862","plaintext":"In my previous two posts I wrote about upload a file to IBM Cloud Functions\n[https://console.bluemix.net/openwhisk/] (serverless), and then how to download\na file from IBM Cloud Object Storage [https://www.ibm.com/cloud/object-storage] \n(COS) using serverless. Both of these posts used the the provided IBM COS SDK\nfor Node.js [https://github.com/ibm/ibm-cos-sdk-js]. In this post I will rehash\nthe upload and download process, but using the COS REST API.\n\nEt tu, SDK?\nThe IBM COS SDK for Node.js is a fork of the AWS S3 SDK for Node.js. As an IBM\ndeveloper advocate, this has always felt a bit ... dirty. It is important that\nthe interface to S3 is a de facto standard used by numerous storage providers.\nThis comes with most of the benefits that would come with any other\nstandardization. \n\nThat \"de facto\" bit, though. The S3 interface is not a standard in the full\nsense - Amazon could change the interface at any point, and the entire industry\nwould have to shift. True standards help developers avoid these types of risks\nthrough structured feedback and oversight.\n\nLiberally Layered Libraries\nLet me just come out and say this right up front - I am not a fan of frameworks,\nand generally try to leverage libraries a sparingly as possible. To be sure,\nframeworks have their place (large development teams), and libraries keep us\nfrom reinventing the wheel. I am not saying that developers should not use these\ntools, but that they should use them appropriately.\n\nIn this case, I just went right for the library (slaps self on wrist). I did not\neven stop to think about how the underlying transaction takes place, and whether\nor not I actually needed the library. Once I had wrapped up the previous posts,\nI went back to revisit those transactions. The interface to IBM COS is an\neasy-to-use, token-based, REST-based API. HTTP is a standard ... I am game!\n\nThe Lion Sleeps Tonight\nTo get a token, you will need your API key from the IBM Cloud Object Storage\ndashboard as covered in my previous post\n[__GHOST_URL__/2019/01/23/upload-files-to]. Plug that into an HTTP request\nagainst the authentication endpoint, and you have yourself a token.\n\nrp( {\n  url: 'https://iam.ng.bluemix.net/oidc/token',\n  method: 'POST',\n  form: {\n    apikey: _YOUR_API_KEY_,\n    response_type: 'cloud_iam',\n    grant_type: 'urn:ibm:params:oauth:grant-type:apikey'\n  },\n  json: true\n} )  \n.then( ( data ) => {\n  console.log( data.access_token );\n} )\n.catch( ( e ) => {\n  console.log( 'Error getting token.' );\n} );\n\n\nI am using \"request-promise\" here because functions operating on callbacks will\nterminate before the callback is ever reached. We need to return a promise that\nlets the system know that we are still executing code, and will get back to it\nwith some data to return.\n\nPut An Object (Upload)\nWith our token in hand, we need one more request to upload the file to Cloud\nObject Storage. I talk about the \"instance ID\" in a previous post. You will find\nthe ID you need in the credentials section of the Cloud Object Storage\ndashboard. Also before you can make the upload, you will need to read the file\nyou want to upload.\n\n// Endpoints vary per region as well as public/private\n// Check the documentation for the URL for your storage\nconst COS_ENDPOINT = 's3.us-east.cloud-object-storage.appdomain.cloud';\nconst FILE_TO_UPLOAD = fs.readFileSync( parts.file );\n\nrp( {\n  url: `https://${COS_ENDPOINT}/${bucket}/${object}`,\n  method: 'PUT',\n  headers: {\n    'ibm-service-instance-id': _YOUR_INSTANCE_ID_\n  },\n  auth: {\n   bearer: _TOKEN_FROM_PREVIOUS_\n  },\n  body: FILE_TO_UPLOAD,\n  resolveWithFullResponse: true      \n} )\n.then( ( response ) => {\n  // ETag comes with nested quotes\n  // Remove quotes\n  let etag = response.headers['etag'].replace( /\"/g,\"\" );\n\n  console.log( etag );\n} )\n.catch( ( e ) => {\n  console.log( 'Error uploading object.' );\n} )\n\n\nThe HTTP request will return with no body. An \"ETag\" will be provided in the\nheaders. The \"ETag\" is a hash of the file provided to allow you to verify that\nthe contents of the file in the cloud match those of the files you have locally.\nStrangely, it comes wrapped in quotes, which we can remove with a splash of\nregular expressions. \n\nTo get access to the headers when using \"request-promise\" we need to include the\n\"resolveWithFullResponse\" property on the outgoing call. Otherwise the result of\nthe promise is just the body of the request.\n\nGet an Object (Download)\nThe request to get a file from COS is almost identical. The only difference\nbeing that this is a \"GET\" request and as such has no body. We also do not want\n\"request-promise\" to be overly aggressive with our file contents, so we set\n\"encoding\" to \"null\" on the request.\n\n// Endpoints vary per region as well as public/private\n// Check the documentation for the URL for your storage\nconst COS_ENDPOINT = 's3.us-east.cloud-object-storage.appdomain.cloud';\n\nrp( {\n  url: `https://${COS_ENDPOINT}/${bucket}/${object}`,\n  method: 'GET',\n  headers: {\n    'ibm-service-instance-id': _YOUR_INSTANCE_ID_\n  },\n  auth: {\n    bearer: _TOKEN_FROM_PREVIOUS_\n  },\n  resolveWithFullResponse: true,\n  encoding: null\n} )\n.then( ( response ) => {\n  console.log( response.headers['content-type'] );\n  console.log( response.body.toString( 'base64' );\n} )\n.catch( ( e ) => {\n  console.log( 'Error getting object.' );\n} );\n\n\nThe \"resolveWithFullResponse\" property is useful here as well - we want both the\nheaders and the body of the response. The headers are useful to get the content\ntype of the file, and the body for the file itself. In the above example, I\napply a Base-64 encoding to the file contents, which is needed for the HTTP\nresponse.\n\nBonus: Delete All Teh Thingz\nGaining more insight into the underlying HTTP interface, I quickly started\nwrapping various operations in their own functions. And because I used\n\"request-promise\" everything surfaced as a promise. This then let me string what\nwould otherwise be complex calls to COS together with \"async/await\".\n\nThe COS REST API provides a means to delete an object. Great! It also provides a\nmeans to delete a bucket. Great! ... If that bucket is empty ... Oh.\n\nThis means that you will first need to get a listing of objects in the given\nbucket. Then you will need to iterate over them - calling the REST API\nrepeatedly to delete each object - using promises. If you have never had to do\nthis before, then it may not seem like a big deal.  Trust me that the firing off\nof any quantity of asynchronous requests in a loop, that then must be completed\nin turn, is a real nightmare. Until \"async/await\". In fact, it is situations\nlike this where \"async/await\" really shines.\n\nconst token = await cos.getToken( \n  COS_AUTH_ENDPOINT, \n  COS_API_KEY \n);\n\nconst objects = await cos.getObjectList( \n  token, \n  COS_ENDPOINT, \n  COS_SERVICE_INSTANCE, \n  bucket \n);\n\n// Mind blowing magic\nfor( let f = 0; f < objects.length; f++ ) {\n  await cos.deleteObject( \n    token, \n    COS_ENDPOINT, \n    COS_SERVICE_INSTANCE, \n    bucket, \n    objects[f].name \n  );\n}\n    \nawait cos.deleteBucket( \n  token, \n  COS_ENDPOINT, \n  COS_SERVICE_INSTANCE, \n  bucket \n); \n\n\nFirst, get your token. Next, the object list. With the object list in hand,\niterate over them, deleting each in turn. Finally, delete the bucket itself. \n\nYou might notice that all you see here are the functions I have used to wrap the\nCOS REST API. And you might otherwise call this a library. And then you might\nsuggest that I said to beware of libraries just a few paragraphs ago. And, on\nthe surface, you would be right.\n\nWhat has changed here is that I now know why and how I am making each of the\ncalls. I understand the transactions, and know that I do not need a whole\nseparate wrapper. A wrapper that does some magic behind the scenes that I am\notherwise oblivious to understanding.\n\nIn the previous post, you will see calls along the lines of\n\"COS_SDK.doSomething().promise().then()\". Where does that \"promise()\" bit come\nfrom? Why do I have to make another method call to get it? Should not the\nfunction just return a promise? Does the \"promise()\" call keep me from using\n\"async/await\" in the proper sense to solve complex, chained calls like deleting\nan entire bucket? Since it is not standard, and not an actual \"Promise\" object,\nI do not know how it will behave. This increases risk - effectively increased my\ndependency on the SDK - and makes me nervous about leveraging the SDK.\n\nIn short, I have distilled it down to the standards level, which gives me peace\nof mind and fine grained control.\n\nNext Steps\nIf you want to take a look at the functions I implemented to access the REST\nSDK, it is available in a GitHub repository\n[https://github.com/krhoyt/ServerlessStorage] for this series of posts. You will\nalso find a complete mobile UI (web standards) for managing your IBM Cloud\nObject Storage instance using IBM Cloud Functions.\n\nThere are two next steps I would like to pursue if I had more time. The first is\nto handle chunked file upload. Effectively upload a very large file by reading\nit in, breaking it apart at a specific point, and then treating each part as its\nown distinct file on the upload, then reassemble those back on the server. The\nCOS REST API supports this, I just did not need it for my application.\n\nThe second step would be to handle paging of object lists. The COS REST API will\nonly return 1,000 records at a time. If the bucket contains more than 1,000\nitems, you will have to page through them. This can be explicit or implicit, but\ncurrently it is something I do not account for at all.\n\nFinally, I think it would be cool to use a bucket as a root, and use naming\nconventions to designate folders. Buckets can only be one layer deep. S3 gets\naround this by using names like \"my_folder/my_object.jpg\" and then logically\ndisplay folders in the user interface. This would allow me to create a bucket\nfor workshop attendees using their email address, and keep each students\nfiles/work separate from the others - all while using a single instance of IBM\nCloud Object Storage.","feature_image":"__GHOST_URL__/content/images/2019/02/whisk.eggs.jpg","featured":0,"status":"published","locale":null,"visibility":"public","author_id":"1","created_at":"2019-02-06T17:32:05.000Z","updated_at":"2019-02-06T19:30:19.000Z","published_at":"2019-02-06T19:30:19.000Z","custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"type":"post","email_recipient_filter":"none"},{"id":"5ca910e8c0479c00c0c80302","uuid":"c99fb390-c1a0-4a56-b669-b3cfe1bb4db1","title":"Functions, Storage, Watson ... Oh, My!","slug":"storage-functions-watson-oh-my","mobiledoc":"{\"version\":\"0.3.1\",\"atoms\":[],\"cards\":[[\"markdown\",{\"markdown\":\"```\\nibmcloud fn trigger create store.this \\\\\\n  --feed /whisk.system/cos-experimental/changes \\\\\\n  --param apikey \\\"_YOUR_API_KEY_\\\" \\\\\\n  --param bucket \\\"show-and-tell\\\" \\\\\\n  --param endpoint \\\"s3.us-east.cloud-object-storage.appdomain.cloud\\\"\\n```\"}],[\"markdown\",{\"markdown\":\"```\\nibmcloud fn trigger create store.this --feed /whisk.system/cos-experimental/changes --param apikey _MY_API_KEY_ --param bucket u-betta-recognize --param endpoint s3.us-east.cloud-object-storage.appdomain.cloud\\n```\"}],[\"markdown\",{\"markdown\":\"```\\nfunction main( params ) {\\n  if( params.status !== 'added' ) {\\n    return;\\n  }\\n  \\n  // Here we go\\n}\\n```\"}],[\"markdown\",{\"markdown\":\"```\\nrp( {\\n  url: 'https://iam.ng.bluemix.net/oidc/token',\\n  method: 'POST',\\n  form: {\\n    apikey: params.COS_API_KEY,\\n    response_type: 'cloud_iam',\\n    grant_type: 'urn:ibm:params:oauth:grant-type:apikey'\\n  },\\n  json: true\\n} );\\n```\"}],[\"markdown\",{\"markdown\":\"```\\nrp( {\\n  url: `https://${params.endpoint}/${params.bucket}/${params.key}`,\\n  method: 'GET',\\n  headers: {\\n    Authorization: `${authorization.token_type} ${authorization.access_token}`,\\n    'ibm-service-instance-id': params.COS_SERVICE_INSTANCE\\n  },\\n  encoding: null\\n} );\\n```\"}],[\"markdown\",{\"markdown\":\"```\\nrp( {\\n  url: 'https://gateway.watsonplatform.net/visual-recognition/api/v3/classify',\\n  method: 'POST',\\n  auth: {\\n    user: params.WATSON_USERNAME,\\n    pass: params.WATSON_PASSWORD\\n  },\\n  formData: {\\n    images_file: {\\n      value: contents,\\n      options: {\\n        filename: params.key\\n      }\\n    }\\n  },\\n  qs: {\\n    version: '2018-03-19'\\n  },\\n  json: true\\n} );\\n```\"}],[\"markdown\",{\"markdown\":\"```\\nclassification.bucket = params.bucket;\\nclassification.key = params.key;\\nclassification.endpoint = params.endpoint;\\n      \\nrp( {\\n  url: `https://${params.CLOUDANT_ACCOUNT}.cloudant.com/${params.CLOUDANT_DATABASE}`,\\n  method: 'POST',\\n  auth: {\\n    user: params.CLOUDANT_USERNAME,\\n    pass: params.CLOUDANT_PASSWORD\\n  },\\n  json: true,\\n  body: classification\\n} );\\n```\"}],[\"markdown\",{\"markdown\":\"```\\nibmcloud fn action create recognize recognize.js\\n```\"}],[\"markdown\",{\"markdown\":\"```\\nibmcloud fn rule create store.recognize store.this recognize\\n```\"}],[\"embed\",{\"url\":\"https://www.youtube.com/watch?v=d_YpimHwHeM\",\"html\":\"<iframe width=\\\"459\\\" height=\\\"344\\\" src=\\\"https://www.youtube.com/embed/d_YpimHwHeM?feature=oembed\\\" frameborder=\\\"0\\\" allow=\\\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\\\" allowfullscreen></iframe>\",\"type\":\"video\",\"caption\":\"See the code in action!\"}]],\"markups\":[[\"a\",[\"href\",\"https://openwhisk.apache.org/\"]],[\"a\",[\"href\",\"https://www.ibm.com/cloud/functions\"]],[\"a\",[\"href\",\"https://www.ibm.com/cloud/object-storage\"]],[\"a\",[\"href\",\"http://couchdb.apache.org/\"]],[\"a\",[\"href\",\"https://www.ibm.com/cloud/message-hub\"]],[\"a\",[\"href\",\"https://kafka.apache.org/\"]],[\"a\",[\"href\",\"__GHOST_URL__/2019/01/23/upload-files-to/\"]],[\"a\",[\"href\",\"__GHOST_URL__/2019/01/30/serverless-download-from-object-storage/\"]],[\"a\",[\"href\",\"__GHOST_URL__/2019/02/06/serverless-storage-redux/\"]],[\"a\",[\"href\",\"https://www.ibm.com/watson/services/visual-recognition\"]],[\"a\",[\"href\",\"https://cloud.ibm.com/docs/openwhisk?topic=cloud-functions-cloud_object_storage#cloud_object_storage\"]],[\"a\",[\"href\",\"https://gist.github.com/krhoyt/50709e6c7322df57f744ecc3bee475ce\"]]],\"sections\":[[1,\"p\",[[0,[],0,\"I have a love-hate relationship with serverless. My style of development definitely trends towards dropping a function in the cloud and calling it done. But the tooling, integration, and best practices are still maturing. It is however, fun to watch the technology evolve. Recently \"],[0,[0],1,\"Apache OpenWhisk\"],[0,[],0,\" (\"],[0,[1],1,\"IBM Cloud Functions\"],[0,[],0,\") released a beta feature around triggers for \"],[0,[2],1,\"Cloud Object Storage\"],[0,[],0,\", and I figured I would take it for a spin.\"]]],[1,\"blockquote\",[[0,[],0,\"Some IBM Cloud Functions commands can get very lengthy - especially if you have several parameters you need to set. Shell scripts are your friend. I tend to have a few scattered around every function for creating and updating the associated actions, sequences, etc. Do not forget to \\\"chmod +x create-action.sh\\\". Here is an example of what using the CLI looks like from a shell script. This is a single command. I have seen scripts from other developers that are respectable applications unto themselves.\"]]],[10,0],[1,\"h2\",[[0,[],0,\"Trigger\"]]],[1,\"p\",[[0,[],0,\"Triggers allow Cloud Functions to be invoked when something happens on an external system. Cloud Functions provides built-in triggers for Cloudant (\"],[0,[3],1,\"CouchDB\"],[0,[],0,\"), \"],[0,[4],1,\"MessageHub\"],[0,[],0,\" (\"],[0,[5],1,\"Apache Kafka\"],[0,[],0,\"), GitHub, and more. Did a document just get added to your database? Invoke an action. Did a message just get placed in a queue? Invoke an action. New code committed? Invoke an action.\"]]],[1,\"p\",[[0,[],0,\"And now, did a file get uploaded to storage? Invoke an action!\"]]],[1,\"h2\",[[0,[],0,\"Storage\"]]],[1,\"p\",[[0,[],0,\"My \"],[0,[6],1,\"recent\"],[0,[],0,\" \"],[0,[7],1,\"blog\"],[0,[],0,\" \"],[0,[8],1,\"posts\"],[0,[],0,\" have been around Cloud Object Storage and Cloud Functions, so this particular trigger definitely piqued my interest. As soon as I heard about the feature, a use-case came to mind. I want to upload a file to storage, have a Cloud Function take the file and send it to \"],[0,[9],1,\"Watson Visual Recognition\"],[0,[],0,\" for classification, and finally put the results in a Cloudant database.\"]]],[1,\"h2\",[[0,[],0,\"Function\"]]],[1,\"p\",[[0,[],0,\"The \"],[0,[10],1,\"documentation\"],[0,[],0,\" for this feature walks through setting up all the necessary parts. The first step is to create the trigger. The documentation walks you through some additional \\\"binding\\\" steps, but I just went straight to the source. After all, it is not that often you need to adjust the actual trigger itself.\"]]],[10,1],[1,\"p\",[[0,[],0,\"Next up is to create the action/function that will get invoked when the trigger runs. The trigger is invoke for a variety of reasons - you added a file is the one I am interested in, but it could also be for being deleted. When the function gets called, various details about the Cloud Object Storage are passed as parameters, such as the key (object name) of the resource that changed, and the bucket in which that object resides.\"]]],[10,2],[1,\"p\",[[0,[],0,\"Cloud Functions has an Object Storage library that comes with it, but as I have written about before, I rather prefer the HTTP/REST interface. The first step in interacting with the REST interface is in getting an authorization token. Where parameters are not provided by the trigger, I prefer to define them using the web-based tooling, and label them in uppercase.\"]]],[10,3],[1,\"p\",[[0,[],0,\"With the token retrieved, I can now go for the file itself. There is a choice to make here. Theoretically, you should be able to get a public URL to a resource on Cloud Object Storage. This is one of the ways we can call Watson - with a URL. However, in this case, I am going to download the file itself to the running function - and keep it in memory.\"]]],[10,4],[1,\"p\",[[0,[],0,\"From there I am going to send the file bytes to Watson for classification. If you are using JavaScript and request/request-promise as I am, then pay particular attention about how the file gets passed. The request documentation shows putting a Buffer, which is the result of downloading the file, directly onto the form. However, you have to use the method that specifies a file name, otherwise the file contents will not be passed.\"]]],[10,5],[1,\"p\",[[0,[],0,\"Last but not least, take the results of the Visual Recognition classification, and put it into a Cloudant database. Of course any database could be used, but Cloudant lends itself to this type of scenario. Being REST-based itself, there are no additional dependencies. Cloudant views are also well suited to taking a bunch of specific, nested, data, like what is returned from Watson, and distill it down to something else.\"]]],[10,6],[1,\"p\",[[0,[],0,\"Before I put the classification results into the database, I also put some additional information about the Object Storage changes. Mostly, details I thought might be helpful in creating views down the road. \\\"What are the top classifications for a given bucket?\\\" type of questions. Creating a document in Cloudant returns with an ID, which you can return at the end of your function if you want.\"]]],[10,7],[1,\"h2\",[[0,[],0,\"Rule\"]]],[1,\"p\",[[0,[],0,\"We create the trigger (store.this). We created the action to be called by the trigger (recognize). Now we just need to glue the two together using a rule. With the trigger and the rule defined, you can iterate on the function as often as you need, without have to touch either the trigger or the rule.\"]]],[10,8],[1,\"p\",[[0,[],0,\"If you change the bucket you want to watch, the storage location, or the frequency of how often the trigger looks for changes (default of once per minute), then you will need to update your trigger. However, the rule and the action can be left untouched.\"]]],[1,\"h2\",[[0,[],0,\"Next Steps\"]]],[1,\"p\",[[0,[],0,\"In days of old, this type of chain of events in JavaScript would be callback hell. However, with request-promise being included as part of Cloud Functions, it becomes a chain of \\\"then\\\" statements, making the code easier to maintain. If you want to specify a modern version of Node.js for Cloud Functions to use, then you can even use async/await. I implemented both, and have put them in a \"],[0,[11],1,\"GitHub Gist\"],[0,[],0,\".\"]]],[1,\"p\",[[0,[],0,\"What is more interesting is the function calls made within this Cloud Function/action. Each of the various steps above could be Cloud Functions themselves, and then a sequence could be made that ties them all together. How granular is too granular? I view all the function calls made to be a part of a single body of work, and in that sense consider it a single action. Do you have an opinion on the matter?\"]]],[1,\"p\",[[0,[],0,\"And that brings us full circle to the as of yet maturing best practices. Exciting times ahead.\"]]],[10,9],[1,\"p\",[]]],\"ghostVersion\":\"3.0\"}","html":"<p>I have a love-hate relationship with serverless. My style of development definitely trends towards dropping a function in the cloud and calling it done. But the tooling, integration, and best practices are still maturing. It is however, fun to watch the technology evolve. Recently <a href=\"https://openwhisk.apache.org/\">Apache OpenWhisk</a> (<a href=\"https://www.ibm.com/cloud/functions\">IBM Cloud Functions</a>) released a beta feature around triggers for <a href=\"https://www.ibm.com/cloud/object-storage\">Cloud Object Storage</a>, and I figured I would take it for a spin.</p><blockquote>Some IBM Cloud Functions commands can get very lengthy - especially if you have several parameters you need to set. Shell scripts are your friend. I tend to have a few scattered around every function for creating and updating the associated actions, sequences, etc. Do not forget to \"chmod +x create-action.sh\". Here is an example of what using the CLI looks like from a shell script. This is a single command. I have seen scripts from other developers that are respectable applications unto themselves.</blockquote><!--kg-card-begin: markdown--><pre><code>ibmcloud fn trigger create store.this \\\n  --feed /whisk.system/cos-experimental/changes \\\n  --param apikey &quot;_YOUR_API_KEY_&quot; \\\n  --param bucket &quot;show-and-tell&quot; \\\n  --param endpoint &quot;s3.us-east.cloud-object-storage.appdomain.cloud&quot;\n</code></pre>\n<!--kg-card-end: markdown--><h2 id=\"trigger\">Trigger</h2><p>Triggers allow Cloud Functions to be invoked when something happens on an external system. Cloud Functions provides built-in triggers for Cloudant (<a href=\"http://couchdb.apache.org/\">CouchDB</a>), <a href=\"https://www.ibm.com/cloud/message-hub\">MessageHub</a> (<a href=\"https://kafka.apache.org/\">Apache Kafka</a>), GitHub, and more. Did a document just get added to your database? Invoke an action. Did a message just get placed in a queue? Invoke an action. New code committed? Invoke an action.</p><p>And now, did a file get uploaded to storage? Invoke an action!</p><h2 id=\"storage\">Storage</h2><p>My <a href=\"__GHOST_URL__/2019/01/23/upload-files-to/\">recent</a> <a href=\"__GHOST_URL__/2019/01/30/serverless-download-from-object-storage/\">blog</a> <a href=\"__GHOST_URL__/2019/02/06/serverless-storage-redux/\">posts</a> have been around Cloud Object Storage and Cloud Functions, so this particular trigger definitely piqued my interest. As soon as I heard about the feature, a use-case came to mind. I want to upload a file to storage, have a Cloud Function take the file and send it to <a href=\"https://www.ibm.com/watson/services/visual-recognition\">Watson Visual Recognition</a> for classification, and finally put the results in a Cloudant database.</p><h2 id=\"function\">Function</h2><p>The <a href=\"https://cloud.ibm.com/docs/openwhisk?topic=cloud-functions-cloud_object_storage#cloud_object_storage\">documentation</a> for this feature walks through setting up all the necessary parts. The first step is to create the trigger. The documentation walks you through some additional \"binding\" steps, but I just went straight to the source. After all, it is not that often you need to adjust the actual trigger itself.</p><!--kg-card-begin: markdown--><pre><code>ibmcloud fn trigger create store.this --feed /whisk.system/cos-experimental/changes --param apikey _MY_API_KEY_ --param bucket u-betta-recognize --param endpoint s3.us-east.cloud-object-storage.appdomain.cloud\n</code></pre>\n<!--kg-card-end: markdown--><p>Next up is to create the action/function that will get invoked when the trigger runs. The trigger is invoke for a variety of reasons - you added a file is the one I am interested in, but it could also be for being deleted. When the function gets called, various details about the Cloud Object Storage are passed as parameters, such as the key (object name) of the resource that changed, and the bucket in which that object resides.</p><!--kg-card-begin: markdown--><pre><code>function main( params ) {\n  if( params.status !== 'added' ) {\n    return;\n  }\n  \n  // Here we go\n}\n</code></pre>\n<!--kg-card-end: markdown--><p>Cloud Functions has an Object Storage library that comes with it, but as I have written about before, I rather prefer the HTTP/REST interface. The first step in interacting with the REST interface is in getting an authorization token. Where parameters are not provided by the trigger, I prefer to define them using the web-based tooling, and label them in uppercase.</p><!--kg-card-begin: markdown--><pre><code>rp( {\n  url: 'https://iam.ng.bluemix.net/oidc/token',\n  method: 'POST',\n  form: {\n    apikey: params.COS_API_KEY,\n    response_type: 'cloud_iam',\n    grant_type: 'urn:ibm:params:oauth:grant-type:apikey'\n  },\n  json: true\n} );\n</code></pre>\n<!--kg-card-end: markdown--><p>With the token retrieved, I can now go for the file itself. There is a choice to make here. Theoretically, you should be able to get a public URL to a resource on Cloud Object Storage. This is one of the ways we can call Watson - with a URL. However, in this case, I am going to download the file itself to the running function - and keep it in memory.</p><!--kg-card-begin: markdown--><pre><code>rp( {\n  url: `https://${params.endpoint}/${params.bucket}/${params.key}`,\n  method: 'GET',\n  headers: {\n    Authorization: `${authorization.token_type} ${authorization.access_token}`,\n    'ibm-service-instance-id': params.COS_SERVICE_INSTANCE\n  },\n  encoding: null\n} );\n</code></pre>\n<!--kg-card-end: markdown--><p>From there I am going to send the file bytes to Watson for classification. If you are using JavaScript and request/request-promise as I am, then pay particular attention about how the file gets passed. The request documentation shows putting a Buffer, which is the result of downloading the file, directly onto the form. However, you have to use the method that specifies a file name, otherwise the file contents will not be passed.</p><!--kg-card-begin: markdown--><pre><code>rp( {\n  url: 'https://gateway.watsonplatform.net/visual-recognition/api/v3/classify',\n  method: 'POST',\n  auth: {\n    user: params.WATSON_USERNAME,\n    pass: params.WATSON_PASSWORD\n  },\n  formData: {\n    images_file: {\n      value: contents,\n      options: {\n        filename: params.key\n      }\n    }\n  },\n  qs: {\n    version: '2018-03-19'\n  },\n  json: true\n} );\n</code></pre>\n<!--kg-card-end: markdown--><p>Last but not least, take the results of the Visual Recognition classification, and put it into a Cloudant database. Of course any database could be used, but Cloudant lends itself to this type of scenario. Being REST-based itself, there are no additional dependencies. Cloudant views are also well suited to taking a bunch of specific, nested, data, like what is returned from Watson, and distill it down to something else.</p><!--kg-card-begin: markdown--><pre><code>classification.bucket = params.bucket;\nclassification.key = params.key;\nclassification.endpoint = params.endpoint;\n      \nrp( {\n  url: `https://${params.CLOUDANT_ACCOUNT}.cloudant.com/${params.CLOUDANT_DATABASE}`,\n  method: 'POST',\n  auth: {\n    user: params.CLOUDANT_USERNAME,\n    pass: params.CLOUDANT_PASSWORD\n  },\n  json: true,\n  body: classification\n} );\n</code></pre>\n<!--kg-card-end: markdown--><p>Before I put the classification results into the database, I also put some additional information about the Object Storage changes. Mostly, details I thought might be helpful in creating views down the road. \"What are the top classifications for a given bucket?\" type of questions. Creating a document in Cloudant returns with an ID, which you can return at the end of your function if you want.</p><!--kg-card-begin: markdown--><pre><code>ibmcloud fn action create recognize recognize.js\n</code></pre>\n<!--kg-card-end: markdown--><h2 id=\"rule\">Rule</h2><p>We create the trigger (store.this). We created the action to be called by the trigger (recognize). Now we just need to glue the two together using a rule. With the trigger and the rule defined, you can iterate on the function as often as you need, without have to touch either the trigger or the rule.</p><!--kg-card-begin: markdown--><pre><code>ibmcloud fn rule create store.recognize store.this recognize\n</code></pre>\n<!--kg-card-end: markdown--><p>If you change the bucket you want to watch, the storage location, or the frequency of how often the trigger looks for changes (default of once per minute), then you will need to update your trigger. However, the rule and the action can be left untouched.</p><h2 id=\"next-steps\">Next Steps</h2><p>In days of old, this type of chain of events in JavaScript would be callback hell. However, with request-promise being included as part of Cloud Functions, it becomes a chain of \"then\" statements, making the code easier to maintain. If you want to specify a modern version of Node.js for Cloud Functions to use, then you can even use async/await. I implemented both, and have put them in a <a href=\"https://gist.github.com/krhoyt/50709e6c7322df57f744ecc3bee475ce\">GitHub Gist</a>.</p><p>What is more interesting is the function calls made within this Cloud Function/action. Each of the various steps above could be Cloud Functions themselves, and then a sequence could be made that ties them all together. How granular is too granular? I view all the function calls made to be a part of a single body of work, and in that sense consider it a single action. Do you have an opinion on the matter?</p><p>And that brings us full circle to the as of yet maturing best practices. Exciting times ahead.</p><figure class=\"kg-card kg-embed-card kg-card-hascaption\"><iframe width=\"459\" height=\"344\" src=\"https://www.youtube.com/embed/d_YpimHwHeM?feature=oembed\" frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe><figcaption>See the code in action!</figcaption></figure>","comment_id":"5ca910e8c0479c00c0c80302","plaintext":"I have a love-hate relationship with serverless. My style of development\ndefinitely trends towards dropping a function in the cloud and calling it done.\nBut the tooling, integration, and best practices are still maturing. It is\nhowever, fun to watch the technology evolve. Recently Apache OpenWhisk\n[https://openwhisk.apache.org/] (IBM Cloud Functions\n[https://www.ibm.com/cloud/functions]) released a beta feature around triggers\nfor Cloud Object Storage [https://www.ibm.com/cloud/object-storage], and I\nfigured I would take it for a spin.\n\n> Some IBM Cloud Functions commands can get very lengthy - especially if you have\nseveral parameters you need to set. Shell scripts are your friend. I tend to\nhave a few scattered around every function for creating and updating the\nassociated actions, sequences, etc. Do not forget to \"chmod +x\ncreate-action.sh\". Here is an example of what using the CLI looks like from a\nshell script. This is a single command. I have seen scripts from other\ndevelopers that are respectable applications unto themselves.\nibmcloud fn trigger create store.this \\\n  --feed /whisk.system/cos-experimental/changes \\\n  --param apikey \"_YOUR_API_KEY_\" \\\n  --param bucket \"show-and-tell\" \\\n  --param endpoint \"s3.us-east.cloud-object-storage.appdomain.cloud\"\n\n\nTrigger\nTriggers allow Cloud Functions to be invoked when something happens on an\nexternal system. Cloud Functions provides built-in triggers for Cloudant (\nCouchDB [http://couchdb.apache.org/]), MessageHub\n[https://www.ibm.com/cloud/message-hub] (Apache Kafka\n[https://kafka.apache.org/]), GitHub, and more. Did a document just get added to\nyour database? Invoke an action. Did a message just get placed in a queue?\nInvoke an action. New code committed? Invoke an action.\n\nAnd now, did a file get uploaded to storage? Invoke an action!\n\nStorage\nMy recent [__GHOST_URL__/2019/01/23/upload-files-to/] blog\n[__GHOST_URL__/2019/01/30/serverless-download-from-object-storage/] posts\n[__GHOST_URL__/2019/02/06/serverless-storage-redux/] have been around Cloud\nObject Storage and Cloud Functions, so this particular trigger definitely piqued\nmy interest. As soon as I heard about the feature, a use-case came to mind. I\nwant to upload a file to storage, have a Cloud Function take the file and send\nit to Watson Visual Recognition\n[https://www.ibm.com/watson/services/visual-recognition] for classification, and\nfinally put the results in a Cloudant database.\n\nFunction\nThe documentation\n[https://cloud.ibm.com/docs/openwhisk?topic=cloud-functions-cloud_object_storage#cloud_object_storage] \nfor this feature walks through setting up all the necessary parts. The first\nstep is to create the trigger. The documentation walks you through some\nadditional \"binding\" steps, but I just went straight to the source. After all,\nit is not that often you need to adjust the actual trigger itself.\n\nibmcloud fn trigger create store.this --feed /whisk.system/cos-experimental/changes --param apikey _MY_API_KEY_ --param bucket u-betta-recognize --param endpoint s3.us-east.cloud-object-storage.appdomain.cloud\n\n\nNext up is to create the action/function that will get invoked when the trigger\nruns. The trigger is invoke for a variety of reasons - you added a file is the\none I am interested in, but it could also be for being deleted. When the\nfunction gets called, various details about the Cloud Object Storage are passed\nas parameters, such as the key (object name) of the resource that changed, and\nthe bucket in which that object resides.\n\nfunction main( params ) {\n  if( params.status !== 'added' ) {\n    return;\n  }\n  \n  // Here we go\n}\n\n\nCloud Functions has an Object Storage library that comes with it, but as I have\nwritten about before, I rather prefer the HTTP/REST interface. The first step in\ninteracting with the REST interface is in getting an authorization token. Where\nparameters are not provided by the trigger, I prefer to define them using the\nweb-based tooling, and label them in uppercase.\n\nrp( {\n  url: 'https://iam.ng.bluemix.net/oidc/token',\n  method: 'POST',\n  form: {\n    apikey: params.COS_API_KEY,\n    response_type: 'cloud_iam',\n    grant_type: 'urn:ibm:params:oauth:grant-type:apikey'\n  },\n  json: true\n} );\n\n\nWith the token retrieved, I can now go for the file itself. There is a choice to\nmake here. Theoretically, you should be able to get a public URL to a resource\non Cloud Object Storage. This is one of the ways we can call Watson - with a\nURL. However, in this case, I am going to download the file itself to the\nrunning function - and keep it in memory.\n\nrp( {\n  url: `https://${params.endpoint}/${params.bucket}/${params.key}`,\n  method: 'GET',\n  headers: {\n    Authorization: `${authorization.token_type} ${authorization.access_token}`,\n    'ibm-service-instance-id': params.COS_SERVICE_INSTANCE\n  },\n  encoding: null\n} );\n\n\nFrom there I am going to send the file bytes to Watson for classification. If\nyou are using JavaScript and request/request-promise as I am, then pay\nparticular attention about how the file gets passed. The request documentation\nshows putting a Buffer, which is the result of downloading the file, directly\nonto the form. However, you have to use the method that specifies a file name,\notherwise the file contents will not be passed.\n\nrp( {\n  url: 'https://gateway.watsonplatform.net/visual-recognition/api/v3/classify',\n  method: 'POST',\n  auth: {\n    user: params.WATSON_USERNAME,\n    pass: params.WATSON_PASSWORD\n  },\n  formData: {\n    images_file: {\n      value: contents,\n      options: {\n        filename: params.key\n      }\n    }\n  },\n  qs: {\n    version: '2018-03-19'\n  },\n  json: true\n} );\n\n\nLast but not least, take the results of the Visual Recognition classification,\nand put it into a Cloudant database. Of course any database could be used, but\nCloudant lends itself to this type of scenario. Being REST-based itself, there\nare no additional dependencies. Cloudant views are also well suited to taking a\nbunch of specific, nested, data, like what is returned from Watson, and distill\nit down to something else.\n\nclassification.bucket = params.bucket;\nclassification.key = params.key;\nclassification.endpoint = params.endpoint;\n      \nrp( {\n  url: `https://${params.CLOUDANT_ACCOUNT}.cloudant.com/${params.CLOUDANT_DATABASE}`,\n  method: 'POST',\n  auth: {\n    user: params.CLOUDANT_USERNAME,\n    pass: params.CLOUDANT_PASSWORD\n  },\n  json: true,\n  body: classification\n} );\n\n\nBefore I put the classification results into the database, I also put some\nadditional information about the Object Storage changes. Mostly, details I\nthought might be helpful in creating views down the road. \"What are the top\nclassifications for a given bucket?\" type of questions. Creating a document in\nCloudant returns with an ID, which you can return at the end of your function if\nyou want.\n\nibmcloud fn action create recognize recognize.js\n\n\nRule\nWe create the trigger (store.this). We created the action to be called by the\ntrigger (recognize). Now we just need to glue the two together using a rule.\nWith the trigger and the rule defined, you can iterate on the function as often\nas you need, without have to touch either the trigger or the rule.\n\nibmcloud fn rule create store.recognize store.this recognize\n\n\nIf you change the bucket you want to watch, the storage location, or the\nfrequency of how often the trigger looks for changes (default of once per\nminute), then you will need to update your trigger. However, the rule and the\naction can be left untouched.\n\nNext Steps\nIn days of old, this type of chain of events in JavaScript would be callback\nhell. However, with request-promise being included as part of Cloud Functions,\nit becomes a chain of \"then\" statements, making the code easier to maintain. If\nyou want to specify a modern version of Node.js for Cloud Functions to use, then\nyou can even use async/await. I implemented both, and have put them in a GitHub\nGist [https://gist.github.com/krhoyt/50709e6c7322df57f744ecc3bee475ce].\n\nWhat is more interesting is the function calls made within this Cloud\nFunction/action. Each of the various steps above could be Cloud Functions\nthemselves, and then a sequence could be made that ties them all together. How\ngranular is too granular? I view all the function calls made to be a part of a\nsingle body of work, and in that sense consider it a single action. Do you have\nan opinion on the matter?\n\nAnd that brings us full circle to the as of yet maturing best practices.\nExciting times ahead.\n\nSee the code in action!","feature_image":"__GHOST_URL__/content/images/2019/04/whisk.eggs.jpg","featured":0,"status":"published","locale":null,"visibility":"public","author_id":"1","created_at":"2019-04-06T20:49:44.000Z","updated_at":"2019-04-18T15:53:36.000Z","published_at":"2019-04-08T18:22:41.000Z","custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"type":"post","email_recipient_filter":"none"},{"id":"60d37d471beb45003bbdb347","uuid":"ce0fd439-d535-4c18-a456-19d636ab1ed9","title":"Building a Calendar with Stencil","slug":"create-a-calendar-with-stencil","mobiledoc":"{\"version\":\"0.3.1\",\"atoms\":[[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}],[\"soft-return\",\"\",{}]],\"cards\":[[\"image\",{\"src\":\"__GHOST_URL__/content/images/2021/06/calendar-edge.png\",\"width\":2500,\"height\":2500,\"caption\":\"\",\"alt\":\"You checked the edge cases, right?\"}],[\"markdown\",{\"markdown\":\"``` ts\\nexport default class Day {\\n  date: number = null;\\n  month: number = null;\\n  year: number = null;\\n  selected: boolean = false;\\n  today: boolean = false;\\n}\\n```\"}]],\"markups\":[],\"sections\":[[1,\"p\",[[0,[],0,\"Take a look at the month view of a calendar and you will see several rows of numbers. The numbers themselves, increasing in value one after the other, are arranged in columns. HTML and CSS provide us with a number of tools to display content in rows and columns. Making a calendar component should be easy, right? Right?\"]]],[10,0],[1,\"p\",[[0,[],0,\"When we look at the first calendar day in a month, it could fall on any day of the week. This in turn can push the last day of the month to also fall on any day of the week. That variation can mean that there are six weeks (six rows) in some months, but only five weeks (rows) in others. Then of course there are the number of days in the month, and oddities such as leap years.\"]]],[1,\"p\",[[0,[],0,\"When we turn to Stencil (and Web Components in general), all of these variations beg the question \\\"What goes in my template if I do not know what I will need to display?\\\" The answer is to figure out what you will need to display before you display it. We can leverage the Stencil component lifecycle to override `componentWillRender()` and perform the required calculations.\"]]],[1,\"h2\",[[0,[],0,\"It is About Time!\"]]],[1,\"p\",[[0,[],0,\"If you think about any single given day on the calendar, you are really taking in several pieces of information at once. First is the integer representing the day of the month - and it is usually plastered in a giant font somewhere in each grid square. The second is that by the time you land on a day of the month, you likely already know the month and year. Finally, as your eyes rest upon a grid square, you are mentally selecting it, and know whether that selection represents today or not.\"]]],[1,\"blockquote\",[[0,[],0,\"These implied and inferred pieces of information fall in no particular order, and any correlation is completely made up by me for the purposes of the flow of this blog post. 🤪\"]]],[1,\"p\",[[0,[],0,\"These are all important pieces of data we will need when rendering out each day in the calendar, so I like to have a class that represents them which I call `Day`. You might think \\\"Why not just use the JavaScript `Date` class? The reason for this decision has to do with the `selected` and `today` properties that are `boolean` values used in helping render the appropriate styles on the calendar grid.\"]]],[10,1],[1,\"h2\",[[0,[],0,\"What Day Is It?\"]]],[1,\"p\",[[0,[],0,\"From a component perspective, there are a few properties we will need to consider. The first is the desired date to display in the calendar versus the date being displayed in the calendar. The desired date might also be considered the \\\"value\\\" of the calendar, and is exposed as a property (`@Prop()`). The date being displayed in the calendar is a matter of how the user has interacted with the component to change its state (`@State()`). We will also need an `Array` of `Day` as a component-internal property to hold the days associated with the rendering of the displayed date.\"],[1,[],0,0]]],[1,\"p\",[[0,[],0,\"``` ts\"]]],[1,\"p\",[[0,[],0,\"@Prop( {mutable: true} ) value: Date = new Date();\"]]],[1,\"p\",[[0,[],0,\"@State() displayedDate: Date = new Date();\"]]],[1,\"p\",[[0,[],0,\"private days: Array<Day>;\"]]],[1,\"p\",[[0,[],0,\"```\"],[1,[],0,1]]],[1,\"p\",[[0,[],0,\"For this calendar, when the user interacts with a day, that date will be set as the new `value`. Since `value` will be modified from inside the component, it will need to be marked as `mutable`. You may also want to emit an event that the `value` (selected date) has changed. Another consideration might be emitting an event for changes in the month being displayed.\"],[1,[],0,2]]],[1,\"p\",[[0,[],0,\"### Back to the Beginning\"],[1,[],0,3]]],[1,\"p\",[[0,[],0,\"As we start implementing `componentWillRender()`, our first major task is to get back to the first day in the displayed month. The JavaScript `Date` constructor is perfectly suited for this by setting the third parameter, the day of the month, to one (1). Being at the first day of the month, does not mean you are at the first (upper-left) grid square, so we also need to go back in the calendar until we are at the first day of the week.\"],[1,[],0,4]]],[1,\"p\",[[0,[],0,\"``` ts\"]]],[1,\"p\",[[0,[],0,\"componentWillRender() {\"]]],[1,\"p\",[[0,[],0,\"const today: Date = new Date();\"]]],[1,\"p\",[[0,[],0,\"// Calendar used in iteration\"]]],[1,\"p\",[[0,[],0,\"const calendar: Date = new Date(\"]]],[1,\"p\",[[0,[],0,\"this.displayedDate.getFullYear(),\"]]],[1,\"p\",[[0,[],0,\"this.displayedDate.getMonth(),\"]]],[1,\"p\",[[0,[],0,\"1\"]]],[1,\"p\",[[0,[],0,\");\"]]],[1,\"p\",[[0,[],0,\"// First day of month may not be first day of week\"]]],[1,\"p\",[[0,[],0,\"// Roll back until first day of week\"]]],[1,\"p\",[[0,[],0,\"calendar.setDate( calendar.getDate() - calendar.getDay() );\"]]],[1,\"p\",[[0,[],0,\"// Clear days to be rendered\"]]],[1,\"p\",[[0,[],0,\"this.days = []; \"],[1,[],0,5]]],[1,\"p\",[[0,[],0,\"// ... more ...\"]]],[1,\"p\",[[0,[],0,\"}\"]]],[1,\"p\",[[0,[],0,\"```\"],[1,[],0,6]]],[1,\"p\",[[0,[],0,\"Perhaps not surprisingly, this now sets us up to iterate through the days of the month. Perhaps surprisingly, that is **not** what I like to do. \"],[1,[],0,7]]],[1,\"p\",[[0,[],0,\"### One Day at a Time\"],[1,[],0,8]]],[1,\"p\",[[0,[],0,\"My preferred approach for this iteration is to count to 42 (6 weeks possible x 7 days per week). If we make it through all 42 iterations, great. If we do not make it through all 42 iterations, we can break out of the loop at any time, and move on to the render. Additionally, it turns out that CSS Grid is perfectly suited for ending the iteration early, and will fill in the \\\"gaps\\\" for us.\"],[1,[],0,9]]],[1,\"p\",[[0,[],0,\"> Though I have seen various takes on how to manage the iteration through the calendar, this \\\"count to 42\\\" approach leads to the most readable version that I have found. YMMV.\"],[1,[],0,10]]],[1,\"p\",[[0,[],0,\"``` ts\"]]],[1,\"p\",[[0,[],0,\"componentWillRender() {\"]]],[1,\"p\",[[0,[],0,\"const today: Date = new Date();\"]]],[1,\"p\",[[0,[],0,\"// Calendar used in iteration\"]]],[1,\"p\",[[0,[],0,\"const calendar: Date = new Date(\"]]],[1,\"p\",[[0,[],0,\"this.displayedDate.getFullYear(),\"]]],[1,\"p\",[[0,[],0,\"this.displayedDate.getMonth(),\"]]],[1,\"p\",[[0,[],0,\"1\"]]],[1,\"p\",[[0,[],0,\");\"]]],[1,\"p\",[[0,[],0,\"// First day of month may not be first day of week\"]]],[1,\"p\",[[0,[],0,\"// Roll back until first day of week\"]]],[1,\"p\",[[0,[],0,\"calendar.setDate( calendar.getDate() - calendar.getDay() );\"]]],[1,\"p\",[[0,[],0,\"// Clear days to be rendered\"]]],[1,\"p\",[[0,[],0,\"this.days = [];\"]]],[1,\"p\",[[0,[],0,\"for( let d: number = 0; d < 42; d++ ) {\"]]],[1,\"p\",[[0,[],0,\"// Day to be rendered\"]]],[1,\"p\",[[0,[],0,\"// Seed with current date in iteration\"]]],[1,\"p\",[[0,[],0,\"const day: Day = new Day();\"]]],[1,\"p\",[[0,[],0,\"day.year = calendar.getFullYear();\"]]],[1,\"p\",[[0,[],0,\"day.month = calendar.getMonth();\"],[1,[],0,11]]],[1,\"p\",[[0,[],0,\"// Populate day in month\"]]],[1,\"p\",[[0,[],0,\"// Undefined date properties are not rendered\"]]],[1,\"p\",[[0,[],0,\"if(\"]]],[1,\"p\",[[0,[],0,\"calendar.getFullYear() === this.displayedDate.getFullYear() &&\"]]],[1,\"p\",[[0,[],0,\"calendar.getMonth() === this.displayedDate.getMonth()\"]]],[1,\"p\",[[0,[],0,\") day.date = calendar.getDate();\"]]],[1,\"p\",[[0,[],0,\"// Check for today\"]]],[1,\"p\",[[0,[],0,\"if(\"]]],[1,\"p\",[[0,[],0,\"calendar.getFullYear() === today.getFullYear() &&\"]]],[1,\"p\",[[0,[],0,\"calendar.getMonth() === today.getMonth() &&\"]]],[1,\"p\",[[0,[],0,\"calendar.getDate() === today.getDate()\"]]],[1,\"p\",[[0,[],0,\") day.today = true;\"]]],[1,\"p\",[[0,[],0,\"// Check for selection\"]]],[1,\"p\",[[0,[],0,\"if(\"]]],[1,\"p\",[[0,[],0,\"calendar.getFullYear() === this.value.getFullYear() &&\"]]],[1,\"p\",[[0,[],0,\"calendar.getMonth() === this.value.getMonth() &&\"]]],[1,\"p\",[[0,[],0,\"calendar.getDate() === this.value.getDate() &&\"]]],[1,\"p\",[[0,[],0,\"calendar.getMonth() === this.value.getMonth()\"]]],[1,\"p\",[[0,[],0,\") day.selected = true;\"]]],[1,\"p\",[[0,[],0,\"// Add to days to be rendered\"]]],[1,\"p\",[[0,[],0,\"this.days.push( day );\"]]],[1,\"p\",[[0,[],0,\"// Keep rolling\"]]],[1,\"p\",[[0,[],0,\"calendar.setDate( calendar.getDate() + 1 );\"]]],[1,\"p\",[[0,[],0,\"// Do not render the last week\"]]],[1,\"p\",[[0,[],0,\"// Depending on calendar layout\"]]],[1,\"p\",[[0,[],0,\"// Some months require five weeks\"]]],[1,\"p\",[[0,[],0,\"// Others six weeks (see May 2021)\"]]],[1,\"p\",[[0,[],0,\"if(\"]]],[1,\"p\",[[0,[],0,\"calendar.getDay() === 0 &&\"]]],[1,\"p\",[[0,[],0,\"calendar.getMonth() !== this.displayedDate.getMonth()\"]]],[1,\"p\",[[0,[],0,\") break;\"]]],[1,\"p\",[[0,[],0,\"}\"]]],[1,\"p\",[[0,[],0,\"}\"]]],[1,\"p\",[[0,[],0,\"```\"],[1,[],0,12]]],[1,\"p\",[[0,[],0,\"Within each iteration we seed the `Day` instance with the month and year, but not necessarily the day of the month. We can use this absence of data in the render. We also check to see if the date in the current iteration is today, or if it matches the `value` property. These flags will help us style the calendar appropriately.  If we are past the end of the month being displayed, we are done, and can return to the component lifecycle to actually render the calendar.\"],[1,[],0,13]]],[1,\"p\",[[0,[],0,\"### Once More, With Feeling\"],[1,[],0,14]]],[1,\"p\",[[0,[],0,\"When it comes to the actual rendering of the calendar, I like to separate it semantically from the rest of the calendar. This helps keep the CSS more concise. \"],[1,[],0,15]]],[1,\"p\",[[0,[],0,\"The most interesting part of the CSS is the use of CSS Grid. The `grid-template-rows` property is specified as `repeat( auto-fill, 1fr )` which is what allows us to get away without having to complete the week at the end of the month, or outright leave a week off the end of the month if there are only five (5) weeks.\"],[1,[],0,16]]],[1,\"p\",[[0,[],0,\"``` css\"]]],[1,\"p\",[[0,[],0,\"article {\"]]],[1,\"p\",[[0,[],0,\"box-sizing: border-box;\"]]],[1,\"p\",[[0,[],0,\"display: grid;\"]]],[1,\"p\",[[0,[],0,\"grid-template-columns: 1fr 1fr 1fr 1fr 1fr 1fr 1fr;\"]]],[1,\"p\",[[0,[],0,\"grid-template-rows: repeat( auto-fill, 1fr );\"]]],[1,\"p\",[[0,[],0,\"gap: 0px 0px;\"]]],[1,\"p\",[[0,[],0,\"min-width: 100vw;\"]]],[1,\"p\",[[0,[],0,\"padding: 0 4px 4px 4px;\"]]],[1,\"p\",[[0,[],0,\"}\"],[1,[],0,17]]],[1,\"p\",[[0,[],0,\"article button {\"]]],[1,\"p\",[[0,[],0,\"appearance: none;\"]]],[1,\"p\",[[0,[],0,\"-webkit-appearance: none;\"]]],[1,\"p\",[[0,[],0,\"background: none;\"]]],[1,\"p\",[[0,[],0,\"border: none;\"]]],[1,\"p\",[[0,[],0,\"box-sizing: border-box;\"]]],[1,\"p\",[[0,[],0,\"cursor: pointer;\"]]],[1,\"p\",[[0,[],0,\"font-family: -apple-system, sans-serif;\"]]],[1,\"p\",[[0,[],0,\"font-size: 20px;\"]]],[1,\"p\",[[0,[],0,\"margin: 3px;\"]]],[1,\"p\",[[0,[],0,\"min-height: calc( ( 100vw / 7 ) - 6px );\"]]],[1,\"p\",[[0,[],0,\"outline: none;\"]]],[1,\"p\",[[0,[],0,\"padding: 0;\"]]],[1,\"p\",[[0,[],0,\"-webkit-tap-highlight-color: rgba( 0, 0, 0, 0 );\"]]],[1,\"p\",[[0,[],0,\"text-align: center;\"]]],[1,\"p\",[[0,[],0,\"}\"],[1,[],0,18]]],[1,\"p\",[[0,[],0,\"article button[disabled] {\"]]],[1,\"p\",[[0,[],0,\"cursor: default;\"]]],[1,\"p\",[[0,[],0,\"}\"],[1,[],0,19]]],[1,\"p\",[[0,[],0,\"article button.selected {\"]]],[1,\"p\",[[0,[],0,\"background-color: rgba( 255, 0, 0, 0.10 );\"]]],[1,\"p\",[[0,[],0,\"border-radius: 100%;\"]]],[1,\"p\",[[0,[],0,\"color: #ff0000;\"]]],[1,\"p\",[[0,[],0,\"font-weight: 600;\"]]],[1,\"p\",[[0,[],0,\"}\"],[1,[],0,20]]],[1,\"p\",[[0,[],0,\"article button.today {\"]]],[1,\"p\",[[0,[],0,\"color: #ff0000;\"]]],[1,\"p\",[[0,[],0,\"}\"],[1,[],0,21]]],[1,\"p\",[[0,[],0,\"article button.selected.today {\"]]],[1,\"p\",[[0,[],0,\"background-color: #ff0000;\"]]],[1,\"p\",[[0,[],0,\"color: #ffffff;\"]]],[1,\"p\",[[0,[],0,\"}\"]]],[1,\"p\",[[0,[],0,\"```\"],[1,[],0,22]]],[1,\"p\",[[0,[],0,\"As you can probably tell from the CSS, each day in the month will be rendered as a `button` element. On that `button` element, we can use `data` attributes to store the day, month, and year that each represents. In the click handler, a reference to the `button` can be obtained using the `target` property of the `MouseEvent`. These values can then be parsed, and a `Date` object then created to represent the date that was selected.  This keeps us from having to store DOM references to each of the days being rendered. A `button` marked `disabled` will not fire the click handler.\"],[1,[],0,23]]],[1,\"p\",[[0,[],0,\"``` jsx\"]]],[1,\"p\",[[0,[],0,\"<article>\"]]],[1,\"p\",[[0,[],0,\"{this.days.map( ( day: Day ) =>\"]]],[1,\"p\",[[0,[],0,\"<button\"]]],[1,\"p\",[[0,[],0,\"class={{\"]]],[1,\"p\",[[0,[],0,\"selected: day.selected,\"]]],[1,\"p\",[[0,[],0,\"today: day.today\"]]],[1,\"p\",[[0,[],0,\"}}\"]]],[1,\"p\",[[0,[],0,\"data-date={day.date}\"]]],[1,\"p\",[[0,[],0,\"data-month={day.month}\"]]],[1,\"p\",[[0,[],0,\"data-year={day.year}\"]]],[1,\"p\",[[0,[],0,\"disabled={day.date === null ? true : false}\"]]],[1,\"p\",[[0,[],0,\"onClick={( evt: MouseEvent ) => this.doSelect( evt )}>\"]]],[1,\"p\",[[0,[],0,\"{day.date}\"]]],[1,\"p\",[[0,[],0,\"</button>\"]]],[1,\"p\",[[0,[],0,\") }\"]]],[1,\"p\",[[0,[],0,\"</article>\"]]],[1,\"p\",[[0,[],0,\"```\"],[1,[],0,24]]],[1,\"p\",[[0,[],0,\"This whole technique of calculating what needs to be displayed before the actual rendering process is what I call \\\"pre-rendering\\\". The approach is valuable not only in calendars, but also in any situation where there can be a variable number of elements to be rendered. Dynamic SVG content is another place where this technique comes in handy.\"],[1,[],0,25]]],[1,\"p\",[[0,[],0,\"> It should be noted that modern calendar designs do not show the dates for the days outside of the month being rendered. This is true for both Material Design and Apple Human Interface Guidelines. You can alter the pre-rendering to meet your requirements should they differ.\"],[1,[],0,26]]],[1,\"p\",[[0,[],0,\"Could you forego the pre-rendering, and do all the calculations in the render proper? With enough braces, brackets, and parenthesis, you probably could. This approach however gives us a clean separation between the data to be rendered and the render itself leading to code that is easy to read and maintain.\"],[1,[],0,27]]],[1,\"p\",[[0,[],0,\"### Next Steps\"],[1,[],0,28]]],[1,\"p\",[[0,[],0,\"There are a number of nuances to calendar rendering which have not been addressed in this article. For example, Sunday is not always the first day of the week depending on your locale. More could also be done to address accessibility. And of course changing the styles for different mobile operating systems would take considerable effort. Does this sound like a lot more work? It is! That is where Ionic Framework can help. The announced Ionic Framework v6 includes modern calendars built to conform to the latest design guidelines, to include accessibility.\"],[1,[],0,29]]],[1,\"p\",[[0,[],0,\"Check out the live [demo](http://temp.kevinhoyt.com/ionic/calendar/) and get the complete [code](https://github.com/krhoyt/Ionic/tree/master/calendar).\"],[1,[],0,30]]]],\"ghostVersion\":\"4.0\"}","html":"<p>Take a look at the month view of a calendar and you will see several rows of numbers. The numbers themselves, increasing in value one after the other, are arranged in columns. HTML and CSS provide us with a number of tools to display content in rows and columns. Making a calendar component should be easy, right? Right?</p><figure class=\"kg-card kg-image-card\"><img src=\"__GHOST_URL__/content/images/2021/06/calendar-edge.png\" class=\"kg-image\" alt=\"You checked the edge cases, right?\" loading=\"lazy\" width=\"2000\" height=\"2000\" srcset=\"__GHOST_URL__/content/images/size/w600/2021/06/calendar-edge.png 600w, __GHOST_URL__/content/images/size/w1000/2021/06/calendar-edge.png 1000w, __GHOST_URL__/content/images/size/w1600/2021/06/calendar-edge.png 1600w, __GHOST_URL__/content/images/size/w2400/2021/06/calendar-edge.png 2400w\" sizes=\"(min-width: 720px) 720px\"></figure><p>When we look at the first calendar day in a month, it could fall on any day of the week. This in turn can push the last day of the month to also fall on any day of the week. That variation can mean that there are six weeks (six rows) in some months, but only five weeks (rows) in others. Then of course there are the number of days in the month, and oddities such as leap years.</p><p>When we turn to Stencil (and Web Components in general), all of these variations beg the question \"What goes in my template if I do not know what I will need to display?\" The answer is to figure out what you will need to display before you display it. We can leverage the Stencil component lifecycle to override `componentWillRender()` and perform the required calculations.</p><h2 id=\"it-is-about-time\">It is About Time!</h2><p>If you think about any single given day on the calendar, you are really taking in several pieces of information at once. First is the integer representing the day of the month - and it is usually plastered in a giant font somewhere in each grid square. The second is that by the time you land on a day of the month, you likely already know the month and year. Finally, as your eyes rest upon a grid square, you are mentally selecting it, and know whether that selection represents today or not.</p><blockquote>These implied and inferred pieces of information fall in no particular order, and any correlation is completely made up by me for the purposes of the flow of this blog post. 🤪</blockquote><p>These are all important pieces of data we will need when rendering out each day in the calendar, so I like to have a class that represents them which I call `Day`. You might think \"Why not just use the JavaScript `Date` class? The reason for this decision has to do with the `selected` and `today` properties that are `boolean` values used in helping render the appropriate styles on the calendar grid.</p><!--kg-card-begin: markdown--><pre><code class=\"language-ts\">export default class Day {\n  date: number = null;\n  month: number = null;\n  year: number = null;\n  selected: boolean = false;\n  today: boolean = false;\n}\n</code></pre>\n<!--kg-card-end: markdown--><h2 id=\"what-day-is-it\">What Day Is It?</h2><p>From a component perspective, there are a few properties we will need to consider. The first is the desired date to display in the calendar versus the date being displayed in the calendar. The desired date might also be considered the \"value\" of the calendar, and is exposed as a property (`@Prop()`). The date being displayed in the calendar is a matter of how the user has interacted with the component to change its state (`@State()`). We will also need an `Array` of `Day` as a component-internal property to hold the days associated with the rendering of the displayed date.<br></p><p>``` ts</p><p>@Prop( {mutable: true} ) value: Date = new Date();</p><p>@State() displayedDate: Date = new Date();</p><p>private days: Array&lt;Day&gt;;</p><p>```<br></p><p>For this calendar, when the user interacts with a day, that date will be set as the new `value`. Since `value` will be modified from inside the component, it will need to be marked as `mutable`. You may also want to emit an event that the `value` (selected date) has changed. Another consideration might be emitting an event for changes in the month being displayed.<br></p><p>### Back to the Beginning<br></p><p>As we start implementing `componentWillRender()`, our first major task is to get back to the first day in the displayed month. The JavaScript `Date` constructor is perfectly suited for this by setting the third parameter, the day of the month, to one (1). Being at the first day of the month, does not mean you are at the first (upper-left) grid square, so we also need to go back in the calendar until we are at the first day of the week.<br></p><p>``` ts</p><p>componentWillRender() {</p><p>const today: Date = new Date();</p><p>// Calendar used in iteration</p><p>const calendar: Date = new Date(</p><p>this.displayedDate.getFullYear(),</p><p>this.displayedDate.getMonth(),</p><p>1</p><p>);</p><p>// First day of month may not be first day of week</p><p>// Roll back until first day of week</p><p>calendar.setDate( calendar.getDate() - calendar.getDay() );</p><p>// Clear days to be rendered</p><p>this.days = []; <br></p><p>// ... more ...</p><p>}</p><p>```<br></p><p>Perhaps not surprisingly, this now sets us up to iterate through the days of the month. Perhaps surprisingly, that is **not** what I like to do. <br></p><p>### One Day at a Time<br></p><p>My preferred approach for this iteration is to count to 42 (6 weeks possible x 7 days per week). If we make it through all 42 iterations, great. If we do not make it through all 42 iterations, we can break out of the loop at any time, and move on to the render. Additionally, it turns out that CSS Grid is perfectly suited for ending the iteration early, and will fill in the \"gaps\" for us.<br></p><p>&gt; Though I have seen various takes on how to manage the iteration through the calendar, this \"count to 42\" approach leads to the most readable version that I have found. YMMV.<br></p><p>``` ts</p><p>componentWillRender() {</p><p>const today: Date = new Date();</p><p>// Calendar used in iteration</p><p>const calendar: Date = new Date(</p><p>this.displayedDate.getFullYear(),</p><p>this.displayedDate.getMonth(),</p><p>1</p><p>);</p><p>// First day of month may not be first day of week</p><p>// Roll back until first day of week</p><p>calendar.setDate( calendar.getDate() - calendar.getDay() );</p><p>// Clear days to be rendered</p><p>this.days = [];</p><p>for( let d: number = 0; d &lt; 42; d++ ) {</p><p>// Day to be rendered</p><p>// Seed with current date in iteration</p><p>const day: Day = new Day();</p><p>day.year = calendar.getFullYear();</p><p>day.month = calendar.getMonth();<br></p><p>// Populate day in month</p><p>// Undefined date properties are not rendered</p><p>if(</p><p>calendar.getFullYear() === this.displayedDate.getFullYear() &amp;&amp;</p><p>calendar.getMonth() === this.displayedDate.getMonth()</p><p>) day.date = calendar.getDate();</p><p>// Check for today</p><p>if(</p><p>calendar.getFullYear() === today.getFullYear() &amp;&amp;</p><p>calendar.getMonth() === today.getMonth() &amp;&amp;</p><p>calendar.getDate() === today.getDate()</p><p>) day.today = true;</p><p>// Check for selection</p><p>if(</p><p>calendar.getFullYear() === this.value.getFullYear() &amp;&amp;</p><p>calendar.getMonth() === this.value.getMonth() &amp;&amp;</p><p>calendar.getDate() === this.value.getDate() &amp;&amp;</p><p>calendar.getMonth() === this.value.getMonth()</p><p>) day.selected = true;</p><p>// Add to days to be rendered</p><p>this.days.push( day );</p><p>// Keep rolling</p><p>calendar.setDate( calendar.getDate() + 1 );</p><p>// Do not render the last week</p><p>// Depending on calendar layout</p><p>// Some months require five weeks</p><p>// Others six weeks (see May 2021)</p><p>if(</p><p>calendar.getDay() === 0 &amp;&amp;</p><p>calendar.getMonth() !== this.displayedDate.getMonth()</p><p>) break;</p><p>}</p><p>}</p><p>```<br></p><p>Within each iteration we seed the `Day` instance with the month and year, but not necessarily the day of the month. We can use this absence of data in the render. We also check to see if the date in the current iteration is today, or if it matches the `value` property. These flags will help us style the calendar appropriately.  If we are past the end of the month being displayed, we are done, and can return to the component lifecycle to actually render the calendar.<br></p><p>### Once More, With Feeling<br></p><p>When it comes to the actual rendering of the calendar, I like to separate it semantically from the rest of the calendar. This helps keep the CSS more concise. <br></p><p>The most interesting part of the CSS is the use of CSS Grid. The `grid-template-rows` property is specified as `repeat( auto-fill, 1fr )` which is what allows us to get away without having to complete the week at the end of the month, or outright leave a week off the end of the month if there are only five (5) weeks.<br></p><p>``` css</p><p>article {</p><p>box-sizing: border-box;</p><p>display: grid;</p><p>grid-template-columns: 1fr 1fr 1fr 1fr 1fr 1fr 1fr;</p><p>grid-template-rows: repeat( auto-fill, 1fr );</p><p>gap: 0px 0px;</p><p>min-width: 100vw;</p><p>padding: 0 4px 4px 4px;</p><p>}<br></p><p>article button {</p><p>appearance: none;</p><p>-webkit-appearance: none;</p><p>background: none;</p><p>border: none;</p><p>box-sizing: border-box;</p><p>cursor: pointer;</p><p>font-family: -apple-system, sans-serif;</p><p>font-size: 20px;</p><p>margin: 3px;</p><p>min-height: calc( ( 100vw / 7 ) - 6px );</p><p>outline: none;</p><p>padding: 0;</p><p>-webkit-tap-highlight-color: rgba( 0, 0, 0, 0 );</p><p>text-align: center;</p><p>}<br></p><p>article button[disabled] {</p><p>cursor: default;</p><p>}<br></p><p>article button.selected {</p><p>background-color: rgba( 255, 0, 0, 0.10 );</p><p>border-radius: 100%;</p><p>color: #ff0000;</p><p>font-weight: 600;</p><p>}<br></p><p>article button.today {</p><p>color: #ff0000;</p><p>}<br></p><p>article button.selected.today {</p><p>background-color: #ff0000;</p><p>color: #ffffff;</p><p>}</p><p>```<br></p><p>As you can probably tell from the CSS, each day in the month will be rendered as a `button` element. On that `button` element, we can use `data` attributes to store the day, month, and year that each represents. In the click handler, a reference to the `button` can be obtained using the `target` property of the `MouseEvent`. These values can then be parsed, and a `Date` object then created to represent the date that was selected.  This keeps us from having to store DOM references to each of the days being rendered. A `button` marked `disabled` will not fire the click handler.<br></p><p>``` jsx</p><p>&lt;article&gt;</p><p>{this.days.map( ( day: Day ) =&gt;</p><p>&lt;button</p><p>class={{</p><p>selected: day.selected,</p><p>today: day.today</p><p>}}</p><p>data-date={day.date}</p><p>data-month={day.month}</p><p>data-year={day.year}</p><p>disabled={day.date === null ? true : false}</p><p>onClick={( evt: MouseEvent ) =&gt; this.doSelect( evt )}&gt;</p><p>{day.date}</p><p>&lt;/button&gt;</p><p>) }</p><p>&lt;/article&gt;</p><p>```<br></p><p>This whole technique of calculating what needs to be displayed before the actual rendering process is what I call \"pre-rendering\". The approach is valuable not only in calendars, but also in any situation where there can be a variable number of elements to be rendered. Dynamic SVG content is another place where this technique comes in handy.<br></p><p>&gt; It should be noted that modern calendar designs do not show the dates for the days outside of the month being rendered. This is true for both Material Design and Apple Human Interface Guidelines. You can alter the pre-rendering to meet your requirements should they differ.<br></p><p>Could you forego the pre-rendering, and do all the calculations in the render proper? With enough braces, brackets, and parenthesis, you probably could. This approach however gives us a clean separation between the data to be rendered and the render itself leading to code that is easy to read and maintain.<br></p><p>### Next Steps<br></p><p>There are a number of nuances to calendar rendering which have not been addressed in this article. For example, Sunday is not always the first day of the week depending on your locale. More could also be done to address accessibility. And of course changing the styles for different mobile operating systems would take considerable effort. Does this sound like a lot more work? It is! That is where Ionic Framework can help. The announced Ionic Framework v6 includes modern calendars built to conform to the latest design guidelines, to include accessibility.<br></p><p>Check out the live [demo](http://temp.kevinhoyt.com/ionic/calendar/) and get the complete [code](https://github.com/krhoyt/Ionic/tree/master/calendar).<br></p>","comment_id":"60d37d471beb45003bbdb347","plaintext":"Take a look at the month view of a calendar and you will see several rows of\nnumbers. The numbers themselves, increasing in value one after the other, are\narranged in columns. HTML and CSS provide us with a number of tools to display\ncontent in rows and columns. Making a calendar component should be easy, right?\nRight?\n\nWhen we look at the first calendar day in a month, it could fall on any day of\nthe week. This in turn can push the last day of the month to also fall on any\nday of the week. That variation can mean that there are six weeks (six rows) in\nsome months, but only five weeks (rows) in others. Then of course there are the\nnumber of days in the month, and oddities such as leap years.\n\nWhen we turn to Stencil (and Web Components in general), all of these variations\nbeg the question \"What goes in my template if I do not know what I will need to\ndisplay?\" The answer is to figure out what you will need to display before you\ndisplay it. We can leverage the Stencil component lifecycle to override\n`componentWillRender()` and perform the required calculations.\n\nIt is About Time!\nIf you think about any single given day on the calendar, you are really taking\nin several pieces of information at once. First is the integer representing the\nday of the month - and it is usually plastered in a giant font somewhere in each\ngrid square. The second is that by the time you land on a day of the month, you\nlikely already know the month and year. Finally, as your eyes rest upon a grid\nsquare, you are mentally selecting it, and know whether that selection\nrepresents today or not.\n\n> These implied and inferred pieces of information fall in no particular order,\nand any correlation is completely made up by me for the purposes of the flow of\nthis blog post. 🤪\nThese are all important pieces of data we will need when rendering out each day\nin the calendar, so I like to have a class that represents them which I call\n`Day`. You might think \"Why not just use the JavaScript `Date` class? The reason\nfor this decision has to do with the `selected` and `today` properties that are\n`boolean` values used in helping render the appropriate styles on the calendar\ngrid.\n\nexport default class Day {\n  date: number = null;\n  month: number = null;\n  year: number = null;\n  selected: boolean = false;\n  today: boolean = false;\n}\n\n\nWhat Day Is It?\nFrom a component perspective, there are a few properties we will need to\nconsider. The first is the desired date to display in the calendar versus the\ndate being displayed in the calendar. The desired date might also be considered\nthe \"value\" of the calendar, and is exposed as a property (`@Prop()`). The date\nbeing displayed in the calendar is a matter of how the user has interacted with\nthe component to change its state (`@State()`). We will also need an `Array` of\n`Day` as a component-internal property to hold the days associated with the\nrendering of the displayed date.\n\n\n``` ts\n\n@Prop( {mutable: true} ) value: Date = new Date();\n\n@State() displayedDate: Date = new Date();\n\nprivate days: Array<Day>;\n\n```\n\n\nFor this calendar, when the user interacts with a day, that date will be set as\nthe new `value`. Since `value` will be modified from inside the component, it\nwill need to be marked as `mutable`. You may also want to emit an event that the\n`value` (selected date) has changed. Another consideration might be emitting an\nevent for changes in the month being displayed.\n\n\n### Back to the Beginning\n\n\nAs we start implementing `componentWillRender()`, our first major task is to get\nback to the first day in the displayed month. The JavaScript `Date` constructor\nis perfectly suited for this by setting the third parameter, the day of the\nmonth, to one (1). Being at the first day of the month, does not mean you are at\nthe first (upper-left) grid square, so we also need to go back in the calendar\nuntil we are at the first day of the week.\n\n\n``` ts\n\ncomponentWillRender() {\n\nconst today: Date = new Date();\n\n// Calendar used in iteration\n\nconst calendar: Date = new Date(\n\nthis.displayedDate.getFullYear(),\n\nthis.displayedDate.getMonth(),\n\n1\n\n);\n\n// First day of month may not be first day of week\n\n// Roll back until first day of week\n\ncalendar.setDate( calendar.getDate() - calendar.getDay() );\n\n// Clear days to be rendered\n\nthis.days = []; \n\n\n// ... more ...\n\n}\n\n```\n\n\nPerhaps not surprisingly, this now sets us up to iterate through the days of the\nmonth. Perhaps surprisingly, that is **not** what I like to do. \n\n\n### One Day at a Time\n\n\nMy preferred approach for this iteration is to count to 42 (6 weeks possible x 7\ndays per week). If we make it through all 42 iterations, great. If we do not\nmake it through all 42 iterations, we can break out of the loop at any time, and\nmove on to the render. Additionally, it turns out that CSS Grid is perfectly\nsuited for ending the iteration early, and will fill in the \"gaps\" for us.\n\n\n> Though I have seen various takes on how to manage the iteration through the\ncalendar, this \"count to 42\" approach leads to the most readable version that I\nhave found. YMMV.\n\n\n``` ts\n\ncomponentWillRender() {\n\nconst today: Date = new Date();\n\n// Calendar used in iteration\n\nconst calendar: Date = new Date(\n\nthis.displayedDate.getFullYear(),\n\nthis.displayedDate.getMonth(),\n\n1\n\n);\n\n// First day of month may not be first day of week\n\n// Roll back until first day of week\n\ncalendar.setDate( calendar.getDate() - calendar.getDay() );\n\n// Clear days to be rendered\n\nthis.days = [];\n\nfor( let d: number = 0; d < 42; d++ ) {\n\n// Day to be rendered\n\n// Seed with current date in iteration\n\nconst day: Day = new Day();\n\nday.year = calendar.getFullYear();\n\nday.month = calendar.getMonth();\n\n\n// Populate day in month\n\n// Undefined date properties are not rendered\n\nif(\n\ncalendar.getFullYear() === this.displayedDate.getFullYear() &&\n\ncalendar.getMonth() === this.displayedDate.getMonth()\n\n) day.date = calendar.getDate();\n\n// Check for today\n\nif(\n\ncalendar.getFullYear() === today.getFullYear() &&\n\ncalendar.getMonth() === today.getMonth() &&\n\ncalendar.getDate() === today.getDate()\n\n) day.today = true;\n\n// Check for selection\n\nif(\n\ncalendar.getFullYear() === this.value.getFullYear() &&\n\ncalendar.getMonth() === this.value.getMonth() &&\n\ncalendar.getDate() === this.value.getDate() &&\n\ncalendar.getMonth() === this.value.getMonth()\n\n) day.selected = true;\n\n// Add to days to be rendered\n\nthis.days.push( day );\n\n// Keep rolling\n\ncalendar.setDate( calendar.getDate() + 1 );\n\n// Do not render the last week\n\n// Depending on calendar layout\n\n// Some months require five weeks\n\n// Others six weeks (see May 2021)\n\nif(\n\ncalendar.getDay() === 0 &&\n\ncalendar.getMonth() !== this.displayedDate.getMonth()\n\n) break;\n\n}\n\n}\n\n```\n\n\nWithin each iteration we seed the `Day` instance with the month and year, but\nnot necessarily the day of the month. We can use this absence of data in the\nrender. We also check to see if the date in the current iteration is today, or\nif it matches the `value` property. These flags will help us style the calendar\nappropriately.  If we are past the end of the month being displayed, we are\ndone, and can return to the component lifecycle to actually render the calendar.\n\n\n### Once More, With Feeling\n\n\nWhen it comes to the actual rendering of the calendar, I like to separate it\nsemantically from the rest of the calendar. This helps keep the CSS more\nconcise. \n\n\nThe most interesting part of the CSS is the use of CSS Grid. The\n`grid-template-rows` property is specified as `repeat( auto-fill, 1fr )` which\nis what allows us to get away without having to complete the week at the end of\nthe month, or outright leave a week off the end of the month if there are only\nfive (5) weeks.\n\n\n``` css\n\narticle {\n\nbox-sizing: border-box;\n\ndisplay: grid;\n\ngrid-template-columns: 1fr 1fr 1fr 1fr 1fr 1fr 1fr;\n\ngrid-template-rows: repeat( auto-fill, 1fr );\n\ngap: 0px 0px;\n\nmin-width: 100vw;\n\npadding: 0 4px 4px 4px;\n\n}\n\n\narticle button {\n\nappearance: none;\n\n-webkit-appearance: none;\n\nbackground: none;\n\nborder: none;\n\nbox-sizing: border-box;\n\ncursor: pointer;\n\nfont-family: -apple-system, sans-serif;\n\nfont-size: 20px;\n\nmargin: 3px;\n\nmin-height: calc( ( 100vw / 7 ) - 6px );\n\noutline: none;\n\npadding: 0;\n\n-webkit-tap-highlight-color: rgba( 0, 0, 0, 0 );\n\ntext-align: center;\n\n}\n\n\narticle button[disabled] {\n\ncursor: default;\n\n}\n\n\narticle button.selected {\n\nbackground-color: rgba( 255, 0, 0, 0.10 );\n\nborder-radius: 100%;\n\ncolor: #ff0000;\n\nfont-weight: 600;\n\n}\n\n\narticle button.today {\n\ncolor: #ff0000;\n\n}\n\n\narticle button.selected.today {\n\nbackground-color: #ff0000;\n\ncolor: #ffffff;\n\n}\n\n```\n\n\nAs you can probably tell from the CSS, each day in the month will be rendered as\na `button` element. On that `button` element, we can use `data` attributes to\nstore the day, month, and year that each represents. In the click handler, a\nreference to the `button` can be obtained using the `target` property of the\n`MouseEvent`. These values can then be parsed, and a `Date` object then created\nto represent the date that was selected.  This keeps us from having to store DOM\nreferences to each of the days being rendered. A `button` marked `disabled` will\nnot fire the click handler.\n\n\n``` jsx\n\n<article>\n\n{this.days.map( ( day: Day ) =>\n\n<button\n\nclass={{\n\nselected: day.selected,\n\ntoday: day.today\n\n}}\n\ndata-date={day.date}\n\ndata-month={day.month}\n\ndata-year={day.year}\n\ndisabled={day.date === null ? true : false}\n\nonClick={( evt: MouseEvent ) => this.doSelect( evt )}>\n\n{day.date}\n\n</button>\n\n) }\n\n</article>\n\n```\n\n\nThis whole technique of calculating what needs to be displayed before the actual\nrendering process is what I call \"pre-rendering\". The approach is valuable not\nonly in calendars, but also in any situation where there can be a variable\nnumber of elements to be rendered. Dynamic SVG content is another place where\nthis technique comes in handy.\n\n\n> It should be noted that modern calendar designs do not show the dates for the\ndays outside of the month being rendered. This is true for both Material Design\nand Apple Human Interface Guidelines. You can alter the pre-rendering to meet\nyour requirements should they differ.\n\n\nCould you forego the pre-rendering, and do all the calculations in the render\nproper? With enough braces, brackets, and parenthesis, you probably could. This\napproach however gives us a clean separation between the data to be rendered and\nthe render itself leading to code that is easy to read and maintain.\n\n\n### Next Steps\n\n\nThere are a number of nuances to calendar rendering which have not been\naddressed in this article. For example, Sunday is not always the first day of\nthe week depending on your locale. More could also be done to address\naccessibility. And of course changing the styles for different mobile operating\nsystems would take considerable effort. Does this sound like a lot more work? It\nis! That is where Ionic Framework can help. The announced Ionic Framework v6\nincludes modern calendars built to conform to the latest design guidelines, to\ninclude accessibility.\n\n\nCheck out the live [demo](http://temp.kevinhoyt.com/ionic/calendar/) and get the\ncomplete [code](https://github.com/krhoyt/Ionic/tree/master/calendar).","feature_image":null,"featured":0,"status":"draft","locale":null,"visibility":"public","author_id":"1","created_at":"2021-06-23T18:28:23.000Z","updated_at":"2021-06-23T18:38:47.000Z","published_at":null,"custom_excerpt":null,"codeinjection_head":null,"codeinjection_foot":null,"custom_template":null,"canonical_url":null,"type":"post","email_recipient_filter":"none"}],"posts_authors":[{"id":"5adb8922351ffe0018a57761","post_id":"5adb8922351ffe0018a5770f","author_id":"1","sort_order":0},{"id":"5adb8922351ffe0018a57765","post_id":"5adb8922351ffe0018a57710","author_id":"1","sort_order":0},{"id":"5adb8922351ffe0018a5776a","post_id":"5adb8922351ffe0018a57711","author_id":"1","sort_order":0},{"id":"5adb8922351ffe0018a5776e","post_id":"5adb8922351ffe0018a57712","author_id":"1","sort_order":0},{"id":"5adb8922351ffe0018a57772","post_id":"5adb8922351ffe0018a57713","author_id":"1","sort_order":0},{"id":"5adb8922351ffe0018a57776","post_id":"5adb8922351ffe0018a57714","author_id":"1","sort_order":0},{"id":"5adb8922351ffe0018a5777a","post_id":"5adb8922351ffe0018a57715","author_id":"1","sort_order":0},{"id":"5adb8922351ffe0018a5777e","post_id":"5adb8922351ffe0018a57716","author_id":"1","sort_order":0},{"id":"5adb8922351ffe0018a57781","post_id":"5adb8922351ffe0018a57717","author_id":"1","sort_order":0},{"id":"5adb8922351ffe0018a57785","post_id":"5adb8922351ffe0018a57718","author_id":"1","sort_order":0},{"id":"5adb8922351ffe0018a57789","post_id":"5adb8922351ffe0018a57719","author_id":"1","sort_order":0},{"id":"5adb8922351ffe0018a5778d","post_id":"5adb8922351ffe0018a5771a","author_id":"1","sort_order":0},{"id":"5adb8923351ffe0018a57792","post_id":"5adb8922351ffe0018a5771b","author_id":"1","sort_order":0},{"id":"5adb8923351ffe0018a57797","post_id":"5adb8922351ffe0018a5771c","author_id":"1","sort_order":0},{"id":"5adb8923351ffe0018a5779b","post_id":"5adb8922351ffe0018a5771d","author_id":"1","sort_order":0},{"id":"5adb8923351ffe0018a577a0","post_id":"5adb8922351ffe0018a5771e","author_id":"1","sort_order":0},{"id":"5adb8923351ffe0018a577a6","post_id":"5adb8922351ffe0018a5771f","author_id":"1","sort_order":0},{"id":"5adb8923351ffe0018a577ad","post_id":"5adb8922351ffe0018a57720","author_id":"1","sort_order":0},{"id":"5adb8923351ffe0018a577af","post_id":"5adb8922351ffe0018a57721","author_id":"1","sort_order":0},{"id":"5adb8923351ffe0018a577b0","post_id":"5adb8922351ffe0018a57722","author_id":"1","sort_order":0},{"id":"5adb8923351ffe0018a577b6","post_id":"5adb8922351ffe0018a57723","author_id":"1","sort_order":0},{"id":"5adb8923351ffe0018a577bd","post_id":"5adb8922351ffe0018a57724","author_id":"1","sort_order":0},{"id":"5adb8923351ffe0018a577bf","post_id":"5adb8922351ffe0018a57725","author_id":"1","sort_order":0},{"id":"5adb8923351ffe0018a577c0","post_id":"5adb8922351ffe0018a57726","author_id":"1","sort_order":0},{"id":"5adb8923351ffe0018a577c5","post_id":"5adb8922351ffe0018a57727","author_id":"1","sort_order":0},{"id":"5adb8923351ffe0018a577c9","post_id":"5adb8922351ffe0018a57728","author_id":"1","sort_order":0},{"id":"5adb8923351ffe0018a577cd","post_id":"5adb8922351ffe0018a57729","author_id":"1","sort_order":0},{"id":"5adb8923351ffe0018a577d1","post_id":"5adb8922351ffe0018a5772a","author_id":"1","sort_order":0},{"id":"5adb8924351ffe0018a577d6","post_id":"5adb8922351ffe0018a5772b","author_id":"1","sort_order":0},{"id":"5adb8924351ffe0018a577d7","post_id":"5adb8922351ffe0018a5772c","author_id":"1","sort_order":0},{"id":"5adb8924351ffe0018a577dc","post_id":"5adb8922351ffe0018a5772d","author_id":"1","sort_order":0},{"id":"5adb8924351ffe0018a577e5","post_id":"5adb8922351ffe0018a5772e","author_id":"1","sort_order":0},{"id":"5adb8924351ffe0018a577eb","post_id":"5adb8922351ffe0018a5772f","author_id":"1","sort_order":0},{"id":"5adb8924351ffe0018a577f1","post_id":"5adb8922351ffe0018a57730","author_id":"1","sort_order":0},{"id":"5adb8924351ffe0018a577f5","post_id":"5adb8922351ffe0018a57731","author_id":"1","sort_order":0},{"id":"5adb8924351ffe0018a577fb","post_id":"5adb8922351ffe0018a57732","author_id":"1","sort_order":0},{"id":"5adb8924351ffe0018a57800","post_id":"5adb8922351ffe0018a57733","author_id":"1","sort_order":0},{"id":"5adb8924351ffe0018a57806","post_id":"5adb8922351ffe0018a57734","author_id":"1","sort_order":0},{"id":"5adb8924351ffe0018a5780a","post_id":"5adb8922351ffe0018a57735","author_id":"1","sort_order":0},{"id":"5adb8924351ffe0018a5780d","post_id":"5adb8922351ffe0018a57736","author_id":"1","sort_order":0},{"id":"5adb8924351ffe0018a57810","post_id":"5adb8922351ffe0018a57737","author_id":"1","sort_order":0},{"id":"5adb8924351ffe0018a57815","post_id":"5adb8922351ffe0018a57738","author_id":"1","sort_order":0},{"id":"5adb8924351ffe0018a57816","post_id":"5adb8922351ffe0018a57739","author_id":"1","sort_order":0},{"id":"5adb8925351ffe0018a5781a","post_id":"5adb8922351ffe0018a5773a","author_id":"1","sort_order":0},{"id":"5adb8925351ffe0018a5781e","post_id":"5adb8922351ffe0018a5773b","author_id":"1","sort_order":0},{"id":"5adb8925351ffe0018a57823","post_id":"5adb8922351ffe0018a5773c","author_id":"1","sort_order":0},{"id":"5adb8925351ffe0018a57829","post_id":"5adb8922351ffe0018a5773d","author_id":"1","sort_order":0},{"id":"5adb8925351ffe0018a57830","post_id":"5adb8922351ffe0018a5773e","author_id":"1","sort_order":0},{"id":"5adb8925351ffe0018a57835","post_id":"5adb8922351ffe0018a5773f","author_id":"1","sort_order":0},{"id":"5adb8925351ffe0018a57839","post_id":"5adb8922351ffe0018a57740","author_id":"1","sort_order":0},{"id":"5adb8925351ffe0018a5783e","post_id":"5adb8922351ffe0018a57741","author_id":"1","sort_order":0},{"id":"5adb8925351ffe0018a57843","post_id":"5adb8922351ffe0018a57742","author_id":"1","sort_order":0},{"id":"5adb8925351ffe0018a57848","post_id":"5adb8922351ffe0018a57743","author_id":"1","sort_order":0},{"id":"5adb8925351ffe0018a57849","post_id":"5adb8922351ffe0018a57744","author_id":"1","sort_order":0},{"id":"5adb8925351ffe0018a57850","post_id":"5adb8922351ffe0018a57745","author_id":"1","sort_order":0},{"id":"5adb8925351ffe0018a57854","post_id":"5adb8922351ffe0018a57746","author_id":"1","sort_order":0},{"id":"5adb8925351ffe0018a57858","post_id":"5adb8922351ffe0018a57747","author_id":"1","sort_order":0},{"id":"5adb8925351ffe0018a5785c","post_id":"5adb8922351ffe0018a57748","author_id":"1","sort_order":0},{"id":"5adb8926351ffe0018a57861","post_id":"5adb8922351ffe0018a57749","author_id":"1","sort_order":0},{"id":"5adb8926351ffe0018a57863","post_id":"5adb8922351ffe0018a5774a","author_id":"1","sort_order":0},{"id":"5adb8926351ffe0018a57869","post_id":"5adb8922351ffe0018a5774b","author_id":"1","sort_order":0},{"id":"5adb8926351ffe0018a5786a","post_id":"5adb8922351ffe0018a5774c","author_id":"1","sort_order":0},{"id":"5adb8926351ffe0018a5786c","post_id":"5adb8922351ffe0018a5774d","author_id":"1","sort_order":0},{"id":"5adb8926351ffe0018a57870","post_id":"5adb8922351ffe0018a5774e","author_id":"1","sort_order":0},{"id":"5adb8926351ffe0018a57873","post_id":"5adb8922351ffe0018a5774f","author_id":"1","sort_order":0},{"id":"5adb8926351ffe0018a57877","post_id":"5adb8922351ffe0018a57750","author_id":"1","sort_order":0},{"id":"5adb8926351ffe0018a5787d","post_id":"5adb8922351ffe0018a57751","author_id":"1","sort_order":0},{"id":"5adb8926351ffe0018a57881","post_id":"5adb8922351ffe0018a57752","author_id":"1","sort_order":0},{"id":"5adb8926351ffe0018a57885","post_id":"5adb8922351ffe0018a57753","author_id":"1","sort_order":0},{"id":"5adb8926351ffe0018a5788b","post_id":"5adb8922351ffe0018a57754","author_id":"1","sort_order":0},{"id":"5adb8926351ffe0018a5788e","post_id":"5adb8922351ffe0018a57755","author_id":"1","sort_order":0},{"id":"5adb8926351ffe0018a57892","post_id":"5adb8922351ffe0018a57756","author_id":"1","sort_order":0},{"id":"5adb8926351ffe0018a57896","post_id":"5adb8922351ffe0018a57757","author_id":"1","sort_order":0},{"id":"5adb8926351ffe0018a5789a","post_id":"5adb8922351ffe0018a57758","author_id":"1","sort_order":0},{"id":"5adb8926351ffe0018a5789c","post_id":"5adb8922351ffe0018a57759","author_id":"1","sort_order":0},{"id":"5adb8927351ffe0018a578a2","post_id":"5adb8922351ffe0018a5775a","author_id":"1","sort_order":0},{"id":"5adb8927351ffe0018a578a5","post_id":"5adb8922351ffe0018a5775b","author_id":"1","sort_order":0},{"id":"5adb8927351ffe0018a578a8","post_id":"5adb8922351ffe0018a5775c","author_id":"1","sort_order":0},{"id":"5adb8927351ffe0018a578aa","post_id":"5adb8922351ffe0018a5775d","author_id":"1","sort_order":0},{"id":"5adb8927351ffe0018a578ab","post_id":"5adb8922351ffe0018a5775e","author_id":"1","sort_order":0},{"id":"5adb8927351ffe0018a578ac","post_id":"5adb8922351ffe0018a5775f","author_id":"1","sort_order":0},{"id":"5adb8927351ffe0018a578af","post_id":"5adb8922351ffe0018a57760","author_id":"1","sort_order":0},{"id":"5adb8abd68339a00221450c1","post_id":"5adb8abd68339a00221450c0","author_id":"1","sort_order":0},{"id":"5bce15eb9b7c4e00bfd17f21","post_id":"5bce15ea9b7c4e00bfd17f20","author_id":"1","sort_order":0},{"id":"5bd72d1da7d50d00bf13e9fb","post_id":"5bd72d1da7d50d00bf13e9fa","author_id":"1","sort_order":0},{"id":"5c1a866ba40d3300bf6913a0","post_id":"5c1a866ba40d3300bf69139f","author_id":"1","sort_order":0},{"id":"5c3fbb70dff84900c0a5620c","post_id":"5c3fbb70dff84900c0a5620b","author_id":"1","sort_order":0},{"id":"5c48c6a19d94d600c0e43dd9","post_id":"5c48c6a19d94d600c0e43dd8","author_id":"1","sort_order":0},{"id":"5c48ed0f9d94d600c0e43e6b","post_id":"5c48ed0f9d94d600c0e43e6a","author_id":"1","sort_order":0},{"id":"5c5b1a15b1b1f400c0b6e863","post_id":"5c5b1a15b1b1f400c0b6e862","author_id":"1","sort_order":0},{"id":"5ca910e8c0479c00c0c80303","post_id":"5ca910e8c0479c00c0c80302","author_id":"1","sort_order":0},{"id":"60d37d471beb45003bbdb348","post_id":"60d37d471beb45003bbdb347","author_id":"1","sort_order":0}],"posts_meta":[],"posts_tags":[{"id":"5adb8922351ffe0018a57762","post_id":"5adb8922351ffe0018a57710","tag_id":"5adb8921351ffe0018a576a5","sort_order":0},{"id":"5adb8922351ffe0018a57763","post_id":"5adb8922351ffe0018a57710","tag_id":"5adb8921351ffe0018a576a6","sort_order":1},{"id":"5adb8922351ffe0018a57764","post_id":"5adb8922351ffe0018a57710","tag_id":"5adb8921351ffe0018a576a7","sort_order":2},{"id":"5adb8922351ffe0018a57766","post_id":"5adb8922351ffe0018a57711","tag_id":"5adb8921351ffe0018a576a6","sort_order":0},{"id":"5adb8922351ffe0018a57767","post_id":"5adb8922351ffe0018a57711","tag_id":"5adb8921351ffe0018a576a8","sort_order":1},{"id":"5adb8922351ffe0018a57768","post_id":"5adb8922351ffe0018a57711","tag_id":"5adb8921351ffe0018a576a9","sort_order":2},{"id":"5adb8922351ffe0018a57769","post_id":"5adb8922351ffe0018a57711","tag_id":"5adb8921351ffe0018a576aa","sort_order":3},{"id":"5adb8922351ffe0018a5776b","post_id":"5adb8922351ffe0018a57712","tag_id":"5adb8921351ffe0018a576a8","sort_order":0},{"id":"5adb8922351ffe0018a5776c","post_id":"5adb8922351ffe0018a57712","tag_id":"5adb8921351ffe0018a576b8","sort_order":1},{"id":"5adb8922351ffe0018a5776d","post_id":"5adb8922351ffe0018a57712","tag_id":"5adb8921351ffe0018a576b9","sort_order":2},{"id":"5adb8922351ffe0018a5776f","post_id":"5adb8922351ffe0018a57713","tag_id":"5adb8921351ffe0018a576a8","sort_order":0},{"id":"5adb8922351ffe0018a57770","post_id":"5adb8922351ffe0018a57713","tag_id":"5adb8921351ffe0018a576ab","sort_order":1},{"id":"5adb8922351ffe0018a57771","post_id":"5adb8922351ffe0018a57713","tag_id":"5adb8921351ffe0018a576ac","sort_order":2},{"id":"5adb8922351ffe0018a57773","post_id":"5adb8922351ffe0018a57714","tag_id":"5adb8921351ffe0018a576a5","sort_order":0},{"id":"5adb8922351ffe0018a57774","post_id":"5adb8922351ffe0018a57714","tag_id":"5adb8921351ffe0018a576a8","sort_order":1},{"id":"5adb8922351ffe0018a57775","post_id":"5adb8922351ffe0018a57714","tag_id":"5adb8921351ffe0018a576ae","sort_order":2},{"id":"5adb8922351ffe0018a57777","post_id":"5adb8922351ffe0018a57715","tag_id":"5adb8921351ffe0018a576a8","sort_order":0},{"id":"5adb8922351ffe0018a57778","post_id":"5adb8922351ffe0018a57715","tag_id":"5adb8921351ffe0018a576a9","sort_order":1},{"id":"5adb8922351ffe0018a57779","post_id":"5adb8922351ffe0018a57715","tag_id":"5adb8921351ffe0018a576ad","sort_order":2},{"id":"5adb8922351ffe0018a5777b","post_id":"5adb8922351ffe0018a57716","tag_id":"5adb8921351ffe0018a576af","sort_order":0},{"id":"5adb8922351ffe0018a5777c","post_id":"5adb8922351ffe0018a57716","tag_id":"5adb8921351ffe0018a576b0","sort_order":1},{"id":"5adb8922351ffe0018a5777d","post_id":"5adb8922351ffe0018a57716","tag_id":"5adb8921351ffe0018a576b1","sort_order":2},{"id":"5adb8922351ffe0018a5777f","post_id":"5adb8922351ffe0018a57717","tag_id":"5adb8921351ffe0018a576ad","sort_order":0},{"id":"5adb8922351ffe0018a57780","post_id":"5adb8922351ffe0018a57717","tag_id":"5adb8921351ffe0018a576b2","sort_order":1},{"id":"5adb8922351ffe0018a57782","post_id":"5adb8922351ffe0018a57718","tag_id":"5adb8921351ffe0018a576ad","sort_order":0},{"id":"5adb8922351ffe0018a57783","post_id":"5adb8922351ffe0018a57718","tag_id":"5adb8921351ffe0018a576b3","sort_order":1},{"id":"5adb8922351ffe0018a57784","post_id":"5adb8922351ffe0018a57718","tag_id":"5adb8921351ffe0018a576b4","sort_order":2},{"id":"5adb8922351ffe0018a57786","post_id":"5adb8922351ffe0018a57719","tag_id":"5adb8921351ffe0018a576ad","sort_order":0},{"id":"5adb8922351ffe0018a57787","post_id":"5adb8922351ffe0018a57719","tag_id":"5adb8921351ffe0018a576b5","sort_order":1},{"id":"5adb8922351ffe0018a57788","post_id":"5adb8922351ffe0018a57719","tag_id":"5adb8921351ffe0018a576b6","sort_order":2},{"id":"5adb8922351ffe0018a5778a","post_id":"5adb8922351ffe0018a5771a","tag_id":"5adb8921351ffe0018a576a8","sort_order":0},{"id":"5adb8922351ffe0018a5778b","post_id":"5adb8922351ffe0018a5771a","tag_id":"5adb8921351ffe0018a576ac","sort_order":1},{"id":"5adb8922351ffe0018a5778c","post_id":"5adb8922351ffe0018a5771a","tag_id":"5adb8921351ffe0018a576b7","sort_order":2},{"id":"5adb8923351ffe0018a5778e","post_id":"5adb8922351ffe0018a5771b","tag_id":"5adb8921351ffe0018a576a8","sort_order":0},{"id":"5adb8923351ffe0018a5778f","post_id":"5adb8922351ffe0018a5771b","tag_id":"5adb8921351ffe0018a576ba","sort_order":1},{"id":"5adb8923351ffe0018a57790","post_id":"5adb8922351ffe0018a5771b","tag_id":"5adb8921351ffe0018a576bb","sort_order":2},{"id":"5adb8923351ffe0018a57791","post_id":"5adb8922351ffe0018a5771b","tag_id":"5adb8921351ffe0018a576bc","sort_order":3},{"id":"5adb8923351ffe0018a57793","post_id":"5adb8922351ffe0018a5771c","tag_id":"5adb8921351ffe0018a576a6","sort_order":0},{"id":"5adb8923351ffe0018a57794","post_id":"5adb8922351ffe0018a5771c","tag_id":"5adb8921351ffe0018a576ba","sort_order":1},{"id":"5adb8923351ffe0018a57795","post_id":"5adb8922351ffe0018a5771c","tag_id":"5adb8921351ffe0018a576bc","sort_order":2},{"id":"5adb8923351ffe0018a57796","post_id":"5adb8922351ffe0018a5771c","tag_id":"5adb8921351ffe0018a576bd","sort_order":3},{"id":"5adb8923351ffe0018a57798","post_id":"5adb8922351ffe0018a5771d","tag_id":"5adb8921351ffe0018a576ba","sort_order":0},{"id":"5adb8923351ffe0018a57799","post_id":"5adb8922351ffe0018a5771d","tag_id":"5adb8921351ffe0018a576bc","sort_order":1},{"id":"5adb8923351ffe0018a5779a","post_id":"5adb8922351ffe0018a5771d","tag_id":"5adb8921351ffe0018a576be","sort_order":2},{"id":"5adb8923351ffe0018a5779c","post_id":"5adb8922351ffe0018a5771e","tag_id":"5adb8921351ffe0018a576ba","sort_order":0},{"id":"5adb8923351ffe0018a5779d","post_id":"5adb8922351ffe0018a5771e","tag_id":"5adb8921351ffe0018a576bc","sort_order":1},{"id":"5adb8923351ffe0018a5779e","post_id":"5adb8922351ffe0018a5771e","tag_id":"5adb8921351ffe0018a576bd","sort_order":2},{"id":"5adb8923351ffe0018a5779f","post_id":"5adb8922351ffe0018a5771e","tag_id":"5adb8921351ffe0018a576bf","sort_order":3},{"id":"5adb8923351ffe0018a577a1","post_id":"5adb8922351ffe0018a5771f","tag_id":"5adb8921351ffe0018a576ba","sort_order":0},{"id":"5adb8923351ffe0018a577a2","post_id":"5adb8922351ffe0018a5771f","tag_id":"5adb8921351ffe0018a576bc","sort_order":1},{"id":"5adb8923351ffe0018a577a3","post_id":"5adb8922351ffe0018a5771f","tag_id":"5adb8921351ffe0018a576bd","sort_order":2},{"id":"5adb8923351ffe0018a577a4","post_id":"5adb8922351ffe0018a5771f","tag_id":"5adb8921351ffe0018a576bf","sort_order":3},{"id":"5adb8923351ffe0018a577a5","post_id":"5adb8922351ffe0018a5771f","tag_id":"5adb8921351ffe0018a576c0","sort_order":4},{"id":"5adb8923351ffe0018a577a7","post_id":"5adb8922351ffe0018a57720","tag_id":"5adb8921351ffe0018a576a5","sort_order":0},{"id":"5adb8923351ffe0018a577a8","post_id":"5adb8922351ffe0018a57720","tag_id":"5adb8921351ffe0018a576a8","sort_order":1},{"id":"5adb8923351ffe0018a577a9","post_id":"5adb8922351ffe0018a57720","tag_id":"5adb8921351ffe0018a576b9","sort_order":2},{"id":"5adb8923351ffe0018a577aa","post_id":"5adb8922351ffe0018a57720","tag_id":"5adb8921351ffe0018a576ba","sort_order":3},{"id":"5adb8923351ffe0018a577ab","post_id":"5adb8922351ffe0018a57720","tag_id":"5adb8921351ffe0018a576bc","sort_order":4},{"id":"5adb8923351ffe0018a577ac","post_id":"5adb8922351ffe0018a57720","tag_id":"5adb8921351ffe0018a576c1","sort_order":5},{"id":"5adb8923351ffe0018a577ae","post_id":"5adb8922351ffe0018a57721","tag_id":"5adb8921351ffe0018a576c2","sort_order":0},{"id":"5adb8923351ffe0018a577b1","post_id":"5adb8922351ffe0018a57723","tag_id":"5adb8921351ffe0018a576a5","sort_order":0},{"id":"5adb8923351ffe0018a577b2","post_id":"5adb8922351ffe0018a57723","tag_id":"5adb8921351ffe0018a576ba","sort_order":1},{"id":"5adb8923351ffe0018a577b3","post_id":"5adb8922351ffe0018a57723","tag_id":"5adb8921351ffe0018a576bc","sort_order":2},{"id":"5adb8923351ffe0018a577b4","post_id":"5adb8922351ffe0018a57723","tag_id":"5adb8921351ffe0018a576cb","sort_order":3},{"id":"5adb8923351ffe0018a577b5","post_id":"5adb8922351ffe0018a57723","tag_id":"5adb8921351ffe0018a576cc","sort_order":4},{"id":"5adb8923351ffe0018a577b7","post_id":"5adb8922351ffe0018a57724","tag_id":"5adb8921351ffe0018a576a5","sort_order":0},{"id":"5adb8923351ffe0018a577b8","post_id":"5adb8922351ffe0018a57724","tag_id":"5adb8921351ffe0018a576ba","sort_order":1},{"id":"5adb8923351ffe0018a577b9","post_id":"5adb8922351ffe0018a57724","tag_id":"5adb8921351ffe0018a576bc","sort_order":2},{"id":"5adb8923351ffe0018a577ba","post_id":"5adb8922351ffe0018a57724","tag_id":"5adb8921351ffe0018a576cd","sort_order":3},{"id":"5adb8923351ffe0018a577bb","post_id":"5adb8922351ffe0018a57724","tag_id":"5adb8921351ffe0018a576ce","sort_order":4},{"id":"5adb8923351ffe0018a577bc","post_id":"5adb8922351ffe0018a57724","tag_id":"5adb8921351ffe0018a576cf","sort_order":5},{"id":"5adb8923351ffe0018a577be","post_id":"5adb8922351ffe0018a57725","tag_id":"5adb8921351ffe0018a576cd","sort_order":0},{"id":"5adb8923351ffe0018a577c1","post_id":"5adb8922351ffe0018a57727","tag_id":"5adb8921351ffe0018a576d0","sort_order":0},{"id":"5adb8923351ffe0018a577c2","post_id":"5adb8922351ffe0018a57727","tag_id":"5adb8921351ffe0018a576d1","sort_order":1},{"id":"5adb8923351ffe0018a577c3","post_id":"5adb8922351ffe0018a57727","tag_id":"5adb8921351ffe0018a576d2","sort_order":2},{"id":"5adb8923351ffe0018a577c4","post_id":"5adb8922351ffe0018a57727","tag_id":"5adb8921351ffe0018a576d3","sort_order":3},{"id":"5adb8923351ffe0018a577c6","post_id":"5adb8922351ffe0018a57728","tag_id":"5adb8921351ffe0018a576ba","sort_order":0},{"id":"5adb8923351ffe0018a577c7","post_id":"5adb8922351ffe0018a57728","tag_id":"5adb8921351ffe0018a576bc","sort_order":1},{"id":"5adb8923351ffe0018a577c8","post_id":"5adb8922351ffe0018a57728","tag_id":"5adb8921351ffe0018a576d4","sort_order":2},{"id":"5adb8923351ffe0018a577ca","post_id":"5adb8922351ffe0018a57729","tag_id":"5adb8921351ffe0018a576ae","sort_order":0},{"id":"5adb8923351ffe0018a577cb","post_id":"5adb8922351ffe0018a57729","tag_id":"5adb8921351ffe0018a576ba","sort_order":1},{"id":"5adb8923351ffe0018a577cc","post_id":"5adb8922351ffe0018a57729","tag_id":"5adb8921351ffe0018a576bc","sort_order":2},{"id":"5adb8923351ffe0018a577ce","post_id":"5adb8922351ffe0018a5772a","tag_id":"5adb8921351ffe0018a576a5","sort_order":0},{"id":"5adb8923351ffe0018a577cf","post_id":"5adb8922351ffe0018a5772a","tag_id":"5adb8921351ffe0018a576d5","sort_order":1},{"id":"5adb8923351ffe0018a577d0","post_id":"5adb8922351ffe0018a5772a","tag_id":"5adb8921351ffe0018a576d6","sort_order":2},{"id":"5adb8924351ffe0018a577d2","post_id":"5adb8922351ffe0018a5772b","tag_id":"5adb8921351ffe0018a576d7","sort_order":0},{"id":"5adb8924351ffe0018a577d3","post_id":"5adb8922351ffe0018a5772b","tag_id":"5adb8921351ffe0018a576d8","sort_order":1},{"id":"5adb8924351ffe0018a577d4","post_id":"5adb8922351ffe0018a5772b","tag_id":"5adb8921351ffe0018a576d9","sort_order":2},{"id":"5adb8924351ffe0018a577d5","post_id":"5adb8922351ffe0018a5772b","tag_id":"5adb8921351ffe0018a576da","sort_order":3},{"id":"5adb8924351ffe0018a577d8","post_id":"5adb8922351ffe0018a5772d","tag_id":"5adb8921351ffe0018a576db","sort_order":0},{"id":"5adb8924351ffe0018a577d9","post_id":"5adb8922351ffe0018a5772d","tag_id":"5adb8921351ffe0018a576dc","sort_order":1},{"id":"5adb8924351ffe0018a577da","post_id":"5adb8922351ffe0018a5772d","tag_id":"5adb8921351ffe0018a576dd","sort_order":2},{"id":"5adb8924351ffe0018a577db","post_id":"5adb8922351ffe0018a5772d","tag_id":"5adb8921351ffe0018a576de","sort_order":3},{"id":"5adb8924351ffe0018a577dd","post_id":"5adb8922351ffe0018a5772e","tag_id":"5adb8921351ffe0018a576a6","sort_order":0},{"id":"5adb8924351ffe0018a577de","post_id":"5adb8922351ffe0018a5772e","tag_id":"5adb8921351ffe0018a576bf","sort_order":1},{"id":"5adb8924351ffe0018a577df","post_id":"5adb8922351ffe0018a5772e","tag_id":"5adb8921351ffe0018a576da","sort_order":2},{"id":"5adb8924351ffe0018a577e0","post_id":"5adb8922351ffe0018a5772e","tag_id":"5adb8921351ffe0018a576db","sort_order":3},{"id":"5adb8924351ffe0018a577e1","post_id":"5adb8922351ffe0018a5772e","tag_id":"5adb8921351ffe0018a576dc","sort_order":4},{"id":"5adb8924351ffe0018a577e2","post_id":"5adb8922351ffe0018a5772e","tag_id":"5adb8921351ffe0018a576dd","sort_order":5},{"id":"5adb8924351ffe0018a577e3","post_id":"5adb8922351ffe0018a5772e","tag_id":"5adb8921351ffe0018a576df","sort_order":6},{"id":"5adb8924351ffe0018a577e4","post_id":"5adb8922351ffe0018a5772e","tag_id":"5adb8921351ffe0018a576e0","sort_order":7},{"id":"5adb8924351ffe0018a577e6","post_id":"5adb8922351ffe0018a5772f","tag_id":"5adb8921351ffe0018a576db","sort_order":0},{"id":"5adb8924351ffe0018a577e7","post_id":"5adb8922351ffe0018a5772f","tag_id":"5adb8921351ffe0018a576dc","sort_order":1},{"id":"5adb8924351ffe0018a577e8","post_id":"5adb8922351ffe0018a5772f","tag_id":"5adb8921351ffe0018a576dd","sort_order":2},{"id":"5adb8924351ffe0018a577e9","post_id":"5adb8922351ffe0018a5772f","tag_id":"5adb8921351ffe0018a576e1","sort_order":3},{"id":"5adb8924351ffe0018a577ea","post_id":"5adb8922351ffe0018a5772f","tag_id":"5adb8921351ffe0018a576e2","sort_order":4},{"id":"5adb8924351ffe0018a577ec","post_id":"5adb8922351ffe0018a57730","tag_id":"5adb8921351ffe0018a576c0","sort_order":0},{"id":"5adb8924351ffe0018a577ed","post_id":"5adb8922351ffe0018a57730","tag_id":"5adb8921351ffe0018a576dc","sort_order":1},{"id":"5adb8924351ffe0018a577ee","post_id":"5adb8922351ffe0018a57730","tag_id":"5adb8921351ffe0018a576df","sort_order":2},{"id":"5adb8924351ffe0018a577ef","post_id":"5adb8922351ffe0018a57730","tag_id":"5adb8921351ffe0018a576e1","sort_order":3},{"id":"5adb8924351ffe0018a577f0","post_id":"5adb8922351ffe0018a57730","tag_id":"5adb8921351ffe0018a576e2","sort_order":4},{"id":"5adb8924351ffe0018a577f2","post_id":"5adb8922351ffe0018a57731","tag_id":"5adb8921351ffe0018a576a5","sort_order":0},{"id":"5adb8924351ffe0018a577f3","post_id":"5adb8922351ffe0018a57731","tag_id":"5adb8921351ffe0018a576ae","sort_order":1},{"id":"5adb8924351ffe0018a577f4","post_id":"5adb8922351ffe0018a57731","tag_id":"5adb8921351ffe0018a576df","sort_order":2},{"id":"5adb8924351ffe0018a577f6","post_id":"5adb8922351ffe0018a57732","tag_id":"5adb8921351ffe0018a576a5","sort_order":0},{"id":"5adb8924351ffe0018a577f7","post_id":"5adb8922351ffe0018a57732","tag_id":"5adb8921351ffe0018a576d6","sort_order":1},{"id":"5adb8924351ffe0018a577f8","post_id":"5adb8922351ffe0018a57732","tag_id":"5adb8921351ffe0018a576db","sort_order":2},{"id":"5adb8924351ffe0018a577f9","post_id":"5adb8922351ffe0018a57732","tag_id":"5adb8921351ffe0018a576dc","sort_order":3},{"id":"5adb8924351ffe0018a577fa","post_id":"5adb8922351ffe0018a57732","tag_id":"5adb8921351ffe0018a576df","sort_order":4},{"id":"5adb8924351ffe0018a577fc","post_id":"5adb8922351ffe0018a57733","tag_id":"5adb8921351ffe0018a576db","sort_order":0},{"id":"5adb8924351ffe0018a577fd","post_id":"5adb8922351ffe0018a57733","tag_id":"5adb8921351ffe0018a576e5","sort_order":1},{"id":"5adb8924351ffe0018a577fe","post_id":"5adb8922351ffe0018a57733","tag_id":"5adb8921351ffe0018a576d6","sort_order":2},{"id":"5adb8924351ffe0018a577ff","post_id":"5adb8922351ffe0018a57733","tag_id":"5adb8921351ffe0018a576d0","sort_order":3},{"id":"5adb8924351ffe0018a57801","post_id":"5adb8922351ffe0018a57734","tag_id":"5adb8921351ffe0018a576a5","sort_order":0},{"id":"5adb8924351ffe0018a57802","post_id":"5adb8922351ffe0018a57734","tag_id":"5adb8921351ffe0018a576ae","sort_order":1},{"id":"5adb8924351ffe0018a57803","post_id":"5adb8922351ffe0018a57734","tag_id":"5adb8921351ffe0018a576c0","sort_order":2},{"id":"5adb8924351ffe0018a57804","post_id":"5adb8922351ffe0018a57734","tag_id":"5adb8921351ffe0018a576df","sort_order":3},{"id":"5adb8924351ffe0018a57805","post_id":"5adb8922351ffe0018a57734","tag_id":"5adb8921351ffe0018a576e3","sort_order":4},{"id":"5adb8924351ffe0018a57807","post_id":"5adb8922351ffe0018a57735","tag_id":"5adb8921351ffe0018a576c0","sort_order":0},{"id":"5adb8924351ffe0018a57808","post_id":"5adb8922351ffe0018a57735","tag_id":"5adb8921351ffe0018a576db","sort_order":1},{"id":"5adb8924351ffe0018a57809","post_id":"5adb8922351ffe0018a57735","tag_id":"5adb8921351ffe0018a576e4","sort_order":2},{"id":"5adb8924351ffe0018a5780b","post_id":"5adb8922351ffe0018a57736","tag_id":"5adb8921351ffe0018a576b9","sort_order":0},{"id":"5adb8924351ffe0018a5780c","post_id":"5adb8922351ffe0018a57736","tag_id":"5adb8921351ffe0018a576dc","sort_order":1},{"id":"5adb8924351ffe0018a5780e","post_id":"5adb8922351ffe0018a57737","tag_id":"5adb8921351ffe0018a576a5","sort_order":0},{"id":"5adb8924351ffe0018a5780f","post_id":"5adb8922351ffe0018a57737","tag_id":"5adb8921351ffe0018a576b9","sort_order":1},{"id":"5adb8924351ffe0018a57811","post_id":"5adb8922351ffe0018a57738","tag_id":"5adb8921351ffe0018a576db","sort_order":0},{"id":"5adb8924351ffe0018a57812","post_id":"5adb8922351ffe0018a57738","tag_id":"5adb8921351ffe0018a576dc","sort_order":1},{"id":"5adb8924351ffe0018a57813","post_id":"5adb8922351ffe0018a57738","tag_id":"5adb8921351ffe0018a576c0","sort_order":2},{"id":"5adb8924351ffe0018a57814","post_id":"5adb8922351ffe0018a57738","tag_id":"5adb8921351ffe0018a576b9","sort_order":3},{"id":"5adb8925351ffe0018a57817","post_id":"5adb8922351ffe0018a5773a","tag_id":"5adb8921351ffe0018a576e5","sort_order":0},{"id":"5adb8925351ffe0018a57818","post_id":"5adb8922351ffe0018a5773a","tag_id":"5adb8921351ffe0018a576e6","sort_order":1},{"id":"5adb8925351ffe0018a57819","post_id":"5adb8922351ffe0018a5773a","tag_id":"5adb8921351ffe0018a576e7","sort_order":2},{"id":"5adb8925351ffe0018a5781b","post_id":"5adb8922351ffe0018a5773b","tag_id":"5adb8921351ffe0018a576e6","sort_order":0},{"id":"5adb8925351ffe0018a5781c","post_id":"5adb8922351ffe0018a5773b","tag_id":"5adb8921351ffe0018a576e7","sort_order":1},{"id":"5adb8925351ffe0018a5781d","post_id":"5adb8922351ffe0018a5773b","tag_id":"5adb8921351ffe0018a576e5","sort_order":2},{"id":"5adb8925351ffe0018a5781f","post_id":"5adb8922351ffe0018a5773c","tag_id":"5adb8921351ffe0018a576db","sort_order":0},{"id":"5adb8925351ffe0018a57820","post_id":"5adb8922351ffe0018a5773c","tag_id":"5adb8921351ffe0018a576e5","sort_order":1},{"id":"5adb8925351ffe0018a57821","post_id":"5adb8922351ffe0018a5773c","tag_id":"5adb8921351ffe0018a576dc","sort_order":2},{"id":"5adb8925351ffe0018a57822","post_id":"5adb8922351ffe0018a5773c","tag_id":"5adb8921351ffe0018a576e0","sort_order":3},{"id":"5adb8925351ffe0018a57824","post_id":"5adb8922351ffe0018a5773d","tag_id":"5adb8921351ffe0018a576a5","sort_order":0},{"id":"5adb8925351ffe0018a57825","post_id":"5adb8922351ffe0018a5773d","tag_id":"5adb8921351ffe0018a576d6","sort_order":1},{"id":"5adb8925351ffe0018a57826","post_id":"5adb8922351ffe0018a5773d","tag_id":"5adb8921351ffe0018a576ae","sort_order":2},{"id":"5adb8925351ffe0018a57827","post_id":"5adb8922351ffe0018a5773d","tag_id":"5adb8921351ffe0018a576e8","sort_order":3},{"id":"5adb8925351ffe0018a57828","post_id":"5adb8922351ffe0018a5773d","tag_id":"5adb8921351ffe0018a576e9","sort_order":4},{"id":"5adb8925351ffe0018a5782a","post_id":"5adb8922351ffe0018a5773e","tag_id":"5adb8921351ffe0018a576e8","sort_order":0},{"id":"5adb8925351ffe0018a5782b","post_id":"5adb8922351ffe0018a5773e","tag_id":"5adb8921351ffe0018a576a5","sort_order":1},{"id":"5adb8925351ffe0018a5782c","post_id":"5adb8922351ffe0018a5773e","tag_id":"5adb8921351ffe0018a576ae","sort_order":2},{"id":"5adb8925351ffe0018a5782d","post_id":"5adb8922351ffe0018a5773e","tag_id":"5adb8921351ffe0018a576ea","sort_order":3},{"id":"5adb8925351ffe0018a5782e","post_id":"5adb8922351ffe0018a5773e","tag_id":"5adb8921351ffe0018a576a6","sort_order":4},{"id":"5adb8925351ffe0018a5782f","post_id":"5adb8922351ffe0018a5773e","tag_id":"5adb8921351ffe0018a576b9","sort_order":5},{"id":"5adb8925351ffe0018a57831","post_id":"5adb8922351ffe0018a5773f","tag_id":"5adb8921351ffe0018a576e8","sort_order":0},{"id":"5adb8925351ffe0018a57832","post_id":"5adb8922351ffe0018a5773f","tag_id":"5adb8921351ffe0018a576a5","sort_order":1},{"id":"5adb8925351ffe0018a57833","post_id":"5adb8922351ffe0018a5773f","tag_id":"5adb8921351ffe0018a576b5","sort_order":2},{"id":"5adb8925351ffe0018a57834","post_id":"5adb8922351ffe0018a5773f","tag_id":"5adb8921351ffe0018a576b6","sort_order":3},{"id":"5adb8925351ffe0018a57836","post_id":"5adb8922351ffe0018a57740","tag_id":"5adb8921351ffe0018a576eb","sort_order":0},{"id":"5adb8925351ffe0018a57837","post_id":"5adb8922351ffe0018a57740","tag_id":"5adb8921351ffe0018a576ec","sort_order":1},{"id":"5adb8925351ffe0018a57838","post_id":"5adb8922351ffe0018a57740","tag_id":"5adb8921351ffe0018a576ed","sort_order":2},{"id":"5adb8925351ffe0018a5783a","post_id":"5adb8922351ffe0018a57741","tag_id":"5adb8921351ffe0018a576eb","sort_order":0},{"id":"5adb8925351ffe0018a5783b","post_id":"5adb8922351ffe0018a57741","tag_id":"5adb8921351ffe0018a576ec","sort_order":1},{"id":"5adb8925351ffe0018a5783c","post_id":"5adb8922351ffe0018a57741","tag_id":"5adb8921351ffe0018a576ed","sort_order":2},{"id":"5adb8925351ffe0018a5783d","post_id":"5adb8922351ffe0018a57741","tag_id":"5adb8921351ffe0018a576ee","sort_order":3},{"id":"5adb8925351ffe0018a5783f","post_id":"5adb8922351ffe0018a57742","tag_id":"5adb8921351ffe0018a576b2","sort_order":0},{"id":"5adb8925351ffe0018a57840","post_id":"5adb8922351ffe0018a57742","tag_id":"5adb8921351ffe0018a576a5","sort_order":1},{"id":"5adb8925351ffe0018a57841","post_id":"5adb8922351ffe0018a57742","tag_id":"5adb8921351ffe0018a576e8","sort_order":2},{"id":"5adb8925351ffe0018a57842","post_id":"5adb8922351ffe0018a57742","tag_id":"5adb8921351ffe0018a576e9","sort_order":3},{"id":"5adb8925351ffe0018a57844","post_id":"5adb8922351ffe0018a57743","tag_id":"5adb8921351ffe0018a576ef","sort_order":0},{"id":"5adb8925351ffe0018a57845","post_id":"5adb8922351ffe0018a57743","tag_id":"5adb8921351ffe0018a576f0","sort_order":1},{"id":"5adb8925351ffe0018a57846","post_id":"5adb8922351ffe0018a57743","tag_id":"5adb8921351ffe0018a576f1","sort_order":2},{"id":"5adb8925351ffe0018a57847","post_id":"5adb8922351ffe0018a57743","tag_id":"5adb8921351ffe0018a576f2","sort_order":3},{"id":"5adb8925351ffe0018a5784a","post_id":"5adb8922351ffe0018a57745","tag_id":"5adb8921351ffe0018a576f3","sort_order":0},{"id":"5adb8925351ffe0018a5784b","post_id":"5adb8922351ffe0018a57745","tag_id":"5adb8921351ffe0018a576f4","sort_order":1},{"id":"5adb8925351ffe0018a5784c","post_id":"5adb8922351ffe0018a57745","tag_id":"5adb8921351ffe0018a576f0","sort_order":2},{"id":"5adb8925351ffe0018a5784d","post_id":"5adb8922351ffe0018a57745","tag_id":"5adb8921351ffe0018a576ef","sort_order":3},{"id":"5adb8925351ffe0018a5784e","post_id":"5adb8922351ffe0018a57745","tag_id":"5adb8921351ffe0018a576f5","sort_order":4},{"id":"5adb8925351ffe0018a5784f","post_id":"5adb8922351ffe0018a57745","tag_id":"5adb8921351ffe0018a576f6","sort_order":5},{"id":"5adb8925351ffe0018a57851","post_id":"5adb8922351ffe0018a57746","tag_id":"5adb8921351ffe0018a576b2","sort_order":0},{"id":"5adb8925351ffe0018a57852","post_id":"5adb8922351ffe0018a57746","tag_id":"5adb8921351ffe0018a576e8","sort_order":1},{"id":"5adb8925351ffe0018a57853","post_id":"5adb8922351ffe0018a57746","tag_id":"5adb8921351ffe0018a576a5","sort_order":2},{"id":"5adb8925351ffe0018a57855","post_id":"5adb8922351ffe0018a57747","tag_id":"5adb8921351ffe0018a576b2","sort_order":0},{"id":"5adb8925351ffe0018a57856","post_id":"5adb8922351ffe0018a57747","tag_id":"5adb8921351ffe0018a576f7","sort_order":1},{"id":"5adb8925351ffe0018a57857","post_id":"5adb8922351ffe0018a57747","tag_id":"5adb8921351ffe0018a576f8","sort_order":2},{"id":"5adb8925351ffe0018a57859","post_id":"5adb8922351ffe0018a57748","tag_id":"5adb8921351ffe0018a576b2","sort_order":0},{"id":"5adb8925351ffe0018a5785a","post_id":"5adb8922351ffe0018a57748","tag_id":"5adb8921351ffe0018a576cd","sort_order":1},{"id":"5adb8925351ffe0018a5785b","post_id":"5adb8922351ffe0018a57748","tag_id":"5adb8921351ffe0018a576a5","sort_order":2},{"id":"5adb8925351ffe0018a5785d","post_id":"5adb8922351ffe0018a57749","tag_id":"5adb8921351ffe0018a576b2","sort_order":0},{"id":"5adb8925351ffe0018a5785e","post_id":"5adb8922351ffe0018a57749","tag_id":"5adb8921351ffe0018a576e8","sort_order":1},{"id":"5adb8925351ffe0018a5785f","post_id":"5adb8922351ffe0018a57749","tag_id":"5adb8921351ffe0018a576a5","sort_order":2},{"id":"5adb8925351ffe0018a57860","post_id":"5adb8922351ffe0018a57749","tag_id":"5adb8921351ffe0018a576cd","sort_order":3},{"id":"5adb8926351ffe0018a57862","post_id":"5adb8922351ffe0018a5774a","tag_id":"5adb8921351ffe0018a576f9","sort_order":0},{"id":"5adb8926351ffe0018a57864","post_id":"5adb8922351ffe0018a5774b","tag_id":"5adb8921351ffe0018a576a6","sort_order":0},{"id":"5adb8926351ffe0018a57865","post_id":"5adb8922351ffe0018a5774b","tag_id":"5adb8921351ffe0018a576ed","sort_order":1},{"id":"5adb8926351ffe0018a57866","post_id":"5adb8922351ffe0018a5774b","tag_id":"5adb8921351ffe0018a576a5","sort_order":2},{"id":"5adb8926351ffe0018a57867","post_id":"5adb8922351ffe0018a5774b","tag_id":"5adb8921351ffe0018a576db","sort_order":3},{"id":"5adb8926351ffe0018a57868","post_id":"5adb8922351ffe0018a5774b","tag_id":"5adb8921351ffe0018a576fa","sort_order":4},{"id":"5adb8926351ffe0018a5786b","post_id":"5adb8922351ffe0018a5774d","tag_id":"5adb8921351ffe0018a576fb","sort_order":0},{"id":"5adb8926351ffe0018a5786d","post_id":"5adb8922351ffe0018a5774e","tag_id":"5adb8921351ffe0018a576e8","sort_order":0},{"id":"5adb8926351ffe0018a5786e","post_id":"5adb8922351ffe0018a5774e","tag_id":"5adb8921351ffe0018a576fc","sort_order":1},{"id":"5adb8926351ffe0018a5786f","post_id":"5adb8922351ffe0018a5774e","tag_id":"5adb8921351ffe0018a576fd","sort_order":2},{"id":"5adb8926351ffe0018a57871","post_id":"5adb8922351ffe0018a5774f","tag_id":"5adb8921351ffe0018a576a6","sort_order":0},{"id":"5adb8926351ffe0018a57872","post_id":"5adb8922351ffe0018a5774f","tag_id":"5adb8921351ffe0018a576fe","sort_order":1},{"id":"5adb8926351ffe0018a57874","post_id":"5adb8922351ffe0018a57750","tag_id":"5adb8921351ffe0018a576ff","sort_order":0},{"id":"5adb8926351ffe0018a57875","post_id":"5adb8922351ffe0018a57750","tag_id":"5adb8921351ffe0018a57700","sort_order":1},{"id":"5adb8926351ffe0018a57876","post_id":"5adb8922351ffe0018a57750","tag_id":"5adb8921351ffe0018a576a6","sort_order":2},{"id":"5adb8926351ffe0018a57878","post_id":"5adb8922351ffe0018a57751","tag_id":"5adb8921351ffe0018a576d5","sort_order":0},{"id":"5adb8926351ffe0018a57879","post_id":"5adb8922351ffe0018a57751","tag_id":"5adb8921351ffe0018a576a5","sort_order":1},{"id":"5adb8926351ffe0018a5787a","post_id":"5adb8922351ffe0018a57751","tag_id":"5adb8921351ffe0018a57701","sort_order":2},{"id":"5adb8926351ffe0018a5787b","post_id":"5adb8922351ffe0018a57751","tag_id":"5adb8921351ffe0018a57702","sort_order":3},{"id":"5adb8926351ffe0018a5787c","post_id":"5adb8922351ffe0018a57751","tag_id":"5adb8921351ffe0018a57703","sort_order":4},{"id":"5adb8926351ffe0018a5787e","post_id":"5adb8922351ffe0018a57752","tag_id":"5adb8921351ffe0018a576a6","sort_order":0},{"id":"5adb8926351ffe0018a5787f","post_id":"5adb8922351ffe0018a57752","tag_id":"5adb8921351ffe0018a576d9","sort_order":1},{"id":"5adb8926351ffe0018a57880","post_id":"5adb8922351ffe0018a57752","tag_id":"5adb8921351ffe0018a57704","sort_order":2},{"id":"5adb8926351ffe0018a57882","post_id":"5adb8922351ffe0018a57753","tag_id":"5adb8921351ffe0018a576e8","sort_order":0},{"id":"5adb8926351ffe0018a57883","post_id":"5adb8922351ffe0018a57753","tag_id":"5adb8921351ffe0018a57705","sort_order":1},{"id":"5adb8926351ffe0018a57884","post_id":"5adb8922351ffe0018a57753","tag_id":"5adb8921351ffe0018a57706","sort_order":2},{"id":"5adb8926351ffe0018a57886","post_id":"5adb8922351ffe0018a57754","tag_id":"5adb8921351ffe0018a576f3","sort_order":0},{"id":"5adb8926351ffe0018a57887","post_id":"5adb8922351ffe0018a57754","tag_id":"5adb8921351ffe0018a576f4","sort_order":1},{"id":"5adb8926351ffe0018a57888","post_id":"5adb8922351ffe0018a57754","tag_id":"5adb8921351ffe0018a576f0","sort_order":2},{"id":"5adb8926351ffe0018a57889","post_id":"5adb8922351ffe0018a57754","tag_id":"5adb8921351ffe0018a57707","sort_order":3},{"id":"5adb8926351ffe0018a5788a","post_id":"5adb8922351ffe0018a57754","tag_id":"5adb8921351ffe0018a57708","sort_order":4},{"id":"5adb8926351ffe0018a5788c","post_id":"5adb8922351ffe0018a57755","tag_id":"5adb8921351ffe0018a57709","sort_order":0},{"id":"5adb8926351ffe0018a5788d","post_id":"5adb8922351ffe0018a57755","tag_id":"5adb8921351ffe0018a576a6","sort_order":1},{"id":"5adb8926351ffe0018a5788f","post_id":"5adb8922351ffe0018a57756","tag_id":"5adb8921351ffe0018a576eb","sort_order":0},{"id":"5adb8926351ffe0018a57890","post_id":"5adb8922351ffe0018a57756","tag_id":"5adb8921351ffe0018a5770a","sort_order":1},{"id":"5adb8926351ffe0018a57891","post_id":"5adb8922351ffe0018a57756","tag_id":"5adb8921351ffe0018a5770b","sort_order":2},{"id":"5adb8926351ffe0018a57893","post_id":"5adb8922351ffe0018a57757","tag_id":"5adb8921351ffe0018a576e8","sort_order":0},{"id":"5adb8926351ffe0018a57894","post_id":"5adb8922351ffe0018a57757","tag_id":"5adb8921351ffe0018a576a5","sort_order":1},{"id":"5adb8926351ffe0018a57895","post_id":"5adb8922351ffe0018a57757","tag_id":"5adb8921351ffe0018a576ae","sort_order":2},{"id":"5adb8926351ffe0018a57897","post_id":"5adb8922351ffe0018a57758","tag_id":"5adb8921351ffe0018a576f3","sort_order":0},{"id":"5adb8926351ffe0018a57898","post_id":"5adb8922351ffe0018a57758","tag_id":"5adb8921351ffe0018a576f4","sort_order":1},{"id":"5adb8926351ffe0018a57899","post_id":"5adb8922351ffe0018a57758","tag_id":"5adb8921351ffe0018a576f0","sort_order":2},{"id":"5adb8926351ffe0018a5789b","post_id":"5adb8922351ffe0018a57759","tag_id":"5adb8921351ffe0018a576fb","sort_order":0},{"id":"5adb8927351ffe0018a5789d","post_id":"5adb8922351ffe0018a5775a","tag_id":"5adb8921351ffe0018a576e8","sort_order":0},{"id":"5adb8927351ffe0018a5789e","post_id":"5adb8922351ffe0018a5775a","tag_id":"5adb8921351ffe0018a576a5","sort_order":1},{"id":"5adb8927351ffe0018a5789f","post_id":"5adb8922351ffe0018a5775a","tag_id":"5adb8921351ffe0018a576ed","sort_order":2},{"id":"5adb8927351ffe0018a578a0","post_id":"5adb8922351ffe0018a5775a","tag_id":"5adb8921351ffe0018a576fa","sort_order":3},{"id":"5adb8927351ffe0018a578a1","post_id":"5adb8922351ffe0018a5775a","tag_id":"5adb8921351ffe0018a576e2","sort_order":4},{"id":"5adb8927351ffe0018a578a3","post_id":"5adb8922351ffe0018a5775b","tag_id":"5adb8921351ffe0018a576a5","sort_order":0},{"id":"5adb8927351ffe0018a578a4","post_id":"5adb8922351ffe0018a5775b","tag_id":"5adb8921351ffe0018a5770c","sort_order":1},{"id":"5adb8927351ffe0018a578a6","post_id":"5adb8922351ffe0018a5775c","tag_id":"5adb8921351ffe0018a5770d","sort_order":0},{"id":"5adb8927351ffe0018a578a7","post_id":"5adb8922351ffe0018a5775c","tag_id":"5adb8921351ffe0018a5770e","sort_order":1},{"id":"5adb8927351ffe0018a578a9","post_id":"5adb8922351ffe0018a5775d","tag_id":"5adb8921351ffe0018a57708","sort_order":0},{"id":"5adb8927351ffe0018a578ad","post_id":"5adb8922351ffe0018a57760","tag_id":"5adb8921351ffe0018a576e8","sort_order":0},{"id":"5adb8927351ffe0018a578ae","post_id":"5adb8922351ffe0018a57760","tag_id":"5adb8921351ffe0018a576a5","sort_order":1},{"id":"5adb8bd968339a00221450c4","post_id":"5adb8abd68339a00221450c0","tag_id":"5adb8bd968339a00221450c2","sort_order":0},{"id":"5adb8bd968339a00221450c5","post_id":"5adb8abd68339a00221450c0","tag_id":"5adb8bd968339a00221450c3","sort_order":1},{"id":"5bcf881da7d50d00bf13e9f5","post_id":"5bce15ea9b7c4e00bfd17f20","tag_id":"5adb8921351ffe0018a576d9","sort_order":0},{"id":"5bcf881da7d50d00bf13e9f6","post_id":"5bce15ea9b7c4e00bfd17f20","tag_id":"5bcf881da7d50d00bf13e9f4","sort_order":1},{"id":"5bcf881da7d50d00bf13e9f7","post_id":"5bce15ea9b7c4e00bfd17f20","tag_id":"5adb8921351ffe0018a576a6","sort_order":2},{"id":"5bd73d6da7d50d00bf13e9fc","post_id":"5bd72d1da7d50d00bf13e9fa","tag_id":"5adb8921351ffe0018a576d9","sort_order":0},{"id":"5bd73d6da7d50d00bf13e9fd","post_id":"5bd72d1da7d50d00bf13e9fa","tag_id":"5bcf881da7d50d00bf13e9f4","sort_order":1},{"id":"5bd73d6da7d50d00bf13e9fe","post_id":"5bd72d1da7d50d00bf13e9fa","tag_id":"5adb8921351ffe0018a576a6","sort_order":2},{"id":"5c1a8820a40d3300bf6913ab","post_id":"5c1a866ba40d3300bf69139f","tag_id":"5c1a8820a40d3300bf6913a9","sort_order":0},{"id":"5c1a8820a40d3300bf6913ac","post_id":"5c1a866ba40d3300bf69139f","tag_id":"5c1a8820a40d3300bf6913aa","sort_order":1},{"id":"5c1a8820a40d3300bf6913ad","post_id":"5c1a866ba40d3300bf69139f","tag_id":"5adb8921351ffe0018a576f3","sort_order":2},{"id":"5c3fcc37dff84900c0a56294","post_id":"5c3fbb70dff84900c0a5620b","tag_id":"5adb8921351ffe0018a57709","sort_order":0},{"id":"5c3fcc37dff84900c0a56295","post_id":"5c3fbb70dff84900c0a5620b","tag_id":"5c3fcc37dff84900c0a56293","sort_order":1},{"id":"5c3fcc37dff84900c0a56296","post_id":"5c3fbb70dff84900c0a5620b","tag_id":"5adb8921351ffe0018a576a6","sort_order":2},{"id":"5c48d5799d94d600c0e43e67","post_id":"5c48c6a19d94d600c0e43dd8","tag_id":"5c3fcc37dff84900c0a56293","sort_order":0},{"id":"5c48d5799d94d600c0e43e68","post_id":"5c48c6a19d94d600c0e43dd8","tag_id":"5adb8921351ffe0018a57709","sort_order":1},{"id":"5c48d5799d94d600c0e43e69","post_id":"5c48c6a19d94d600c0e43dd8","tag_id":"5c48d5799d94d600c0e43e66","sort_order":2},{"id":"5c5b31eab1b1f400c0b6e958","post_id":"5c5b1a15b1b1f400c0b6e862","tag_id":"5c48d5799d94d600c0e43e66","sort_order":0},{"id":"5c5b31eab1b1f400c0b6e959","post_id":"5c5b1a15b1b1f400c0b6e862","tag_id":"5adb8921351ffe0018a57709","sort_order":1},{"id":"5c5b31eab1b1f400c0b6e95a","post_id":"5c5b1a15b1b1f400c0b6e862","tag_id":"5adb8921351ffe0018a576e2","sort_order":2},{"id":"5c5b31eab1b1f400c0b6e95b","post_id":"5c5b1a15b1b1f400c0b6e862","tag_id":"5c3fcc37dff84900c0a56293","sort_order":3},{"id":"5ca9216fc0479c00c0c8045e","post_id":"5ca910e8c0479c00c0c80302","tag_id":"5c3fcc37dff84900c0a56293","sort_order":0},{"id":"5ca9216fc0479c00c0c8045f","post_id":"5ca910e8c0479c00c0c80302","tag_id":"5c48d5799d94d600c0e43e66","sort_order":1},{"id":"5ca9216fc0479c00c0c80460","post_id":"5ca910e8c0479c00c0c80302","tag_id":"5adb8921351ffe0018a576e8","sort_order":2}],"roles":[{"id":"5adb891f351ffe0018a575b9","name":"Administrator","description":"Administrators","created_at":"2018-04-21T18:55:27.000Z","updated_at":"2018-04-21T18:55:27.000Z"},{"id":"5adb891f351ffe0018a575ba","name":"Editor","description":"Editors","created_at":"2018-04-21T18:55:27.000Z","updated_at":"2018-04-21T18:55:27.000Z"},{"id":"5adb891f351ffe0018a575bb","name":"Author","description":"Authors","created_at":"2018-04-21T18:55:27.000Z","updated_at":"2018-04-21T18:55:27.000Z"},{"id":"5adb891f351ffe0018a575bc","name":"Contributor","description":"Contributors","created_at":"2018-04-21T18:55:27.000Z","updated_at":"2018-04-21T18:55:27.000Z"},{"id":"5adb891f351ffe0018a575bd","name":"Owner","description":"Blog Owner","created_at":"2018-04-21T18:55:27.000Z","updated_at":"2018-04-21T18:55:27.000Z"},{"id":"5c1a84f065391c00b42f0f19","name":"Admin Integration","description":"External Apps","created_at":"2018-12-19T17:50:40.000Z","updated_at":"2018-12-19T17:50:40.000Z"},{"id":"5d4aa21d6ebbd7002ddccf8d","name":"DB Backup Integration","description":"Internal DB Backup Client","created_at":"2019-08-07T10:04:13.000Z","updated_at":"2019-08-07T10:04:13.000Z"},{"id":"5d5d385e3b0505002de8f3b0","name":"Scheduler Integration","description":"Internal Scheduler Client","created_at":"2019-08-21T12:26:06.000Z","updated_at":"2019-08-21T12:26:06.000Z"}],"roles_users":[{"id":"5adb8920351ffe0018a575f3","role_id":"5adb891f351ffe0018a575bb","user_id":"5951f5fca366002ebd5dbef7"},{"id":"5adb8921351ffe0018a57683","role_id":"5adb891f351ffe0018a575bd","user_id":"1"}],"settings":[{"id":"5adb8921351ffe0018a57684","key":"db_hash","value":"2a80479d-7588-430d-84b3-c02041ce4dca","type":"string","created_at":"2018-04-21T18:55:29.000Z","updated_at":"2018-04-21T18:55:29.000Z","group":"core","flags":null},{"id":"5adb8921351ffe0018a57685","key":"next_update_check","value":"1625174803","type":"number","created_at":"2018-04-21T18:55:29.000Z","updated_at":"2021-06-30T21:26:43.000Z","group":"core","flags":null},{"id":"5adb8921351ffe0018a57686","key":"notifications","value":"[{\"dismissible\":true,\"location\":\"bottom\",\"status\":\"alert\",\"id\":\"045f7e0c-5305-44bf-8b32-6f955d461212\",\"custom\":true,\"createdAt\":\"2018-08-21T19:05:35.000Z\",\"type\":\"info\",\"top\":true,\"message\":\"<strong>Ghost 2.0 is now available</strong> - This is a major update which requires a manual upgrade. <a href=\\\"https://my.ghost.org\\\" target=\\\"_blank\\\" rel=\\\"noopener\\\">Click here</a> to get started!\",\"seen\":true,\"addedAt\":\"2018-10-22T18:22:17.197Z\"}]","type":"array","created_at":"2018-04-21T18:55:29.000Z","updated_at":"2018-10-22T18:23:37.000Z","group":"core","flags":null},{"id":"5adb8921351ffe0018a57687","key":"title","value":"Kevin Hoyt","type":"string","created_at":"2014-01-01T16:57:03.000Z","updated_at":"2018-04-21T18:55:11.000Z","group":"site","flags":"PUBLIC"},{"id":"5adb8921351ffe0018a57688","key":"description","value":"The Internet of Things at the crossroads of design and development.","type":"string","created_at":"2014-01-01T16:57:03.000Z","updated_at":"2018-04-21T18:55:11.000Z","group":"site","flags":"PUBLIC"},{"id":"5adb8921351ffe0018a57689","key":"logo","value":null,"type":"string","created_at":"2014-01-01T16:57:03.000Z","updated_at":"2018-04-21T18:55:11.000Z","group":"site","flags":"PUBLIC"},{"id":"5adb8921351ffe0018a5768a","key":"cover_image","value":"__GHOST_URL__/content/images/2014/10/wonder-twins.jpg","type":"string","created_at":"2014-01-01T16:57:03.000Z","updated_at":"2019-02-26T22:43:58.000Z","group":"site","flags":null},{"id":"5adb8921351ffe0018a5768b","key":"icon","value":null,"type":"string","created_at":"2018-04-21T18:55:29.000Z","updated_at":"2018-04-21T18:55:29.000Z","group":"site","flags":null},{"id":"5adb8921351ffe0018a57690","key":"amp","value":"true","type":"boolean","created_at":"2017-01-12T18:05:37.000Z","updated_at":"2018-04-21T18:55:11.000Z","group":"amp","flags":null},{"id":"5adb8921351ffe0018a57693","key":"facebook","value":null,"type":"string","created_at":"2016-05-18T11:30:57.000Z","updated_at":"2018-04-21T18:55:11.000Z","group":"site","flags":null},{"id":"5adb8921351ffe0018a57694","key":"twitter","value":"@krhoyt","type":"string","created_at":"2016-05-18T11:30:57.000Z","updated_at":"2018-12-19T17:48:51.000Z","group":"site","flags":null},{"id":"5adb8921351ffe0018a57696","key":"navigation","value":"[{\"label\":\"Home\",\"url\":\"/\"},{\"label\":\"About\",\"url\":\"/about/\"},{\"label\":\"Events\",\"url\":\"/events/\"},{\"label\":\"Lounge\",\"url\":\"/lounge/\"}]","type":"array","created_at":"2015-03-28T20:40:48.000Z","updated_at":"2018-04-21T18:59:17.000Z","group":"site","flags":null},{"id":"5adb8921351ffe0018a57698","key":"unsplash","value":"true","type":"boolean","created_at":"2018-04-21T18:55:29.000Z","updated_at":"2018-04-21T18:59:17.000Z","group":"unsplash","flags":null},{"id":"5adb8921351ffe0018a57699","key":"active_theme","value":"casper","type":"string","created_at":"2018-04-21T18:55:29.000Z","updated_at":"2018-12-19T17:46:25.000Z","group":"theme","flags":"RO"},{"id":"5adb8921351ffe0018a5769c","key":"is_private","value":"false","type":"boolean","created_at":"2015-05-14T18:19:24.000Z","updated_at":"2018-04-21T18:55:11.000Z","group":"private","flags":null},{"id":"5adb8921351ffe0018a5769d","key":"password","value":null,"type":"string","created_at":"2015-05-14T18:19:24.000Z","updated_at":"2018-04-21T18:55:11.000Z","group":"private","flags":null},{"id":"5adb8921351ffe0018a5769e","key":"public_hash","value":"5f0ad0a2efa0cbb09f9b1300a0883f","type":"string","created_at":"2018-04-21T18:55:29.000Z","updated_at":"2018-04-21T18:55:29.000Z","group":"private","flags":null},{"id":"5d384df4573a8d0038501371","key":"meta_title","value":null,"type":"string","created_at":"2019-07-24T12:24:20.000Z","updated_at":"2019-07-24T12:24:20.000Z","group":"site","flags":null},{"id":"5d384df4573a8d0038501372","key":"meta_description","value":null,"type":"string","created_at":"2019-07-24T12:24:20.000Z","updated_at":"2019-07-24T12:24:20.000Z","group":"site","flags":null},{"id":"5d384df4573a8d0038501373","key":"og_image","value":null,"type":"string","created_at":"2019-07-24T12:24:20.000Z","updated_at":"2019-07-24T12:24:20.000Z","group":"site","flags":null},{"id":"5d384df4573a8d0038501374","key":"og_title","value":null,"type":"string","created_at":"2019-07-24T12:24:20.000Z","updated_at":"2019-07-24T12:24:20.000Z","group":"site","flags":null},{"id":"5d384df4573a8d0038501375","key":"og_description","value":null,"type":"string","created_at":"2019-07-24T12:24:20.000Z","updated_at":"2019-07-24T12:24:20.000Z","group":"site","flags":null},{"id":"5d384df4573a8d0038501376","key":"twitter_image","value":null,"type":"string","created_at":"2019-07-24T12:24:20.000Z","updated_at":"2019-07-24T12:24:20.000Z","group":"site","flags":null},{"id":"5d384df4573a8d0038501377","key":"twitter_title","value":null,"type":"string","created_at":"2019-07-24T12:24:20.000Z","updated_at":"2019-07-24T12:24:20.000Z","group":"site","flags":null},{"id":"5d384df4573a8d0038501378","key":"twitter_description","value":null,"type":"string","created_at":"2019-07-24T12:24:20.000Z","updated_at":"2019-07-24T12:24:20.000Z","group":"site","flags":null},{"id":"5d9dea40d1c5ac00384fd2a8","key":"default_content_visibility","value":"public","type":"string","created_at":"2019-10-09T14:10:08.000Z","updated_at":"2019-10-09T14:10:08.000Z","group":"members","flags":null},{"id":"5de9487acbc8f200449b80b6","key":"secondary_navigation","value":"[]","type":"array","created_at":"2019-12-05T18:12:10.000Z","updated_at":"2019-12-05T18:12:10.000Z","group":"site","flags":null},{"id":"5eb3e2be99bf20003911cc08","key":"session_secret","value":"e8c39d835f45e40ca8b34921cd984787a56e3d74e8a93051405452c28ff4c761","type":"string","created_at":"2020-05-07T10:28:14.000Z","updated_at":"2020-05-07T10:28:14.000Z","group":"core","flags":null},{"id":"5eb3e2be99bf20003911cc09","key":"theme_session_secret","value":"724bc46b7310987519b72d536ebf4faa2db737973ad33fd285fa8060cce1acb6","type":"string","created_at":"2020-05-07T10:28:14.000Z","updated_at":"2020-05-07T10:28:14.000Z","group":"core","flags":null},{"id":"5eb3e2be99bf20003911cc0a","key":"ghost_public_key","value":"-----BEGIN RSA PUBLIC KEY-----\nMIGJAoGBAIDeNtomH7/SdSSWBJnfBajMoPjfT0/vIOylOQcT+4Ri2SwRD3GEDssQmZmZX79K\ndoE+AnJMSFva1qcmw2HwgwUlgsPj/WKKneIoTzbWvgU8x1fykp+ggeRahfQYDPAEnno4pd4E\ne08q3cyEPfyYnalquz2cwGNMLDERqQHelPX3AgMBAAE=\n-----END RSA PUBLIC KEY-----\n","type":"string","created_at":"2020-05-07T10:28:14.000Z","updated_at":"2020-05-07T10:28:14.000Z","group":"core","flags":null},{"id":"5eb3e2be99bf20003911cc0b","key":"ghost_private_key","value":"-----BEGIN RSA PRIVATE KEY-----\nMIICXgIBAAKBgQCA3jbaJh+/0nUklgSZ3wWozKD4309P7yDspTkHE/uEYtksEQ9xhA7LEJmZ\nmV+/SnaBPgJyTEhb2tanJsNh8IMFJYLD4/1iip3iKE821r4FPMdX8pKfoIHkWoX0GAzwBJ56\nOKXeBHtPKt3MhD38mJ2pars9nMBjTCwxEakB3pT19wIDAQABAoGAcuw4xJDNLZb1CtySW9qN\nRHzAbVFQfLuHhCLQWaSTX/t7KxZHzHb1blhucpFZ1tEEH7v6GI441Vmypbwm79e6Lv8NgOm9\nJk1BxNTa3qbbUPJoeNLle96bHiX161nJJ6JhtOUaoGGpWyM07VQ5Vhei3GpiAK6YxWmW0As6\n5x23g0kCQQDCg3EZr6QVKft0pp4WNJzpPgK0JeA4Sd+JPKQzNxAz64sT3/En1XVP/CDN9pbH\nRpnE5Ksml8LY3Nl5MxdeQNErAkEAqZqQ7S/tiQcJH3lK9y8g3EMb2383L/JV49qLaKVsVoi4\nmRdR83KoAGtgiMLvZAhhLfWa/vROLiDW3tU6NG9QZQJBAKvvl+fGk9mJVJZWOWMTeqfJgoZO\n/JWE5yzeB56ly5K5YCRfTUGSs7HSAgVNNRZM19ts7cSdXURoY2t4t+0+iiUCQQCi0s3Rt5J+\nW6/JdIOgvLD6ZL7owb/0mt0US2pNA4r3pHg8GORHKMra5Dsf8K0sRtMs8T5lB5vKsyI1eVVQ\no5D1AkEAq83523ekDUDU0UPOPtpV85NUGxFmsI5LIxcf2QBx775ohJeIqgjqwG56STbtpd7o\ntM8O3Et0bFEBc+YHF5KhTw==\n-----END RSA PRIVATE KEY-----\n","type":"string","created_at":"2020-05-07T10:28:14.000Z","updated_at":"2020-05-07T10:28:14.000Z","group":"core","flags":null},{"id":"5eb3e2be99bf20003911cc0c","key":"members_public_key","value":"-----BEGIN RSA PUBLIC KEY-----\nMIGJAoGBAPEkKaAPl+Y9fKxIljhUXnvFwDpKu1AaXZLOn5Y5ZuV+UO+gryPc7buuNmMonM8l\nHtUPe4roMkaGL++QO7KAw1bwnHWT8lZHXOq0I1PXDE4tck5Q5D6Y5q4Mxgngn9bJ9MuiyNhf\ny9UnwquS+hWln3z5Ted6U1M7vi6+18y/M2ubAgMBAAE=\n-----END RSA PUBLIC KEY-----\n","type":"string","created_at":"2020-05-07T10:28:14.000Z","updated_at":"2020-05-07T10:28:14.000Z","group":"core","flags":null},{"id":"5eb3e2be99bf20003911cc0d","key":"members_private_key","value":"-----BEGIN RSA PRIVATE KEY-----\nMIICXAIBAAKBgQDxJCmgD5fmPXysSJY4VF57xcA6SrtQGl2Szp+WOWblflDvoK8j3O27rjZj\nKJzPJR7VD3uK6DJGhi/vkDuygMNW8Jx1k/JWR1zqtCNT1wxOLXJOUOQ+mOauDMYJ4J/WyfTL\nosjYX8vVJ8KrkvoVpZ98+U3nelNTO74uvtfMvzNrmwIDAQABAoGATBspuAaz3krRql5OawDV\nwSKshLlJXQK1PbxQyczcGNpBGSARprP3pCBbsfgDZa1FieEVEMYv9+op/uX6sV/KRm1ccFot\n4MNi+9pY711REq28xa5lqv00oN/RSr5Z3Jk5+iwMQ3UjICmtVWohS3/RAjW5k/buTxDeswA6\n4DEllOECQQD5VjSgiEZ0LEILy+qtOqlf8ZlunjkfpKseS6JgGfzpnsZBtJur05mDVR+M/b73\ngfSkW46NsjhbqYmpgaICZz/dAkEA95XjmRYPzy7iz8nmI+wUmp3S5TPVz/+lga9LolQcFAnv\nuDsB0m8VNwx5qbiLJMbrgn7gXmreyL22X97mUZfd1wJAcoj4CdovywPdnv+gIM02qt0XpynF\ngdSpynPh9hVaCZMm7V3iuK6t8Rd0M9cIFzg6QhnQxSReITUjCJuE6We2fQJBAPMzETa3LSBe\n9f0WXjK7HCIlFZx/pm+Ct5kEEE+MP2iPuiJGFCwZYcWJQ5kr5cJig8TTzKHyaJTXheX2lldf\nzlECQHK0NEAfgs5lUdUoEqyLAz27NlFzsGax0M/5dFo/LlG17WHo1SCwmmP0SLu/7OLqC1LM\nhxlWPd1hGubBfmuwsp0=\n-----END RSA PRIVATE KEY-----\n","type":"string","created_at":"2020-05-07T10:28:14.000Z","updated_at":"2020-05-07T10:28:14.000Z","group":"core","flags":null},{"id":"5eb3e2be99bf20003911cc0f","key":"members_email_auth_secret","value":"1005bb59ab4d30f8479bd7dae67dfb52ec74d3e0416afcefbe1d48bbb9753669b20e30263fdcd16d1c64814c4a70768742df020e1dbbf0a3bf5bdf4d6d7176fc","type":"string","created_at":"2020-05-07T10:28:14.000Z","updated_at":"2020-05-07T10:28:14.000Z","group":"core","flags":null},{"id":"5ee20707d666df0039db55e2","key":"shared_views","value":"{}","type":"array","created_at":"2020-06-11T11:27:19.000Z","updated_at":"2020-06-11T11:27:19.000Z","group":"views","flags":null},{"id":"5efa1062be411e003926d398","key":"portal_name","value":"true","type":"boolean","created_at":"2020-06-29T16:01:38.000Z","updated_at":"2020-06-29T16:01:38.000Z","group":"portal","flags":null},{"id":"5efa1062be411e003926d399","key":"portal_button","value":"false","type":"boolean","created_at":"2020-06-29T16:01:38.000Z","updated_at":"2020-06-29T16:01:38.000Z","group":"portal","flags":null},{"id":"5efa1062be411e003926d39a","key":"portal_plans","value":"[\"free\",\"monthly\",\"yearly\"]","type":"array","created_at":"2020-06-29T16:01:38.000Z","updated_at":"2021-06-23T18:27:30.000Z","group":"portal","flags":null},{"id":"5f0de948d0c47f002d4967e2","key":"accent_color","value":"#15171A","type":"string","created_at":"2020-07-14T18:20:08.000Z","updated_at":"2020-07-14T18:20:08.000Z","group":"site","flags":"PUBLIC"},{"id":"5f0de948d0c47f002d4967e3","key":"lang","value":"en","type":"string","created_at":"2020-07-14T18:20:08.000Z","updated_at":"2020-07-14T18:20:08.000Z","group":"site","flags":null},{"id":"5f0de948d0c47f002d4967e4","key":"timezone","value":"Etc/UTC","type":"string","created_at":"2020-07-14T18:20:08.000Z","updated_at":"2020-07-14T18:20:08.000Z","group":"site","flags":null},{"id":"5f0de948d0c47f002d4967e5","key":"codeinjection_head","value":"<script>\n  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){\n  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),\n  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)\n  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');\n\n  ga('create', 'UA-66675460-1', 'auto');\n  ga('send', 'pageview');\n\n</script>","type":"string","created_at":"2020-07-14T18:20:08.000Z","updated_at":"2020-07-14T18:20:08.000Z","group":"site","flags":null},{"id":"5f0de948d0c47f002d4967e6","key":"codeinjection_foot","value":"<!-- You can safely delete this line if your theme does not require jQuery -->\n<script type=\"text/javascript\" src=\"https://code.jquery.com/jquery-1.11.3.min.js\"></script>\n","type":"string","created_at":"2020-07-14T18:20:08.000Z","updated_at":"2020-07-14T18:20:08.000Z","group":"site","flags":null},{"id":"5f0de948d0c47f002d4967e8","key":"members_from_address","value":"noreply@www.kevinhoyt.com","type":"string","created_at":"2020-07-14T18:20:08.000Z","updated_at":"2020-07-14T18:20:08.000Z","group":"members","flags":"RO"},{"id":"5f0de948d0c47f002d4967e9","key":"stripe_product_name","value":"Ghost Subscription","type":"string","created_at":"2020-07-14T18:20:08.000Z","updated_at":"2020-07-14T18:20:08.000Z","group":"members","flags":""},{"id":"5f0de948d0c47f002d4967ec","key":"stripe_plans","value":"[{\"name\":\"Monthly\",\"currency\":\"usd\",\"interval\":\"month\",\"amount\":0},{\"name\":\"Yearly\",\"currency\":\"usd\",\"interval\":\"year\",\"amount\":0}]","type":"array","created_at":"2020-07-14T18:20:08.000Z","updated_at":"2020-07-14T18:20:08.000Z","group":"members","flags":""},{"id":"5f0de948d0c47f002d4967ef","key":"stripe_connect_livemode","value":"","type":"boolean","created_at":"2020-07-14T18:20:08.000Z","updated_at":"2020-07-14T18:20:08.000Z","group":"members","flags":""},{"id":"5f0de948d0c47f002d4967f0","key":"stripe_connect_display_name","value":"","type":"string","created_at":"2020-07-14T18:20:08.000Z","updated_at":"2020-07-14T18:20:08.000Z","group":"members","flags":""},{"id":"5f0de948d0c47f002d4967f2","key":"portal_button_style","value":"icon-and-text","type":"string","created_at":"2020-07-14T18:20:08.000Z","updated_at":"2020-07-14T18:20:08.000Z","group":"portal","flags":null},{"id":"5f0de948d0c47f002d4967f3","key":"portal_button_icon","value":null,"type":"string","created_at":"2020-07-14T18:20:08.000Z","updated_at":"2020-07-14T18:20:08.000Z","group":"portal","flags":null},{"id":"5f0de948d0c47f002d4967f4","key":"portal_button_signup_text","value":"Subscribe","type":"string","created_at":"2020-07-14T18:20:08.000Z","updated_at":"2020-07-14T18:20:08.000Z","group":"portal","flags":null},{"id":"5f0de948d0c47f002d4967f5","key":"mailgun_domain","value":"","type":"string","created_at":"2020-07-14T18:20:08.000Z","updated_at":"2020-07-14T18:20:08.000Z","group":"email","flags":null},{"id":"5f0de948d0c47f002d4967f6","key":"mailgun_api_key","value":"","type":"string","created_at":"2020-07-14T18:20:08.000Z","updated_at":"2020-07-14T18:20:08.000Z","group":"email","flags":null},{"id":"5f0de948d0c47f002d4967f7","key":"mailgun_base_url","value":"","type":"string","created_at":"2020-07-14T18:20:08.000Z","updated_at":"2020-07-14T18:20:08.000Z","group":"email","flags":null},{"id":"5f187119a52626002d783819","key":"amp_gtag_id","value":null,"type":"string","created_at":"2020-07-22T18:02:17.000Z","updated_at":"2020-07-22T18:02:17.000Z","group":"amp","flags":null},{"id":"5f578e055da313002d39184c","key":"routes_hash","value":"956f6fcca08e70eab80fd2c35e9fb1d6","type":"string","created_at":"2020-09-08T14:58:29.000Z","updated_at":"2020-09-16T16:19:54.000Z","group":"core","flags":null},{"id":"5f578e055da313002d39184d","key":"members_support_address","value":"noreply@www.kevinhoyt.com","type":"string","created_at":"2020-09-08T14:58:29.000Z","updated_at":"2020-09-08T14:58:29.000Z","group":"members","flags":"PUBLIC,RO"},{"id":"5f578e055da313002d39184e","key":"members_reply_address","value":"newsletter","type":"string","created_at":"2020-09-08T14:58:29.000Z","updated_at":"2020-09-08T14:58:29.000Z","group":"members","flags":null},{"id":"5fb70d7897b721002debb68e","key":"newsletter_show_badge","value":"true","type":"boolean","created_at":"2020-11-20T00:27:36.000Z","updated_at":"2020-11-20T00:27:36.000Z","group":"newsletter","flags":null},{"id":"5fb70d7897b721002debb690","key":"newsletter_body_font_category","value":"sans_serif","type":"string","created_at":"2020-11-20T00:27:36.000Z","updated_at":"2020-11-20T00:27:36.000Z","group":"newsletter","flags":null},{"id":"5fb70d7897b721002debb691","key":"newsletter_footer_content","value":"","type":"string","created_at":"2020-11-20T00:27:36.000Z","updated_at":"2020-11-20T00:27:36.000Z","group":"newsletter","flags":null},{"id":"5fd1091222070e002d3e18be","key":"members_free_signup_redirect","value":"/","type":"string","created_at":"2020-12-09T17:27:46.000Z","updated_at":"2020-12-09T17:27:46.000Z","group":"members","flags":null},{"id":"5fd1091222070e002d3e18bf","key":"members_paid_signup_redirect","value":"/","type":"string","created_at":"2020-12-09T17:27:46.000Z","updated_at":"2020-12-09T17:27:46.000Z","group":"members","flags":null},{"id":"5fd1091222070e002d3e18c0","key":"email_track_opens","value":"true","type":"boolean","created_at":"2020-12-09T17:27:46.000Z","updated_at":"2020-12-09T17:27:46.000Z","group":"email","flags":null},{"id":"6012b9d6e88041002d7b18a7","key":"firstpromoter","value":"false","type":"boolean","created_at":"2021-01-28T13:19:18.000Z","updated_at":"2021-01-28T13:19:18.000Z","group":"firstpromoter","flags":null},{"id":"6012b9d6e88041002d7b18a8","key":"firstpromoter_id","value":null,"type":"string","created_at":"2021-01-28T13:19:18.000Z","updated_at":"2021-01-28T13:19:18.000Z","group":"firstpromoter","flags":null},{"id":"60d37be5a62c80002f1da412","key":"slack_url","value":"","type":"string","created_at":"2021-06-23T19:22:29.000Z","updated_at":"2021-06-23T18:27:30.000Z","group":"slack","flags":null},{"id":"60d37be5a62c80002f1da413","key":"slack_username","value":"Ghost","type":"string","created_at":"2021-06-23T19:22:29.000Z","updated_at":"2021-06-23T18:27:30.000Z","group":"slack","flags":null},{"id":"60d37be8a62c80002f1da42e","key":"members_signup_access","value":"all","type":"string","created_at":"2021-06-23T19:22:32.000Z","updated_at":"2021-06-23T19:22:32.000Z","group":"members","flags":null},{"id":"60d37be9a62c80002f1da431","key":"labs","value":"{}","type":"object","created_at":"2021-06-23T19:22:33.000Z","updated_at":"2021-06-23T19:22:33.000Z","group":"labs","flags":null},{"id":"60d37bef028de7003e482b0d","key":"members_free_price_name","value":"Free","type":"string","created_at":"2021-06-23T19:22:39.000Z","updated_at":"2021-06-23T19:22:39.000Z","group":"members","flags":null},{"id":"60d37bef028de7003e482b0e","key":"members_free_price_description","value":"Free preview","type":"string","created_at":"2021-06-23T19:22:39.000Z","updated_at":"2021-06-23T19:22:39.000Z","group":"members","flags":null},{"id":"60d37bef028de7003e482b0f","key":"members_monthly_price_id","value":null,"type":"string","created_at":"2021-06-23T19:22:39.000Z","updated_at":"2021-06-23T19:22:39.000Z","group":"members","flags":null},{"id":"60d37bef028de7003e482b10","key":"members_yearly_price_id","value":null,"type":"string","created_at":"2021-06-23T19:22:39.000Z","updated_at":"2021-06-23T19:22:39.000Z","group":"members","flags":null},{"id":"60d37bef028de7003e482b11","key":"newsletter_header_image","value":null,"type":"string","created_at":"2021-06-23T19:22:39.000Z","updated_at":"2021-06-23T19:22:39.000Z","group":"newsletter","flags":null},{"id":"60d37bef028de7003e482b12","key":"newsletter_show_header_icon","value":"true","type":"boolean","created_at":"2021-06-23T19:22:39.000Z","updated_at":"2021-06-23T19:22:39.000Z","group":"newsletter","flags":null},{"id":"60d37bef028de7003e482b13","key":"newsletter_show_header_title","value":"true","type":"boolean","created_at":"2021-06-23T19:22:39.000Z","updated_at":"2021-06-23T19:22:39.000Z","group":"newsletter","flags":null},{"id":"60d37bef028de7003e482b14","key":"newsletter_title_alignment","value":"center","type":"string","created_at":"2021-06-23T19:22:39.000Z","updated_at":"2021-06-23T19:22:39.000Z","group":"newsletter","flags":null},{"id":"60d37bef028de7003e482b15","key":"newsletter_title_font_category","value":"sans_serif","type":"string","created_at":"2021-06-23T19:22:39.000Z","updated_at":"2021-06-23T19:22:39.000Z","group":"newsletter","flags":null},{"id":"60d37bef028de7003e482b16","key":"newsletter_show_feature_image","value":"true","type":"boolean","created_at":"2021-06-23T19:22:39.000Z","updated_at":"2021-06-23T19:22:39.000Z","group":"newsletter","flags":null},{"id":"60d37bef028de7003e482b19","key":"editor_default_email_recipients","value":"visibility","type":"string","created_at":"2021-06-23T19:22:39.000Z","updated_at":"2021-06-23T19:22:39.000Z","group":"editor","flags":null},{"id":"60d37bef028de7003e482b1a","key":"editor_default_email_recipients_filter","value":"all","type":"string","created_at":"2021-06-23T19:22:39.000Z","updated_at":"2021-06-23T19:22:39.000Z","group":"editor","flags":null},{"id":"60d4a2ba10d6de002f6649e0","key":"portal_products","value":"[\"60d37be8a62c80002f1da41f\"]","type":"array","created_at":"2021-06-24T16:20:26.000Z","updated_at":"2021-06-24T16:20:26.000Z","group":"portal","flags":null}],"tags":[{"id":"5adb891f351ffe0018a575b4","name":"Getting Started","slug":"getting-started","description":null,"feature_image":null,"parent_id":null,"visibility":"public","meta_title":null,"meta_description":null,"created_at":"2018-04-21T18:55:27.000Z","updated_at":"2018-04-21T18:55:27.000Z","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null},{"id":"5adb8921351ffe0018a576a5","name":"iot","slug":"iot","description":null,"feature_image":null,"parent_id":null,"visibility":"public","meta_title":null,"meta_description":null,"created_at":"2015-04-20T19:51:28.000Z","updated_at":"2015-04-20T19:51:28.000Z","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null},{"id":"5adb8921351ffe0018a576a6","name":"web","slug":"web","description":null,"feature_image":null,"parent_id":null,"visibility":"public","meta_title":null,"meta_description":null,"created_at":"2015-04-20T19:51:28.000Z","updated_at":"2015-04-20T19:51:28.000Z","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null},{"id":"5adb8921351ffe0018a576a7","name":"architecture","slug":"architecture","description":null,"feature_image":null,"parent_id":null,"visibility":"public","meta_title":null,"meta_description":null,"created_at":"2015-04-20T19:51:28.000Z","updated_at":"2015-04-20T19:51:28.000Z","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null},{"id":"5adb8921351ffe0018a576a8","name":"arduino","slug":"arduino","description":null,"feature_image":null,"parent_id":null,"visibility":"public","meta_title":null,"meta_description":null,"created_at":"2015-04-20T20:09:25.000Z","updated_at":"2015-04-20T20:09:25.000Z","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null},{"id":"5adb8921351ffe0018a576a9","name":"yun","slug":"yun","description":null,"feature_image":null,"parent_id":null,"visibility":"public","meta_title":null,"meta_description":null,"created_at":"2015-04-20T20:09:25.000Z","updated_at":"2015-04-20T20:09:25.000Z","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null},{"id":"5adb8921351ffe0018a576aa","name":"led","slug":"led","description":null,"feature_image":null,"parent_id":null,"visibility":"public","meta_title":null,"meta_description":null,"created_at":"2015-04-20T20:09:25.000Z","updated_at":"2015-04-20T20:09:25.000Z","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null},{"id":"5adb8921351ffe0018a576ab","name":"thermistor","slug":"thermistor","description":null,"feature_image":null,"parent_id":null,"visibility":"public","meta_title":null,"meta_description":null,"created_at":"2015-04-20T20:46:51.000Z","updated_at":"2015-04-20T20:46:51.000Z","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null},{"id":"5adb8921351ffe0018a576ac","name":"circuit","slug":"circuit","description":null,"feature_image":null,"parent_id":null,"visibility":"public","meta_title":null,"meta_description":null,"created_at":"2015-04-20T20:46:51.000Z","updated_at":"2015-04-20T20:46:51.000Z","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null},{"id":"5adb8921351ffe0018a576ad","name":"parse","slug":"parse","description":null,"feature_image":null,"parent_id":null,"visibility":"public","meta_title":null,"meta_description":null,"created_at":"2015-04-20T20:58:51.000Z","updated_at":"2015-04-20T20:58:51.000Z","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null},{"id":"5adb8921351ffe0018a576ae","name":"pubsub","slug":"pubsub","description":null,"feature_image":null,"parent_id":null,"visibility":"public","meta_title":null,"meta_description":null,"created_at":"2015-04-20T21:16:44.000Z","updated_at":"2015-04-20T21:16:44.000Z","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null},{"id":"5adb8921351ffe0018a576af","name":"xbee","slug":"xbee","description":null,"feature_image":null,"parent_id":null,"visibility":"public","meta_title":null,"meta_description":null,"created_at":"2015-04-21T13:24:52.000Z","updated_at":"2015-04-21T13:24:52.000Z","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null},{"id":"5adb8921351ffe0018a576b0","name":"mesh","slug":"mesh","description":null,"feature_image":null,"parent_id":null,"visibility":"public","meta_title":null,"meta_description":null,"created_at":"2015-04-21T13:24:52.000Z","updated_at":"2015-04-21T13:24:52.000Z","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null},{"id":"5adb8921351ffe0018a576b1","name":"networking","slug":"networking","description":null,"feature_image":null,"parent_id":null,"visibility":"public","meta_title":null,"meta_description":null,"created_at":"2015-04-21T13:24:52.000Z","updated_at":"2015-04-21T13:24:52.000Z","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null},{"id":"5adb8921351ffe0018a576b2","name":"tessel","slug":"tessel","description":null,"feature_image":null,"parent_id":null,"visibility":"public","meta_title":null,"meta_description":null,"created_at":"2015-04-21T13:33:50.000Z","updated_at":"2015-04-21T13:33:50.000Z","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null},{"id":"5adb8921351ffe0018a576b3","name":"spark","slug":"spark","description":null,"feature_image":null,"parent_id":null,"visibility":"public","meta_title":null,"meta_description":null,"created_at":"2015-04-21T13:47:13.000Z","updated_at":"2015-04-21T13:47:13.000Z","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null},{"id":"5adb8921351ffe0018a576b4","name":"core","slug":"core","description":null,"feature_image":null,"parent_id":null,"visibility":"public","meta_title":null,"meta_description":null,"created_at":"2015-04-21T13:47:13.000Z","updated_at":"2015-04-21T13:47:13.000Z","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null},{"id":"5adb8921351ffe0018a576b5","name":"electric","slug":"electric","description":null,"feature_image":null,"parent_id":null,"visibility":"public","meta_title":null,"meta_description":null,"created_at":"2015-04-21T13:57:41.000Z","updated_at":"2015-04-21T13:57:41.000Z","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null},{"id":"5adb8921351ffe0018a576b6","name":"imp","slug":"imp","description":null,"feature_image":null,"parent_id":null,"visibility":"public","meta_title":null,"meta_description":null,"created_at":"2015-04-21T13:57:41.000Z","updated_at":"2015-04-21T13:57:41.000Z","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null},{"id":"5adb8921351ffe0018a576b7","name":"dht","slug":"dht","description":null,"feature_image":null,"parent_id":null,"visibility":"public","meta_title":null,"meta_description":null,"created_at":"2015-04-21T14:04:35.000Z","updated_at":"2015-04-21T14:04:35.000Z","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null},{"id":"5adb8921351ffe0018a576b8","name":"photocell","slug":"photocell","description":null,"feature_image":null,"parent_id":null,"visibility":"public","meta_title":null,"meta_description":null,"created_at":"2015-04-21T14:20:34.000Z","updated_at":"2015-04-21T14:20:34.000Z","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null},{"id":"5adb8921351ffe0018a576b9","name":"websocket","slug":"websocket","description":null,"feature_image":null,"parent_id":null,"visibility":"public","meta_title":null,"meta_description":null,"created_at":"2015-04-21T14:20:34.000Z","updated_at":"2015-04-21T14:20:34.000Z","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null},{"id":"5adb8921351ffe0018a576ba","name":"kaazing","slug":"kaazing","description":null,"feature_image":null,"parent_id":null,"visibility":"public","meta_title":null,"meta_description":null,"created_at":"2015-04-21T14:57:08.000Z","updated_at":"2015-04-21T14:57:08.000Z","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null},{"id":"5adb8921351ffe0018a576bb","name":"projects","slug":"projects","description":null,"feature_image":null,"parent_id":null,"visibility":"public","meta_title":null,"meta_description":null,"created_at":"2015-04-21T14:57:08.000Z","updated_at":"2015-04-21T14:57:08.000Z","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null},{"id":"5adb8921351ffe0018a576bc","name":"cross","slug":"cross","description":null,"feature_image":null,"parent_id":null,"visibility":"public","meta_title":null,"meta_description":null,"created_at":"2015-04-21T14:57:08.000Z","updated_at":"2015-04-21T14:57:08.000Z","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null},{"id":"5adb8921351ffe0018a576bd","name":"chat","slug":"chat","description":null,"feature_image":null,"parent_id":null,"visibility":"public","meta_title":null,"meta_description":null,"created_at":"2015-04-21T15:06:13.000Z","updated_at":"2015-04-21T15:06:13.000Z","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null},{"id":"5adb8921351ffe0018a576be","name":"amqp","slug":"amqp","description":null,"feature_image":null,"parent_id":null,"visibility":"public","meta_title":null,"meta_description":null,"created_at":"2015-04-21T15:18:55.000Z","updated_at":"2015-04-21T15:18:55.000Z","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null},{"id":"5adb8921351ffe0018a576bf","name":"java","slug":"java","description":null,"feature_image":null,"parent_id":null,"visibility":"public","meta_title":null,"meta_description":null,"created_at":"2015-04-21T15:32:14.000Z","updated_at":"2015-04-21T15:32:14.000Z","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null},{"id":"5adb8921351ffe0018a576c0","name":"android","slug":"android","description":null,"feature_image":null,"parent_id":null,"visibility":"public","meta_title":null,"meta_description":null,"created_at":"2015-04-21T15:45:16.000Z","updated_at":"2015-04-21T15:45:16.000Z","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null},{"id":"5adb8921351ffe0018a576c1","name":"telemetry","slug":"telemetry","description":null,"feature_image":null,"parent_id":null,"visibility":"public","meta_title":null,"meta_description":null,"created_at":"2015-04-22T15:31:01.000Z","updated_at":"2015-04-22T15:31:01.000Z","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null},{"id":"5adb8921351ffe0018a576c2","name":"vehicle","slug":"vehicle","description":null,"feature_image":null,"parent_id":null,"visibility":"public","meta_title":null,"meta_description":null,"created_at":"2015-04-23T20:19:50.000Z","updated_at":"2015-04-23T20:19:50.000Z","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null},{"id":"5adb8921351ffe0018a576c3","name":"about","slug":"about","description":null,"feature_image":null,"parent_id":null,"visibility":"public","meta_title":null,"meta_description":null,"created_at":"2015-04-27T13:21:55.000Z","updated_at":"2015-04-27T13:21:55.000Z","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null},{"id":"5adb8921351ffe0018a576c4","name":"kevin","slug":"kevin","description":null,"feature_image":null,"parent_id":null,"visibility":"public","meta_title":null,"meta_description":null,"created_at":"2015-04-27T13:21:56.000Z","updated_at":"2015-04-27T13:21:56.000Z","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null},{"id":"5adb8921351ffe0018a576c5","name":"hoyt","slug":"hoyt","description":null,"feature_image":null,"parent_id":null,"visibility":"public","meta_title":null,"meta_description":null,"created_at":"2015-04-27T13:21:56.000Z","updated_at":"2015-04-27T13:21:56.000Z","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null},{"id":"5adb8921351ffe0018a576c6","name":"bio","slug":"bio","description":null,"feature_image":null,"parent_id":null,"visibility":"public","meta_title":null,"meta_description":null,"created_at":"2015-04-27T13:21:56.000Z","updated_at":"2015-04-27T13:21:56.000Z","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null},{"id":"5adb8921351ffe0018a576c7","name":"demo","slug":"demo","description":null,"feature_image":null,"parent_id":null,"visibility":"public","meta_title":null,"meta_description":null,"created_at":"2015-04-27T13:42:17.000Z","updated_at":"2015-04-27T13:42:17.000Z","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null},{"id":"5adb8921351ffe0018a576c8","name":"portfolio","slug":"portfolio","description":null,"feature_image":null,"parent_id":null,"visibility":"public","meta_title":null,"meta_description":null,"created_at":"2015-04-27T13:42:17.000Z","updated_at":"2015-04-27T13:42:17.000Z","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null},{"id":"5adb8921351ffe0018a576c9","name":"work","slug":"work","description":null,"feature_image":null,"parent_id":null,"visibility":"public","meta_title":null,"meta_description":null,"created_at":"2015-04-27T13:42:17.000Z","updated_at":"2015-04-27T13:42:17.000Z","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null},{"id":"5adb8921351ffe0018a576ca","name":"smart","slug":"smart","description":null,"feature_image":null,"parent_id":null,"visibility":"public","meta_title":null,"meta_description":null,"created_at":"2015-04-30T19:50:34.000Z","updated_at":"2015-04-30T19:50:34.000Z","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null},{"id":"5adb8921351ffe0018a576cb","name":"buildings","slug":"buildings","description":null,"feature_image":null,"parent_id":null,"visibility":"public","meta_title":null,"meta_description":null,"created_at":"2015-04-30T19:50:34.000Z","updated_at":"2015-04-30T19:50:34.000Z","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null},{"id":"5adb8921351ffe0018a576cc","name":"hvac","slug":"hvac","description":null,"feature_image":null,"parent_id":null,"visibility":"public","meta_title":null,"meta_description":null,"created_at":"2015-04-30T19:50:34.000Z","updated_at":"2015-04-30T19:50:34.000Z","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null},{"id":"5adb8921351ffe0018a576cd","name":"barcode","slug":"barcode","description":null,"feature_image":null,"parent_id":null,"visibility":"public","meta_title":null,"meta_description":null,"created_at":"2015-05-12T20:55:36.000Z","updated_at":"2015-05-12T20:55:36.000Z","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null},{"id":"5adb8921351ffe0018a576ce","name":"intel","slug":"intel","description":null,"feature_image":null,"parent_id":null,"visibility":"public","meta_title":null,"meta_description":null,"created_at":"2015-05-12T20:55:36.000Z","updated_at":"2015-05-12T20:55:36.000Z","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null},{"id":"5adb8921351ffe0018a576cf","name":"edison","slug":"edison","description":null,"feature_image":null,"parent_id":null,"visibility":"public","meta_title":null,"meta_description":null,"created_at":"2015-05-12T20:55:36.000Z","updated_at":"2015-05-12T20:55:36.000Z","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null},{"id":"5adb8921351ffe0018a576d0","name":"ponoko","slug":"ponoko","description":null,"feature_image":null,"parent_id":null,"visibility":"public","meta_title":null,"meta_description":null,"created_at":"2015-05-29T15:42:22.000Z","updated_at":"2015-05-29T15:42:22.000Z","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null},{"id":"5adb8921351ffe0018a576d1","name":"laser","slug":"laser","description":null,"feature_image":null,"parent_id":null,"visibility":"public","meta_title":null,"meta_description":null,"created_at":"2015-05-29T15:42:22.000Z","updated_at":"2015-05-29T15:42:22.000Z","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null},{"id":"5adb8921351ffe0018a576d2","name":"fabrication","slug":"fabrication","description":null,"feature_image":null,"parent_id":null,"visibility":"public","meta_title":null,"meta_description":null,"created_at":"2015-05-29T15:42:22.000Z","updated_at":"2015-05-29T15:42:22.000Z","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null},{"id":"5adb8921351ffe0018a576d3","name":"wood","slug":"wood","description":null,"feature_image":null,"parent_id":null,"visibility":"public","meta_title":null,"meta_description":null,"created_at":"2015-05-29T15:42:22.000Z","updated_at":"2015-05-29T15:42:22.000Z","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null},{"id":"5adb8921351ffe0018a576d4","name":"game","slug":"game","description":null,"feature_image":null,"parent_id":null,"visibility":"public","meta_title":null,"meta_description":null,"created_at":"2015-06-01T20:19:39.000Z","updated_at":"2015-06-01T20:19:39.000Z","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null},{"id":"5adb8921351ffe0018a576d5","name":"particle","slug":"particle","description":null,"feature_image":null,"parent_id":null,"visibility":"public","meta_title":null,"meta_description":null,"created_at":"2015-06-09T18:56:20.000Z","updated_at":"2015-06-09T18:56:20.000Z","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null},{"id":"5adb8921351ffe0018a576d6","name":"photon","slug":"photon","description":null,"feature_image":null,"parent_id":null,"visibility":"public","meta_title":null,"meta_description":null,"created_at":"2015-06-09T18:56:20.000Z","updated_at":"2015-06-09T18:56:20.000Z","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null},{"id":"5adb8921351ffe0018a576d7","name":"canvas","slug":"canvas","description":null,"feature_image":null,"parent_id":null,"visibility":"public","meta_title":null,"meta_description":null,"created_at":"2015-06-18T15:05:44.000Z","updated_at":"2015-06-18T15:05:44.000Z","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null},{"id":"5adb8921351ffe0018a576d8","name":"camera","slug":"camera","description":null,"feature_image":null,"parent_id":null,"visibility":"public","meta_title":null,"meta_description":null,"created_at":"2015-06-18T15:05:44.000Z","updated_at":"2015-06-18T15:05:44.000Z","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null},{"id":"5adb8921351ffe0018a576d9","name":"image","slug":"image","description":null,"feature_image":null,"parent_id":null,"visibility":"public","meta_title":null,"meta_description":null,"created_at":"2015-06-18T15:05:44.000Z","updated_at":"2015-06-18T15:05:44.000Z","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null},{"id":"5adb8921351ffe0018a576da","name":"php","slug":"php","description":null,"feature_image":null,"parent_id":null,"visibility":"public","meta_title":null,"meta_description":null,"created_at":"2015-06-18T15:05:44.000Z","updated_at":"2015-06-18T15:05:44.000Z","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null},{"id":"5adb8921351ffe0018a576db","name":"ibm","slug":"ibm","description":null,"feature_image":null,"parent_id":null,"visibility":"public","meta_title":null,"meta_description":null,"created_at":"2015-07-23T14:54:09.000Z","updated_at":"2015-07-23T14:54:09.000Z","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null},{"id":"5adb8921351ffe0018a576dc","name":"bluemix","slug":"bluemix","description":null,"feature_image":null,"parent_id":null,"visibility":"public","meta_title":null,"meta_description":null,"created_at":"2015-07-23T14:54:09.000Z","updated_at":"2015-07-23T14:54:09.000Z","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null},{"id":"5adb8921351ffe0018a576dd","name":"runtime","slug":"runtime","description":null,"feature_image":null,"parent_id":null,"visibility":"public","meta_title":null,"meta_description":null,"created_at":"2015-07-23T21:01:00.000Z","updated_at":"2015-07-23T21:01:00.000Z","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null},{"id":"5adb8921351ffe0018a576de","name":"dashboard","slug":"dashboard-tag","description":null,"feature_image":null,"parent_id":null,"visibility":"public","meta_title":null,"meta_description":null,"created_at":"2015-07-23T21:01:00.000Z","updated_at":"2015-07-23T21:01:00.000Z","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null},{"id":"5adb8921351ffe0018a576df","name":"weather","slug":"weather","description":null,"feature_image":null,"parent_id":null,"visibility":"public","meta_title":null,"meta_description":null,"created_at":"2015-07-29T13:48:42.000Z","updated_at":"2015-07-29T13:48:42.000Z","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null},{"id":"5adb8921351ffe0018a576e0","name":"nodejs","slug":"nodejs","description":null,"feature_image":null,"parent_id":null,"visibility":"public","meta_title":null,"meta_description":null,"created_at":"2015-07-29T13:48:42.000Z","updated_at":"2015-07-29T13:48:42.000Z","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null},{"id":"5adb8921351ffe0018a576e1","name":"mobile","slug":"mobile","description":null,"feature_image":null,"parent_id":null,"visibility":"public","meta_title":null,"meta_description":null,"created_at":"2015-08-03T03:10:40.000Z","updated_at":"2015-08-03T03:10:40.000Z","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null},{"id":"5adb8921351ffe0018a576e2","name":"cloud","slug":"cloud","description":null,"feature_image":null,"parent_id":null,"visibility":"public","meta_title":null,"meta_description":null,"created_at":"2015-08-03T03:10:40.000Z","updated_at":"2015-08-03T03:10:40.000Z","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null},{"id":"5adb8921351ffe0018a576e3","name":"rest","slug":"rest","description":null,"feature_image":null,"parent_id":null,"visibility":"public","meta_title":null,"meta_description":null,"created_at":"2015-08-18T16:33:35.000Z","updated_at":"2015-08-18T16:33:35.000Z","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null},{"id":"5adb8921351ffe0018a576e4","name":"mobilefirst","slug":"mobilefirst","description":null,"feature_image":null,"parent_id":null,"visibility":"public","meta_title":null,"meta_description":null,"created_at":"2015-08-20T20:23:13.000Z","updated_at":"2015-08-20T20:23:13.000Z","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null},{"id":"5adb8921351ffe0018a576e5","name":"maker","slug":"maker","description":null,"feature_image":null,"parent_id":null,"visibility":"public","meta_title":null,"meta_description":null,"created_at":"2016-01-04T17:36:18.000Z","updated_at":"2016-01-04T17:36:18.000Z","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null},{"id":"5adb8921351ffe0018a576e6","name":"reading","slug":"reading","description":null,"feature_image":null,"parent_id":null,"visibility":"public","meta_title":null,"meta_description":null,"created_at":"2016-01-04T17:36:18.000Z","updated_at":"2016-01-04T17:36:18.000Z","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null},{"id":"5adb8921351ffe0018a576e7","name":"light","slug":"light","description":null,"feature_image":null,"parent_id":null,"visibility":"public","meta_title":null,"meta_description":null,"created_at":"2016-01-04T17:36:18.000Z","updated_at":"2016-01-04T17:36:18.000Z","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null},{"id":"5adb8921351ffe0018a576e8","name":"watson","slug":"watson","description":null,"feature_image":null,"parent_id":null,"visibility":"public","meta_title":null,"meta_description":null,"created_at":"2016-04-27T17:25:21.000Z","updated_at":"2016-04-27T17:25:21.000Z","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null},{"id":"5adb8921351ffe0018a576e9","name":"mqtt","slug":"mqtt","description":null,"feature_image":null,"parent_id":null,"visibility":"public","meta_title":null,"meta_description":null,"created_at":"2016-04-27T17:25:21.000Z","updated_at":"2016-04-27T17:25:21.000Z","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null},{"id":"5adb8921351ffe0018a576ea","name":"browser","slug":"browser","description":null,"feature_image":null,"parent_id":null,"visibility":"public","meta_title":null,"meta_description":null,"created_at":"2016-04-27T20:45:24.000Z","updated_at":"2016-04-27T20:45:24.000Z","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null},{"id":"5adb8921351ffe0018a576eb","name":"swift","slug":"swift","description":null,"feature_image":null,"parent_id":null,"visibility":"public","meta_title":null,"meta_description":null,"created_at":"2016-05-20T00:59:10.000Z","updated_at":"2016-05-20T00:59:10.000Z","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null},{"id":"5adb8921351ffe0018a576ec","name":"ble","slug":"ble","description":null,"feature_image":null,"parent_id":null,"visibility":"public","meta_title":null,"meta_description":null,"created_at":"2016-05-20T00:59:10.000Z","updated_at":"2016-05-20T00:59:10.000Z","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null},{"id":"5adb8921351ffe0018a576ed","name":"bluetooth","slug":"bluetooth","description":null,"feature_image":null,"parent_id":null,"visibility":"public","meta_title":null,"meta_description":null,"created_at":"2016-05-20T00:59:10.000Z","updated_at":"2016-05-20T00:59:10.000Z","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null},{"id":"5adb8921351ffe0018a576ee","name":"beacon","slug":"beacon","description":null,"feature_image":null,"parent_id":null,"visibility":"public","meta_title":null,"meta_description":null,"created_at":"2016-05-20T02:03:44.000Z","updated_at":"2016-05-20T02:03:44.000Z","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null},{"id":"5adb8921351ffe0018a576ef","name":"evangelism","slug":"evangelism","description":null,"feature_image":null,"parent_id":null,"visibility":"public","meta_title":null,"meta_description":null,"created_at":"2016-06-07T16:26:42.000Z","updated_at":"2016-06-07T16:26:42.000Z","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null},{"id":"5adb8921351ffe0018a576f0","name":"advocacy","slug":"advocacy","description":null,"feature_image":null,"parent_id":null,"visibility":"public","meta_title":null,"meta_description":null,"created_at":"2016-06-07T16:26:42.000Z","updated_at":"2016-06-07T16:26:42.000Z","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null},{"id":"5adb8921351ffe0018a576f1","name":"community","slug":"community","description":null,"feature_image":null,"parent_id":null,"visibility":"public","meta_title":null,"meta_description":null,"created_at":"2016-06-07T16:26:42.000Z","updated_at":"2016-06-07T16:26:42.000Z","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null},{"id":"5adb8921351ffe0018a576f2","name":"business","slug":"business","description":null,"feature_image":null,"parent_id":null,"visibility":"public","meta_title":null,"meta_description":null,"created_at":"2016-06-07T16:26:42.000Z","updated_at":"2016-06-07T16:26:42.000Z","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null},{"id":"5adb8921351ffe0018a576f3","name":"developer","slug":"developer","description":null,"feature_image":null,"parent_id":null,"visibility":"public","meta_title":null,"meta_description":null,"created_at":"2016-08-26T21:42:42.000Z","updated_at":"2016-08-26T21:42:42.000Z","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null},{"id":"5adb8921351ffe0018a576f4","name":"relations","slug":"relations","description":null,"feature_image":null,"parent_id":null,"visibility":"public","meta_title":null,"meta_description":null,"created_at":"2016-08-26T21:42:42.000Z","updated_at":"2016-08-26T21:42:42.000Z","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null},{"id":"5adb8921351ffe0018a576f5","name":"career","slug":"career","description":null,"feature_image":null,"parent_id":null,"visibility":"public","meta_title":null,"meta_description":null,"created_at":"2016-08-26T21:42:42.000Z","updated_at":"2016-08-26T21:42:42.000Z","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null},{"id":"5adb8921351ffe0018a576f6","name":"management","slug":"management","description":null,"feature_image":null,"parent_id":null,"visibility":"public","meta_title":null,"meta_description":null,"created_at":"2016-08-26T21:42:42.000Z","updated_at":"2016-08-26T21:42:42.000Z","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null},{"id":"5adb8921351ffe0018a576f7","name":"i2c","slug":"i2c","description":null,"feature_image":null,"parent_id":null,"visibility":"public","meta_title":null,"meta_description":null,"created_at":"2016-09-12T17:52:57.000Z","updated_at":"2016-09-12T17:52:57.000Z","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null},{"id":"5adb8921351ffe0018a576f8","name":"climate","slug":"climate","description":null,"feature_image":null,"parent_id":null,"visibility":"public","meta_title":null,"meta_description":null,"created_at":"2016-09-12T17:52:57.000Z","updated_at":"2016-09-12T17:52:57.000Z","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null},{"id":"5adb8921351ffe0018a576f9","name":"travel","slug":"travel","description":null,"feature_image":null,"parent_id":null,"visibility":"public","meta_title":null,"meta_description":null,"created_at":"2017-01-26T03:15:59.000Z","updated_at":"2017-01-26T03:15:59.000Z","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null},{"id":"5adb8921351ffe0018a576fa","name":"bean","slug":"bean","description":null,"feature_image":null,"parent_id":null,"visibility":"public","meta_title":null,"meta_description":null,"created_at":"2017-02-24T19:46:40.000Z","updated_at":"2017-02-24T19:46:40.000Z","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null},{"id":"5adb8921351ffe0018a576fb","name":"blockchain","slug":"blockchain","description":null,"feature_image":null,"parent_id":null,"visibility":"public","meta_title":null,"meta_description":null,"created_at":"2017-04-11T18:26:08.000Z","updated_at":"2017-04-11T18:26:08.000Z","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null},{"id":"5adb8921351ffe0018a576fc","name":"python","slug":"python","description":null,"feature_image":null,"parent_id":null,"visibility":"public","meta_title":null,"meta_description":null,"created_at":"2017-04-21T21:26:41.000Z","updated_at":"2017-04-21T21:26:41.000Z","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null},{"id":"5adb8921351ffe0018a576fd","name":"nlp","slug":"nlp","description":null,"feature_image":null,"parent_id":null,"visibility":"public","meta_title":null,"meta_description":null,"created_at":"2017-04-21T21:26:41.000Z","updated_at":"2017-04-21T21:26:41.000Z","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null},{"id":"5adb8921351ffe0018a576fe","name":"svg","slug":"svg","description":null,"feature_image":null,"parent_id":null,"visibility":"public","meta_title":null,"meta_description":null,"created_at":"2017-05-18T09:29:46.000Z","updated_at":"2017-05-18T09:29:46.000Z","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null},{"id":"5adb8921351ffe0018a576ff","name":"machine","slug":"machine","description":null,"feature_image":null,"parent_id":null,"visibility":"public","meta_title":null,"meta_description":null,"created_at":"2017-05-18T23:57:34.000Z","updated_at":"2017-05-18T23:57:34.000Z","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null},{"id":"5adb8921351ffe0018a57700","name":"vision","slug":"vision","description":null,"feature_image":null,"parent_id":null,"visibility":"public","meta_title":null,"meta_description":null,"created_at":"2017-05-18T23:57:34.000Z","updated_at":"2017-05-18T23:57:34.000Z","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null},{"id":"5adb8921351ffe0018a57701","name":"google","slug":"google","description":null,"feature_image":null,"parent_id":null,"visibility":"public","meta_title":null,"meta_description":null,"created_at":"2017-05-28T20:52:00.000Z","updated_at":"2017-05-28T20:52:00.000Z","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null},{"id":"5adb8921351ffe0018a57702","name":"location","slug":"location","description":null,"feature_image":null,"parent_id":null,"visibility":"public","meta_title":null,"meta_description":null,"created_at":"2017-05-28T20:52:00.000Z","updated_at":"2017-05-28T20:52:00.000Z","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null},{"id":"5adb8921351ffe0018a57703","name":"map","slug":"map","description":null,"feature_image":null,"parent_id":null,"visibility":"public","meta_title":null,"meta_description":null,"created_at":"2017-05-28T20:52:00.000Z","updated_at":"2017-05-28T20:52:00.000Z","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null},{"id":"5adb8921351ffe0018a57704","name":"javascript","slug":"javascript","description":null,"feature_image":null,"parent_id":null,"visibility":"public","meta_title":null,"meta_description":null,"created_at":"2017-05-31T19:37:27.000Z","updated_at":"2017-05-31T19:37:27.000Z","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null},{"id":"5adb8921351ffe0018a57705","name":"conversation","slug":"conversation","description":null,"feature_image":null,"parent_id":null,"visibility":"public","meta_title":null,"meta_description":null,"created_at":"2017-06-05T21:11:43.000Z","updated_at":"2017-06-05T21:11:43.000Z","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null},{"id":"5adb8921351ffe0018a57706","name":"nlu","slug":"nlu","description":null,"feature_image":null,"parent_id":null,"visibility":"public","meta_title":null,"meta_description":null,"created_at":"2017-06-05T21:11:43.000Z","updated_at":"2017-06-05T21:11:43.000Z","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null},{"id":"5adb8921351ffe0018a57707","name":"ideas","slug":"ideas","description":null,"feature_image":null,"parent_id":null,"visibility":"public","meta_title":null,"meta_description":null,"created_at":"2017-06-13T21:52:55.000Z","updated_at":"2017-06-13T21:52:55.000Z","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null},{"id":"5adb8921351ffe0018a57708","name":"events","slug":"events","description":null,"feature_image":null,"parent_id":null,"visibility":"public","meta_title":null,"meta_description":null,"created_at":"2017-06-13T21:52:55.000Z","updated_at":"2017-06-13T21:52:55.000Z","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null},{"id":"5adb8921351ffe0018a57709","name":"openwhisk","slug":"openwhisk","description":null,"feature_image":null,"parent_id":null,"visibility":"public","meta_title":null,"meta_description":null,"created_at":"2017-06-15T20:39:41.000Z","updated_at":"2017-06-15T20:39:41.000Z","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null},{"id":"5adb8921351ffe0018a5770a","name":"kitura","slug":"kitura","description":null,"feature_image":null,"parent_id":null,"visibility":"public","meta_title":null,"meta_description":null,"created_at":"2017-06-19T21:11:17.000Z","updated_at":"2017-06-19T21:11:17.000Z","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null},{"id":"5adb8921351ffe0018a5770b","name":"http","slug":"http","description":null,"feature_image":null,"parent_id":null,"visibility":"public","meta_title":null,"meta_description":null,"created_at":"2017-06-19T21:11:17.000Z","updated_at":"2017-06-19T21:11:17.000Z","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null},{"id":"5adb8921351ffe0018a5770c","name":"cloudant","slug":"cloudant","description":null,"feature_image":null,"parent_id":null,"visibility":"public","meta_title":null,"meta_description":null,"created_at":"2017-10-12T23:03:11.000Z","updated_at":"2017-10-12T23:03:11.000Z","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null},{"id":"5adb8921351ffe0018a5770d","name":"ocr","slug":"ocr","description":null,"feature_image":null,"parent_id":null,"visibility":"public","meta_title":null,"meta_description":null,"created_at":"2017-12-23T22:23:16.000Z","updated_at":"2017-12-23T22:23:16.000Z","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null},{"id":"5adb8921351ffe0018a5770e","name":"tesseract","slug":"tesseract","description":null,"feature_image":null,"parent_id":null,"visibility":"public","meta_title":null,"meta_description":null,"created_at":"2017-12-23T22:23:16.000Z","updated_at":"2017-12-23T22:23:16.000Z","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null},{"id":"5adb8bd968339a00221450c2","name":"cigar","slug":"cigar","description":null,"feature_image":null,"parent_id":null,"visibility":"public","meta_title":null,"meta_description":null,"created_at":"2018-04-21T19:07:05.000Z","updated_at":"2018-04-21T19:07:05.000Z","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null},{"id":"5adb8bd968339a00221450c3","name":"lounge","slug":"lounge","description":null,"feature_image":null,"parent_id":null,"visibility":"public","meta_title":null,"meta_description":null,"created_at":"2018-04-21T19:07:05.000Z","updated_at":"2018-04-21T19:07:05.000Z","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null},{"id":"5bcf881da7d50d00bf13e9f4","name":"processing","slug":"processing","description":null,"feature_image":null,"parent_id":null,"visibility":"public","meta_title":null,"meta_description":null,"created_at":"2018-10-23T20:44:13.000Z","updated_at":"2018-10-23T20:44:13.000Z","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null},{"id":"5c1a8820a40d3300bf6913a9","name":"mac","slug":"mac","description":null,"feature_image":null,"parent_id":null,"visibility":"public","meta_title":null,"meta_description":null,"created_at":"2018-12-19T18:04:16.000Z","updated_at":"2018-12-19T18:04:16.000Z","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null},{"id":"5c1a8820a40d3300bf6913aa","name":"install","slug":"install","description":null,"feature_image":null,"parent_id":null,"visibility":"public","meta_title":null,"meta_description":null,"created_at":"2018-12-19T18:04:16.000Z","updated_at":"2018-12-19T18:04:16.000Z","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null},{"id":"5c3fcc37dff84900c0a56293","name":"serverless","slug":"serverless","description":null,"feature_image":null,"parent_id":null,"visibility":"public","meta_title":null,"meta_description":null,"created_at":"2019-01-17T00:28:39.000Z","updated_at":"2019-01-17T00:28:39.000Z","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null},{"id":"5c48d5799d94d600c0e43e66","name":"storage","slug":"storage","description":null,"feature_image":null,"parent_id":null,"visibility":"public","meta_title":null,"meta_description":null,"created_at":"2019-01-23T20:58:33.000Z","updated_at":"2019-01-23T20:58:33.000Z","og_image":null,"og_title":null,"og_description":null,"twitter_image":null,"twitter_title":null,"twitter_description":null,"codeinjection_head":null,"codeinjection_foot":null,"canonical_url":null,"accent_color":null}],"users":[{"id":"1","name":"Kevin Hoyt","slug":"krhoyt","password":"$2a$10$Jknc7btlwExVtxFJQXevOu/vVxRgg8zr60YvP1z.ZwPkQWMpL1a0.","email":"parkerkrhoyt@gmail.com","profile_image":"__GHOST_URL__/content/images/2018/12/kevin.hoyt.jpg","cover_image":null,"bio":null,"website":"http://www.kevinhoyt.com","location":"Parker, CO","facebook":null,"twitter":"@krhoyt","accessibility":"{\"launchComplete\":true}","status":"active","locale":null,"visibility":"public","meta_title":null,"meta_description":null,"tour":"[\"getting-started\",\"using-the-editor\"]","last_seen":"2021-06-30T21:26:44.000Z","created_at":"2014-01-01T16:57:50.000Z","updated_at":"2021-06-30T21:26:44.000Z"}]}}]}